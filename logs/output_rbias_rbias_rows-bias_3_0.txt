Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=512, canonical_testing=False, cs=0.5, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, hidden=111, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=0.1, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=6, quant_actNM=8, quant_inp=4, quant_w=None, random_seed=193012823, rows_bias=3, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=640, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
3d22a2a2-04c5-4b7b-aae9-4b2ce3c76edc
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 161, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 228, in forward
    activated_input = quant_pass(pact_a_bmm(input_gate_out * activation_out, self.a8), self.abNM, self.a8)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 89, in forward
    x01q =  torch.round(x01 * step_d ) / step_d
RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 10.76 GiB total capacity; 8.54 GiB already allocated; 3.12 MiB free; 9.80 GiB reserved in total by PyTorch)
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=512, canonical_testing=False, cs=0.5, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, hidden=111, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=0.1, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=6, quant_actNM=8, quant_inp=4, quant_w=None, random_seed=193012823, rows_bias=3, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=640, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
ab3cd90a-102d-4e81-af02-f190a054e87c
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 161, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 228, in forward
    activated_input = quant_pass(pact_a_bmm(input_gate_out * activation_out, self.a8), self.abNM, self.a8)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 89, in forward
    x01q =  torch.round(x01 * step_d ) / step_d
RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 10.76 GiB total capacity; 8.54 GiB already allocated; 3.12 MiB free; 9.80 GiB reserved in total by PyTorch)
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=512, canonical_testing=False, cs=0.5, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, hidden=111, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=0.1, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=6, quant_actNM=8, quant_inp=4, quant_w=None, random_seed=193012823, rows_bias=3, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=640, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
7d60a1a2-baf4-4a74-aa9c-70e0569e3ed9
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.6507      0.0762     0.0904     11.6448
00100     1.6079      0.4746     0.5188     69.7610
00200     1.1999      0.6289     0.6314     70.0446
00300     1.0855      0.6504     0.7157     71.0204
00400     1.0361      0.6680     0.7522     70.5174
00500     0.8607      0.7188     0.7549     70.6854
00600     0.8785      0.7480     0.7970     71.2245
00700     0.7791      0.7480     0.7970     69.9925
00800     0.7922      0.7441     0.7970     71.7372
00900     0.7885      0.7695     0.7970     70.1665
01000     0.7867      0.7715     0.7975     69.2780
01100     0.6459      0.7910     0.8113     71.1322
01200     0.7466      0.7754     0.8113     70.5863
01300     0.7909      0.7402     0.8113     70.4319
01400     0.7473      0.7383     0.8149     71.2301
01500     0.6878      0.7734     0.8180     71.4419
01600     0.6934      0.7871     0.8180     70.5729
01700     0.7195      0.7695     0.8180     68.7858
01800     0.7117      0.7871     0.8180     71.9429
01900     0.7244      0.7617     0.8180     71.4245
02000     0.6847      0.8086     0.8180     69.9037
02100     0.6258      0.8145     0.8180     71.5586
02200     0.6752      0.8027     0.8289     69.4202
02300     0.7414      0.7617     0.8289     68.6836
02400     0.6721      0.7910     0.8289     70.6447
02500     0.6422      0.8145     0.8289     69.1307
02600     0.6428      0.8125     0.8289     68.9784
02700     0.6022      0.8047     0.8289     69.6851
02800     0.7033      0.7891     0.8457     69.6505
02900     0.6511      0.7988     0.8457     69.9780
03000     0.6873      0.7793     0.8457     69.8895
03100     0.5960      0.8008     0.8457     71.1892
03200     0.6146      0.8184     0.8457     71.2222
03300     0.6541      0.7891     0.8457     70.4115
03400     0.6479      0.7969     0.8457     71.7788
03500     0.6344      0.7910     0.8457     70.4431
03600     0.5851      0.8184     0.8457     71.3484
03700     0.6068      0.8223     0.8457     70.9397
03800     0.5436      0.8496     0.8457     70.5755
03900     0.6362      0.8223     0.8457     70.7316
04000     0.5425      0.8438     0.8457     70.6767
04100     0.5677      0.8301     0.8457     72.6867
04200     0.6324      0.7949     0.8457     71.1034
04300     0.5212      0.8438     0.8457     70.4176
04400     0.6080      0.7969     0.8457     71.8353
04500     0.6022      0.8184     0.8457     72.3083
04600     0.6859      0.7871     0.8457     71.3547
04700     0.5559      0.8262     0.8457     69.9754
04800     0.6087      0.8145     0.8457     71.0045
04900     0.6628      0.7891     0.8457     69.2358
05000     0.5605      0.8320     0.8457     71.4870
05100     0.6049      0.8086     0.8457     70.3775
05200     0.6760      0.7852     0.8457     69.8825
05300     0.5677      0.8301     0.8457     71.2233
05400     0.6050      0.8086     0.8457     69.4261
05500     0.5548      0.8438     0.8457     71.8886
05600     0.5729      0.8477     0.8457     71.2910
05700     0.5656      0.8281     0.8457     70.1280
05800     0.5119      0.8496     0.8457     72.1835
05900     0.5352      0.8320     0.8457     70.2224
06000     0.5660      0.8242     0.8457     69.5806
06100     0.6974      0.7988     0.8457     70.5805
06200     0.6031      0.8301     0.8457     70.2957
06300     0.5461      0.8262     0.8457     71.8851
06400     0.5382      0.8398     0.8457     70.4764
06500     0.5602      0.8281     0.8457     71.0102
06600     0.4992      0.8496     0.8457     73.0790
06700     0.5915      0.8184     0.8457     71.6442
06800     0.5376      0.8398     0.8457     73.2534
06900     0.5271      0.8398     0.8457     72.9492
07000     0.5197      0.8457     0.8457     69.7142
07100     0.5290      0.8496     0.8457     72.2056
07200     0.6034      0.8125     0.8457     70.0584
07300     0.5304      0.8535     0.8457     72.4328
07400     0.5392      0.8223     0.8457     70.4836
07500     0.5811      0.8301     0.8615     71.2658
07600     0.5293      0.8242     0.8615     70.9008
07700     0.4862      0.8711     0.8615     70.4852
07800     0.5117      0.8496     0.8615     70.6956
07900     0.5599      0.8223     0.8615     69.5664
08000     0.4836      0.8652     0.8615     71.0230
08100     0.5020      0.8516     0.8615     70.6186
08200     0.5542      0.8281     0.8615     70.3764
08300     0.5346      0.8535     0.8615     70.4373
08400     0.5709      0.8203     0.8615     71.7218
08500     0.6151      0.8223     0.8625     70.7804
08600     0.5096      0.8379     0.8625     70.7454
08700     0.5727      0.8242     0.8625     70.5556
08800     0.5007      0.8633     0.8625     70.8627
08900     0.5372      0.8301     0.8625     70.7862
09000     0.4995      0.8613     0.8625     72.4325
09100     0.5328      0.8516     0.8625     70.7958
09200     0.5122      0.8457     0.8625     68.9394
09300     0.4662      0.8633     0.8625     69.6654
09400     0.4808      0.8477     0.8625     71.0730
09500     0.4851      0.8711     0.8625     70.1999
09600     0.5085      0.8398     0.8625     69.5342
09700     0.4922      0.8496     0.8625     71.8773
09800     0.5173      0.8496     0.8625     70.9239
09900     0.4421      0.8906     0.8625     71.3188
10000     0.4723      0.8652     0.8625     70.3665
10100     0.4421      0.8750     0.8625     71.3208
10200     0.5397      0.8398     0.8625     70.5407
10300     0.4797      0.8477     0.8625     70.5099
10400     0.5080      0.8438     0.8625     70.2036
10500     0.4856      0.8320     0.8625     72.9538
10600     0.4718      0.8574     0.8625     70.8299
10700     0.4864      0.8555     0.8625     71.1630
10800     0.4688      0.8809     0.8625     71.6205
10900     0.4837      0.8711     0.8625     69.9584
11000     0.4446      0.8633     0.8625     71.3853
11100     0.4163      0.8809     0.8625     71.1195
11200     0.5054      0.8457     0.8625     70.6304
11300     0.4850      0.8438     0.8625     71.8676
11400     0.4672      0.8652     0.8625     71.0386
11500     0.4391      0.8652     0.8625     71.2998
11600     0.4253      0.8730     0.8625     69.8593
11700     0.4605      0.8633     0.8625     70.0473
11800     0.4133      0.8867     0.8625     71.8619
11900     0.4210      0.8750     0.8625     70.8312
12000     0.4385      0.8809     0.8625     69.7661
12100     0.3686      0.8965     0.8625     71.8848
12200     0.4400      0.8711     0.8625     69.1146
12300     0.4203      0.8672     0.8625     70.9941
12400     0.4536      0.8613     0.8625     71.4094
12500     0.4097      0.8848     0.8625     71.6547
12600     0.5074      0.8340     0.8625     69.6709
12700     0.4326      0.8691     0.8625     72.2594
12800     0.4561      0.8594     0.8625     71.4292
12900     0.4804      0.8633     0.8625     70.6693
13000     0.4257      0.8633     0.8625     69.7738
13100     0.4360      0.8613     0.8625     69.8622
13200     0.4465      0.8711     0.8625     71.0534
13300     0.3882      0.8867     0.8625     71.8576
13400     0.3602      0.9082     0.8625     70.7460
13500     0.4760      0.8477     0.8625     69.5518
13600     0.5045      0.8691     0.8625     70.3337
13700     0.4008      0.8906     0.8625     70.9561
13800     0.3783      0.8945     0.8625     71.5193
13900     0.4894      0.8398     0.8625     71.1524
14000     0.4061      0.8750     0.8625     70.3960
14100     0.4886      0.8555     0.8625     72.8240
14200     0.4433      0.8594     0.8625     70.6378
14300     0.4093      0.8828     0.8625     71.0582
14400     0.4040      0.8848     0.8625     70.6279
14500     0.5354      0.8457     0.8625     71.9342
14600     0.3828      0.8867     0.8625     72.1033
14700     0.4832      0.8516     0.8625     72.5001
14800     0.4930      0.8477     0.8625     71.1811
14900     0.4367      0.8652     0.8625     71.6904
15000     0.4163      0.8828     0.8625     70.2638
15100     0.4806      0.8340     0.8625     70.5922
15200     0.4578      0.8770     0.8625     70.0902
15300     0.4480      0.8730     0.8625     71.4896
15400     0.4804      0.8633     0.8625     69.8887
15500     0.4620      0.8691     0.8625     71.0952
15600     0.4260      0.8750     0.8625     71.4708
15700     0.4002      0.8965     0.8625     70.1391
15800     0.4305      0.8711     0.8625     71.2529
15900     0.4634      0.8613     0.8625     70.6243
16000     0.4275      0.8770     0.8625     70.3325
16100     0.4846      0.8594     0.8625     69.9937
16200     0.5455      0.8340     0.8625     71.8150
16300     0.4775      0.8535     0.8625     72.5833
16400     0.4870      0.8496     0.8625     70.3189
16500     0.4225      0.8750     0.8625     70.6658
16600     0.4142      0.8750     0.8625     70.7738
16700     0.4076      0.8848     0.8625     71.2854
16800     0.4339      0.8691     0.8625     71.0217
16900     0.4129      0.8711     0.8625     71.8338
17000     0.3942      0.8984     0.8625     70.3979
17100     0.4373      0.8750     0.8625     70.8440
17200     0.4730      0.8848     0.8625     70.7497
17300     0.4215      0.8809     0.8625     70.1212
17400     0.3652      0.8945     0.8625     72.5410
17500     0.4126      0.8906     0.8625     68.7882
17600     0.4970      0.8652     0.8625     72.1857
17700     0.4019      0.8789     0.8625     72.8820
17800     0.4560      0.8613     0.8625     70.8013
17900     0.4252      0.8652     0.8625     70.9737
18000     0.4077      0.8848     0.8625     69.6680
18100     0.4530      0.8828     0.8625     71.0847
18200     0.4614      0.8398     0.8625     71.3361
18300     0.3937      0.8945     0.8625     71.5188
18400     0.3745      0.8848     0.8625     70.0234
18500     0.4207      0.8770     0.8625     70.9394
18600     0.4109      0.8809     0.8625     72.2147
18700     0.4226      0.8809     0.8625     70.6829
18800     0.4462      0.8633     0.8625     69.7781
18900     0.4727      0.8477     0.8625     70.9407
19000     0.4115      0.8672     0.8625     71.5402
19100     0.4206      0.8691     0.8625     71.3001
19200     0.4158      0.8848     0.8625     70.3035
19300     0.3423      0.9102     0.8625     71.6961
19400     0.4492      0.8770     0.8625     71.5721
19500     0.4319      0.8770     0.8625     71.6729
19600     0.3873      0.8965     0.8625     70.6731
19700     0.4209      0.8809     0.8625     70.9302
19800     0.3900      0.8945     0.8625     70.7737
19900     0.4374      0.8711     0.8625     70.3529
20000     0.4254      0.8770     0.8625     69.0154
20100     0.3899      0.8828     0.8625     70.8249
20199     0.3901      0.8809     0.8625     69.5850
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     0.6341      0.7949     0.7777     10.6649
00100     0.4240      0.8809     0.8644     70.1846
00200     0.4838      0.8613     0.8679     71.0764
00300     0.4775      0.8652     0.8906     70.4334
00400     0.4906      0.8594     0.8906     69.4588
00500     0.4522      0.8613     0.8906     70.9751
00600     0.5526      0.8301     0.8906     72.7555
00700     0.4484      0.8789     0.8906     73.1496
00800     0.3994      0.8906     0.8906     71.3247
00900     0.4340      0.8730     0.8906     70.7589
01000     0.4239      0.8867     0.8906     69.7843
01100     0.5058      0.8594     0.8906     72.1600
01200     0.4712      0.8574     0.8906     70.2062
01300     0.4137      0.8809     0.8906     70.8240
01400     0.4886      0.8496     0.8906     70.2891
01500     0.4503      0.8613     0.8906     72.4860
01600     0.4267      0.8770     0.8906     69.7038
01700     0.4475      0.8652     0.8906     71.5596
01800     0.4320      0.8770     0.8906     69.8888
01900     0.4675      0.8613     0.8906     70.7844
02000     0.4439      0.8691     0.8906     70.4508
02100     0.4084      0.8828     0.8906     71.3947
02200     0.4138      0.8809     0.8906     71.5211
02300     0.4669      0.8711     0.8906     70.7646
02400     0.3979      0.8770     0.8906     69.7123
02500     0.4075      0.8750     0.8906     73.1088
02600     0.4145      0.8691     0.8906     70.9941
02700     0.3604      0.8867     0.8910     71.0388
02800     0.4556      0.8789     0.8910     72.7706
02900     0.4690      0.8574     0.8910     71.3854
03000     0.4499      0.8711     0.8910     71.4018
03100     0.3664      0.8945     0.8910     69.7394
03200     0.3969      0.8770     0.8910     70.7042
03300     0.4447      0.8730     0.8910     72.4522
03400     0.3895      0.9023     0.8910     70.3718
03500     0.4022      0.8809     0.8910     71.6643
03600     0.4207      0.8691     0.8910     70.1661
03700     0.3750      0.8906     0.8910     71.7971
03800     0.4315      0.8672     0.8910     73.1751
03900     0.4263      0.8672     0.8910     70.7398
04000     0.3964      0.8809     0.8910     70.7932
04100     0.4964      0.8457     0.8910     70.5762
04200     0.4475      0.8652     0.8910     73.4364
04300     0.4712      0.8613     0.8910     71.7530
04400     0.4357      0.8750     0.8910     70.2539
04500     0.3869      0.8945     0.8910     71.7336
04600     0.4562      0.8730     0.8910     70.9020
04700     0.3596      0.9023     0.8910     72.0379
04800     0.4446      0.8613     0.8910     71.5796
04900     0.4685      0.8633     0.8910     71.3765
05000     0.3112      0.9004     0.8910     71.1333
05100     0.4174      0.8711     0.8911     72.5829
05200     0.4670      0.8418     0.8911     71.5453
05300     0.4975      0.8398     0.8911     72.3730
05400     0.4097      0.8848     0.8911     72.1991
05500     0.4182      0.8770     0.8911     69.2440
05600     0.3970      0.8906     0.8911     70.0829
05700     0.4124      0.8750     0.8911     70.8037
05800     0.4315      0.8691     0.8928     71.9309
05900     0.4263      0.8945     0.8928     71.4689
06000     0.4539      0.8750     0.8928     72.5292
06100     0.5099      0.8398     0.8928     72.4063
06200     0.4022      0.8691     0.8928     69.9589
06300     0.3995      0.8809     0.8928     70.6886
06400     0.4365      0.8672     0.8928     73.1335
06500     0.4419      0.8594     0.8928     73.0715
06600     0.3780      0.8887     0.8928     70.6697
06700     0.4751      0.8516     0.8928     71.5579
06800     0.4780      0.8613     0.8928     69.7656
06900     0.3896      0.8789     0.8928     73.2359
07000     0.4711      0.8711     0.8928     73.0445
07100     0.4251      0.8730     0.8928     72.6515
07200     0.4049      0.8926     0.8928     70.8800
07300     0.4030      0.8770     0.8928     71.1771
07400     0.5033      0.8359     0.8928     70.2470
07500     0.4004      0.8789     0.8928     72.4773
07600     0.4451      0.8652     0.8928     70.4879
07700     0.4143      0.8828     0.8928     71.4411
07800     0.4261      0.8711     0.8928     69.6279
07900     0.4277      0.8789     0.8928     72.1240
08000     0.4917      0.8555     0.8928     70.8763
08100     0.3805      0.8926     0.8928     69.0718
08200     0.4688      0.8672     0.8928     71.2800
08300     0.3721      0.9023     0.8928     70.2062
08400     0.3964      0.8965     0.8928     72.2702
08500     0.4869      0.8730     0.8928     69.6626
08600     0.4033      0.8848     0.8928     71.9721
08700     0.3683      0.8926     0.8928     72.2141
08800     0.4299      0.8613     0.8936     70.8697
08900     0.3855      0.8926     0.8936     71.6171
09000     0.4158      0.8828     0.8936     71.1898
09100     0.4337      0.8789     0.8936     70.3989
09200     0.3564      0.8906     0.8936     70.5371
09300     0.4080      0.8730     0.8967     70.1372
09400     0.4990      0.8574     0.8967     65.8452
09500     0.4547      0.8711     0.8967     67.4608
09600     0.4224      0.8789     0.8967     66.5559
09700     0.4213      0.8789     0.8967     66.7921
09800     0.4923      0.8418     0.8967     68.1371
09900     0.4454      0.8691     0.8967     67.5370
Start testing:
Test Accuracy: 0.8769
