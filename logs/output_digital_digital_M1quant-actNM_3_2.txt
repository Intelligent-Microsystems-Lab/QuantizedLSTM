Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 362, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 362, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.0, finetuning_epochs=0, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.0005,0.0001,0.00002', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,10000', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
077e92b0-d6a7-4808-845b-11bad3c867a9
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.5352      0.0781     0.0808     14.2380
00100     2.5221      0.0844     0.0811     77.2678
00200     2.5140      0.0907     0.0811     77.1740
00300     2.5096      0.0696     0.0811     75.8649
00400     2.5072      0.0738     0.0811     75.4370
00500     2.5057      0.0907     0.0811     75.2866
00600     2.5047      0.0886     0.0811     75.4554
00700     2.5039      0.0886     0.0811     76.3169
00800     2.5034      0.0738     0.0811     76.4404
00900     2.5029      0.0675     0.0811     76.9680
01000     2.5025      0.0759     0.0811     74.8737
01100     2.5022      0.0696     0.0811     77.6438
01200     2.5020      0.0928     0.0811     75.2172
01300     2.5017      0.0759     0.0811     75.8636
01400     2.5015      0.0823     0.0811     76.9258
01500     2.5014      0.1013     0.0811     75.5929
01600     2.5012      0.0633     0.0811     77.6659
01700     2.5011      0.0970     0.0811     77.0030
01800     2.5010      0.0928     0.0811     78.3650
01900     2.5008      0.0612     0.0811     76.0535
02000     2.5007      0.0759     0.0811     74.6781
02100     2.5007      0.0949     0.0811     74.7773
02200     2.5006      0.0865     0.0811     76.0356
02300     2.5005      0.0759     0.0811     77.2219
02400     2.5004      0.0738     0.0811     76.8194
02500     2.5004      0.0802     0.0811     75.2449
02600     2.5003      0.0759     0.0811     74.8349
02700     2.5003      0.0949     0.0811     75.5888
02800     2.5002      0.0738     0.0811     75.3380
02900     2.5002      0.0823     0.0811     75.2230
03000     2.5001      0.0823     0.0811     75.1243
03100     2.5001      0.1076     0.0811     78.7592
03200     2.5000      0.0759     0.0811     78.5615
03300     2.5000      0.0696     0.0811     76.7107
03400     2.5000      0.1013     0.0811     75.9954
03500     2.4999      0.0738     0.0811     76.4846
03600     2.4999      0.0844     0.0811     76.0126
03700     2.4999      0.0717     0.0811     76.6069
03800     2.4999      0.0696     0.0811     76.6950
03900     2.4998      0.0844     0.0811     75.4876
04000     2.4998      0.0570     0.0811     77.9967
04100     2.4998      0.0759     0.0814     75.1102
04200     2.4998      0.0949     0.0814     75.4263
04300     2.4997      0.0759     0.0814     77.0979
04400     2.4997      0.0654     0.0814     75.9624
04500     2.4997      0.0949     0.0814     77.7211
04600     2.4997      0.0654     0.0814     77.5156
04700     2.4997      0.1034     0.0814     76.0810
04800     2.4996      0.0675     0.0814     75.8613
04900     2.4996      0.0865     0.0814     76.7478
05000     2.4996      0.0654     0.0814     75.6759
05100     2.4996      0.0654     0.0814     76.5960
05200     2.4996      0.0886     0.0814     74.9309
05300     2.4995      0.0654     0.0814     76.7338
05400     2.4995      0.0696     0.0814     75.6095
05500     2.4995      0.1013     0.0814     75.5423
05600     2.4995      0.0717     0.0814     75.4428
05700     2.4995      0.0696     0.0814     75.1368
05800     2.4994      0.0781     0.0814     75.4192
05900     2.4994      0.0612     0.0814     75.1347
06000     2.4994      0.0675     0.0814     75.8723
06100     2.4994      0.0717     0.0814     76.8573
06200     2.4994      0.0928     0.0814     75.3198
06300     2.4993      0.0759     0.0814     75.3289
06400     2.4993      0.0823     0.0814     78.2854
06500     2.4993      0.0865     0.0814     77.7878
06600     2.4993      0.0802     0.0814     75.8467
06700     2.4993      0.0949     0.0814     76.9047
06800     2.4992      0.0781     0.0814     76.8460
06900     2.4992      0.0633     0.0814     76.6138
07000     2.4992      0.0802     0.0814     75.7792
07100     2.4992      0.1076     0.0814     77.5557
07200     2.4992      0.1118     0.0814     78.2643
07300     2.4992      0.0949     0.0814     76.1966
07400     2.4991      0.0949     0.0814     77.2513
07500     2.4991      0.0823     0.0814     77.3173
07600     2.4991      0.0781     0.0814     75.6258
07700     2.4991      0.0992     0.0814     76.1300
07800     2.4991      0.0717     0.0814     76.4266
07900     2.4990      0.0717     0.0816     77.3845
08000     2.4990      0.0928     0.0816     77.5792
08100     2.4990      0.0759     0.0816     76.1747
08200     2.4990      0.0675     0.0816     76.2340
08300     2.4990      0.1055     0.0816     75.9416
08400     2.4989      0.0802     0.0816     76.7194
08500     2.4989      0.0696     0.0816     78.0774
08600     2.4989      0.0738     0.0816     74.6342
08700     2.4989      0.0738     0.0816     78.6443
08800     2.4989      0.0570     0.0816     75.1294
08900     2.4989      0.0781     0.0816     76.1809
09000     2.4988      0.0865     0.0816     76.9866
09100     2.4988      0.0865     0.0816     77.2239
09200     2.4988      0.0759     0.0816     77.8739
09300     2.4988      0.0696     0.0816     74.4983
09400     2.4988      0.0992     0.0816     77.1781
09500     2.4987      0.0802     0.0816     75.9462
09600     2.4987      0.0717     0.0816     76.2316
09700     2.4987      0.0696     0.0816     75.7532
09800     2.4987      0.0907     0.0816     76.2536
09900     2.4987      0.0591     0.0816     76.0986
10000     2.4986      0.0759     0.0816     75.8771
10100     2.4986      0.0823     0.0816     76.5072
10200     2.4986      0.0696     0.0816     76.3664
10300     2.4986      0.0907     0.0816     76.5680
10400     2.4986      0.0802     0.0816     77.2448
10500     2.4986      0.0802     0.0816     76.8032
10600     2.4986      0.0865     0.0816     77.5404
10700     2.4986      0.0591     0.0816     77.0375
10800     2.4986      0.1118     0.0816     75.5400
10900     2.4986      0.1076     0.0816     75.4754
11000     2.4986      0.0696     0.0816     75.9479
11100     2.4986      0.0633     0.0816     76.0090
11200     2.4986      0.0696     0.0816     76.7697
11300     2.4986      0.0970     0.0816     75.7020
11400     2.4986      0.0443     0.0816     78.4911
11500     2.4986      0.0907     0.0816     77.0936
11600     2.4986      0.0738     0.0816     77.4583
11700     2.4986      0.0886     0.0816     76.2836
11800     2.4986      0.0886     0.0816     76.5031
11900     2.4986      0.1139     0.0816     75.7257
12000     2.4986      0.0633     0.0816     76.6162
12100     2.4986      0.0570     0.0816     76.9553
12200     2.4986      0.0802     0.0816     76.8418
12300     2.4986      0.0675     0.0816     76.3592
12400     2.4986      0.0654     0.0816     75.5187
12500     2.4986      0.1013     0.0816     75.3976
12600     2.4986      0.0802     0.0816     77.2316
12700     2.4985      0.0865     0.0816     75.0610
12800     2.4985      0.0823     0.0816     77.6298
12900     2.4985      0.1034     0.0816     75.9413
13000     2.4985      0.0823     0.0816     77.7185
13100     2.4985      0.0823     0.0816     76.5908
13200     2.4985      0.1013     0.0816     78.2001
13300     2.4985      0.0802     0.0816     74.5046
13400     2.4985      0.0717     0.0816     76.8279
13500     2.4985      0.0781     0.0816     76.4444
13600     2.4985      0.0865     0.0816     75.7716
13700     2.4985      0.0633     0.0816     75.2468
13800     2.4985      0.0738     0.0816     76.3020
13900     2.4985      0.0717     0.0816     77.8922
14000     2.4985      0.0759     0.0816     76.2426
14100     2.4985      0.0759     0.0816     76.2354
14200     2.4985      0.0907     0.0816     77.0805
14300     2.4985      0.0738     0.0816     76.5527
14400     2.4985      0.1034     0.0816     76.9428
14500     2.4985      0.0865     0.0816     76.5610
14600     2.4985      0.0738     0.0816     75.6180
14700     2.4985      0.0907     0.0816     76.1108
14800     2.4985      0.0738     0.0816     75.9618
14900     2.4985      0.0549     0.0816     76.4964
15000     2.4985      0.0844     0.0816     77.8111
15100     2.4985      0.0886     0.0816     76.2316
15200     2.4985      0.0823     0.0816     76.9970
15300     2.4985      0.0907     0.0816     76.9722
15400     2.4985      0.0696     0.0816     75.7805
15500     2.4984      0.0928     0.0816     76.7453
15600     2.4984      0.0633     0.0816     74.9354
15700     2.4984      0.1055     0.0816     74.0984
15800     2.4984      0.0738     0.0816     75.6929
15900     2.4984      0.0633     0.0816     75.6122
16000     2.4984      0.0928     0.0816     76.2122
16100     2.4984      0.0738     0.0816     76.9187
16200     2.4984      0.0928     0.0816     75.3041
16300     2.4984      0.0781     0.0816     75.0369
16400     2.4984      0.0823     0.0816     76.8034
16500     2.4984      0.0823     0.0816     75.8810
16600     2.4984      0.0823     0.0816     76.0837
16700     2.4984      0.0738     0.0816     76.4062
16800     2.4984      0.0970     0.0816     76.3986
16900     2.4984      0.0823     0.0816     76.2349
17000     2.4984      0.0717     0.0816     76.0534
17100     2.4984      0.0675     0.0816     76.1260
17200     2.4984      0.0675     0.0816     77.9758
17300     2.4984      0.0654     0.0816     76.5711
17400     2.4984      0.0717     0.0816     76.0150
17500     2.4984      0.0738     0.0816     76.3356
17600     2.4984      0.0612     0.0816     76.7687
17700     2.4984      0.0675     0.0816     75.7176
17800     2.4984      0.0654     0.0816     76.0355
17900     2.4984      0.0886     0.0816     76.6015
18000     2.4984      0.1097     0.0816     75.3028
18100     2.4984      0.0886     0.0816     77.5990
18200     2.4984      0.0717     0.0816     75.7913
18300     2.4983      0.0696     0.0816     75.0465
18400     2.4983      0.0738     0.0816     78.3471
18500     2.4983      0.0633     0.0816     76.7484
18600     2.4983      0.0907     0.0816     75.1887
18700     2.4983      0.0781     0.0816     74.8542
18800     2.4983      0.1181     0.0816     74.4392
18900     2.4983      0.0759     0.0816     75.5655
19000     2.4983      0.0696     0.0816     77.1836
19100     2.4983      0.0570     0.0816     74.2556
19200     2.4983      0.0907     0.0816     76.4273
19300     2.4983      0.0485     0.0816     76.5085
19400     2.4983      0.0781     0.0816     74.8600
19500     2.4983      0.0570     0.0816     75.6774
19600     2.4983      0.0949     0.0816     74.6912
19700     2.4983      0.0570     0.0816     76.2851
19800     2.4983      0.0633     0.0816     77.7222
19900     2.4983      0.0675     0.0816     74.9467
20000     2.4983      0.0633     0.0816     76.7243
20100     2.4983      0.0907     0.0816     76.8987
20200     2.4983      0.0717     0.0816     76.8597
20300     2.4983      0.0865     0.0816     77.4415
20400     2.4983      0.0992     0.0816     77.2439
20500     2.4983      0.0781     0.0816     76.5329
20600     2.4983      0.0759     0.0816     76.6532
20700     2.4983      0.0591     0.0816     75.9071
20800     2.4983      0.0865     0.0816     75.5685
20900     2.4983      0.0675     0.0816     76.6842
21000     2.4983      0.0759     0.0816     75.3321
21100     2.4983      0.0865     0.0816     77.1083
21200     2.4983      0.0738     0.0816     77.4251
21300     2.4983      0.0992     0.0816     77.3776
21400     2.4983      0.0802     0.0816     78.0746
21500     2.4983      0.0802     0.0816     78.9065
21600     2.4983      0.0907     0.0816     79.4745
21700     2.4983      0.0759     0.0816     75.7845
21800     2.4983      0.0781     0.0816     76.9511
21900     2.4983      0.0844     0.0816     75.1892
22000     2.4983      0.0591     0.0816     77.1520
22100     2.4983      0.0907     0.0816     75.1678
22200     2.4983      0.0527     0.0816     76.3022
22300     2.4983      0.0759     0.0816     75.4656
22400     2.4983      0.0612     0.0816     77.5120
22500     2.4983      0.0781     0.0816     76.8640
22600     2.4983      0.0949     0.0816     77.2782
22700     2.4983      0.0781     0.0816     75.5641
22800     2.4983      0.0992     0.0816     78.1086
22900     2.4983      0.0612     0.0816     75.8103
23000     2.4983      0.0612     0.0816     76.6284
23100     2.4983      0.0675     0.0816     76.9898
23200     2.4983      0.0591     0.0816     76.6212
23300     2.4983      0.0759     0.0816     77.3501
23400     2.4983      0.1160     0.0816     76.4020
23500     2.4983      0.0717     0.0816     76.8689
23600     2.4983      0.0802     0.0816     76.3048
23700     2.4983      0.0781     0.0816     77.5286
23800     2.4983      0.0675     0.0816     75.8479
23900     2.4983      0.0759     0.0816     76.8389
24000     2.4983      0.0886     0.0816     77.0626
24100     2.4983      0.0759     0.0816     75.6082
24200     2.4983      0.0781     0.0816     77.1862
24300     2.4983      0.0802     0.0816     76.2213
24400     2.4983      0.0612     0.0816     76.4037
24500     2.4982      0.0675     0.0816     74.7742
24600     2.4982      0.0823     0.0816     77.4252
24700     2.4982      0.0802     0.0816     75.8479
24800     2.4982      0.0886     0.0816     77.6358
24900     2.4982      0.0759     0.0816     76.7136
25000     2.4982      0.0907     0.0816     78.6347
25100     2.4982      0.1076     0.0816     74.2897
25200     2.4982      0.0633     0.0816     77.4277
25300     2.4982      0.1013     0.0816     76.6204
25400     2.4982      0.0886     0.0816     78.4484
25500     2.4982      0.0844     0.0816     76.6679
25600     2.4982      0.1013     0.0816     76.2576
25700     2.4982      0.0654     0.0816     76.2918
25800     2.4982      0.0928     0.0816     75.9582
25900     2.4982      0.0633     0.0816     77.2453
26000     2.4982      0.1055     0.0816     75.8798
26100     2.4982      0.0781     0.0816     76.1652
26200     2.4982      0.0949     0.0816     75.8250
26300     2.4982      0.1034     0.0816     78.0952
26400     2.4982      0.0844     0.0816     75.6867
26500     2.4982      0.0928     0.0816     78.0584
26600     2.4982      0.0612     0.0816     75.0216
26700     2.4982      0.0527     0.0816     75.5296
26800     2.4982      0.0570     0.0816     74.8587
26900     2.4982      0.0781     0.0816     74.4507
27000     2.4982      0.0781     0.0816     74.9257
27100     2.4982      0.0823     0.0816     64.9317
27200     2.4982      0.0591     0.0816     69.9561
27300     2.4982      0.0970     0.0816     67.6074
27400     2.4982      0.0675     0.0816     70.8263
27500     2.4982      0.0844     0.0816     66.7697
27600     2.4982      0.0886     0.0816     66.4728
27700     2.4982      0.0570     0.0816     69.2214
27800     2.4982      0.0675     0.0816     69.7658
27900     2.4982      0.0844     0.0816     67.5112
28000     2.4982      0.0970     0.0816     69.2003
28100     2.4982      0.0717     0.0816     69.5386
28200     2.4982      0.0738     0.0816     66.3293
28300     2.4982      0.0802     0.0816     66.9269
28400     2.4982      0.0591     0.0816     67.2467
28500     2.4982      0.0949     0.0816     65.7971
28600     2.4982      0.0675     0.0816     67.7532
28700     2.4982      0.0633     0.0816     66.3708
28800     2.4982      0.0654     0.0816     68.2966
28900     2.4982      0.0591     0.0816     67.7094
29000     2.4982      0.0844     0.0816     68.7646
29100     2.4982      0.0549     0.0816     68.4121
29200     2.4982      0.0781     0.0816     67.1614
29300     2.4982      0.0802     0.0816     66.5237
29400     2.4982      0.0781     0.0816     67.7954
29500     2.4982      0.0654     0.0816     69.0179
29600     2.4982      0.0485     0.0816     67.5567
29700     2.4982      0.0696     0.0816     67.7787
29800     2.4982      0.0675     0.0816     69.2728
29900     2.4982      0.0823     0.0816     64.1593
29999     2.4982      0.0907     0.0816     66.1703
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Start testing:
Test Accuracy: 0.0789
