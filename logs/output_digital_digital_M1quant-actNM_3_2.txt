Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 362, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 362, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.0, finetuning_epochs=0, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.0005,0.0001,0.00002', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,10000', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
077e92b0-d6a7-4808-845b-11bad3c867a9
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.5352      0.0781     0.0808     14.2380
00100     2.5221      0.0844     0.0811     77.2678
00200     2.5140      0.0907     0.0811     77.1740
00300     2.5096      0.0696     0.0811     75.8649
00400     2.5072      0.0738     0.0811     75.4370
00500     2.5057      0.0907     0.0811     75.2866
00600     2.5047      0.0886     0.0811     75.4554
00700     2.5039      0.0886     0.0811     76.3169
00800     2.5034      0.0738     0.0811     76.4404
00900     2.5029      0.0675     0.0811     76.9680
01000     2.5025      0.0759     0.0811     74.8737
01100     2.5022      0.0696     0.0811     77.6438
01200     2.5020      0.0928     0.0811     75.2172
01300     2.5017      0.0759     0.0811     75.8636
01400     2.5015      0.0823     0.0811     76.9258
01500     2.5014      0.1013     0.0811     75.5929
01600     2.5012      0.0633     0.0811     77.6659
01700     2.5011      0.0970     0.0811     77.0030
01800     2.5010      0.0928     0.0811     78.3650
01900     2.5008      0.0612     0.0811     76.0535
02000     2.5007      0.0759     0.0811     74.6781
02100     2.5007      0.0949     0.0811     74.7773
02200     2.5006      0.0865     0.0811     76.0356
02300     2.5005      0.0759     0.0811     77.2219
02400     2.5004      0.0738     0.0811     76.8194
02500     2.5004      0.0802     0.0811     75.2449
02600     2.5003      0.0759     0.0811     74.8349
02700     2.5003      0.0949     0.0811     75.5888
02800     2.5002      0.0738     0.0811     75.3380
02900     2.5002      0.0823     0.0811     75.2230
03000     2.5001      0.0823     0.0811     75.1243
03100     2.5001      0.1076     0.0811     78.7592
03200     2.5000      0.0759     0.0811     78.5615
03300     2.5000      0.0696     0.0811     76.7107
03400     2.5000      0.1013     0.0811     75.9954
03500     2.4999      0.0738     0.0811     76.4846
03600     2.4999      0.0844     0.0811     76.0126
03700     2.4999      0.0717     0.0811     76.6069
03800     2.4999      0.0696     0.0811     76.6950
03900     2.4998      0.0844     0.0811     75.4876
04000     2.4998      0.0570     0.0811     77.9967
04100     2.4998      0.0759     0.0814     75.1102
04200     2.4998      0.0949     0.0814     75.4263
04300     2.4997      0.0759     0.0814     77.0979
04400     2.4997      0.0654     0.0814     75.9624
04500     2.4997      0.0949     0.0814     77.7211
04600     2.4997      0.0654     0.0814     77.5156
04700     2.4997      0.1034     0.0814     76.0810
04800     2.4996      0.0675     0.0814     75.8613
04900     2.4996      0.0865     0.0814     76.7478
05000     2.4996      0.0654     0.0814     75.6759
05100     2.4996      0.0654     0.0814     76.5960
05200     2.4996      0.0886     0.0814     74.9309
05300     2.4995      0.0654     0.0814     76.7338
05400     2.4995      0.0696     0.0814     75.6095
05500     2.4995      0.1013     0.0814     75.5423
05600     2.4995      0.0717     0.0814     75.4428
05700     2.4995      0.0696     0.0814     75.1368
05800     2.4994      0.0781     0.0814     75.4192
05900     2.4994      0.0612     0.0814     75.1347
06000     2.4994      0.0675     0.0814     75.8723
06100     2.4994      0.0717     0.0814     76.8573
06200     2.4994      0.0928     0.0814     75.3198
06300     2.4993      0.0759     0.0814     75.3289
06400     2.4993      0.0823     0.0814     78.2854
06500     2.4993      0.0865     0.0814     77.7878
06600     2.4993      0.0802     0.0814     75.8467
06700     2.4993      0.0949     0.0814     76.9047
06800     2.4992      0.0781     0.0814     76.8460
06900     2.4992      0.0633     0.0814     76.6138
07000     2.4992      0.0802     0.0814     75.7792
07100     2.4992      0.1076     0.0814     77.5557
07200     2.4992      0.1118     0.0814     78.2643
07300     2.4992      0.0949     0.0814     76.1966
07400     2.4991      0.0949     0.0814     77.2513
07500     2.4991      0.0823     0.0814     77.3173
07600     2.4991      0.0781     0.0814     75.6258
07700     2.4991      0.0992     0.0814     76.1300
07800     2.4991      0.0717     0.0814     76.4266
07900     2.4990      0.0717     0.0816     77.3845
08000     2.4990      0.0928     0.0816     77.5792
08100     2.4990      0.0759     0.0816     76.1747
08200     2.4990      0.0675     0.0816     76.2340
08300     2.4990      0.1055     0.0816     75.9416
08400     2.4989      0.0802     0.0816     76.7194
08500     2.4989      0.0696     0.0816     78.0774
08600     2.4989      0.0738     0.0816     74.6342
08700     2.4989      0.0738     0.0816     78.6443
08800     2.4989      0.0570     0.0816     75.1294
08900     2.4989      0.0781     0.0816     76.1809
09000     2.4988      0.0865     0.0816     76.9866
09100     2.4988      0.0865     0.0816     77.2239
09200     2.4988      0.0759     0.0816     77.8739
09300     2.4988      0.0696     0.0816     74.4983
09400     2.4988      0.0992     0.0816     77.1781
09500     2.4987      0.0802     0.0816     75.9462
09600     2.4987      0.0717     0.0816     76.2316
09700     2.4987      0.0696     0.0816     75.7532
09800     2.4987      0.0907     0.0816     76.2536
09900     2.4987      0.0591     0.0816     76.0986
10000     2.4986      0.0759     0.0816     75.8771
10100     2.4986      0.0823     0.0816     76.5072
10200     2.4986      0.0696     0.0816     76.3664
10300     2.4986      0.0907     0.0816     76.5680
10400     2.4986      0.0802     0.0816     77.2448
10500     2.4986      0.0802     0.0816     76.8032
10600     2.4986      0.0865     0.0816     77.5404
10700     2.4986      0.0591     0.0816     77.0375
10800     2.4986      0.1118     0.0816     75.5400
10900     2.4986      0.1076     0.0816     75.4754
11000     2.4986      0.0696     0.0816     75.9479
11100     2.4986      0.0633     0.0816     76.0090
11200     2.4986      0.0696     0.0816     76.7697
11300     2.4986      0.0970     0.0816     75.7020
11400     2.4986      0.0443     0.0816     78.4911
11500     2.4986      0.0907     0.0816     77.0936
11600     2.4986      0.0738     0.0816     77.4583
11700     2.4986      0.0886     0.0816     76.2836
11800     2.4986      0.0886     0.0816     76.5031
11900     2.4986      0.1139     0.0816     75.7257
12000     2.4986      0.0633     0.0816     76.6162
12100     2.4986      0.0570     0.0816     76.9553
12200     2.4986      0.0802     0.0816     76.8418
12300     2.4986      0.0675     0.0816     76.3592
12400     2.4986      0.0654     0.0816     75.5187
12500     2.4986      0.1013     0.0816     75.3976
12600     2.4986      0.0802     0.0816     77.2316
12700     2.4985      0.0865     0.0816     75.0610
12800     2.4985      0.0823     0.0816     77.6298
12900     2.4985      0.1034     0.0816     75.9413
13000     2.4985      0.0823     0.0816     77.7185
13100     2.4985      0.0823     0.0816     76.5908
13200     2.4985      0.1013     0.0816     78.2001
13300     2.4985      0.0802     0.0816     74.5046
13400     2.4985      0.0717     0.0816     76.8279
13500     2.4985      0.0781     0.0816     76.4444
13600     2.4985      0.0865     0.0816     75.7716
13700     2.4985      0.0633     0.0816     75.2468
13800     2.4985      0.0738     0.0816     76.3020
13900     2.4985      0.0717     0.0816     77.8922
14000     2.4985      0.0759     0.0816     76.2426
14100     2.4985      0.0759     0.0816     76.2354
14200     2.4985      0.0907     0.0816     77.0805
14300     2.4985      0.0738     0.0816     76.5527
14400     2.4985      0.1034     0.0816     76.9428
14500     2.4985      0.0865     0.0816     76.5610
14600     2.4985      0.0738     0.0816     75.6180
14700     2.4985      0.0907     0.0816     76.1108
14800     2.4985      0.0738     0.0816     75.9618
14900     2.4985      0.0549     0.0816     76.4964
15000     2.4985      0.0844     0.0816     77.8111
15100     2.4985      0.0886     0.0816     76.2316
15200     2.4985      0.0823     0.0816     76.9970
15300     2.4985      0.0907     0.0816     76.9722
15400     2.4985      0.0696     0.0816     75.7805
15500     2.4984      0.0928     0.0816     76.7453
15600     2.4984      0.0633     0.0816     74.9354
15700     2.4984      0.1055     0.0816     74.0984
15800     2.4984      0.0738     0.0816     75.6929
15900     2.4984      0.0633     0.0816     75.6122
16000     2.4984      0.0928     0.0816     76.2122
16100     2.4984      0.0738     0.0816     76.9187
16200     2.4984      0.0928     0.0816     75.3041
16300     2.4984      0.0781     0.0816     75.0369
16400     2.4984      0.0823     0.0816     76.8034
16500     2.4984      0.0823     0.0816     75.8810
16600     2.4984      0.0823     0.0816     76.0837
16700     2.4984      0.0738     0.0816     76.4062
16800     2.4984      0.0970     0.0816     76.3986
16900     2.4984      0.0823     0.0816     76.2349
17000     2.4984      0.0717     0.0816     76.0534
17100     2.4984      0.0675     0.0816     76.1260
17200     2.4984      0.0675     0.0816     77.9758
17300     2.4984      0.0654     0.0816     76.5711
17400     2.4984      0.0717     0.0816     76.0150
17500     2.4984      0.0738     0.0816     76.3356
17600     2.4984      0.0612     0.0816     76.7687
17700     2.4984      0.0675     0.0816     75.7176
17800     2.4984      0.0654     0.0816     76.0355
17900     2.4984      0.0886     0.0816     76.6015
18000     2.4984      0.1097     0.0816     75.3028
18100     2.4984      0.0886     0.0816     77.5990
18200     2.4984      0.0717     0.0816     75.7913
18300     2.4983      0.0696     0.0816     75.0465
18400     2.4983      0.0738     0.0816     78.3471
18500     2.4983      0.0633     0.0816     76.7484
18600     2.4983      0.0907     0.0816     75.1887
18700     2.4983      0.0781     0.0816     74.8542
18800     2.4983      0.1181     0.0816     74.4392
18900     2.4983      0.0759     0.0816     75.5655
19000     2.4983      0.0696     0.0816     77.1836
19100     2.4983      0.0570     0.0816     74.2556
19200     2.4983      0.0907     0.0816     76.4273
19300     2.4983      0.0485     0.0816     76.5085
19400     2.4983      0.0781     0.0816     74.8600
19500     2.4983      0.0570     0.0816     75.6774
19600     2.4983      0.0949     0.0816     74.6912
19700     2.4983      0.0570     0.0816     76.2851
19800     2.4983      0.0633     0.0816     77.7222
19900     2.4983      0.0675     0.0816     74.9467
20000     2.4983      0.0633     0.0816     76.7243
20100     2.4983      0.0907     0.0816     76.8987
20200     2.4983      0.0717     0.0816     76.8597
20300     2.4983      0.0865     0.0816     77.4415
20400     2.4983      0.0992     0.0816     77.2439
20500     2.4983      0.0781     0.0816     76.5329
20600     2.4983      0.0759     0.0816     76.6532
20700     2.4983      0.0591     0.0816     75.9071
20800     2.4983      0.0865     0.0816     75.5685
20900     2.4983      0.0675     0.0816     76.6842
21000     2.4983      0.0759     0.0816     75.3321
21100     2.4983      0.0865     0.0816     77.1083
21200     2.4983      0.0738     0.0816     77.4251
21300     2.4983      0.0992     0.0816     77.3776
21400     2.4983      0.0802     0.0816     78.0746
21500     2.4983      0.0802     0.0816     78.9065
21600     2.4983      0.0907     0.0816     79.4745
21700     2.4983      0.0759     0.0816     75.7845
21800     2.4983      0.0781     0.0816     76.9511
21900     2.4983      0.0844     0.0816     75.1892
22000     2.4983      0.0591     0.0816     77.1520
22100     2.4983      0.0907     0.0816     75.1678
22200     2.4983      0.0527     0.0816     76.3022
22300     2.4983      0.0759     0.0816     75.4656
22400     2.4983      0.0612     0.0816     77.5120
22500     2.4983      0.0781     0.0816     76.8640
22600     2.4983      0.0949     0.0816     77.2782
22700     2.4983      0.0781     0.0816     75.5641
22800     2.4983      0.0992     0.0816     78.1086
22900     2.4983      0.0612     0.0816     75.8103
23000     2.4983      0.0612     0.0816     76.6284
23100     2.4983      0.0675     0.0816     76.9898
23200     2.4983      0.0591     0.0816     76.6212
23300     2.4983      0.0759     0.0816     77.3501
23400     2.4983      0.1160     0.0816     76.4020
23500     2.4983      0.0717     0.0816     76.8689
23600     2.4983      0.0802     0.0816     76.3048
23700     2.4983      0.0781     0.0816     77.5286
23800     2.4983      0.0675     0.0816     75.8479
23900     2.4983      0.0759     0.0816     76.8389
24000     2.4983      0.0886     0.0816     77.0626
24100     2.4983      0.0759     0.0816     75.6082
24200     2.4983      0.0781     0.0816     77.1862
24300     2.4983      0.0802     0.0816     76.2213
24400     2.4983      0.0612     0.0816     76.4037
24500     2.4982      0.0675     0.0816     74.7742
24600     2.4982      0.0823     0.0816     77.4252
24700     2.4982      0.0802     0.0816     75.8479
24800     2.4982      0.0886     0.0816     77.6358
24900     2.4982      0.0759     0.0816     76.7136
25000     2.4982      0.0907     0.0816     78.6347
25100     2.4982      0.1076     0.0816     74.2897
25200     2.4982      0.0633     0.0816     77.4277
25300     2.4982      0.1013     0.0816     76.6204
25400     2.4982      0.0886     0.0816     78.4484
25500     2.4982      0.0844     0.0816     76.6679
25600     2.4982      0.1013     0.0816     76.2576
25700     2.4982      0.0654     0.0816     76.2918
25800     2.4982      0.0928     0.0816     75.9582
25900     2.4982      0.0633     0.0816     77.2453
26000     2.4982      0.1055     0.0816     75.8798
26100     2.4982      0.0781     0.0816     76.1652
26200     2.4982      0.0949     0.0816     75.8250
26300     2.4982      0.1034     0.0816     78.0952
26400     2.4982      0.0844     0.0816     75.6867
26500     2.4982      0.0928     0.0816     78.0584
26600     2.4982      0.0612     0.0816     75.0216
26700     2.4982      0.0527     0.0816     75.5296
26800     2.4982      0.0570     0.0816     74.8587
26900     2.4982      0.0781     0.0816     74.4507
27000     2.4982      0.0781     0.0816     74.9257
27100     2.4982      0.0823     0.0816     64.9317
27200     2.4982      0.0591     0.0816     69.9561
27300     2.4982      0.0970     0.0816     67.6074
27400     2.4982      0.0675     0.0816     70.8263
27500     2.4982      0.0844     0.0816     66.7697
27600     2.4982      0.0886     0.0816     66.4728
27700     2.4982      0.0570     0.0816     69.2214
27800     2.4982      0.0675     0.0816     69.7658
27900     2.4982      0.0844     0.0816     67.5112
28000     2.4982      0.0970     0.0816     69.2003
28100     2.4982      0.0717     0.0816     69.5386
28200     2.4982      0.0738     0.0816     66.3293
28300     2.4982      0.0802     0.0816     66.9269
28400     2.4982      0.0591     0.0816     67.2467
28500     2.4982      0.0949     0.0816     65.7971
28600     2.4982      0.0675     0.0816     67.7532
28700     2.4982      0.0633     0.0816     66.3708
28800     2.4982      0.0654     0.0816     68.2966
28900     2.4982      0.0591     0.0816     67.7094
29000     2.4982      0.0844     0.0816     68.7646
29100     2.4982      0.0549     0.0816     68.4121
29200     2.4982      0.0781     0.0816     67.1614
29300     2.4982      0.0802     0.0816     66.5237
29400     2.4982      0.0781     0.0816     67.7954
29500     2.4982      0.0654     0.0816     69.0179
29600     2.4982      0.0485     0.0816     67.5567
29700     2.4982      0.0696     0.0816     67.7787
29800     2.4982      0.0675     0.0816     69.2728
29900     2.4982      0.0823     0.0816     64.1593
29999     2.4982      0.0907     0.0816     66.1703
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Start testing:
Test Accuracy: 0.0789
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
78f83a51-6b1a-457a-934b-f994383fbba5
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.5352      0.0802     0.0805     12.2060
00100     2.5068      0.0675     0.0807     59.1531
00200     2.5033      0.0717     0.0808     58.1065
00300     2.5020      0.0781     0.0808     59.1500
00400     2.5013      0.0717     0.0808     58.4754
00500     2.5008      0.0949     0.0808     58.8595
00600     2.5005      0.0654     0.0812     58.7554
00700     2.5003      0.0928     0.0812     58.5777
00800     2.5001      0.1055     0.0812     59.3409
00900     2.5000      0.0949     0.0812     58.6051
01000     2.4999      0.0928     0.0812     58.4013
01100     2.4998      0.0823     0.0812     59.3739
01200     2.4997      0.0907     0.0812     58.5428
01300     2.4996      0.0675     0.0812     58.5201
01400     2.4995      0.0506     0.0812     59.1287
01500     2.4994      0.0970     0.0812     58.8289
01600     2.4993      0.0738     0.0812     59.6165
01700     2.4993      0.0928     0.0812     59.0385
01800     2.4992      0.0802     0.0812     58.5627
01900     2.4991      0.0949     0.0812     59.1440
02000     2.4990      0.0633     0.0812     58.8231
02100     2.4990      0.0759     0.0812     58.9875
02200     2.4989      0.0802     0.0812     59.3742
02300     2.4988      0.0696     0.0812     58.9452
02400     2.4987      0.0907     0.0812     60.0822
02500     2.4987      0.0907     0.0812     59.3785
02600     2.4986      0.1034     0.0812     59.1786
02700     2.4985      0.0865     0.0812     59.9468
02800     2.4984      0.0717     0.0812     59.5490
02900     2.4984      0.0844     0.0812     60.0495
03000     2.4983      0.0907     0.0812     59.5768
03100     2.4982      0.1266     0.0812     59.0337
03200     2.4982      0.0802     0.0812     59.8352
03300     2.4981      0.0781     0.0812     59.4696
03400     2.4980      0.0802     0.0812     59.8274
03500     2.4979      0.0759     0.0812     59.7877
03600     2.4979      0.0675     0.0812     58.9030
03700     2.4978      0.0738     0.0812     59.4486
03800     2.4977      0.1013     0.0812     59.4515
03900     2.4977      0.0886     0.0812     59.4625
04000     2.4976      0.0781     0.0812     60.0579
04100     2.4975      0.0823     0.0812     60.6260
04200     2.4975      0.0781     0.0812     60.0802
04300     2.4974      0.0717     0.0812     62.0276
04400     2.4973      0.0696     0.0812     60.9258
04500     2.4972      0.0970     0.0812     61.2786
04600     2.4972      0.0907     0.0812     60.3585
04700     2.4971      0.0717     0.0812     61.4535
04800     2.4970      0.0759     0.0812     62.0286
04900     2.4970      0.0633     0.0812     61.6701
05000     2.4969      0.0759     0.0812     59.7872
05100     2.4968      0.0717     0.0812     61.8898
05200     2.4968      0.0675     0.0812     61.8132
05300     2.4967      0.0759     0.0812     62.1376
05400     2.4966      0.0802     0.0812     62.7457
05500     2.4966      0.0802     0.0812     61.1060
05600     2.4965      0.0886     0.0812     61.6289
05700     2.4964      0.0844     0.0812     60.2153
05800     2.4964      0.0485     0.0812     61.3983
05900     2.4963      0.0928     0.0812     61.9353
06000     2.4962      0.1034     0.0812     60.4944
06100     2.4962      0.0949     0.0812     61.4758
06200     2.4961      0.0781     0.0812     61.3062
06300     2.4960      0.0696     0.0812     60.9365
06400     2.4960      0.0802     0.0812     61.7266
06500     2.4959      0.0844     0.0812     61.1168
06600     2.4958      0.0717     0.0812     62.4210
06700     2.4958      0.1013     0.0812     62.9768
06800     2.4957      0.0717     0.0812     61.2907
06900     2.4956      0.0781     0.0812     61.7359
07000     2.4956      0.0654     0.0812     60.7692
07100     2.4955      0.0823     0.0812     60.7326
07200     2.4954      0.0886     0.0812     61.3735
07300     2.4954      0.0696     0.0812     61.8461
07400     2.4953      0.0717     0.0812     60.6554
07500     2.4952      0.0865     0.0812     62.9911
07600     2.4952      0.0928     0.0812     61.8219
07700     2.4951      0.0781     0.0813     62.7929
07800     2.4950      0.0886     0.0813     63.5815
07900     2.4950      0.0696     0.0813     60.9460
08000     2.4949      0.0612     0.0813     60.0688
08100     2.4949      0.1013     0.0813     57.0420
08200     2.4948      0.0675     0.0813     59.1539
08300     2.4947      0.0506     0.0813     59.0769
08400     2.4947      0.0823     0.0813     57.2266
08500     2.4946      0.0759     0.0813     57.3241
08600     2.4945      0.0633     0.0813     57.2779
08700     2.4945      0.0865     0.0813     58.1146
08800     2.4944      0.0823     0.0813     58.3209
08900     2.4943      0.0781     0.0813     57.1920
09000     2.4943      0.0675     0.0813     59.1699
09100     2.4942      0.0802     0.0813     58.8080
09200     2.4942      0.1013     0.0813     59.8732
09300     2.4941      0.0907     0.0813     57.4237
09400     2.4940      0.0738     0.0813     59.0398
09500     2.4940      0.0949     0.0813     57.8845
09600     2.4939      0.0865     0.0813     59.2143
09700     2.4938      0.0654     0.0813     57.4277
09800     2.4938      0.0654     0.0813     56.7358
09900     2.4937      0.0759     0.0813     59.7681
10000     2.4936      0.0823     0.0813     57.6271
10100     2.4936      0.0802     0.0813     58.4427
10200     2.4936      0.0696     0.0813     58.2473
10300     2.4936      0.0865     0.0813     57.5668
10400     2.4936      0.0675     0.0813     57.6740
10500     2.4936      0.0992     0.0813     57.9558
10600     2.4936      0.0717     0.0813     58.7524
10700     2.4935      0.0886     0.0813     58.9356
10800     2.4935      0.0823     0.0813     57.4563
10900     2.4935      0.0612     0.0813     57.9447
11000     2.4935      0.0928     0.0813     58.9141
11100     2.4935      0.0591     0.0813     57.9334
11200     2.4935      0.0675     0.0813     58.3219
11300     2.4934      0.0844     0.0813     58.2621
11400     2.4934      0.0844     0.0813     57.5285
11500     2.4934      0.1076     0.0813     58.8152
11600     2.4934      0.0717     0.0814     58.7703
11700     2.4934      0.0633     0.0814     59.2082
11800     2.4934      0.0928     0.0814     59.0084
11900     2.4933      0.0844     0.0814     58.2991
12000     2.4933      0.1160     0.0814     60.0843
12100     2.4933      0.0886     0.0814     58.2904
12200     2.4933      0.0886     0.0814     59.6872
12300     2.4933      0.0591     0.0814     58.6001
12400     2.4933      0.0738     0.0814     56.6176
12500     2.4933      0.0844     0.0814     58.6202
12600     2.4932      0.0802     0.0814     58.4687
12700     2.4932      0.0781     0.0814     57.7627
12800     2.4932      0.0781     0.0814     60.2444
12900     2.4932      0.0886     0.0814     57.2810
13000     2.4932      0.0928     0.0814     58.4915
13100     2.4932      0.0949     0.0814     59.1100
13200     2.4931      0.0675     0.0814     58.2382
13300     2.4931      0.0802     0.0814     56.7214
13400     2.4931      0.0802     0.0814     58.3844
13500     2.4931      0.0549     0.0814     59.0106
13600     2.4931      0.0738     0.0814     58.4714
13700     2.4931      0.0823     0.0814     57.4355
13800     2.4930      0.0844     0.0814     58.1604
13900     2.4930      0.0844     0.0814     58.2305
14000     2.4930      0.0886     0.0814     59.0815
14100     2.4930      0.0696     0.0814     58.4717
14200     2.4930      0.0823     0.0814     58.9943
14300     2.4930      0.0781     0.0814     57.7837
14400     2.4930      0.0844     0.0814     58.2575
14500     2.4929      0.0781     0.0814     57.4101
14600     2.4929      0.0781     0.0814     59.3592
14700     2.4929      0.0886     0.0814     59.8915
14800     2.4929      0.0738     0.0814     58.1349
14900     2.4929      0.0865     0.0814     58.3062
15000     2.4929      0.0823     0.0814     59.0095
15100     2.4928      0.0738     0.0814     57.1004
15200     2.4928      0.0844     0.0814     59.8454
15300     2.4928      0.0759     0.0814     57.5328
15400     2.4928      0.0717     0.0814     57.9835
15500     2.4928      0.0759     0.0814     58.3656
15600     2.4928      0.0886     0.0814     58.3133
15700     2.4928      0.0675     0.0814     58.0098
15800     2.4927      0.0844     0.0814     59.0276
15900     2.4927      0.1076     0.0814     58.5368
16000     2.4927      0.0612     0.0814     59.4668
16100     2.4927      0.0928     0.0814     57.4205
16200     2.4927      0.0802     0.0814     57.9829
16300     2.4927      0.0802     0.0814     59.2885
16400     2.4926      0.0738     0.0814     58.8617
16500     2.4926      0.0844     0.0814     57.8088
16600     2.4926      0.1076     0.0814     58.4162
16700     2.4926      0.0886     0.0814     57.4170
16800     2.4926      0.0612     0.0814     57.9412
16900     2.4926      0.0612     0.0814     57.2058
17000     2.4925      0.0612     0.0814     58.6605
17100     2.4925      0.0886     0.0814     60.0075
17200     2.4925      0.0717     0.0814     57.3644
17300     2.4925      0.0928     0.0814     58.3765
17400     2.4925      0.0844     0.0814     58.5263
17500     2.4925      0.0802     0.0817     58.4842
17600     2.4925      0.0886     0.0817     60.6404
17700     2.4924      0.0759     0.0817     59.6064
17800     2.4924      0.0844     0.0817     58.5253
17900     2.4924      0.0802     0.0817     58.6934
18000     2.4924      0.0633     0.0817     58.4962
18100     2.4924      0.0717     0.0817     57.5529
18200     2.4924      0.0823     0.0817     59.2853
18300     2.4923      0.0970     0.0817     58.3170
18400     2.4923      0.0738     0.0817     58.3658
18500     2.4923      0.1055     0.0817     58.2495
18600     2.4923      0.0781     0.0817     59.3344
18700     2.4923      0.0865     0.0817     59.0717
18800     2.4923      0.0823     0.0817     57.6810
18900     2.4922      0.0823     0.0817     59.1963
19000     2.4922      0.0970     0.0817     58.3211
19100     2.4922      0.0886     0.0817     59.1227
19200     2.4922      0.0907     0.0817     58.7781
19300     2.4922      0.0992     0.0817     58.3097
19400     2.4922      0.0865     0.0817     57.6493
19500     2.4922      0.0485     0.0817     60.1979
19600     2.4921      0.0823     0.0817     57.7975
19700     2.4921      0.0570     0.0817     58.8150
19800     2.4921      0.1055     0.0817     58.0692
19900     2.4921      0.0717     0.0817     57.8528
20000     2.4921      0.0675     0.0817     58.8593
20100     2.4921      0.0823     0.0817     57.8937
20199     2.4921      0.0844     0.0817     56.8239
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.4921      0.1076     0.0807     9.1162
00100     2.4921      0.0696     0.0807     56.1654
00200     2.4921      0.0823     0.0809     57.7367
00300     2.4921      0.0802     0.0809     59.0385
00400     2.4921      0.0865     0.0809     56.2262
00500     2.4921      0.0781     0.0809     55.9725
00600     2.4921      0.0865     0.0809     58.1399
00700     2.4921      0.0844     0.0809     56.0022
00800     2.4921      0.0612     0.0809     56.7936
00900     2.4920      0.0992     0.0810     56.5042
01000     2.4920      0.0654     0.0810     56.0672
01100     2.4920      0.0907     0.0810     56.5236
01200     2.4920      0.0907     0.0810     56.6013
01300     2.4920      0.0633     0.0813     56.2688
01400     2.4920      0.0696     0.0813     57.4663
01500     2.4920      0.0802     0.0813     56.1425
01600     2.4920      0.0696     0.0813     57.5886
01700     2.4920      0.0802     0.0813     56.2643
01800     2.4920      0.0823     0.0813     57.1888
01900     2.4920      0.0717     0.0813     56.1772
02000     2.4920      0.0907     0.0813     56.2068
02100     2.4920      0.0717     0.0813     56.2695
02200     2.4920      0.0759     0.0813     57.9009
02300     2.4920      0.0802     0.0813     56.9450
02400     2.4920      0.0759     0.0813     57.0602
02500     2.4920      0.0823     0.0813     57.8520
02600     2.4920      0.0675     0.0813     56.1463
02700     2.4920      0.0633     0.0813     58.6133
02800     2.4920      0.0802     0.0813     56.5023
02900     2.4920      0.0759     0.0813     55.7953
03000     2.4920      0.0970     0.0813     57.7021
03100     2.4920      0.0717     0.0813     58.2297
03200     2.4920      0.0886     0.0813     56.6347
03300     2.4920      0.0844     0.0813     57.6529
03400     2.4920      0.1224     0.0813     55.9824
03500     2.4920      0.0633     0.0813     59.2052
03600     2.4920      0.0759     0.0813     57.9514
03700     2.4920      0.0781     0.0813     57.6566
03800     2.4920      0.0865     0.0813     56.9416
03900     2.4920      0.0549     0.0813     57.1144
04000     2.4920      0.0823     0.0813     56.2277
04100     2.4920      0.0675     0.0813     59.0297
04200     2.4920      0.0759     0.0813     56.6463
04300     2.4920      0.0654     0.0813     57.3005
04400     2.4920      0.0844     0.0813     56.2820
04500     2.4920      0.0696     0.0813     57.1258
04600     2.4920      0.0738     0.0813     56.2269
04700     2.4920      0.0717     0.0813     58.2232
04800     2.4920      0.0865     0.0813     56.8353
04900     2.4920      0.0949     0.0813     56.4758
05000     2.4920      0.1013     0.0813     56.7199
05100     2.4919      0.0823     0.0813     57.0018
05200     2.4919      0.0549     0.0813     56.0546
05300     2.4919      0.0844     0.0813     58.0912
05400     2.4919      0.0865     0.0813     57.5175
05500     2.4919      0.0802     0.0813     56.5276
05600     2.4919      0.0970     0.0813     56.9112
05700     2.4919      0.0802     0.0813     57.7576
05800     2.4919      0.0844     0.0813     57.3329
05900     2.4919      0.0675     0.0813     57.4983
06000     2.4919      0.0886     0.0813     56.5660
06100     2.4919      0.0886     0.0813     57.3208
06200     2.4919      0.0907     0.0813     58.4207
06300     2.4919      0.0907     0.0813     56.5877
06400     2.4919      0.0970     0.0813     56.5979
06500     2.4919      0.0886     0.0813     57.0506
06600     2.4919      0.0717     0.0813     57.3299
06700     2.4919      0.0781     0.0813     57.5589
06800     2.4919      0.0738     0.0813     56.9844
06900     2.4919      0.0675     0.0813     57.0144
07000     2.4919      0.0823     0.0813     57.1221
07100     2.4919      0.0696     0.0813     56.6901
07200     2.4919      0.0591     0.0813     56.3993
07300     2.4919      0.0781     0.0813     56.7811
07400     2.4919      0.0802     0.0813     56.5815
07500     2.4919      0.0970     0.0813     56.8957
07600     2.4919      0.0612     0.0813     57.4509
07700     2.4919      0.0485     0.0813     56.8899
07800     2.4919      0.0970     0.0813     56.7713
07900     2.4919      0.0781     0.0813     58.5060
08000     2.4919      0.0612     0.0813     57.6315
08100     2.4919      0.1034     0.0813     56.8933
08200     2.4919      0.0844     0.0813     56.9599
08300     2.4919      0.0802     0.0813     58.8328
08400     2.4919      0.0865     0.0813     56.8273
08500     2.4919      0.0865     0.0813     57.1966
08600     2.4919      0.0738     0.0813     56.9226
08700     2.4919      0.0802     0.0813     56.4019
08800     2.4919      0.0823     0.0813     56.9677
08900     2.4919      0.0570     0.0813     59.6788
09000     2.4919      0.0928     0.0813     57.1899
09100     2.4919      0.0802     0.0813     58.3836
09200     2.4919      0.0591     0.0813     56.5391
09300     2.4918      0.0759     0.0813     56.9852
09400     2.4918      0.0485     0.0813     56.6896
09500     2.4918      0.0844     0.0813     57.1781
09600     2.4918      0.0527     0.0813     57.6359
09700     2.4918      0.0949     0.0813     60.7179
09800     2.4918      0.0717     0.0813     57.9444
09900     2.4918      0.0759     0.0813     58.7964
Start testing:
Test Accuracy: 0.0789
