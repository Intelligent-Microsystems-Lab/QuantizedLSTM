Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=10, quant_actNM=10, quant_inp=10, quant_w=10, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
b6de7615-4859-4637-a259-799a340e3446
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, 1.)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 82, in forward
    if len(x_range) > 1:
TypeError: object of type 'float' has no len()
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=10, quant_actNM=10, quant_inp=10, quant_w=10, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
32dd6334-ed2c-406a-884d-29c051c7b56d
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, torch.tensor([1.]))
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 94, in forward
    x_scaled = x/x_range
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=10, quant_actNM=10, quant_inp=10, quant_w=10, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
95d65a49-3749-47a1-b4dd-8e502be1e2fa
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, torch.tensor([1.]).device(input.device))
TypeError: 'torch.device' object is not callable
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=10, quant_actNM=10, quant_inp=10, quant_w=10, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
4f576316-2084-4349-930f-2409e81c842a
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.8255      0.0696     0.0875     9.7765
00100     2.3697      0.2152     0.2559     66.6650
00200     1.8009      0.4262     0.4615     66.6026
00300     1.4694      0.5253     0.5532     69.1031
00400     1.3468      0.5802     0.6037     68.2219
00500     1.2060      0.6139     0.6491     66.6149
00600     1.0788      0.6561     0.6947     68.1197
00700     0.9565      0.7194     0.7194     66.0473
00800     0.9531      0.7131     0.7357     67.7022
00900     0.9639      0.6899     0.7371     66.8057
01000     0.9243      0.7278     0.7523     66.3583
01100     0.8864      0.7405     0.7523     68.6884
01200     0.8954      0.7215     0.7614     67.8519
01300     0.7754      0.7975     0.7662     68.3571
01400     0.8754      0.7447     0.7748     66.6522
01500     0.8060      0.7426     0.7792     66.1194
01600     0.7922      0.7468     0.7859     68.5733
01700     0.7090      0.8038     0.7859     67.0214
01800     0.7849      0.7405     0.7859     67.2436
01900     0.7381      0.7785     0.7952     68.3701
02000     0.7575      0.7806     0.8018     66.5014
02100     0.6967      0.7996     0.8018     67.8534
02200     0.7147      0.7785     0.8018     66.6205
02300     0.7552      0.7595     0.8018     66.5609
02400     0.6703      0.7954     0.8018     66.7153
02500     0.7861      0.7869     0.8042     67.7219
02600     0.7828      0.7574     0.8042     67.7203
02700     0.6875      0.7827     0.8042     66.4010
02800     0.6922      0.7785     0.8113     65.9902
02900     0.8076      0.7700     0.8113     65.7487
03000     0.6867      0.7911     0.8117     66.9269
03100     0.7112      0.7806     0.8117     65.5050
03200     0.6484      0.8038     0.8117     68.3036
03300     0.6280      0.8228     0.8131     66.3010
03400     0.7446      0.7722     0.8131     67.9735
03500     0.7169      0.7954     0.8151     69.3465
03600     0.7181      0.7848     0.8151     66.6528
03700     0.7082      0.7700     0.8161     67.3913
03800     0.6523      0.7954     0.8161     68.7109
03900     0.7310      0.7658     0.8161     67.7020
04000     0.6780      0.8059     0.8161     68.5232
04100     0.6149      0.8080     0.8161     66.2847
04200     0.6539      0.8017     0.8161     67.2508
04300     0.6813      0.8038     0.8161     65.2347
04400     0.6685      0.8101     0.8161     65.9869
04500     0.5887      0.8122     0.8161     65.5617
04600     0.6627      0.7911     0.8196     69.9969
04700     0.5730      0.8291     0.8208     69.4639
04800     0.6080      0.8312     0.8208     70.6691
04900     0.5687      0.8270     0.8208     69.7932
05000     0.6545      0.8101     0.8208     71.0405
05100     0.6813      0.7848     0.8301     67.8900
05200     0.6438      0.8101     0.8301     69.3988
05300     0.6445      0.8143     0.8301     71.7560
05400     0.6090      0.8376     0.8301     68.8330
05500     0.6525      0.8080     0.8301     68.1680
05600     0.5802      0.8228     0.8301     68.8604
05700     0.6629      0.8080     0.8301     68.7090
05800     0.7013      0.7932     0.8301     69.8665
05900     0.6151      0.8059     0.8301     71.4751
06000     0.6438      0.8101     0.8301     67.9039
06100     0.6476      0.7975     0.8301     70.2655
06200     0.6768      0.7911     0.8301     69.4776
06300     0.5563      0.8270     0.8301     67.9370
06400     0.5980      0.8059     0.8301     68.5774
06500     0.6088      0.8207     0.8301     71.6303
06600     0.6467      0.8143     0.8301     69.2076
06700     0.6733      0.7975     0.8301     70.6623
06800     0.6243      0.8122     0.8301     68.4772
06900     0.6086      0.8038     0.8301     68.6261
07000     0.6558      0.8101     0.8301     68.3191
07100     0.6345      0.7975     0.8301     68.5082
07200     0.6458      0.7890     0.8301     67.8805
07300     0.7077      0.7911     0.8301     68.1530
07400     0.6272      0.8059     0.8301     67.4903
07500     0.5651      0.8376     0.8301     70.0289
07600     0.6609      0.8080     0.8301     68.3151
07700     0.5413      0.8502     0.8301     69.0629
07800     0.5687      0.8101     0.8301     68.5477
07900     0.6548      0.8017     0.8301     69.5558
08000     0.5879      0.8228     0.8301     69.8315
08100     0.7135      0.7827     0.8301     69.4812
08200     0.4915      0.8544     0.8301     67.7231
08300     0.5330      0.8228     0.8301     69.2878
08400     0.6302      0.8059     0.8301     68.7601
08500     0.6289      0.8143     0.8301     69.0208
08600     0.6283      0.8122     0.8301     69.2290
08700     0.5384      0.8481     0.8308     68.7557
08800     0.6491      0.8101     0.8308     69.2279
08900     0.6284      0.8080     0.8308     68.3747
09000     0.5852      0.8228     0.8308     69.0861
09100     0.6059      0.8165     0.8308     69.6046
09200     0.6392      0.8122     0.8308     69.6405
09300     0.5314      0.8481     0.8308     68.9832
09400     0.6249      0.8186     0.8308     69.4868
09500     0.6789      0.7806     0.8308     70.0916
09600     0.5165      0.8418     0.8308     69.1742
09700     0.5742      0.8291     0.8308     68.6535
09800     0.5831      0.8397     0.8308     68.0634
09900     0.5551      0.8291     0.8308     70.7005
10000     0.5422      0.8502     0.8308     70.4839
10100     0.6322      0.8165     0.8308     68.6042
10200     0.5548      0.8270     0.8308     69.9561
10300     0.5340      0.8376     0.8308     69.2865
10400     0.5486      0.8228     0.8308     69.3537
10500     0.5781      0.8228     0.8308     69.8940
10600     0.5392      0.8418     0.8308     67.9698
10700     0.5614      0.8312     0.8308     70.1648
10800     0.5626      0.8270     0.8308     70.9188
10900     0.5712      0.8249     0.8308     68.6044
11000     0.4652      0.8629     0.8308     68.4550
11100     0.5191      0.8397     0.8339     69.4545
11200     0.4844      0.8608     0.8339     68.8401
11300     0.5666      0.8376     0.8339     68.9643
11400     0.6105      0.8186     0.8339     68.6388
11500     0.5381      0.8376     0.8339     67.8707
11600     0.5863      0.8333     0.8339     68.9124
11700     0.5956      0.8101     0.8339     68.5163
11800     0.6074      0.8165     0.8339     67.8476
11900     0.5419      0.8418     0.8339     67.9419
12000     0.4642      0.8565     0.8339     68.6679
12100     0.5462      0.8565     0.8339     70.1469
12200     0.5001      0.8481     0.8339     70.3266
12300     0.5835      0.8270     0.8339     67.4049
12400     0.5741      0.8312     0.8339     68.7947
12500     0.5602      0.8291     0.8339     68.5018
12600     0.5903      0.8291     0.8339     70.4212
12700     0.6220      0.8249     0.8339     70.4367
12800     0.5020      0.8523     0.8339     70.4426
12900     0.6341      0.8143     0.8339     69.3856
13000     0.5253      0.8312     0.8339     68.9088
13100     0.5945      0.8080     0.8339     70.8029
13200     0.4959      0.8502     0.8339     67.8717
13300     0.5532      0.8418     0.8339     70.7735
13400     0.5549      0.8333     0.8358     68.1065
13500     0.5780      0.8207     0.8358     69.6890
13600     0.5315      0.8523     0.8358     69.0766
13700     0.6118      0.8312     0.8358     69.5072
13800     0.5464      0.8143     0.8358     69.8688
13900     0.5267      0.8481     0.8358     70.2976
14000     0.5116      0.8840     0.8358     69.4913
14100     0.5631      0.8397     0.8358     70.0590
14200     0.4959      0.8565     0.8358     67.8213
14300     0.5667      0.8312     0.8358     68.0948
14400     0.5727      0.8397     0.8358     68.3978
14500     0.5909      0.8291     0.8358     71.8528
14600     0.5416      0.8291     0.8358     70.1075
14700     0.6152      0.8059     0.8395     70.5600
14800     0.5957      0.8291     0.8395     70.0998
14900     0.5359      0.8481     0.8395     69.4008
15000     0.5415      0.8333     0.8395     69.0882
15100     0.5521      0.8270     0.8395     69.1956
15200     0.5819      0.8207     0.8395     70.5921
15300     0.5936      0.8186     0.8395     69.3087
15400     0.5357      0.8312     0.8395     69.9040
15500     0.6597      0.8017     0.8395     68.8341
15600     0.5323      0.8312     0.8395     69.4707
15700     0.5323      0.8333     0.8395     68.3047
15800     0.6626      0.8017     0.8395     72.8397
15900     0.4782      0.8629     0.8395     70.3531
16000     0.5591      0.8270     0.8395     70.2708
16100     0.6321      0.7975     0.8395     68.7025
16200     0.5862      0.8165     0.8395     68.9686
16300     0.5564      0.8291     0.8395     68.6960
16400     0.5359      0.8418     0.8395     68.1799
16500     0.4681      0.8671     0.8395     69.8560
16600     0.5304      0.8460     0.8395     69.2389
16700     0.5017      0.8544     0.8395     70.1794
16800     0.5094      0.8397     0.8395     69.4999
16900     0.5511      0.8333     0.8395     70.2792
17000     0.6207      0.8080     0.8395     70.6653
17100     0.5197      0.8586     0.8395     68.7555
17200     0.6350      0.8017     0.8395     68.6114
17300     0.6232      0.8143     0.8395     69.1852
17400     0.5817      0.8059     0.8395     69.6487
17500     0.6298      0.8122     0.8395     68.5256
17600     0.5849      0.8291     0.8395     68.0809
17700     0.5031      0.8502     0.8395     69.7248
17800     0.4999      0.8312     0.8395     67.6798
17900     0.5147      0.8481     0.8395     68.6708
18000     0.5081      0.8565     0.8395     70.4282
18100     0.5206      0.8586     0.8395     67.9803
18200     0.5567      0.8418     0.8395     69.5028
18300     0.5121      0.8481     0.8395     69.5350
18400     0.5904      0.8418     0.8395     68.7611
18500     0.4879      0.8376     0.8395     68.0130
18600     0.5831      0.8312     0.8395     68.5504
18700     0.5163      0.8439     0.8395     71.7448
18800     0.5491      0.8312     0.8395     69.8129
18900     0.6012      0.8143     0.8395     69.2236
19000     0.5098      0.8544     0.8395     68.3506
19100     0.4133      0.8924     0.8395     69.6647
19200     0.5075      0.8608     0.8395     70.0739
19300     0.5235      0.8544     0.8395     69.4673
19400     0.5341      0.8249     0.8395     69.1136
19500     0.4979      0.8544     0.8395     68.9615
19600     0.5406      0.8439     0.8395     70.2078
19700     0.5417      0.8460     0.8395     69.4105
19800     0.5743      0.8312     0.8395     72.4287
19900     0.6319      0.7996     0.8395     71.6126
20000     0.5815      0.8481     0.8395     70.4518
20100     0.5377      0.8565     0.8395     68.6435
20199     0.6007      0.8186     0.8395     69.6532
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     0.5068      0.8586     0.8344     9.8711
00100     0.3732      0.8797     0.8344     69.1977
00200     0.5324      0.8270     0.8360     70.2128
00300     0.4916      0.8608     0.8360     69.3807
00400     0.3962      0.8882     0.8360     71.3094
00500     0.4451      0.8544     0.8360     69.3147
00600     0.4581      0.8608     0.8360     70.2477
00700     0.4127      0.8861     0.8360     69.2332
00800     0.4020      0.8819     0.8360     70.6719
00900     0.4795      0.8608     0.8360     71.5242
01000     0.4367      0.8734     0.8360     70.4497
01100     0.4814      0.8397     0.8360     74.2196
01200     0.5217      0.8397     0.8360     71.8871
01300     0.4458      0.8819     0.8360     73.9290
01400     0.5339      0.8460     0.8368     70.5238
01500     0.4444      0.8734     0.8369     71.4301
01600     0.5001      0.8418     0.8369     70.1633
01700     0.4679      0.8713     0.8369     71.4523
01800     0.3871      0.8861     0.8369     70.6929
01900     0.3939      0.8861     0.8369     70.7364
02000     0.5038      0.8629     0.8369     69.5250
02100     0.4669      0.8608     0.8369     68.6076
02200     0.4319      0.8713     0.8369     72.4903
02300     0.4530      0.8819     0.8369     72.3360
02400     0.3919      0.8903     0.8369     69.7106
02500     0.4910      0.8502     0.8369     70.3474
02600     0.4887      0.8629     0.8409     70.2577
02700     0.4251      0.8903     0.8409     69.4268
02800     0.4392      0.8671     0.8409     69.2781
02900     0.4859      0.8481     0.8409     68.8661
03000     0.4766      0.8671     0.8409     70.2573
03100     0.4578      0.8502     0.8409     71.5140
03200     0.4563      0.8713     0.8409     71.6297
03300     0.3964      0.8840     0.8409     72.0596
03400     0.4452      0.8565     0.8409     72.3666
03500     0.4937      0.8418     0.8409     72.3133
03600     0.4527      0.8692     0.8409     71.8029
03700     0.5544      0.8376     0.8409     70.8952
03800     0.4380      0.8671     0.8409     71.6423
03900     0.3909      0.8861     0.8409     69.5344
04000     0.4814      0.8481     0.8409     69.6424
04100     0.4749      0.8629     0.8409     71.1358
04200     0.4805      0.8544     0.8437     70.5697
04300     0.4235      0.8819     0.8437     70.8113
04400     0.4006      0.8797     0.8437     69.7258
04500     0.4810      0.8586     0.8437     68.7546
04600     0.4635      0.8544     0.8437     69.0322
04700     0.4916      0.8629     0.8437     71.8477
04800     0.4280      0.8776     0.8437     72.2914
04900     0.4793      0.8586     0.8437     68.7503
05000     0.4641      0.8586     0.8437     69.7593
05100     0.3981      0.8924     0.8437     68.4805
05200     0.4965      0.8502     0.8437     68.2381
05300     0.4347      0.8692     0.8437     69.7633
05400     0.4808      0.8481     0.8437     68.8752
05500     0.4062      0.8776     0.8437     69.4268
05600     0.4490      0.8755     0.8437     69.1638
05700     0.4945      0.8586     0.8437     68.3747
05800     0.4177      0.8840     0.8437     67.4631
05900     0.4875      0.8608     0.8437     71.6934
06000     0.4608      0.8671     0.8437     69.6486
06100     0.5003      0.8397     0.8437     69.3187
06200     0.4360      0.8734     0.8437     68.1946
06300     0.4615      0.8734     0.8437     68.8326
06400     0.4261      0.8987     0.8437     70.3987
06500     0.4542      0.8734     0.8437     70.1722
06600     0.4282      0.8755     0.8437     69.5465
06700     0.4742      0.8692     0.8437     69.2119
06800     0.5077      0.8418     0.8437     71.5532
06900     0.3518      0.9008     0.8437     69.7913
07000     0.4735      0.8544     0.8437     69.5940
07100     0.4738      0.8460     0.8437     69.4379
07200     0.4366      0.8861     0.8437     69.7874
07300     0.5199      0.8397     0.8437     70.4192
07400     0.4894      0.8586     0.8437     69.3457
07500     0.3612      0.8966     0.8437     68.4945
07600     0.4912      0.8418     0.8437     68.3395
07700     0.4489      0.8671     0.8437     67.2858
07800     0.4270      0.8776     0.8437     69.2778
07900     0.5372      0.8228     0.8437     68.9835
08000     0.4548      0.8797     0.8437     69.0226
08100     0.4425      0.8797     0.8437     69.0520
08200     0.4336      0.8713     0.8437     68.5027
08300     0.3975      0.8945     0.8437     69.4495
08400     0.4063      0.8776     0.8437     68.5349
08500     0.4418      0.8565     0.8437     68.7452
08600     0.4881      0.8608     0.8437     68.7833
08700     0.4283      0.8797     0.8437     69.5677
08800     0.5009      0.8565     0.8437     69.4481
08900     0.4888      0.8713     0.8437     70.3096
09000     0.4751      0.8586     0.8437     68.4575
09100     0.4416      0.8861     0.8437     69.0702
09200     0.4308      0.8713     0.8437     68.8653
09300     0.4930      0.8565     0.8437     69.2941
09400     0.5119      0.8692     0.8437     70.9736
09500     0.4680      0.8544     0.8437     69.1799
09600     0.4421      0.8734     0.8437     71.5421
09700     0.5082      0.8481     0.8437     68.4092
09800     0.4284      0.8755     0.8437     69.3383
09900     0.4475      0.8629     0.8437     69.7709
Start testing:
Test Accuracy: 0.8317
