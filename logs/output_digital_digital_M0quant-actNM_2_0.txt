Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 279, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.0, finetuning_epochs=0, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.0005,0.0001,0.00002', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,10000', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
004da9de-4cf3-4424-9d2e-a6260c4c8fa8
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.5257      0.0970     0.0998     12.2607
00100     2.5256      0.1076     0.1003     69.6074
00200     2.5256      0.0717     0.1003     69.1646
00300     2.5256      0.1076     0.1003     71.9662
00400     2.5256      0.0886     0.1003     71.7339
00500     2.5256      0.0738     0.1003     68.6386
00600     2.5255      0.0928     0.1003     72.2409
00700     2.5255      0.1097     0.1003     71.1446
00800     2.5255      0.0886     0.1003     71.1414
00900     2.5255      0.1371     0.1003     71.1996
01000     2.5254      0.0992     0.1003     70.5969
01100     2.5254      0.1013     0.1003     73.3754
01200     2.5254      0.0928     0.1003     71.0742
01300     2.5254      0.1160     0.1003     71.1408
01400     2.5254      0.1203     0.1003     73.2273
01500     2.5253      0.0865     0.1008     70.5894
01600     2.5253      0.1118     0.1008     71.7079
01700     2.5253      0.0907     0.1008     69.7989
01800     2.5253      0.1055     0.1008     70.8738
01900     2.5252      0.1160     0.1008     71.4303
02000     2.5252      0.1055     0.1008     70.2816
02100     2.5252      0.1118     0.1008     70.9001
02200     2.5252      0.0949     0.1008     70.8706
02300     2.5252      0.0992     0.1008     71.1364
02400     2.5251      0.0949     0.1008     70.4753
02500     2.5251      0.1160     0.1008     69.2788
02600     2.5251      0.0865     0.1008     70.3087
02700     2.5251      0.1203     0.1008     71.4752
02800     2.5251      0.0970     0.1008     69.9458
02900     2.5250      0.0844     0.1008     73.3926
03000     2.5250      0.0865     0.1008     73.5652
03100     2.5250      0.1034     0.1008     71.4920
03200     2.5250      0.1097     0.1008     74.4004
03300     2.5249      0.0886     0.1008     70.3236
03400     2.5249      0.1118     0.1008     71.6582
03500     2.5249      0.1076     0.1008     71.1104
03600     2.5249      0.1139     0.1008     71.1599
03700     2.5249      0.0759     0.1008     72.3491
03800     2.5248      0.0823     0.1008     70.8217
03900     2.5248      0.0992     0.1008     71.8615
04000     2.5248      0.1013     0.1008     71.8568
04100     2.5248      0.0781     0.1008     69.9943
04200     2.5248      0.0992     0.1008     70.5403
04300     2.5247      0.1160     0.1008     70.4969
04400     2.5247      0.1097     0.1008     71.0277
04500     2.5247      0.0992     0.1008     70.8021
04600     2.5247      0.0886     0.1008     69.3831
04700     2.5247      0.1076     0.1008     69.2942
04800     2.5246      0.0844     0.1008     71.8989
04900     2.5246      0.1076     0.1008     71.5279
05000     2.5246      0.0865     0.1008     71.6326
05100     2.5246      0.0928     0.1008     69.9112
05200     2.5246      0.0865     0.1008     71.9273
05300     2.5245      0.0738     0.1008     72.2531
05400     2.5245      0.0865     0.1008     72.0550
05500     2.5245      0.0970     0.1008     72.0313
05600     2.5245      0.1266     0.1008     73.9849
05700     2.5245      0.1076     0.1008     70.2147
05800     2.5244      0.0928     0.1008     70.8195
05900     2.5244      0.0759     0.1008     71.8193
06000     2.5244      0.1013     0.1008     73.2642
06100     2.5244      0.1076     0.1008     72.7256
06200     2.5244      0.1203     0.1008     71.9475
06300     2.5243      0.1097     0.1008     74.0404
06400     2.5243      0.1055     0.1008     73.8888
06500     2.5243      0.0992     0.1008     73.4279
06600     2.5243      0.1139     0.1008     72.5871
06700     2.5243      0.0612     0.1008     71.8469
06800     2.5242      0.1034     0.1008     71.8391
06900     2.5242      0.0823     0.1008     72.1632
07000     2.5242      0.0970     0.1008     70.6377
07100     2.5242      0.1034     0.1008     70.3434
07200     2.5242      0.0823     0.1008     72.4709
07300     2.5242      0.0970     0.1008     71.2081
07400     2.5241      0.0886     0.1008     71.1379
07500     2.5241      0.0970     0.1008     71.1536
07600     2.5241      0.1139     0.1008     71.1091
07700     2.5241      0.1055     0.1008     71.0580
07800     2.5241      0.1055     0.1008     69.7494
07900     2.5240      0.0992     0.1008     71.0298
08000     2.5240      0.0928     0.1008     70.8981
08100     2.5240      0.1013     0.1008     72.1185
08200     2.5240      0.1266     0.1008     70.6551
08300     2.5240      0.0886     0.1008     71.0307
08400     2.5239      0.1013     0.1008     73.5883
08500     2.5239      0.0865     0.1008     71.5962
08600     2.5239      0.0992     0.1008     72.9593
08700     2.5239      0.0992     0.1008     73.7021
08800     2.5239      0.1245     0.1008     71.8700
08900     2.5239      0.0970     0.1008     70.0515
09000     2.5238      0.0886     0.1008     70.9176
09100     2.5238      0.0907     0.1008     70.3225
09200     2.5238      0.1371     0.1008     71.6494
09300     2.5238      0.0949     0.1008     70.9244
09400     2.5238      0.1076     0.1009     73.1346
09500     2.5237      0.0928     0.1009     71.3106
09600     2.5237      0.0781     0.1009     72.1721
09700     2.5237      0.1181     0.1009     72.7128
09800     2.5237      0.0992     0.1009     69.6543
09900     2.5237      0.1097     0.1009     70.0747
10000     2.5236      0.1076     0.1009     70.9516
10100     2.5236      0.1118     0.1009     70.8348
10200     2.5236      0.0907     0.1009     72.3721
10300     2.5236      0.1266     0.1009     70.1564
10400     2.5236      0.0928     0.1009     71.2122
10500     2.5236      0.0928     0.1009     70.1530
10600     2.5236      0.0865     0.1009     69.5228
10700     2.5236      0.0907     0.1009     70.8673
10800     2.5236      0.0992     0.1009     71.2590
10900     2.5236      0.1224     0.1009     70.9904
11000     2.5236      0.0886     0.1009     72.6533
11100     2.5236      0.1160     0.1009     70.4651
11200     2.5236      0.0802     0.1009     75.1118
11300     2.5236      0.0717     0.1009     71.8001
11400     2.5236      0.0992     0.1009     72.4604
11500     2.5236      0.0717     0.1009     70.2093
11600     2.5236      0.0949     0.1009     69.8748
11700     2.5236      0.0886     0.1009     69.5106
11800     2.5236      0.0865     0.1009     70.6634
11900     2.5236      0.0907     0.1009     70.9084
12000     2.5236      0.0928     0.1009     72.5370
12100     2.5236      0.0949     0.1009     70.1178
12200     2.5236      0.1118     0.1009     71.5671
12300     2.5236      0.1013     0.1009     72.1101
12400     2.5236      0.1160     0.1009     70.8379
12500     2.5236      0.0907     0.1009     69.1553
12600     2.5236      0.0781     0.1009     72.1733
12700     2.5235      0.1076     0.1009     70.2603
12800     2.5235      0.1055     0.1009     70.0542
12900     2.5235      0.0970     0.1009     69.5716
13000     2.5235      0.1203     0.1009     71.3096
13100     2.5235      0.0970     0.1009     72.6097
13200     2.5235      0.0823     0.1009     72.2091
13300     2.5235      0.0886     0.1009     70.8181
13400     2.5235      0.1139     0.1009     72.6413
13500     2.5235      0.0907     0.1009     72.7165
13600     2.5235      0.0949     0.1009     70.2021
13700     2.5235      0.0992     0.1009     70.3692
13800     2.5235      0.1034     0.1009     70.0558
13900     2.5235      0.0865     0.1009     71.9156
14000     2.5235      0.0886     0.1009     70.6320
14100     2.5235      0.1076     0.1009     72.6545
14200     2.5235      0.1055     0.1009     71.0912
14300     2.5235      0.1055     0.1009     70.2370
14400     2.5235      0.0738     0.1009     69.4239
14500     2.5235      0.0844     0.1009     69.5249
14600     2.5235      0.0675     0.1009     70.5837
14700     2.5235      0.1224     0.1009     71.6981
14800     2.5235      0.0823     0.1009     70.1752
14900     2.5235      0.1139     0.1009     70.5066
15000     2.5235      0.1266     0.1009     70.4033
15100     2.5235      0.0992     0.1009     70.9251
15200     2.5235      0.1160     0.1009     71.0482
15300     2.5235      0.1076     0.1009     69.6550
15400     2.5235      0.0928     0.1009     69.5793
15500     2.5234      0.0781     0.1009     71.1070
15600     2.5234      0.0928     0.1009     70.8924
15700     2.5234      0.1034     0.1009     71.7504
15800     2.5234      0.0970     0.1009     71.2114
15900     2.5234      0.1034     0.1009     72.4265
16000     2.5234      0.0949     0.1009     71.6399
16100     2.5234      0.0865     0.1009     70.7122
16200     2.5234      0.0970     0.1009     70.8425
16300     2.5234      0.1076     0.1009     70.9524
16400     2.5234      0.0886     0.1009     70.6487
16500     2.5234      0.0992     0.1009     70.0042
16600     2.5234      0.1160     0.1009     70.9443
16700     2.5234      0.0949     0.1009     70.4332
16800     2.5234      0.1266     0.1009     69.5741
16900     2.5234      0.0907     0.1009     72.3103
17000     2.5234      0.0844     0.1009     71.7380
17100     2.5234      0.1055     0.1009     70.3097
17200     2.5234      0.0970     0.1009     70.3077
17300     2.5234      0.0886     0.1009     72.4796
17400     2.5234      0.0992     0.1009     73.1661
17500     2.5234      0.1055     0.1009     73.1278
17600     2.5234      0.1013     0.1009     72.5009
17700     2.5234      0.1224     0.1009     70.2942
17800     2.5234      0.1034     0.1009     70.3891
17900     2.5234      0.0928     0.1009     70.4776
18000     2.5234      0.1034     0.1009     70.6289
18100     2.5234      0.1034     0.1011     70.6808
18200     2.5234      0.1118     0.1011     71.9980
18300     2.5233      0.0886     0.1011     72.5507
18400     2.5233      0.1013     0.1011     70.5444
18500     2.5233      0.1118     0.1011     70.0670
18600     2.5233      0.0865     0.1011     73.1205
18700     2.5233      0.1181     0.1011     70.8696
18800     2.5233      0.1076     0.1011     73.1036
18900     2.5233      0.0781     0.1011     70.9787
19000     2.5233      0.0886     0.1011     70.6033
19100     2.5233      0.0717     0.1011     69.5345
19200     2.5233      0.0949     0.1011     70.6492
19300     2.5233      0.1308     0.1011     71.7211
19400     2.5233      0.0907     0.1011     72.8627
19500     2.5233      0.0928     0.1011     71.2100
19600     2.5233      0.1266     0.1011     70.1594
19700     2.5233      0.1203     0.1011     70.4981
19800     2.5233      0.0928     0.1011     71.6241
19900     2.5233      0.1097     0.1011     71.6991
20000     2.5233      0.0865     0.1011     72.1666
20100     2.5233      0.0992     0.1011     73.5615
20200     2.5233      0.1160     0.1011     72.1017
20300     2.5233      0.0844     0.1011     73.0852
20400     2.5233      0.1034     0.1011     73.2852
20500     2.5233      0.1013     0.1011     72.7299
20600     2.5233      0.0992     0.1011     72.1269
20700     2.5233      0.0949     0.1011     70.2787
20800     2.5233      0.1076     0.1011     74.6753
20900     2.5233      0.1224     0.1011     72.4598
21000     2.5233      0.1160     0.1011     70.5309
21100     2.5233      0.0970     0.1011     72.0441
21200     2.5233      0.0992     0.1011     71.1276
21300     2.5233      0.0781     0.1011     72.1524
21400     2.5233      0.1203     0.1011     72.0413
21500     2.5233      0.0823     0.1011     70.9701
21600     2.5233      0.1034     0.1011     71.2169
21700     2.5233      0.0865     0.1011     72.7327
21800     2.5233      0.0823     0.1011     71.2260
21900     2.5233      0.1076     0.1011     71.9887
22000     2.5233      0.0865     0.1011     73.7002
22100     2.5233      0.0949     0.1011     71.9623
22200     2.5233      0.0907     0.1011     71.4811
22300     2.5233      0.0949     0.1011     72.3087
22400     2.5233      0.0823     0.1011     72.6659
22500     2.5233      0.1034     0.1011     73.1904
22600     2.5233      0.1034     0.1011     73.0694
22700     2.5233      0.0844     0.1011     71.6629
22800     2.5233      0.0928     0.1011     70.2190
22900     2.5233      0.1224     0.1011     72.2441
23000     2.5233      0.0886     0.1011     72.4774
23100     2.5233      0.1055     0.1011     72.4722
23200     2.5233      0.1013     0.1011     71.7811
23300     2.5233      0.0738     0.1011     73.0874
23400     2.5233      0.0992     0.1011     70.5762
23500     2.5233      0.0823     0.1011     73.1454
23600     2.5233      0.1013     0.1011     72.0879
23700     2.5233      0.0949     0.1011     71.4912
23800     2.5233      0.1350     0.1011     71.8414
23900     2.5233      0.1350     0.1011     70.8167
24000     2.5233      0.0844     0.1011     72.7423
24100     2.5233      0.1013     0.1011     71.5639
24200     2.5233      0.0907     0.1011     71.3490
24300     2.5233      0.0949     0.1011     71.4940
24400     2.5233      0.0992     0.1011     71.2155
24500     2.5232      0.0738     0.1011     71.5807
24600     2.5232      0.1034     0.1011     72.6951
24700     2.5232      0.1034     0.1011     72.4016
24800     2.5232      0.1203     0.1011     72.0028
24900     2.5232      0.1055     0.1011     71.0666
25000     2.5232      0.0949     0.1011     71.4862
25100     2.5232      0.1308     0.1011     70.6787
25200     2.5232      0.1139     0.1011     71.0375
25300     2.5232      0.0949     0.1011     72.1395
25400     2.5232      0.1013     0.1011     73.3711
25500     2.5232      0.0675     0.1011     72.9139
25600     2.5232      0.1076     0.1011     72.5051
25700     2.5232      0.1224     0.1011     71.4017
25800     2.5232      0.0970     0.1011     71.7600
25900     2.5232      0.0949     0.1011     70.7037
26000     2.5232      0.0992     0.1011     71.7322
26100     2.5232      0.1097     0.1011     75.1650
26200     2.5232      0.1076     0.1011     71.2349
26300     2.5232      0.0823     0.1011     73.1131
26400     2.5232      0.1160     0.1011     74.3033
26500     2.5232      0.1245     0.1011     74.0177
26600     2.5232      0.1118     0.1011     71.2647
26700     2.5232      0.0738     0.1011     73.0170
26800     2.5232      0.0781     0.1011     72.0520
26900     2.5232      0.0907     0.1011     71.6925
27000     2.5232      0.0907     0.1011     71.5196
27100     2.5232      0.1034     0.1011     71.5913
27200     2.5232      0.1013     0.1011     71.9202
27300     2.5232      0.1034     0.1011     72.3020
27400     2.5232      0.1034     0.1011     73.2282
27500     2.5232      0.0781     0.1011     72.4699
27600     2.5232      0.0759     0.1011     72.3141
27700     2.5232      0.0886     0.1011     70.5947
27800     2.5232      0.0823     0.1011     71.9706
27900     2.5232      0.0781     0.1011     73.6159
28000     2.5232      0.0928     0.1011     73.0401
28100     2.5232      0.0802     0.1011     73.2251
28200     2.5232      0.0992     0.1011     75.3425
28300     2.5232      0.0907     0.1011     72.1859
28400     2.5232      0.0781     0.1011     71.3038
28500     2.5232      0.1266     0.1011     71.1267
28600     2.5232      0.0928     0.1011     69.8595
28700     2.5232      0.1076     0.1011     71.9820
28800     2.5232      0.0949     0.1011     75.8954
28900     2.5232      0.0949     0.1011     75.1345
29000     2.5232      0.0865     0.1011     73.5038
29100     2.5232      0.0928     0.1011     71.8582
29200     2.5232      0.1076     0.1011     67.4019
29300     2.5232      0.0907     0.1011     69.2274
29400     2.5232      0.0992     0.1011     71.2083
29500     2.5232      0.0949     0.1011     72.7368
29600     2.5232      0.0886     0.1012     73.8038
29700     2.5232      0.1034     0.1012     71.7020
29800     2.5232      0.0970     0.1012     70.0559
29900     2.5232      0.0696     0.1012     73.2979
29999     2.5232      0.1076     0.1012     71.0825
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Start testing:
Test Accuracy: 0.1002
