Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=11, quant_actNM=11, quant_inp=11, quant_w=11, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
30645ce9-4b67-493c-bd11-b5c56c5c3fdd
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 373, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, 1.)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 82, in forward
    if len(x_range) > 1:
TypeError: object of type 'float' has no len()
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=11, quant_actNM=11, quant_inp=11, quant_w=11, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
7066b5e8-3374-44a5-bd68-4f2fac3ceb13
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 373, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, torch.tensor([1.]).device(input.device))
TypeError: 'torch.device' object is not callable
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=11, quant_actNM=11, quant_inp=11, quant_w=11, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
b67445a1-b8ff-402e-8a2c-2e201276a7a4
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     3.5754      0.0633     0.0679     10.4829
00100     2.0838      0.2911     0.3392     74.1822
00200     1.5754      0.4852     0.5030     74.1372
00300     1.3017      0.5907     0.5968     74.8560
00400     1.1669      0.6181     0.6553     74.0149
00500     1.1280      0.6561     0.6849     73.9781
00600     0.9277      0.7089     0.7087     74.8998
00700     0.9753      0.6899     0.7288     74.3269
00800     0.8200      0.7384     0.7296     74.7638
00900     0.8555      0.7426     0.7455     74.7863
01000     0.8077      0.7595     0.7630     74.9213
01100     0.8368      0.7321     0.7630     75.2691
01200     0.8105      0.7321     0.7802     74.7903
01300     0.7697      0.7532     0.7802     74.0486
01400     0.7733      0.7637     0.7815     74.6289
01500     0.8186      0.7489     0.7842     74.4963
01600     0.7648      0.7468     0.7891     74.2110
01700     0.6920      0.7954     0.7925     73.9196
01800     0.6555      0.8080     0.7925     74.6335
01900     0.6819      0.7806     0.8018     75.2369
02000     0.6993      0.7468     0.8018     74.2676
02100     0.6103      0.7890     0.8113     74.0444
02200     0.7648      0.7679     0.8113     74.9553
02300     0.6250      0.7975     0.8113     75.4132
02400     0.6408      0.7806     0.8113     74.8157
02500     0.6916      0.7827     0.8113     74.6090
02600     0.6327      0.8143     0.8113     74.2264
02700     0.7331      0.7785     0.8113     75.3808
02800     0.6621      0.8080     0.8115     74.0231
02900     0.6337      0.8038     0.8128     74.4440
03000     0.6711      0.7827     0.8128     74.6400
03100     0.6795      0.7848     0.8128     74.3875
03200     0.6181      0.7975     0.8139     75.5891
03300     0.7057      0.7743     0.8160     74.9887
03400     0.5597      0.8186     0.8160     76.9576
03500     0.6831      0.7911     0.8160     74.6423
03600     0.5531      0.8165     0.8227     74.3872
03700     0.6292      0.7869     0.8227     75.0199
03800     0.5643      0.8291     0.8227     74.5545
03900     0.5637      0.8165     0.8227     73.8289
04000     0.6170      0.8143     0.8227     74.4793
04100     0.6281      0.7996     0.8227     74.4312
04200     0.6411      0.7975     0.8227     74.4312
04300     0.5329      0.8354     0.8227     74.9363
04400     0.6111      0.8038     0.8227     73.7296
04500     0.6102      0.7954     0.8227     74.2184
04600     0.5016      0.8291     0.8237     74.7219
04700     0.5369      0.8207     0.8237     73.8299
04800     0.5266      0.8333     0.8237     73.9745
04900     0.5961      0.8165     0.8237     73.8816
05000     0.6648      0.7911     0.8237     74.2486
05100     0.5545      0.8122     0.8237     75.0654
05200     0.6046      0.8101     0.8237     73.6219
05300     0.5353      0.8186     0.8237     75.3893
05400     0.5568      0.8165     0.8237     75.8261
05500     0.7111      0.7532     0.8237     74.5623
05600     0.6225      0.8143     0.8245     75.6662
05700     0.5641      0.8291     0.8245     74.1699
05800     0.5695      0.8122     0.8275     74.6433
05900     0.6078      0.8165     0.8275     74.9726
06000     0.5612      0.8143     0.8285     74.8803
06100     0.5568      0.8270     0.8293     74.9932
06200     0.5748      0.8228     0.8293     75.0346
06300     0.5793      0.8059     0.8293     74.5786
06400     0.5914      0.7975     0.8330     74.7473
06500     0.5516      0.8312     0.8330     75.2266
06600     0.5225      0.8376     0.8330     75.1411
06700     0.5418      0.8186     0.8382     74.6112
06800     0.5925      0.8291     0.8382     74.4009
06900     0.4867      0.8439     0.8382     74.5094
07000     0.5580      0.8228     0.8382     74.5101
07100     0.5881      0.8017     0.8382     74.9732
07200     0.5621      0.8165     0.8385     75.1880
07300     0.6003      0.8207     0.8385     74.2454
07400     0.4809      0.8502     0.8385     74.4707
07500     0.5156      0.8207     0.8385     74.7690
07600     0.5863      0.8186     0.8385     74.3521
07700     0.5322      0.8249     0.8385     74.9540
07800     0.5305      0.8228     0.8385     74.6829
07900     0.4914      0.8354     0.8385     74.6903
08000     0.5430      0.8249     0.8385     75.3745
08100     0.4486      0.8692     0.8385     74.8555
08200     0.5298      0.8207     0.8385     74.5818
08300     0.6004      0.8101     0.8385     74.9864
08400     0.5651      0.8143     0.8398     74.5342
08500     0.5243      0.8397     0.8414     74.5289
08600     0.5789      0.8038     0.8414     75.3104
08700     0.5206      0.8186     0.8414     74.8787
08800     0.5640      0.8249     0.8414     74.6957
08900     0.4188      0.8692     0.8455     74.2936
09000     0.4913      0.8397     0.8455     74.1944
09100     0.5645      0.8312     0.8455     75.7493
09200     0.5394      0.8186     0.8455     74.2455
09300     0.5486      0.8143     0.8455     75.2124
09400     0.6541      0.7848     0.8455     75.6232
09500     0.4949      0.8397     0.8455     74.9180
09600     0.5278      0.8397     0.8455     76.1575
09700     0.5184      0.8270     0.8455     75.0231
09800     0.6000      0.7975     0.8455     74.8701
09900     0.5226      0.8523     0.8455     75.1763
10000     0.5272      0.8333     0.8463     74.5811
10100     0.5229      0.8418     0.8463     74.6081
10200     0.4636      0.8418     0.8468     74.9710
10300     0.4973      0.8418     0.8468     74.4410
10400     0.4473      0.8692     0.8468     74.9348
10500     0.4832      0.8481     0.8468     74.7730
10600     0.4156      0.8797     0.8501     74.3772
10700     0.4895      0.8397     0.8501     75.4898
10800     0.5498      0.8101     0.8501     75.4614
10900     0.5392      0.8481     0.8501     74.5066
11000     0.5026      0.8207     0.8501     75.0202
11100     0.4972      0.8544     0.8501     74.7466
11200     0.4616      0.8586     0.8501     74.8952
11300     0.4659      0.8650     0.8501     74.5975
11400     0.4172      0.8650     0.8501     75.0418
11500     0.4047      0.8650     0.8501     74.9729
11600     0.5001      0.8228     0.8501     74.7280
11700     0.4792      0.8418     0.8501     74.7884
11800     0.5286      0.8418     0.8501     75.4689
11900     0.5325      0.8312     0.8501     75.0749
12000     0.4779      0.8544     0.8501     75.8425
12100     0.4809      0.8586     0.8501     75.2187
12200     0.4891      0.8481     0.8501     74.9273
12300     0.4170      0.8692     0.8501     74.7051
12400     0.4973      0.8249     0.8501     75.0610
12500     0.4409      0.8671     0.8501     74.1759
12600     0.4340      0.8692     0.8501     75.0806
12700     0.4308      0.8608     0.8501     75.1855
12800     0.4919      0.8650     0.8501     74.6678
12900     0.4909      0.8418     0.8501     74.9709
13000     0.4671      0.8397     0.8501     75.3529
13100     0.5086      0.8397     0.8501     75.8967
13200     0.4680      0.8565     0.8501     74.4754
13300     0.4556      0.8481     0.8501     74.3800
13400     0.4767      0.8354     0.8501     75.5572
13500     0.5755      0.8080     0.8501     74.8363
13600     0.5256      0.8565     0.8501     74.5651
13700     0.5148      0.8312     0.8501     75.3086
13800     0.5046      0.8397     0.8501     74.9922
13900     0.4286      0.8692     0.8501     74.5591
14000     0.4511      0.8523     0.8501     74.5320
14100     0.5284      0.8460     0.8508     75.4210
14200     0.5642      0.8186     0.8508     75.6736
14300     0.4335      0.8544     0.8508     74.8081
14400     0.4690      0.8608     0.8508     75.4530
14500     0.4407      0.8755     0.8508     75.1796
14600     0.4500      0.8523     0.8508     73.4638
14700     0.4737      0.8481     0.8508     75.0631
14800     0.4336      0.8755     0.8508     74.4035
14900     0.4788      0.8460     0.8508     73.9662
15000     0.5034      0.8354     0.8508     73.8144
15100     0.4954      0.8481     0.8508     74.2338
15200     0.5818      0.8207     0.8508     74.6150
15300     0.4797      0.8333     0.8508     74.6949
15400     0.4233      0.8650     0.8508     74.3137
15500     0.4252      0.8755     0.8508     74.5850
15600     0.4708      0.8481     0.8508     74.1255
15700     0.4695      0.8460     0.8508     73.6096
15800     0.4063      0.8755     0.8508     73.5829
15900     0.4051      0.8776     0.8508     74.2458
16000     0.4690      0.8734     0.8508     74.7435
16100     0.4666      0.8354     0.8508     74.1170
16200     0.4468      0.8608     0.8508     74.1007
16300     0.4410      0.8692     0.8508     74.8088
16400     0.5542      0.8354     0.8518     74.8162
16500     0.3599      0.8924     0.8518     74.6994
16600     0.4430      0.8671     0.8518     74.9878
16700     0.4287      0.8650     0.8518     74.8758
16800     0.5054      0.8418     0.8518     74.6699
16900     0.5247      0.8333     0.8518     74.2696
17000     0.4891      0.8565     0.8518     74.8750
17100     0.4491      0.8608     0.8518     74.3711
17200     0.4739      0.8523     0.8518     74.1584
17300     0.4587      0.8565     0.8518     74.6038
17400     0.5133      0.8291     0.8518     74.5653
17500     0.5230      0.8460     0.8518     74.4068
17600     0.4894      0.8481     0.8518     74.2935
17700     0.4613      0.8629     0.8518     75.2216
17800     0.5302      0.8249     0.8518     74.6928
17900     0.4582      0.8523     0.8518     74.3329
18000     0.4507      0.8586     0.8518     74.7062
18100     0.4467      0.8671     0.8518     74.4898
18200     0.3938      0.8776     0.8518     74.6354
18300     0.5289      0.8228     0.8518     75.1429
18400     0.4546      0.8565     0.8518     75.4427
18500     0.4553      0.8565     0.8518     74.9608
18600     0.4723      0.8502     0.8518     74.9800
18700     0.4826      0.8354     0.8541     75.1844
18800     0.4818      0.8439     0.8541     74.0705
18900     0.4767      0.8650     0.8541     74.5529
19000     0.4552      0.8460     0.8541     74.5509
19100     0.4858      0.8671     0.8541     74.1166
19200     0.5057      0.8333     0.8550     75.6084
19300     0.5153      0.8249     0.8550     75.4585
19400     0.4341      0.8565     0.8550     74.9583
19500     0.5072      0.8418     0.8550     74.5480
19600     0.4951      0.8502     0.8550     74.1109
19700     0.5694      0.8122     0.8550     74.4398
19800     0.5878      0.8122     0.8550     74.7248
19900     0.4529      0.8502     0.8550     74.3656
20000     0.5444      0.8249     0.8550     75.2359
20100     0.5045      0.8312     0.8550     75.1216
20199     0.5383      0.8312     0.8550     73.3761
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     0.3822      0.8903     0.8519     10.2425
00100     0.3909      0.8840     0.8519     74.6014
00200     0.3851      0.8903     0.8519     74.2669
00300     0.4216      0.8629     0.8519     74.7756
00400     0.4245      0.8565     0.8519     75.1969
00500     0.3835      0.8819     0.8519     74.5942
00600     0.3830      0.8629     0.8519     74.5522
00700     0.3443      0.8924     0.8519     74.4877
00800     0.4614      0.8376     0.8527     74.4903
00900     0.4079      0.8840     0.8527     74.0259
01000     0.4076      0.8713     0.8527     74.6693
01100     0.3846      0.8671     0.8527     74.8481
01200     0.4032      0.8713     0.8527     74.1346
01300     0.4168      0.8671     0.8527     74.1118
01400     0.4209      0.8692     0.8527     73.9023
01500     0.4195      0.8882     0.8527     73.3176
01600     0.4027      0.8608     0.8527     74.2382
01700     0.3963      0.8819     0.8527     74.5426
01800     0.4448      0.8608     0.8527     74.4536
01900     0.2819      0.9030     0.8527     74.5594
02000     0.4885      0.8397     0.8527     74.4802
02100     0.3863      0.8861     0.8527     74.2756
02200     0.3428      0.8987     0.8527     74.2994
02300     0.4399      0.8629     0.8527     74.6015
02400     0.3755      0.8819     0.8527     74.2647
02500     0.3677      0.8945     0.8529     74.5626
02600     0.4156      0.8755     0.8529     74.6012
02700     0.4173      0.8776     0.8529     75.3435
02800     0.4382      0.8544     0.8529     74.6092
02900     0.4229      0.8565     0.8529     74.5478
03000     0.3972      0.8713     0.8529     75.1246
03100     0.4066      0.8713     0.8529     74.9852
03200     0.3866      0.8861     0.8529     74.5654
03300     0.3726      0.8840     0.8529     74.3236
03400     0.3820      0.8819     0.8529     74.4246
03500     0.4429      0.8586     0.8529     74.7336
03600     0.3860      0.8819     0.8529     73.7237
03700     0.4195      0.8608     0.8529     73.9981
03800     0.3994      0.8755     0.8529     75.0904
03900     0.3840      0.8734     0.8529     74.4143
04000     0.3752      0.8797     0.8529     74.3953
04100     0.3775      0.8776     0.8529     74.7019
04200     0.3886      0.8882     0.8529     74.6025
04300     0.3854      0.8671     0.8529     73.3367
04400     0.3830      0.8797     0.8529     74.0312
04500     0.3715      0.8945     0.8529     73.8858
04600     0.4350      0.8608     0.8529     75.0708
04700     0.3346      0.9030     0.8529     75.0141
04800     0.4470      0.8523     0.8529     74.3815
04900     0.3940      0.8671     0.8529     74.5327
05000     0.3981      0.8650     0.8529     74.1819
05100     0.4051      0.8840     0.8529     75.1700
05200     0.3676      0.8966     0.8529     74.7728
05300     0.3506      0.8819     0.8529     74.8539
05400     0.4116      0.8692     0.8529     75.2823
05500     0.4199      0.8776     0.8529     74.7814
05600     0.4332      0.8734     0.8529     74.9844
05700     0.4040      0.8840     0.8529     74.6772
05800     0.4337      0.8650     0.8529     74.9872
05900     0.3577      0.8861     0.8529     75.2623
06000     0.3065      0.8945     0.8529     74.8239
06100     0.3616      0.9008     0.8529     74.6912
06200     0.3772      0.8734     0.8529     75.6671
06300     0.4260      0.8692     0.8529     74.7066
06400     0.3738      0.8882     0.8529     74.8543
06500     0.4355      0.8797     0.8529     74.5737
06600     0.4624      0.8418     0.8529     75.0131
06700     0.4976      0.8354     0.8547     74.6249
06800     0.3872      0.8713     0.8547     75.6653
06900     0.3630      0.8987     0.8547     75.0207
07000     0.3867      0.8819     0.8547     75.2096
07100     0.3647      0.9008     0.8547     76.1023
07200     0.4239      0.8586     0.8547     75.2823
07300     0.3910      0.8692     0.8547     75.2001
07400     0.3917      0.8692     0.8547     74.6428
07500     0.3996      0.8776     0.8547     75.2456
07600     0.4319      0.8544     0.8547     75.4331
07700     0.3863      0.8797     0.8547     74.8569
07800     0.4451      0.8650     0.8547     74.7600
07900     0.3904      0.8819     0.8547     74.7317
08000     0.3301      0.8987     0.8549     75.0578
08100     0.3163      0.9008     0.8549     75.4052
08200     0.4198      0.8734     0.8549     74.9043
08300     0.3838      0.8734     0.8549     75.4007
08400     0.4663      0.8523     0.8549     74.5212
08500     0.4116      0.8734     0.8549     75.8271
08600     0.3340      0.8903     0.8549     75.5807
08700     0.3610      0.8840     0.8549     75.1666
08800     0.3315      0.8987     0.8549     74.8576
08900     0.3762      0.8713     0.8549     81.4909
09000     0.4302      0.8650     0.8549     79.3410
09100     0.3725      0.8945     0.8549     78.0225
09200     0.3476      0.8924     0.8549     75.8778
09300     0.3893      0.8776     0.8549     76.2900
09400     0.3378      0.8903     0.8549     75.7515
09500     0.4371      0.8755     0.8549     75.3488
09600     0.3801      0.8840     0.8549     75.1208
09700     0.4198      0.8608     0.8549     75.0761
09800     0.4367      0.8544     0.8549     75.3153
09900     0.3605      0.8903     0.8549     75.4516
Start testing:
Test Accuracy: 0.8527
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
