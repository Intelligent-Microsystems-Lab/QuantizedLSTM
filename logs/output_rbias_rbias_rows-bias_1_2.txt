Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=512, canonical_testing=False, cs=0.5, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, hidden=113, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=0.1, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=6, quant_actNM=8, quant_inp=4, quant_w=None, random_seed=8627169, rows_bias=1, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=640, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
ffb5f16b-99d1-4b8e-afd3-e2a22150df08
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 161, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 228, in forward
    activated_input = quant_pass(pact_a_bmm(input_gate_out * activation_out, self.a8), self.abNM, self.a8)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 124, in pact_a_bmm
    return torch.sign(x) * .5 * (torch.abs(x) - torch.abs(torch.abs(x) - a) + a)
RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 10.76 GiB total capacity; 8.69 GiB already allocated; 3.12 MiB free; 9.80 GiB reserved in total by PyTorch)
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=512, canonical_testing=False, cs=0.5, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, hidden=113, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=0.1, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=6, quant_actNM=8, quant_inp=4, quant_w=None, random_seed=8627169, rows_bias=1, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=640, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
9c844d8f-93e3-4960-b230-1206c57895e2
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 161, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 228, in forward
    activated_input = quant_pass(pact_a_bmm(input_gate_out * activation_out, self.a8), self.abNM, self.a8)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 124, in pact_a_bmm
    return torch.sign(x) * .5 * (torch.abs(x) - torch.abs(torch.abs(x) - a) + a)
RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 10.76 GiB total capacity; 8.69 GiB already allocated; 3.12 MiB free; 9.80 GiB reserved in total by PyTorch)
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=512, canonical_testing=False, cs=0.5, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, hidden=113, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=0.1, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=6, quant_actNM=8, quant_inp=4, quant_w=None, random_seed=8627169, rows_bias=1, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=640, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
95722181-bc35-45b7-9398-f1c043052d17
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.7573      0.1230     0.0882     11.7676
00100     1.5617      0.4863     0.5087     73.9980
00200     1.1768      0.6582     0.6710     72.8711
00300     1.0183      0.6836     0.7412     72.2783
00400     0.8843      0.7344     0.7412     72.3655
00500     0.9181      0.7305     0.7591     72.1861
00600     0.6959      0.7695     0.7889     70.8750
00700     0.7971      0.7461     0.7889     73.1167
00800     0.7932      0.7559     0.8007     72.9251
00900     0.8364      0.7520     0.8007     72.5166
01000     0.7016      0.7871     0.8045     72.0582
01100     0.7129      0.7930     0.8106     72.1818
01200     0.6943      0.8008     0.8117     74.3166
01300     0.6994      0.7871     0.8117     73.5036
01400     0.6614      0.7988     0.8126     72.1897
01500     0.7343      0.7852     0.8126     71.7559
01600     0.7146      0.7715     0.8126     72.4438
01700     0.6779      0.7852     0.8126     73.3020
01800     0.7216      0.7793     0.8133     71.4868
01900     0.6250      0.8262     0.8133     71.5234
02000     0.7303      0.7852     0.8289     73.5653
02100     0.7621      0.7832     0.8289     73.5188
02200     0.6455      0.8105     0.8289     71.8809
02300     0.6625      0.7832     0.8289     72.0199
02400     0.6133      0.8008     0.8289     71.0798
02500     0.6165      0.8145     0.8289     72.8660
02600     0.6509      0.7969     0.8289     72.9919
02700     0.6831      0.7910     0.8289     72.5218
02800     0.6732      0.7754     0.8289     72.5498
02900     0.5950      0.8262     0.8289     72.9720
03000     0.6214      0.8086     0.8289     72.6390
03100     0.6758      0.7812     0.8334     72.6301
03200     0.6028      0.8047     0.8334     70.8841
03300     0.5773      0.8262     0.8334     72.4636
03400     0.6275      0.8145     0.8334     72.7886
03500     0.6801      0.7754     0.8334     71.3471
03600     0.6978      0.7852     0.8334     72.0465
03700     0.5952      0.8164     0.8334     72.3958
03800     0.5954      0.8105     0.8334     71.9298
03900     0.6266      0.8086     0.8334     73.0984
04000     0.5831      0.8281     0.8334     71.2401
04100     0.5746      0.8438     0.8334     74.1566
04200     0.5919      0.8320     0.8334     71.9555
04300     0.6161      0.8203     0.8334     73.5598
04400     0.6626      0.7930     0.8334     73.0079
04500     0.6120      0.8281     0.8334     72.5230
04600     0.5446      0.8457     0.8334     72.1352
04700     0.6224      0.8125     0.8334     72.7461
04800     0.5207      0.8496     0.8334     71.8453
04900     0.5556      0.8398     0.8384     72.6527
05000     0.6292      0.8008     0.8384     73.7917
05100     0.6051      0.8027     0.8384     72.2904
05200     0.5076      0.8477     0.8406     72.8094
05300     0.5623      0.8379     0.8406     71.8824
05400     0.6257      0.8066     0.8406     72.5651
05500     0.5396      0.8457     0.8548     73.3971
05600     0.6071      0.8145     0.8548     73.1352
05700     0.6049      0.8184     0.8548     72.6273
05800     0.5523      0.8223     0.8548     71.8489
05900     0.5619      0.8164     0.8548     71.2450
06000     0.5690      0.8340     0.8548     73.2406
06100     0.5376      0.8418     0.8548     73.0860
06200     0.4954      0.8496     0.8548     74.3837
06300     0.5389      0.8379     0.8548     74.0328
06400     0.5750      0.8359     0.8548     71.5003
06500     0.5344      0.8496     0.8548     72.7024
06600     0.5125      0.8418     0.8548     71.5741
06700     0.5342      0.8320     0.8548     72.3928
06800     0.5689      0.8223     0.8548     72.0863
06900     0.5685      0.8340     0.8548     71.3806
07000     0.6296      0.8164     0.8548     72.9861
07100     0.6044      0.8301     0.8548     74.8269
07200     0.5677      0.8281     0.8548     74.2574
07300     0.5644      0.8398     0.8548     73.6301
07400     0.5862      0.8125     0.8548     71.3067
07500     0.4959      0.8691     0.8548     72.9631
07600     0.5786      0.8242     0.8548     72.5560
07700     0.4749      0.8633     0.8548     73.2133
07800     0.5036      0.8574     0.8548     73.5915
07900     0.5694      0.8281     0.8548     70.6985
08000     0.5193      0.8340     0.8548     71.8951
08100     0.4973      0.8457     0.8548     73.7515
08200     0.5195      0.8477     0.8548     71.9429
08300     0.5040      0.8477     0.8570     70.6636
08400     0.5430      0.8281     0.8570     74.1807
08500     0.5589      0.8262     0.8636     74.3067
08600     0.5342      0.8418     0.8636     74.1609
08700     0.4969      0.8516     0.8636     74.9637
08800     0.4867      0.8477     0.8636     73.3151
08900     0.5159      0.8477     0.8636     74.5331
09000     0.5607      0.8242     0.8636     71.6320
09100     0.5210      0.8262     0.8636     71.7656
09200     0.4646      0.8574     0.8636     72.9007
09300     0.4650      0.8633     0.8636     71.6959
09400     0.5220      0.8379     0.8636     75.0945
09500     0.4584      0.8770     0.8636     74.0393
09600     0.4942      0.8516     0.8636     72.6983
09700     0.4746      0.8555     0.8636     73.5279
09800     0.5271      0.8262     0.8636     73.0932
09900     0.4719      0.8613     0.8636     75.5168
10000     0.4980      0.8438     0.8636     71.8055
10100     0.4915      0.8613     0.8636     73.3760
10200     0.5049      0.8496     0.8636     73.4143
10300     0.4784      0.8691     0.8636     71.9660
10400     0.4658      0.8711     0.8636     72.4316
10500     0.5051      0.8477     0.8636     74.3987
10600     0.5974      0.8105     0.8636     73.8430
10700     0.5096      0.8457     0.8636     72.0600
10800     0.4931      0.8555     0.8636     72.8245
10900     0.4756      0.8594     0.8636     73.4616
11000     0.5005      0.8457     0.8636     74.0115
11100     0.4423      0.8594     0.8636     72.4456
11200     0.4623      0.8770     0.8636     72.9459
11300     0.5064      0.8398     0.8636     74.2492
11400     0.4819      0.8477     0.8636     72.6289
11500     0.4772      0.8652     0.8636     72.3904
11600     0.4829      0.8535     0.8636     74.1703
11700     0.4831      0.8711     0.8636     73.9724
11800     0.5063      0.8438     0.8636     74.9358
11900     0.4996      0.8398     0.8636     71.4231
12000     0.4911      0.8496     0.8636     71.6565
12100     0.4058      0.8789     0.8636     72.8899
12200     0.4367      0.8750     0.8636     73.5107
12300     0.4298      0.8809     0.8636     71.1122
12400     0.5112      0.8438     0.8636     74.2273
12500     0.4579      0.8652     0.8636     73.4818
12600     0.5078      0.8398     0.8636     74.1215
12700     0.5037      0.8418     0.8636     73.1158
12800     0.5215      0.8418     0.8636     72.6895
12900     0.4454      0.8672     0.8636     74.3023
13000     0.4673      0.8633     0.8636     75.4800
13100     0.4494      0.8672     0.8636     72.7694
13200     0.4214      0.8789     0.8636     71.6839
13300     0.3674      0.8965     0.8636     72.9768
13400     0.4864      0.8535     0.8636     73.9267
13500     0.4564      0.8594     0.8636     73.3357
13600     0.4489      0.8730     0.8636     72.8784
13700     0.5490      0.8320     0.8636     74.9029
13800     0.4732      0.8652     0.8636     74.5867
13900     0.4726      0.8711     0.8636     72.4319
14000     0.4300      0.8652     0.8636     71.5474
14100     0.4288      0.8672     0.8636     70.6840
14200     0.4868      0.8418     0.8636     72.3719
14300     0.3953      0.8828     0.8636     71.4318
14400     0.4744      0.8613     0.8636     70.8309
14500     0.4681      0.8633     0.8636     71.1735
14600     0.4174      0.8711     0.8636     72.1163
14700     0.4732      0.8711     0.8636     70.4260
14800     0.4243      0.8633     0.8636     71.5970
14900     0.4702      0.8652     0.8636     70.5319
15000     0.4999      0.8613     0.8636     72.8744
15100     0.4469      0.8848     0.8636     72.3103
15200     0.4406      0.8750     0.8636     72.4782
15300     0.4505      0.8691     0.8636     71.2262
15400     0.3824      0.8926     0.8636     72.3074
15500     0.4050      0.8828     0.8636     71.3351
15600     0.4762      0.8555     0.8636     72.1044
15700     0.4015      0.8848     0.8636     71.8763
15800     0.4472      0.8496     0.8636     72.1891
15900     0.4266      0.8926     0.8636     70.3073
16000     0.4459      0.8672     0.8636     71.6709
16100     0.4918      0.8496     0.8636     70.6737
16200     0.4773      0.8613     0.8636     71.0888
16300     0.4815      0.8691     0.8636     70.6239
16400     0.4134      0.8809     0.8636     69.6485
16500     0.4521      0.8535     0.8636     71.0020
16600     0.4534      0.8594     0.8636     70.1596
16700     0.4417      0.8691     0.8636     70.8466
16800     0.4483      0.8730     0.8636     71.1578
16900     0.4242      0.8672     0.8636     70.1013
17000     0.4513      0.8789     0.8636     70.2604
17100     0.4180      0.8750     0.8636     70.3907
17200     0.4385      0.8691     0.8636     70.4077
17300     0.4379      0.8633     0.8636     71.6763
17400     0.4229      0.8789     0.8636     71.2648
17500     0.3910      0.8848     0.8636     70.7382
17600     0.4509      0.8672     0.8636     72.3283
17700     0.4710      0.8613     0.8636     72.2815
17800     0.4397      0.8730     0.8636     71.9985
17900     0.4141      0.8906     0.8636     73.1708
18000     0.4679      0.8613     0.8636     75.0680
18100     0.4825      0.8496     0.8636     71.1157
18200     0.4179      0.8691     0.8636     73.6495
18300     0.3937      0.8867     0.8636     73.5344
18400     0.4114      0.8809     0.8636     71.8756
18500     0.4619      0.8613     0.8636     71.3173
18600     0.4484      0.8652     0.8636     72.8416
18700     0.5668      0.8223     0.8636     73.2424
18800     0.4243      0.8867     0.8636     72.8224
18900     0.4504      0.8691     0.8636     72.5338
19000     0.3733      0.9102     0.8636     71.8248
19100     0.3399      0.9082     0.8636     71.6563
19200     0.5001      0.8613     0.8636     69.8860
19300     0.4073      0.8965     0.8636     72.9372
19400     0.4530      0.8574     0.8636     75.3411
19500     0.4707      0.8477     0.8636     74.3632
19600     0.4585      0.8809     0.8636     74.5494
19700     0.3847      0.8770     0.8636     74.3640
19800     0.4782      0.8633     0.8636     73.8255
19900     0.4144      0.8672     0.8636     71.8962
20000     0.4491      0.8672     0.8636     74.0957
20100     0.4179      0.8848     0.8636     71.5760
20199     0.3965      0.8887     0.8636     71.7153
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     0.7133      0.7852     0.7784     9.9098
00100     0.4438      0.8711     0.8799     71.1637
00200     0.4161      0.8848     0.8799     70.0514
00300     0.4693      0.8418     0.8799     71.0407
00400     0.4032      0.8809     0.8815     70.2455
00500     0.4458      0.8770     0.8815     71.2572
00600     0.4473      0.8770     0.8852     73.1861
00700     0.4817      0.8535     0.8852     71.6965
00800     0.4610      0.8594     0.8852     72.3833
00900     0.3943      0.8789     0.8852     71.4729
01000     0.4823      0.8477     0.8852     72.2196
01100     0.4927      0.8516     0.8852     71.2430
01200     0.4746      0.8418     0.8852     71.2369
01300     0.4724      0.8535     0.8852     70.8456
01400     0.4422      0.8828     0.8852     71.1912
01500     0.3795      0.8828     0.8852     70.2850
01600     0.4860      0.8496     0.8852     71.2035
01700     0.4726      0.8672     0.8852     71.4988
01800     0.4109      0.8691     0.8852     71.3989
01900     0.3923      0.8926     0.8852     71.4009
02000     0.4395      0.8789     0.8852     72.4131
02100     0.4425      0.8770     0.8852     71.6458
02200     0.4334      0.8711     0.8887     70.1916
02300     0.4529      0.8750     0.8887     69.5068
02400     0.5480      0.8379     0.8887     70.2769
02500     0.4558      0.8574     0.8887     74.3497
02600     0.4333      0.8809     0.8887     71.2840
02700     0.4111      0.8770     0.8887     71.0590
02800     0.4933      0.8438     0.8887     68.4939
02900     0.4608      0.8457     0.8887     70.6692
03000     0.4283      0.8828     0.8887     71.2642
03100     0.5408      0.8477     0.8887     73.4179
03200     0.3985      0.8809     0.8887     70.4539
03300     0.4131      0.8789     0.8887     71.5948
03400     0.4097      0.8809     0.8887     72.7663
03500     0.4272      0.8730     0.8887     70.3659
03600     0.4135      0.8809     0.8887     71.1334
03700     0.4556      0.8613     0.8887     72.8006
03800     0.4475      0.8789     0.8887     71.7030
03900     0.3813      0.9023     0.8887     70.3105
04000     0.4314      0.8770     0.8887     73.5364
04100     0.4448      0.8574     0.8887     71.5329
04200     0.4409      0.8633     0.8887     71.7728
04300     0.4658      0.8633     0.8887     72.7761
04400     0.4454      0.8691     0.8887     73.0036
04500     0.4105      0.8828     0.8887     72.6975
04600     0.4180      0.8789     0.8887     71.9796
04700     0.3905      0.8809     0.8887     72.5412
04800     0.4167      0.8809     0.8887     70.2007
04900     0.4114      0.8906     0.8887     72.6090
05000     0.4634      0.8652     0.8887     71.1859
05100     0.5123      0.8379     0.8887     70.6499
05200     0.4484      0.8730     0.8887     70.3142
05300     0.3785      0.8906     0.8887     69.8517
05400     0.4640      0.8613     0.8887     73.4874
05500     0.3868      0.8926     0.8887     70.9786
05600     0.3954      0.8887     0.8887     72.5676
05700     0.3793      0.8906     0.8887     72.2063
05800     0.4627      0.8613     0.8887     70.8889
05900     0.4733      0.8672     0.8887     72.4132
06000     0.4814      0.8516     0.8887     71.6233
06100     0.4458      0.8613     0.8887     72.4036
06200     0.3885      0.8926     0.8887     71.3154
06300     0.3661      0.8906     0.8887     72.0708
06400     0.4195      0.8750     0.8897     73.4770
06500     0.4943      0.8555     0.8897     71.6388
06600     0.4138      0.8867     0.8897     71.8643
06700     0.4274      0.8555     0.8897     70.9834
06800     0.4487      0.8750     0.8897     71.0569
06900     0.3552      0.8945     0.8897     70.2414
07000     0.3983      0.8828     0.8897     71.4474
07100     0.4340      0.8594     0.8897     71.4189
07200     0.4230      0.8672     0.8897     70.7608
07300     0.3775      0.9043     0.8897     71.9903
07400     0.4685      0.8652     0.8897     71.3845
07500     0.5091      0.8477     0.8897     72.7367
07600     0.4602      0.8672     0.8897     71.5505
07700     0.5057      0.8496     0.8897     71.4886
07800     0.4871      0.8438     0.8897     71.2883
07900     0.4141      0.8730     0.8897     71.9202
08000     0.4454      0.8770     0.8897     71.8971
08100     0.4490      0.8672     0.8897     71.1982
08200     0.3836      0.8867     0.8897     70.2180
08300     0.3864      0.8848     0.8897     71.6544
08400     0.4357      0.8750     0.8897     71.6789
08500     0.4400      0.8496     0.8897     73.0944
08600     0.4303      0.8633     0.8897     70.8264
08700     0.3949      0.8809     0.8897     71.1670
08800     0.4776      0.8496     0.8897     71.1155
08900     0.5064      0.8340     0.8897     70.8859
09000     0.4375      0.8809     0.8897     72.9115
09100     0.4361      0.8730     0.8897     72.9666
09200     0.4430      0.8613     0.8897     71.1252
09300     0.3673      0.9102     0.8897     72.3602
09400     0.4887      0.8555     0.8897     73.4221
09500     0.4041      0.8750     0.8897     72.1234
09600     0.4367      0.8828     0.8897     71.5037
09700     0.4282      0.8809     0.8897     71.9498
09800     0.4064      0.8730     0.8897     73.7203
09900     0.4075      0.9062     0.8897     70.4418
Start testing:
Test Accuracy: 0.8814
