Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=7, quant_actNM=7, quant_inp=7, quant_w=7, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
b735f7cc-7ff9-4bfc-8f3e-2b55f296d140
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 373, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, 1.)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 82, in forward
    if len(x_range) > 1:
TypeError: object of type 'float' has no len()
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=7, quant_actNM=7, quant_inp=7, quant_w=7, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
b45b5799-7deb-4397-9023-9b468ed9e6e3
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 373, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, torch.tensor([1.]))
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 94, in forward
    x_scaled = x/x_range
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=7, quant_actNM=7, quant_inp=7, quant_w=7, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
3eaf1228-f0e3-4b9b-a406-ed267e3a46ff
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     3.6525      0.0802     0.0873     11.3489
00100     2.3993      0.2110     0.2282     78.2080
00200     2.0026      0.3059     0.3339     77.3440
00300     1.8387      0.3565     0.4062     75.2501
00400     1.5342      0.4873     0.4895     74.3961
00500     1.4594      0.5295     0.5319     73.3238
00600     1.3243      0.5844     0.5867     75.0347
00700     1.4093      0.5211     0.6058     74.3392
00800     1.2988      0.6013     0.6263     74.0179
00900     1.2271      0.5928     0.6545     74.3179
01000     1.1976      0.5928     0.6739     74.6371
01100     0.9885      0.6561     0.6739     73.7390
01200     1.1200      0.6287     0.6986     75.3041
01300     1.1189      0.6203     0.7011     73.1307
01400     0.9702      0.6983     0.7153     74.8891
01500     1.0102      0.6730     0.7237     73.8303
01600     0.9536      0.6920     0.7282     74.1420
01700     0.9788      0.7110     0.7336     74.2055
01800     0.9864      0.6814     0.7336     75.5054
01900     0.9635      0.6561     0.7354     74.6145
02000     0.9121      0.6793     0.7379     73.9670
02100     0.9064      0.7068     0.7438     74.9658
02200     0.9016      0.6962     0.7438     74.6181
02300     0.9056      0.7004     0.7457     74.0677
02400     0.9146      0.6983     0.7457     74.9397
02500     0.8254      0.7300     0.7540     73.6681
02600     0.8951      0.7004     0.7540     73.7685
02700     0.8291      0.7215     0.7592     75.0784
02800     0.8564      0.7257     0.7630     74.9762
02900     0.8784      0.7131     0.7630     75.3890
03000     0.8451      0.7679     0.7630     75.1232
03100     0.7893      0.7363     0.7675     74.4666
03200     0.9537      0.6899     0.7675     76.7398
03300     0.8032      0.7342     0.7706     76.0813
03400     0.8954      0.7300     0.7760     75.3195
03500     0.7953      0.7426     0.7760     76.0921
03600     0.7409      0.7785     0.7760     74.9770
03700     0.6813      0.7574     0.7760     76.3508
03800     0.8347      0.7278     0.7760     75.4570
03900     0.7593      0.7574     0.7760     74.5569
04000     0.6603      0.7743     0.7760     74.5540
04100     0.7409      0.7489     0.7760     74.7562
04200     0.7912      0.7511     0.7760     74.6654
04300     0.7967      0.7131     0.7760     77.1129
04400     0.7979      0.7405     0.7760     75.2876
04500     0.7551      0.7574     0.7760     73.5636
04600     0.7274      0.7658     0.7760     75.1344
04700     0.7087      0.7869     0.7765     75.2866
04800     0.7798      0.7342     0.7776     74.2403
04900     0.7340      0.7574     0.7808     75.7547
05000     0.8211      0.7321     0.7808     75.5628
05100     0.7847      0.7426     0.7837     75.0509
05200     0.7283      0.7869     0.7871     75.7266
05300     0.7009      0.7700     0.7871     74.6250
05400     0.6838      0.7954     0.7877     75.6185
05500     0.8116      0.7489     0.7877     73.6753
05600     0.7446      0.7743     0.7877     75.1547
05700     0.7270      0.7595     0.7877     73.9449
05800     0.7387      0.7363     0.7877     74.3916
05900     0.6465      0.7975     0.7877     73.4473
06000     0.7189      0.7975     0.7877     74.7450
06100     0.7480      0.7658     0.7948     75.5559
06200     0.7100      0.7574     0.7948     74.3902
06300     0.6016      0.7996     0.7948     73.8296
06400     0.6731      0.7679     0.7948     77.2001
06500     0.6794      0.7869     0.7948     73.9287
06600     0.6748      0.7785     0.7950     73.8472
06700     0.7454      0.7468     0.7950     75.7036
06800     0.6784      0.7890     0.7958     73.1705
06900     0.6327      0.8017     0.7958     73.8248
07000     0.6899      0.7827     0.7958     73.5584
07100     0.6345      0.7848     0.7978     74.1163
07200     0.6701      0.7848     0.7978     73.7168
07300     0.7310      0.7911     0.7992     74.1905
07400     0.6731      0.7911     0.7992     74.4008
07500     0.8439      0.7405     0.7992     74.3034
07600     0.6665      0.7806     0.8015     75.0832
07700     0.6919      0.7785     0.8015     73.5583
07800     0.6868      0.7679     0.8015     75.1750
07900     0.7237      0.7658     0.8021     74.2758
08000     0.7188      0.7574     0.8053     74.0281
08100     0.6641      0.7848     0.8053     73.8694
08200     0.6968      0.7700     0.8053     73.3891
08300     0.6796      0.7785     0.8053     74.7317
08400     0.6795      0.7743     0.8053     74.5330
08500     0.6458      0.7869     0.8053     74.0364
08600     0.7651      0.7532     0.8053     74.2194
08700     0.6864      0.7827     0.8061     74.2626
08800     0.7317      0.7384     0.8061     74.2340
08900     0.6027      0.7890     0.8061     74.5480
09000     0.7350      0.7722     0.8061     73.9097
09100     0.7340      0.7553     0.8061     74.6825
09200     0.6555      0.7764     0.8061     74.8748
09300     0.6183      0.7932     0.8061     74.6280
09400     0.6934      0.7869     0.8061     75.2857
09500     0.6751      0.8017     0.8061     74.6587
09600     0.5936      0.8101     0.8064     75.1556
09700     0.7229      0.7679     0.8097     76.2687
09800     0.5469      0.8228     0.8097     75.1643
09900     0.7139      0.7532     0.8164     75.0157
10000     0.6187      0.7890     0.8164     76.0623
10100     0.6403      0.7764     0.8164     74.8802
10200     0.6351      0.8017     0.8164     75.2011
10300     0.5889      0.8270     0.8164     75.3688
10400     0.5852      0.7869     0.8164     75.3485
10500     0.6651      0.7616     0.8164     74.3531
10600     0.7078      0.7806     0.8164     75.2179
10700     0.6390      0.7954     0.8164     74.6967
10800     0.5812      0.8017     0.8164     75.9235
10900     0.7663      0.7700     0.8164     73.4340
11000     0.6693      0.7932     0.8181     73.6623
11100     0.6437      0.7932     0.8181     76.2992
11200     0.5642      0.8080     0.8181     74.2393
11300     0.5988      0.7996     0.8181     75.9281
11400     0.6483      0.7764     0.8181     74.0189
11500     0.5499      0.8228     0.8181     76.6965
11600     0.6278      0.7954     0.8181     73.0894
11700     0.6509      0.7890     0.8181     75.6079
11800     0.5595      0.8186     0.8181     76.9726
11900     0.7223      0.7595     0.8181     73.9320
12000     0.5461      0.8270     0.8181     74.0970
12100     0.6599      0.7764     0.8181     73.9284
12200     0.6578      0.7785     0.8181     75.1197
12300     0.6815      0.7869     0.8181     77.0601
12400     0.5931      0.7932     0.8181     75.0743
12500     0.6372      0.7679     0.8181     75.0866
12600     0.5699      0.8249     0.8181     76.4310
12700     0.6899      0.7869     0.8181     74.2991
12800     0.6064      0.8101     0.8181     74.5144
12900     0.6676      0.7869     0.8181     75.8206
13000     0.6827      0.7806     0.8181     75.2102
13100     0.5461      0.8207     0.8181     74.9862
13200     0.5832      0.8059     0.8181     75.9065
13300     0.5476      0.8101     0.8181     73.4712
13400     0.6267      0.8059     0.8181     75.8846
13500     0.6125      0.7848     0.8181     74.8572
13600     0.5176      0.8333     0.8181     74.1642
13700     0.5413      0.8080     0.8181     74.3583
13800     0.6311      0.7890     0.8181     75.7951
13900     0.5627      0.8038     0.8181     75.6737
14000     0.5875      0.8038     0.8181     74.4148
14100     0.5656      0.8101     0.8181     75.5369
14200     0.5859      0.8143     0.8181     74.4613
14300     0.5511      0.8228     0.8181     74.7706
14400     0.6541      0.7869     0.8181     75.3722
14500     0.6026      0.8186     0.8181     75.8928
14600     0.5268      0.8333     0.8181     74.7208
14700     0.6139      0.8101     0.8181     74.9273
14800     0.6071      0.8059     0.8181     74.2030
14900     0.5913      0.7932     0.8181     75.4202
15000     0.7047      0.7806     0.8181     73.5865
15100     0.5841      0.8101     0.8181     75.8615
15200     0.6419      0.7911     0.8181     76.0008
15300     0.6411      0.7954     0.8181     73.9449
15400     0.5972      0.8165     0.8194     74.2200
15500     0.6032      0.8186     0.8194     74.8667
15600     0.5877      0.8101     0.8194     75.3071
15700     0.6038      0.7954     0.8194     76.0143
15800     0.5735      0.8165     0.8194     76.9795
15900     0.6479      0.8143     0.8223     75.9158
16000     0.6079      0.7996     0.8223     74.9664
16100     0.6116      0.7700     0.8223     74.0423
16200     0.6238      0.8059     0.8223     72.7368
16300     0.7159      0.7616     0.8223     75.4454
16400     0.5551      0.8080     0.8223     75.2653
16500     0.7489      0.7426     0.8223     73.9215
16600     0.6273      0.7827     0.8223     74.6750
16700     0.5808      0.8038     0.8223     74.2768
16800     0.5845      0.8165     0.8223     74.2109
16900     0.6099      0.8059     0.8223     76.2414
17000     0.6121      0.7996     0.8223     73.6263
17100     0.5799      0.8291     0.8223     73.9036
17200     0.5982      0.8080     0.8223     74.1410
17300     0.5918      0.8059     0.8223     74.8816
17400     0.5429      0.8186     0.8223     73.1660
17500     0.6010      0.7975     0.8223     74.5602
17600     0.5443      0.8270     0.8223     73.7547
17700     0.5377      0.8333     0.8223     74.4793
17800     0.5537      0.8143     0.8223     74.1532
17900     0.6649      0.7722     0.8223     74.9948
18000     0.5725      0.8017     0.8223     73.6690
18100     0.6173      0.7785     0.8223     75.3868
18200     0.6441      0.7954     0.8223     75.1581
18300     0.5625      0.8143     0.8223     76.0965
18400     0.5999      0.8228     0.8223     74.1413
18500     0.6355      0.7827     0.8223     74.1034
18600     0.5692      0.8523     0.8223     74.1560
18700     0.6037      0.7996     0.8223     75.1675
18800     0.6087      0.7869     0.8223     75.5496
18900     0.5839      0.8165     0.8223     75.6761
19000     0.6324      0.7848     0.8223     75.0493
19100     0.5912      0.8017     0.8223     75.3115
19200     0.6326      0.8080     0.8223     73.8077
19300     0.6593      0.7975     0.8223     73.7364
19400     0.5736      0.8080     0.8223     75.6042
19500     0.5647      0.8122     0.8223     75.9275
19600     0.5770      0.8207     0.8225     72.9900
19700     0.5165      0.8418     0.8225     75.8025
19800     0.6147      0.8038     0.8225     74.2986
19900     0.6235      0.7911     0.8225     73.5120
20000     0.6089      0.7975     0.8233     73.2535
20100     0.5733      0.8059     0.8233     74.6723
20199     0.5570      0.8080     0.8233     75.4108
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     0.5301      0.8186     0.8151     10.5865
00100     0.4839      0.8460     0.8253     75.6896
00200     0.5012      0.8207     0.8253     74.2881
00300     0.5349      0.8165     0.8253     76.3119
00400     0.4772      0.8439     0.8276     76.0155
00500     0.4882      0.8354     0.8276     76.2792
00600     0.4967      0.8122     0.8276     74.8284
00700     0.5397      0.8080     0.8276     75.1768
00800     0.5182      0.8186     0.8276     75.5189
00900     0.5895      0.7827     0.8276     76.3515
01000     0.5076      0.8291     0.8276     75.0743
01100     0.3970      0.8650     0.8296     75.3376
01200     0.4985      0.8629     0.8296     74.5711
01300     0.4746      0.8586     0.8296     74.5550
01400     0.5477      0.8122     0.8296     75.1616
01500     0.5860      0.8228     0.8296     74.6416
01600     0.5287      0.8333     0.8296     75.4443
01700     0.4428      0.8502     0.8296     74.9438
01800     0.5739      0.8207     0.8296     75.6329
01900     0.5558      0.8186     0.8296     74.6948
02000     0.5614      0.8101     0.8296     74.5167
02100     0.5095      0.8439     0.8296     74.9181
02200     0.4930      0.8439     0.8296     74.9419
02300     0.4652      0.8586     0.8296     75.4397
02400     0.4891      0.8354     0.8296     74.5314
02500     0.5250      0.8376     0.8296     74.1737
02600     0.5197      0.8333     0.8296     74.7419
02700     0.5243      0.8333     0.8296     75.7392
02800     0.5641      0.8291     0.8296     75.0344
02900     0.5349      0.8122     0.8296     76.1197
03000     0.5617      0.8080     0.8296     75.0170
03100     0.4788      0.8586     0.8296     74.1724
03200     0.5674      0.8165     0.8296     73.7431
03300     0.5477      0.8270     0.8296     76.5746
03400     0.5427      0.8312     0.8296     74.2074
03500     0.4976      0.8333     0.8296     73.4855
03600     0.4680      0.8460     0.8296     72.6232
03700     0.5311      0.8186     0.8296     73.6410
03800     0.4712      0.8523     0.8296     74.7081
03900     0.5592      0.8207     0.8308     73.4204
04000     0.4696      0.8523     0.8308     74.3227
04100     0.4750      0.8523     0.8308     74.6853
04200     0.5106      0.8502     0.8308     75.0343
04300     0.5035      0.8460     0.8308     73.7640
04400     0.4944      0.8418     0.8308     72.9797
04500     0.4955      0.8376     0.8308     73.7599
04600     0.5007      0.8439     0.8327     75.6645
04700     0.4991      0.8333     0.8327     74.2796
04800     0.5210      0.8312     0.8327     73.6227
04900     0.5170      0.8291     0.8327     73.8352
05000     0.4999      0.8397     0.8327     76.7255
05100     0.4738      0.8523     0.8327     75.5223
05200     0.5363      0.8165     0.8327     72.8912
05300     0.5396      0.8460     0.8327     77.2096
05400     0.4658      0.8544     0.8327     75.4290
05500     0.5990      0.8122     0.8327     75.7262
05600     0.5350      0.8165     0.8327     75.1206
05700     0.5199      0.8439     0.8327     76.0083
05800     0.4776      0.8397     0.8327     75.0712
05900     0.4723      0.8481     0.8327     72.7461
06000     0.4343      0.8650     0.8327     74.1003
06100     0.4521      0.8608     0.8327     75.0635
06200     0.5615      0.8122     0.8327     75.7569
06300     0.4717      0.8671     0.8327     73.8822
06400     0.5461      0.8270     0.8327     77.0464
06500     0.5901      0.8017     0.8334     74.3377
06600     0.4749      0.8629     0.8334     75.5444
06700     0.5682      0.8270     0.8334     73.8486
06800     0.4686      0.8586     0.8334     75.1061
06900     0.4476      0.8734     0.8334     75.7806
07000     0.5059      0.8397     0.8334     75.6970
07100     0.4906      0.8544     0.8334     74.7821
07200     0.5078      0.8207     0.8334     75.6405
07300     0.5541      0.8186     0.8334     73.7604
07400     0.5141      0.8333     0.8334     74.8760
07500     0.5922      0.7996     0.8334     74.0171
07600     0.5487      0.7996     0.8334     74.4069
07700     0.4644      0.8481     0.8334     75.1208
07800     0.4328      0.8481     0.8334     74.8295
07900     0.4640      0.8671     0.8334     75.2545
08000     0.4938      0.8333     0.8334     74.2137
08100     0.5258      0.8481     0.8334     75.0787
08200     0.4550      0.8544     0.8334     75.4115
08300     0.4616      0.8502     0.8334     75.7757
08400     0.4768      0.8397     0.8334     75.6082
08500     0.5128      0.8333     0.8334     75.0040
08600     0.5465      0.8270     0.8334     74.6114
08700     0.6054      0.8017     0.8334     73.0888
08800     0.4953      0.8291     0.8334     75.5241
08900     0.5222      0.8376     0.8334     73.3779
09000     0.4856      0.8418     0.8334     74.6745
09100     0.4892      0.8312     0.8334     76.1739
09200     0.5206      0.8186     0.8334     73.6009
09300     0.5480      0.8122     0.8334     76.4963
09400     0.5329      0.8354     0.8334     77.9052
09500     0.4755      0.8376     0.8334     75.0044
09600     0.5219      0.8270     0.8334     74.4400
09700     0.5229      0.8186     0.8334     75.3722
09800     0.5319      0.8101     0.8334     76.2169
09900     0.5307      0.8270     0.8334     76.7428
Start testing:
Test Accuracy: 0.8201
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=7, quant_actNM=7, quant_inp=7, quant_w=7, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
31cca6f9-f789-4306-b85b-3f84ce14022d
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     3.6525      0.0802     0.0873     9.9861
00100     2.3993      0.2110     0.2282     56.7809
00200     2.0026      0.3059     0.3339     56.5268
00300     1.8387      0.3565     0.4062     56.0645
00400     1.5342      0.4873     0.4895     55.4314
00500     1.4594      0.5295     0.5319     56.2513
00600     1.2980      0.5949     0.5775     56.0286
00700     1.4525      0.5338     0.6046     55.9449
00800     1.3023      0.5802     0.6242     56.5612
00900     1.1821      0.6160     0.6446     56.4923
01000     1.1530      0.6160     0.6678     55.2501
01100     0.9848      0.6667     0.6894     57.3243
01200     1.1014      0.6414     0.6963     56.5223
01300     1.1088      0.6203     0.7064     56.7474
01400     0.9383      0.7068     0.7167     56.5866
01500     0.9617      0.6835     0.7168     55.7090
01600     1.0050      0.6962     0.7326     56.4303
01700     0.9934      0.6751     0.7326     56.8443
01800     0.9687      0.6751     0.7375     56.8278
01900     0.9656      0.6709     0.7375     56.6780
02000     0.9196      0.6835     0.7375     55.5775
02100     0.9201      0.6878     0.7446     55.5607
02200     0.9031      0.6962     0.7446     57.1963
02300     0.8850      0.7257     0.7469     56.6040
02400     0.8894      0.6899     0.7496     58.8550
02500     0.8035      0.7173     0.7535     57.5397
02600     0.9468      0.6793     0.7535     57.7974
02700     0.8165      0.7300     0.7535     56.7212
02800     0.8341      0.7300     0.7597     55.5447
02900     0.8539      0.7215     0.7597     55.8548
03000     0.8138      0.7194     0.7597     57.6535
03100     0.7976      0.7468     0.7688     57.9745
03200     0.9120      0.6899     0.7709     57.5161
03300     0.8233      0.7489     0.7709     56.4900
03400     0.9435      0.6983     0.7709     56.0695
03500     0.8760      0.7300     0.7709     55.7928
03600     0.7690      0.7532     0.7711     55.6236
03700     0.7199      0.7806     0.7711     56.6761
03800     0.8058      0.7321     0.7711     56.3107
03900     0.7678      0.7595     0.7725     55.9723
04000     0.7010      0.7553     0.7741     56.4392
04100     0.7591      0.7658     0.7749     57.5128
04200     0.8033      0.7257     0.7749     56.6845
04300     0.8150      0.7152     0.7773     56.5571
04400     0.7807      0.7489     0.7865     56.1169
04500     0.7425      0.7300     0.7865     56.3337
04600     0.7718      0.7384     0.7865     56.9078
04700     0.6995      0.7869     0.7865     56.0211
04800     0.7691      0.7300     0.7879     57.4487
04900     0.7461      0.7616     0.7910     56.2620
05000     0.7994      0.7679     0.7910     56.1331
05100     0.7285      0.7848     0.7910     56.2131
05200     0.7157      0.7785     0.7910     56.1313
05300     0.7300      0.7806     0.7956     55.8612
05400     0.7038      0.7932     0.7956     56.2603
05500     0.7863      0.7468     0.7956     55.3108
05600     0.7713      0.7574     0.7956     56.4032
05700     0.7200      0.7764     0.7990     55.4717
05800     0.7604      0.7447     0.7990     55.5231
05900     0.6495      0.7743     0.7990     56.8883
06000     0.6959      0.7996     0.7990     56.6630
06100     0.7422      0.7785     0.7990     56.0603
06200     0.6374      0.7722     0.8046     57.1622
06300     0.5457      0.8333     0.8046     56.9500
06400     0.7067      0.7658     0.8046     56.2937
06500     0.6836      0.7869     0.8046     56.3280
06600     0.7131      0.7743     0.8046     55.4646
06700     0.7209      0.7975     0.8046     56.9201
06800     0.6194      0.8080     0.8085     56.2696
06900     0.6462      0.7827     0.8085     55.5364
07000     0.6632      0.7743     0.8085     56.8569
07100     0.6392      0.7954     0.8085     56.3553
07200     0.6995      0.7869     0.8085     56.9821
07300     0.7708      0.7785     0.8085     56.0017
07400     0.7133      0.7700     0.8085     55.9468
07500     0.7728      0.7426     0.8085     56.9971
07600     0.6430      0.7890     0.8098     56.1728
07700     0.6763      0.7764     0.8098     56.6559
07800     0.6603      0.7743     0.8098     56.3895
