Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=494, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=6, quant_actNM=6, quant_inp=6, quant_w=6, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
81708e14-95ab-4ab2-bd98-3509c679ce16
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 167, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 290, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 229, in forward
    activated_input = quant_pass(pact_a_bmm(input_gate_out * activation_out, self.a8), self.abNM, self.a8)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 89, in forward
    x01q =  torch.round(x01 * step_d ) / step_d
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 22.17 GiB total capacity; 19.10 GiB already allocated; 18.50 MiB free; 21.18 GiB reserved in total by PyTorch)
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=100, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=494, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=6, quant_actNM=6, quant_inp=6, quant_w=6, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
3c432721-7e0c-4f79-987f-34aaf36daaf3
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 167, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 290, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 211, in forward
    part2 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(hx, self.a11), self.ib, self.a11), self.weight_hh * w_mask, self.bias_hh, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 146, in forward
    wq = quant_pass(weight, wb, torch.tensor([1.]).to(input.device))
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 98, in forward
    x01q =  torch.round(x01 * step_d ) / step_d
RuntimeError: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 10.76 GiB total capacity; 8.64 GiB already allocated; 11.12 MiB free; 9.79 GiB reserved in total by PyTorch)
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=100, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=494, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=6, quant_actNM=6, quant_inp=6, quant_w=6, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
0d6c5c41-ec78-4dc8-a10c-8973ce47ac97
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.9684      0.0900     0.0875     19.8396
00100     1.4806      0.5600     0.0875     63.8062
00200     1.0440      0.6200     0.1026     62.6917
00300     0.7571      0.8000     0.1026     65.5784
00400     0.6919      0.7800     0.1143     64.3750
00500     0.6445      0.7900     0.1143     66.3761
00600     0.6155      0.8400     0.1143     62.5530
00700     0.5201      0.8000     0.1143     63.5220
00800     0.6800      0.7900     0.1143     62.0693
00900     0.5197      0.8300     0.1143     63.2792
01000     0.5890      0.8200     0.1192     64.0762
01100     0.6397      0.8300     0.1220     68.6547
01200     0.4317      0.8600     0.1220     62.5417
01300     0.5018      0.8200     0.1220     64.0101
01400     0.5021      0.8100     0.1251     64.1318
01500     0.3779      0.8800     0.1251     66.6303
01600     0.5214      0.9000     0.1251     62.0803
01700     0.3730      0.8700     0.1255     63.8068
01800     0.3103      0.9100     0.1264     65.1462
01900     0.3804      0.8700     0.1309     63.4629
02000     0.4264      0.8300     0.1309     63.2039
02100     0.4888      0.8500     0.1358     66.3352
02200     0.3809      0.9200     0.1462     70.2707
02300     0.3295      0.8800     0.1491     66.4373
02400     0.3932      0.8800     0.1491     64.4820
02500     0.4348      0.9000     0.1491     67.7382
02600     0.2197      0.9200     0.1491     65.9545
02700     0.2513      0.9200     0.1491     63.5819
02800     0.3477      0.8800     0.1564     64.4977
02900     0.3692      0.8500     0.1566     63.6606
03000     0.3547      0.9100     0.1652     67.3274
03100     0.4865      0.8500     0.1677     65.1360
03200     0.3048      0.9300     0.1677     62.6316
03300     0.2950      0.9200     0.1677     65.1456
03400     0.3016      0.9300     0.1734     68.5005
03500     0.2889      0.9300     0.1734     65.1294
03600     0.3071      0.9100     0.1734     62.9125
03700     0.3367      0.8900     0.1734     66.0680
03800     0.2664      0.9400     0.1834     69.4429
03900     0.3940      0.8800     0.1834     66.1190
04000     0.3118      0.9200     0.2043     67.2597
04100     0.2920      0.9400     0.2098     68.4381
04200     0.2117      0.9500     0.2098     62.4330
04300     0.3440      0.9200     0.2098     64.9860
04400     0.3246      0.9000     0.2098     66.8332
04500     0.3488      0.9200     0.2098     65.5847
04600     0.2580      0.9300     0.2098     64.9740
04700     0.4162      0.8700     0.2098     63.6320
04800     0.3380      0.8800     0.2098     63.7921
04900     0.3130      0.9100     0.2178     63.6782
05000     0.3185      0.9100     0.2178     62.7523
05100     0.3772      0.9200     0.2330     68.4512
05200     0.2844      0.9000     0.2330     63.2485
05300     0.3975      0.9000     0.2330     66.5053
05400     0.4620      0.8500     0.2330     64.8108
05500     0.4107      0.8700     0.2330     64.4162
05600     0.2516      0.9100     0.2415     64.8713
05700     0.2549      0.9200     0.2415     64.2983
05800     0.2635      0.9100     0.2415     63.4913
05900     0.1772      0.9700     0.2415     63.7853
06000     0.3131      0.9000     0.2587     65.5991
06100     0.4045      0.8500     0.2680     64.6175
06200     0.2190      0.9400     0.2680     65.5972
06300     0.2027      0.9500     0.2680     62.1535
06400     0.3519      0.8900     0.2695     66.0135
06500     0.3447      0.9100     0.2864     64.3197
06600     0.2403      0.9200     0.2977     62.5034
06700     0.2466      0.9400     0.2988     63.8661
06800     0.2315      0.9400     0.2988     66.0869
06900     0.2843      0.9300     0.2988     66.5685
07000     0.3398      0.9100     0.2988     62.5144
07100     0.3212      0.9200     0.3091     65.0278
07200     0.2783      0.9200     0.3091     66.4027
07300     0.3371      0.9200     0.3091     63.6378
07400     0.2890      0.9000     0.3262     64.4738
07500     0.1656      0.9500     0.3341     64.0414
07600     0.2031      0.9500     0.3341     63.5880
07700     0.2263      0.9200     0.3341     66.1687
07800     0.3163      0.9300     0.3341     64.9235
07900     0.1805      0.9500     0.3430     65.0030
08000     0.2568      0.9300     0.3430     62.7241
08100     0.2082      0.9500     0.3430     69.5484
08200     0.2717      0.9400     0.3430     63.4917
08300     0.3778      0.8900     0.3704     64.8371
08400     0.1777      0.9600     0.3734     63.9942
08500     0.2167      0.9400     0.3734     62.9007
08600     0.2991      0.9000     0.3734     67.3129
08700     0.2401      0.9000     0.3734     65.6501
08800     0.2858      0.9300     0.3734     64.9014
08900     0.2199      0.9600     0.3734     63.9826
09000     0.1527      0.9700     0.3734     64.3870
09100     0.3483      0.9100     0.4120     63.0729
09200     0.1767      0.9400     0.4120     62.8250
09300     0.2912      0.9200     0.4120     62.8745
09400     0.2119      0.9600     0.4120     63.0112
09500     0.2707      0.9500     0.4120     64.4187
09600     0.2361      0.9400     0.4120     62.3311
09700     0.3049      0.9000     0.4120     66.6090
09800     0.2011      0.9500     0.4120     66.8688
09900     0.1993      0.9500     0.4120     67.4429
10000     0.3025      0.9200     0.4120     62.9753
10100     0.2396      0.9400     0.4120     64.4272
10200     0.2135      0.9400     0.4120     63.3411
10300     0.3441      0.9100     0.4120     66.6119
10400     0.2225      0.9300     0.4120     65.2774
10500     0.1340      0.9800     0.4120     62.6879
10600     0.1596      0.9600     0.4120     63.6239
10700     0.2665      0.8900     0.4120     68.1899
10800     0.1425      0.9800     0.4120     62.8291
10900     0.1316      0.9700     0.4120     64.3757
11000     0.1737      0.9600     0.4120     62.9136
11100     0.1603      0.9600     0.4120     63.7303
11200     0.1546      0.9700     0.4120     63.1925
11300     0.2268      0.9500     0.4156     66.7180
11400     0.1784      0.9600     0.4156     64.0724
11500     0.1641      0.9500     0.4156     68.1319
11600     0.1213      0.9600     0.4301     66.3566
11700     0.0946      0.9800     0.4301     63.7000
11800     0.1200      0.9800     0.4301     66.2109
11900     0.1205      0.9700     0.4301     63.6869
12000     0.1871      0.9400     0.4301     66.7684
12100     0.1237      0.9800     0.4301     62.8160
12200     0.1939      0.9400     0.4301     63.8278
12300     0.0977      0.9800     0.4301     63.6740
12400     0.1250      0.9700     0.4301     64.6973
12500     0.1445      0.9600     0.4301     67.2820
12600     0.1592      0.9600     0.4301     62.9643
12700     0.1062      0.9600     0.4301     65.6018
12800     0.1182      0.9800     0.4301     62.3532
12900     0.2706      0.9300     0.4301     63.0069
13000     0.1147      0.9900     0.4301     66.7528
13100     0.2387      0.9400     0.4301     64.3077
13200     0.1422      0.9500     0.4301     66.4453
13300     0.1344      0.9800     0.4301     64.6714
13400     0.1672      0.9600     0.4301     66.3673
13500     0.1323      0.9800     0.4301     66.9119
13600     0.1232      0.9700     0.4301     66.7522
13700     0.0594      1.0000     0.4433     64.7362
13800     0.2496      0.9300     0.4433     63.2026
13900     0.1806      0.9500     0.4433     65.9451
14000     0.1477      0.9600     0.4433     65.4276
14100     0.1895      0.9400     0.4433     68.4664
14200     0.0900      0.9800     0.4433     65.7423
14300     0.1478      0.9500     0.4433     62.6828
14400     0.1560      0.9700     0.4433     64.0021
14500     0.1320      0.9700     0.4433     66.1605
14600     0.2084      0.9300     0.4433     64.1896
14700     0.0870      0.9800     0.4433     65.2121
14800     0.1729      0.9500     0.4433     62.6609
14900     0.1376      0.9800     0.4433     64.4005
15000     0.0681      0.9900     0.4433     65.4964
15100     0.1068      0.9800     0.4433     62.4726
15200     0.1288      0.9600     0.4433     66.7912
15300     0.1079      0.9600     0.4433     61.5168
15400     0.1796      0.9600     0.4433     61.1548
15500     0.2068      0.9600     0.4433     61.9294
15600     0.1714      0.9500     0.4433     60.8673
15700     0.1396      0.9700     0.4433     64.1349
15800     0.0701      0.9900     0.4433     60.1728
15900     0.0948      0.9800     0.4433     60.1263
16000     0.1174      0.9800     0.4433     59.7718
16100     0.1593      0.9500     0.4433     61.4476
16200     0.2042      0.9300     0.4433     63.2916
16300     0.0619      0.9900     0.4433     62.5705
16400     0.2161      0.9400     0.4433     65.7236
16500     0.2011      0.9700     0.4433     61.3625
16600     0.1592      0.9600     0.4433     60.1885
16700     0.1196      0.9700     0.4433     62.1557
16800     0.2671      0.9100     0.4433     61.7896
16900     0.1232      0.9600     0.4433     65.5402
17000     0.1201      0.9700     0.4433     62.7108
17100     0.1545      0.9600     0.4433     62.8222
17200     0.2013      0.9300     0.4433     62.0099
17300     0.2302      0.9100     0.4433     62.0112
17400     0.1029      0.9700     0.4433     62.2648
17500     0.1576      0.9600     0.4433     63.0051
17600     0.1807      0.9500     0.4433     61.9250
17700     0.2223      0.9400     0.4433     65.4989
17800     0.1338      0.9700     0.4433     65.2716
17900     0.1457      0.9500     0.4433     68.1377
18000     0.0895      0.9800     0.4433     62.0612
18100     0.1433      0.9600     0.4433     61.3885
18200     0.0834      0.9800     0.4433     64.0517
18300     0.1560      0.9600     0.4433     62.3325
18400     0.1520      0.9500     0.4433     61.9039
18500     0.1349      0.9600     0.4433     63.3229
18600     0.1021      0.9800     0.4433     65.0702
18700     0.1206      0.9500     0.4433     61.9185
18800     0.1790      0.9600     0.4433     60.9217
18900     0.1036      0.9700     0.4433     60.5764
19000     0.1263      0.9700     0.4433     61.1717
19100     0.1578      0.9500     0.4433     64.6460
19200     0.0740      0.9800     0.4433     64.1741
19300     0.0883      0.9800     0.4433     63.2439
19400     0.1511      0.9600     0.4433     63.3595
19500     0.1697      0.9500     0.4433     64.1294
19600     0.1040      0.9800     0.4433     62.4686
19700     0.0665      1.0000     0.4497     62.5735
19800     0.1887      0.9500     0.4497     60.7536
19900     0.1112      0.9800     0.4497     60.6331
20000     0.1292      0.9500     0.4497     63.5234
20100     0.1543      0.9400     0.4497     63.1876
20199     0.0826      1.0000     0.4497     62.7005
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     4.9116      0.2300     0.4247     16.6249
00100     1.0863      0.7300     0.6729     63.0983
00200     0.6700      0.8300     0.7268     61.9517
00300     0.7542      0.7600     0.7527     61.1431
00400     0.8517      0.8000     0.7752     61.1891
00500     1.1485      0.6900     0.7882     62.3048
00600     0.6244      0.8400     0.7946     65.4044
00700     0.7615      0.8400     0.8010     65.2816
00800     0.5497      0.8400     0.8010     60.4978
00900     0.9017      0.7200     0.8024     61.8061
01000     0.6178      0.8000     0.8144     64.1352
01100     0.7455      0.7800     0.8144     66.3715
01200     0.5059      0.8400     0.8144     62.2406
01300     0.7136      0.8000     0.8213     64.6961
01400     0.5114      0.8700     0.8301     63.3407
01500     0.5141      0.8400     0.8301     64.2777
01600     0.4141      0.8600     0.8301     62.9456
01700     0.6403      0.7900     0.8301     64.7767
01800     0.4809      0.8400     0.8306     60.0363
01900     0.4322      0.8600     0.8306     61.2300
02000     0.7788      0.7800     0.8306     64.6539
02100     0.5702      0.8600     0.8306     63.5859
02200     0.6842      0.8200     0.8322     64.0577
02300     0.4966      0.8400     0.8345     62.3929
02400     0.5977      0.8500     0.8421     60.6959
02500     0.4754      0.8800     0.8421     62.3206
02600     0.5738      0.8900     0.8421     63.5735
02700     0.5983      0.8400     0.8421     63.8593
02800     0.5608      0.8700     0.8473     64.2298
02900     0.6220      0.8200     0.8473     60.7589
03000     0.6901      0.8000     0.8473     60.5975
03100     0.5293      0.8500     0.8498     62.0995
03200     0.6627      0.7900     0.8498     61.6688
03300     0.5565      0.8400     0.8498     60.3762
03400     0.6625      0.8300     0.8498     66.4597
03500     0.4447      0.9000     0.8498     62.7353
03600     0.5362      0.8400     0.8498     62.7237
03700     0.5884      0.8000     0.8498     61.2688
03800     0.7915      0.7900     0.8512     64.0490
03900     0.5621      0.7900     0.8512     63.4213
04000     0.5270      0.8400     0.8512     63.3800
04100     0.3760      0.8800     0.8512     61.2197
04200     0.6855      0.8200     0.8512     62.9004
04300     0.5192      0.8400     0.8545     62.4183
04400     0.8129      0.8100     0.8545     62.7658
04500     0.5791      0.8500     0.8545     63.9310
04600     0.2808      0.9100     0.8545     61.3038
04700     0.6181      0.8300     0.8573     60.9317
04800     0.5121      0.8700     0.8573     62.4885
04900     0.5574      0.8600     0.8573     63.6712
05000     0.6403      0.8400     0.8573     62.6102
05100     0.4365      0.8800     0.8573     62.5082
05200     0.4334      0.8800     0.8573     62.2082
05300     0.3195      0.9100     0.8573     62.6382
05400     0.4662      0.9000     0.8573     60.5117
05500     0.5846      0.8400     0.8573     62.0567
05600     0.4651      0.8500     0.8573     61.1827
05700     0.3227      0.8900     0.8573     59.1025
05800     0.6556      0.7900     0.8573     61.4984
05900     0.4130      0.8900     0.8589     62.8339
06000     0.4763      0.8500     0.8614     65.7648
06100     0.3746      0.8900     0.8614     61.7394
06200     0.6053      0.8200     0.8634     60.9386
06300     0.4082      0.9100     0.8634     63.2519
06400     0.3463      0.9300     0.8634     61.1065
06500     0.2999      0.9000     0.8640     62.4702
06600     0.4284      0.8500     0.8640     60.6566
06700     0.2816      0.9300     0.8640     62.4765
06800     0.5320      0.9100     0.8640     61.3159
06900     0.3851      0.8700     0.8640     65.4733
07000     0.6107      0.8500     0.8640     63.4371
07100     0.2129      0.9400     0.8640     63.6649
07200     0.6022      0.8400     0.8640     64.5381
07300     0.2919      0.9300     0.8640     64.4609
07400     0.6088      0.8000     0.8640     63.1239
07500     0.2291      0.9200     0.8640     66.8694
07600     0.2605      0.9200     0.8640     67.7724
07700     0.4398      0.8600     0.8657     65.8483
07800     0.4945      0.9100     0.8664     66.5485
07900     0.1935      0.9300     0.8678     62.8519
08000     0.6434      0.8300     0.8720     63.2517
08100     0.5203      0.8600     0.8720     63.6939
08200     0.2710      0.9400     0.8720     66.2961
08300     0.7030      0.7900     0.8720     66.2591
08400     0.3721      0.9100     0.8720     63.0831
08500     0.4978      0.8400     0.8720     65.8617
08600     0.4923      0.8700     0.8720     65.5667
08700     0.4705      0.8700     0.8720     66.4245
08800     0.5426      0.9000     0.8720     67.0236
08900     0.4381      0.8800     0.8720     67.8851
09000     0.4596      0.8700     0.8720     66.9864
09100     0.4779      0.8600     0.8720     65.2049
09200     0.5743      0.8600     0.8720     65.9158
09300     0.4707      0.8500     0.8720     65.8782
09400     0.5601      0.8700     0.8720     63.8290
09500     0.6057      0.8800     0.8720     65.7345
09600     0.5455      0.8700     0.8720     67.0434
09700     0.4711      0.8400     0.8720     66.3283
09800     0.4101      0.8900     0.8720     64.4422
09900     0.5302      0.8600     0.8738     65.6750
Start testing:
Test Accuracy: 0.9016
