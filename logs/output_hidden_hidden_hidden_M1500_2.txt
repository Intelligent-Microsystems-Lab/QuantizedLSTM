Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=494, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=0.1, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=6, quant_actNM=8, quant_inp=4, quant_w=None, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
85dbfde5-ab52-4b0d-a470-f75e97a1e2d2
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 167, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 374, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 229, in forward
    activated_input = quant_pass(pact_a_bmm(input_gate_out * activation_out, self.a8), self.abNM, self.a8)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 87, in forward
    x01 = torch.clamp(x_scaled,-1+(1./step_d),1-(1./step_d))
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 22.17 GiB total capacity; 19.09 GiB already allocated; 8.50 MiB free; 21.19 GiB reserved in total by PyTorch)
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=100, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=494, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=0.1, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=6, quant_actNM=8, quant_inp=4, quant_w=None, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
36501ff3-bea3-4326-9286-d8d38779fdff
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 167, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 374, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 211, in forward
    part2 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(hx, self.a11), self.ib, self.a11), self.weight_hh * w_mask, self.bias_hh, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 141, in forward
    noise_w = torch.randn(weight.shape, device = input.device) * max_w * nl
RuntimeError: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 10.76 GiB total capacity; 8.64 GiB already allocated; 13.12 MiB free; 9.79 GiB reserved in total by PyTorch)
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=100, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=494, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=0.1, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=6, quant_actNM=8, quant_inp=4, quant_w=None, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
dd278279-861e-4ffe-bca7-c98373a669cc
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     3.7470      0.0900     0.1136     16.6451
00100     1.5497      0.4900     0.5119     61.1734
00200     1.1911      0.5500     0.6138     59.8964
00300     1.1817      0.5900     0.7095     59.6916
00400     1.0421      0.6800     0.7165     60.1267
00500     1.1844      0.6600     0.7332     60.1021
00600     1.0165      0.6700     0.7608     60.5012
00700     0.8097      0.7500     0.7718     60.3507
00800     0.8681      0.7400     0.7718     59.4811
00900     0.8984      0.7200     0.8046     60.1732
01000     0.8826      0.7200     0.8046     60.5036
01100     0.6223      0.8400     0.8046     60.6579
01200     0.7617      0.7600     0.8138     60.3005
01300     1.0044      0.6400     0.8138     59.5125
01400     0.7474      0.7400     0.8159     61.1614
01500     0.6691      0.7800     0.8159     60.2077
01600     0.7647      0.7500     0.8205     61.4560
01700     0.7178      0.7700     0.8286     61.1097
01800     0.6856      0.7900     0.8325     61.0056
01900     0.7793      0.7900     0.8325     60.6850
02000     0.5754      0.8200     0.8325     61.6566
02100     0.4872      0.8500     0.8426     61.6763
02200     0.6824      0.7900     0.8426     60.4328
02300     0.6542      0.8000     0.8439     60.8590
02400     0.6054      0.7700     0.8439     65.1560
02500     0.6314      0.8200     0.8494     63.2500
02600     0.7106      0.8000     0.8494     61.9969
02700     0.5683      0.8200     0.8494     62.4447
02800     0.5093      0.8600     0.8494     62.2384
02900     0.5912      0.7700     0.8494     64.7523
03000     0.5217      0.8700     0.8502     64.0967
03100     0.7486      0.7600     0.8502     61.8535
03200     0.5664      0.8400     0.8552     61.6639
03300     0.6487      0.8100     0.8606     61.9226
03400     0.7235      0.7800     0.8606     61.8045
03500     0.6745      0.7600     0.8606     62.7919
03600     0.7241      0.7200     0.8606     61.5858
03700     0.7764      0.7300     0.8606     61.2971
03800     0.5624      0.8200     0.8606     63.9385
03900     0.6097      0.8000     0.8606     63.4989
04000     0.5699      0.8400     0.8606     62.3525
04100     0.5972      0.8100     0.8606     61.9625
04200     0.6336      0.8300     0.8614     63.5836
04300     0.5650      0.8300     0.8614     64.0775
04400     0.6066      0.7900     0.8614     62.5932
04500     0.2846      0.9100     0.8619     61.0684
04600     0.5685      0.7800     0.8619     61.7292
04700     0.7209      0.7600     0.8619     60.9621
04800     0.5992      0.7700     0.8619     62.1685
04900     0.5090      0.8400     0.8619     61.8125
05000     0.5746      0.8100     0.8619     61.7879
05100     0.4990      0.8400     0.8619     59.7086
05200     0.5594      0.7900     0.8647     61.9877
05300     0.4725      0.8700     0.8736     61.6364
05400     0.5246      0.7900     0.8736     62.7321
05500     0.5208      0.8000     0.8736     61.1001
05600     0.3614      0.8800     0.8736     62.4895
05700     0.6703      0.8000     0.8743     62.5749
05800     0.4489      0.8400     0.8743     62.8711
05900     0.7139      0.7600     0.8743     64.0899
06000     0.3825      0.8900     0.8743     62.4439
06100     0.6305      0.7300     0.8743     61.4796
06200     0.5858      0.8500     0.8812     61.8267
06300     0.5906      0.7900     0.8812     60.0205
06400     0.6242      0.8000     0.8812     59.5794
06500     0.3854      0.8400     0.8812     59.6817
06600     0.6522      0.8300     0.8812     59.3215
06700     0.4981      0.8200     0.8812     59.4678
06800     0.3881      0.9000     0.8812     59.9501
06900     0.5576      0.8300     0.8812     59.2348
07000     0.4097      0.8600     0.8812     61.2844
07100     0.5424      0.8300     0.8812     60.7426
07200     0.6964      0.7700     0.8815     61.1135
07300     0.6330      0.7900     0.8819     63.4481
07400     0.4715      0.8600     0.8819     62.0847
07500     0.5215      0.8000     0.8819     61.3730
07600     0.5853      0.8400     0.8833     62.8306
07700     0.6022      0.8400     0.8833     61.4124
07800     0.5639      0.8300     0.8833     62.3999
07900     0.4443      0.8600     0.8833     62.1232
08000     0.5812      0.8200     0.8833     60.6705
08100     0.4024      0.8700     0.8833     60.7182
08200     0.4174      0.8800     0.8833     59.8883
08300     0.4202      0.8700     0.8833     60.1722
08400     0.6847      0.8200     0.8833     61.2113
08500     0.6094      0.8100     0.8833     61.2144
08600     0.5933      0.7700     0.8833     61.3212
08700     0.6182      0.7500     0.8833     60.3336
08800     0.5119      0.8400     0.8833     63.7719
08900     0.3238      0.9000     0.8833     63.4440
09000     0.4758      0.8300     0.8833     62.9108
09100     0.5834      0.8300     0.8833     60.8647
09200     0.6396      0.7700     0.8833     62.7482
09300     0.6267      0.8400     0.8833     61.0297
09400     0.5478      0.8200     0.8833     60.9702
09500     0.3566      0.9100     0.8833     60.9986
09600     0.4185      0.9000     0.8833     60.7747
09700     0.4243      0.8700     0.8833     60.8617
09800     0.5500      0.8200     0.8833     60.0284
09900     0.6891      0.7900     0.8934     61.0792
10000     0.5584      0.8100     0.8934     61.5639
10100     0.4736      0.8600     0.8934     62.6372
10200     0.3053      0.9000     0.8934     63.3736
10300     0.3249      0.9100     0.8943     61.8368
10400     0.2945      0.9300     0.8943     61.5189
10500     0.3396      0.9000     0.8943     61.5186
10600     0.3106      0.9300     0.8943     61.3101
10700     0.3338      0.9200     0.8943     63.1053
10800     0.4361      0.8900     0.8955     60.5881
10900     0.3665      0.8800     0.8955     59.6193
11000     0.3596      0.8900     0.8955     58.8082
11100     0.3558      0.9100     0.8958     59.7411
11200     0.3845      0.8900     0.8995     60.0315
11300     0.5947      0.8200     0.8995     60.4994
11400     0.2234      0.9500     0.8995     59.8513
11500     0.2019      0.9500     0.8995     59.3922
11600     0.3213      0.9300     0.8995     59.3077
11700     0.4087      0.8900     0.8995     59.6811
11800     0.4952      0.8600     0.8995     62.0215
11900     0.3172      0.9100     0.8995     61.3360
12000     0.5408      0.8100     0.8995     62.2497
12100     0.2595      0.9300     0.9013     60.5842
12200     0.1727      0.9600     0.9013     60.8215
12300     0.3053      0.9100     0.9013     60.1466
12400     0.2700      0.8800     0.9013     61.9068
12500     0.2854      0.9400     0.9013     61.1240
12600     0.4983      0.8400     0.9013     59.9938
12700     0.3317      0.9100     0.9013     60.8806
12800     0.3229      0.9000     0.9013     61.8272
12900     0.2850      0.9100     0.9013     60.9000
13000     0.3671      0.8700     0.9013     61.8777
13100     0.2785      0.9000     0.9013     61.1214
13200     0.2314      0.9500     0.9013     60.4592
13300     0.3994      0.8900     0.9013     61.1245
13400     0.4369      0.9000     0.9013     59.1868
13500     0.2689      0.9100     0.9029     61.4028
13600     0.2954      0.9000     0.9029     58.8514
13700     0.3596      0.9000     0.9029     59.3349
13800     0.3667      0.9000     0.9029     59.4191
13900     0.2186      0.9500     0.9029     59.1809
14000     0.3324      0.8900     0.9029     59.3392
14100     0.4306      0.8900     0.9029     59.9747
14200     0.2084      0.9300     0.9029     59.0992
14300     0.5544      0.8100     0.9029     58.6758
14400     0.2118      0.9300     0.9029     59.0618
14500     0.5669      0.8200     0.9029     59.8209
14600     0.3940      0.8500     0.9029     59.3090
14700     0.2899      0.9400     0.9029     59.0403
14800     0.3469      0.9100     0.9029     60.1388
14900     0.2229      0.9600     0.9029     59.3379
15000     0.2996      0.9200     0.9029     59.6292
15100     0.2997      0.9000     0.9029     59.1769
15200     0.2480      0.9400     0.9029     60.1871
15300     0.3851      0.9000     0.9029     59.6505
15400     0.2210      0.9100     0.9029     60.0015
15500     0.2127      0.9400     0.9029     58.5503
15600     0.4782      0.8600     0.9029     60.2207
15700     0.5365      0.8500     0.9029     60.0560
15800     0.3507      0.9100     0.9029     59.2126
15900     0.2461      0.9300     0.9029     59.4072
16000     0.2793      0.9200     0.9029     59.9241
16100     0.2910      0.9300     0.9029     62.3045
16200     0.2449      0.9500     0.9029     61.9934
16300     0.2637      0.8800     0.9029     61.3136
16400     0.3609      0.8600     0.9029     60.1388
16500     0.3425      0.8600     0.9029     60.5949
16600     0.3248      0.8800     0.9029     60.3634
16700     0.3012      0.9100     0.9033     60.2328
16800     0.2953      0.9300     0.9033     58.8254
16900     0.3430      0.8600     0.9060     59.3523
17000     0.1938      0.9400     0.9060     58.1194
17100     0.2637      0.9300     0.9060     58.5820
17200     0.3760      0.8500     0.9060     58.3275
17300     0.4327      0.8700     0.9060     60.1990
17400     0.3304      0.9000     0.9060     59.5514
17500     0.3771      0.9000     0.9060     59.9666
17600     0.3195      0.9100     0.9060     59.1049
17700     0.2937      0.9100     0.9060     59.2634
17800     0.2720      0.9300     0.9060     60.1476
17900     0.2785      0.8900     0.9060     59.8976
18000     0.1818      0.9600     0.9060     59.9652
18100     0.3295      0.9000     0.9060     58.6555
18200     0.3825      0.8800     0.9060     58.7018
18300     0.3332      0.8800     0.9060     58.3598
18400     0.3431      0.8900     0.9060     58.6538
18500     0.1714      0.9700     0.9088     59.5398
18600     0.1952      0.9300     0.9088     58.9281
18700     0.2869      0.8900     0.9088     58.4531
18800     0.2756      0.9100     0.9088     58.5817
18900     0.2742      0.9200     0.9088     58.7380
19000     0.2644      0.9500     0.9088     58.4511
19100     0.2810      0.9100     0.9088     58.2969
19200     0.2525      0.9200     0.9088     58.7353
19300     0.2960      0.8900     0.9088     59.4183
19400     0.3078      0.9100     0.9088     58.4728
19500     0.4640      0.8200     0.9088     58.7797
19600     0.3355      0.9000     0.9088     58.5708
19700     0.2701      0.8900     0.9088     58.6302
19800     0.2950      0.9000     0.9088     58.3207
19900     0.3338      0.8900     0.9088     58.6849
20000     0.4781      0.8600     0.9088     58.3960
20100     0.2553      0.9000     0.9088     58.2599
20199     0.3404      0.8700     0.9088     57.6452
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     0.4798      0.8600     0.8742     15.6183
00100     0.2916      0.8900     0.9064     59.3917
00200     0.3395      0.8900     0.9064     58.6008
00300     0.4461      0.8700     0.9100     58.8200
00400     0.2722      0.9300     0.9100     57.8977
00500     0.3751      0.8900     0.9100     58.6874
00600     0.3877      0.9000     0.9100     58.5804
00700     0.3120      0.9000     0.9101     58.9451
00800     0.3320      0.8900     0.9101     59.2563
00900     0.2664      0.9000     0.9101     58.9212
01000     0.3667      0.8900     0.9101     59.0081
01100     0.2138      0.9300     0.9126     60.9367
01200     0.2151      0.9100     0.9126     59.2434
01300     0.2570      0.9600     0.9126     58.7586
01400     0.2344      0.9300     0.9126     58.0370
01500     0.2513      0.9100     0.9126     57.9011
01600     0.4274      0.8400     0.9126     59.2930
01700     0.2833      0.9200     0.9127     59.9108
01800     0.2712      0.9300     0.9127     59.7172
01900     0.4874      0.8500     0.9127     58.8304
02000     0.3214      0.8900     0.9127     58.2914
02100     0.2694      0.9100     0.9127     58.2425
02200     0.2603      0.9200     0.9130     58.6888
02300     0.2971      0.9400     0.9130     58.5355
02400     0.3040      0.9000     0.9130     58.5447
02500     0.1841      0.9500     0.9130     58.9581
02600     0.2818      0.9100     0.9130     58.5110
02700     0.2747      0.9200     0.9130     58.0277
02800     0.2267      0.9300     0.9130     57.9877
02900     0.3486      0.8900     0.9139     59.6290
03000     0.3075      0.9000     0.9139     58.9344
03100     0.2107      0.9400     0.9139     59.6541
03200     0.3137      0.9100     0.9139     60.5594
03300     0.2752      0.8900     0.9139     56.6500
03400     0.5383      0.8400     0.9139     56.0710
03500     0.3422      0.9000     0.9139     56.9884
03600     0.2398      0.9200     0.9139     56.2511
03700     0.2698      0.9300     0.9139     55.7658
03800     0.3403      0.9000     0.9139     56.2082
03900     0.2125      0.9100     0.9139     57.0876
04000     0.4131      0.8500     0.9139     56.7522
04100     0.2850      0.9000     0.9139     57.2811
04200     0.3687      0.8900     0.9139     56.0508
04300     0.3588      0.8500     0.9139     57.3408
04400     0.4288      0.8800     0.9139     57.9213
04500     0.2051      0.9500     0.9139     56.4254
04600     0.2088      0.9400     0.9139     57.2925
04700     0.3609      0.8900     0.9139     56.3481
04800     0.3388      0.9000     0.9139     57.2156
04900     0.3481      0.8900     0.9139     55.5770
05000     0.2425      0.9100     0.9139     56.6423
05100     0.2708      0.9300     0.9140     56.1000
05200     0.2688      0.9200     0.9140     55.9204
05300     0.2918      0.9100     0.9140     57.7651
05400     0.2493      0.9400     0.9140     56.3249
05500     0.2829      0.9000     0.9140     55.9727
05600     0.3785      0.8700     0.9140     56.3227
05700     0.3450      0.9000     0.9140     57.6118
05800     0.2241      0.9300     0.9140     57.0231
05900     0.2939      0.8800     0.9140     56.0066
06000     0.3024      0.9100     0.9140     55.8811
06100     0.3853      0.8800     0.9140     57.5810
06200     0.1498      0.9700     0.9140     56.3988
06300     0.3299      0.8800     0.9140     56.5204
06400     0.3300      0.9300     0.9140     56.2811
06500     0.3408      0.8900     0.9140     55.9051
06600     0.3084      0.9100     0.9140     56.3764
06700     0.2085      0.9600     0.9140     56.6855
06800     0.4077      0.8700     0.9140     55.8650
06900     0.3126      0.9000     0.9140     55.3048
07000     0.2431      0.9100     0.9140     57.6559
07100     0.3732      0.9000     0.9140     55.9436
07200     0.1499      0.9600     0.9140     56.3355
07300     0.3840      0.9100     0.9140     57.5852
07400     0.1520      0.9600     0.9140     56.2776
07500     0.2887      0.9100     0.9140     55.7597
07600     0.3136      0.8800     0.9140     56.3856
07700     0.2200      0.9100     0.9140     56.9243
07800     0.2265      0.9500     0.9140     56.6577
07900     0.2369      0.9100     0.9140     56.9717
08000     0.3461      0.9100     0.9140     56.8124
08100     0.2619      0.9300     0.9140     57.8957
08200     0.2839      0.9200     0.9141     57.8510
08300     0.2259      0.9300     0.9146     56.9523
08400     0.2247      0.9400     0.9146     57.6960
08500     0.2109      0.9600     0.9146     56.5345
08600     0.2900      0.8900     0.9146     56.4270
08700     0.2737      0.9100     0.9146     56.8231
08800     0.1699      0.9400     0.9146     56.3202
08900     0.3060      0.9000     0.9146     56.0959
09000     0.1755      0.9500     0.9146     58.5045
09100     0.2406      0.9400     0.9146     56.6734
09200     0.1543      0.9700     0.9146     57.0398
09300     0.3782      0.8700     0.9146     57.6535
09400     0.2503      0.9400     0.9146     56.8265
09500     0.1881      0.9400     0.9146     55.8369
09600     0.1843      0.9300     0.9146     56.2716
09700     0.2459      0.9200     0.9146     55.9672
09800     0.3732      0.8700     0.9146     55.7348
09900     0.2260      0.9500     0.9146     56.9401
Start testing:
Test Accuracy: 0.9104
