Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 49, in __init__
    torch.nn.init.uniform_(self.cell.weight_ih, a = -limit2, b = limit2)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 49, in __init__
    torch.nn.init.uniform_(self.cell.weight_ih, a = -limit2, b = limit2)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 49, in __init__
    torch.nn.init.uniform_(self.cell.weight_ih, a = -limit2, b = limit2)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 49, in __init__
    torch.nn.init.uniform_(self.cell.weight_ih, a = -limit2, b = limit2)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 279, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.0, finetuning_epochs=0, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.0005,0.0001,0.00002', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,10000', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
5c3c470b-0e75-4505-8fbb-472d44340fe2
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.7903      0.1245     0.1179     14.5292
00100     2.4475      0.1055     0.1179     75.6115
00200     2.4847      0.1139     0.1683     75.1492
00300     2.1778      0.2278     0.2504     75.1740
00400     2.2108      0.1814     0.2504     75.9720
00500     2.0867      0.2637     0.2898     75.2790
00600     2.0631      0.2975     0.3340     76.1184
00700     1.9393      0.3143     0.3340     75.2594
00800     1.8903      0.3312     0.3590     74.9989
00900     1.8503      0.3966     0.3883     76.2656
01000     1.7299      0.4114     0.4235     75.3808
01100     1.7031      0.4156     0.4465     76.9790
01200     1.7268      0.4135     0.4634     76.3656
01300     1.5976      0.4810     0.4819     76.7005
01400     1.6345      0.4346     0.4819     76.9408
01500     1.5464      0.5105     0.5060     77.7587
01600     1.5640      0.5021     0.5060     77.0529
01700     1.5454      0.4768     0.5098     77.1118
01800     1.4254      0.5169     0.5374     77.1098
01900     1.4079      0.5127     0.5391     76.3867
02000     1.4848      0.4768     0.5391     76.0255
02100     1.3599      0.5591     0.5449     75.5125
02200     1.5186      0.4873     0.5661     77.2073
02300     1.4424      0.5063     0.5661     75.9358
02400     1.3017      0.5591     0.5661     77.7168
02500     1.3570      0.5359     0.5744     77.0122
02600     1.4771      0.4916     0.5744     76.7346
02700     1.3517      0.5464     0.6035     76.2484
02800     1.2203      0.5992     0.6050     76.2419
02900     1.3849      0.5506     0.6050     77.1155
03000     1.3416      0.5506     0.6193     77.2998
03100     1.3515      0.5696     0.6193     77.0211
03200     1.1883      0.5823     0.6193     77.0029
03300     1.3053      0.5549     0.6193     77.5649
03400     1.2257      0.6203     0.6234     77.0598
03500     1.2084      0.5823     0.6338     77.6344
03600     1.2184      0.6203     0.6685     77.1075
03700     1.1822      0.5865     0.6685     76.2012
03800     1.1278      0.6287     0.6879     80.3180
03900     1.0270      0.6498     0.7013     76.3782
04000     1.1383      0.6329     0.7013     78.5033
04100     1.0709      0.6371     0.7013     76.4263
04200     1.0755      0.6245     0.7067     76.5025
04300     1.0652      0.6392     0.7067     78.2409
04400     1.1207      0.6266     0.7146     77.9834
04500     1.1080      0.6266     0.7266     77.8451
04600     1.1687      0.6224     0.7266     79.4901
04700     1.1112      0.6435     0.7266     76.6778
04800     1.0539      0.6667     0.7266     77.8656
04900     1.1475      0.6308     0.7266     78.0293
05000     1.0948      0.6646     0.7266     76.0519
05100     1.2718      0.6181     0.7266     78.5549
05200     1.1351      0.6181     0.7266     76.4367
05300     1.0903      0.6456     0.7266     77.0975
05400     1.0802      0.6582     0.7266     78.7465
05500     1.0091      0.6603     0.7266     76.0073
05600     0.9692      0.6751     0.7266     78.2071
05700     1.0154      0.6498     0.7266     79.1000
05800     1.0920      0.6181     0.7266     76.2241
05900     1.0065      0.6941     0.7266     75.9700
06000     0.9482      0.7089     0.7266     76.9203
06100     1.0241      0.6878     0.7266     76.7108
06200     0.9449      0.6624     0.7371     76.8880
06300     0.9583      0.6624     0.7371     77.6268
06400     0.9895      0.6667     0.7371     78.0384
06500     1.1026      0.6624     0.7371     78.0407
06600     0.9687      0.6878     0.7371     78.1647
06700     1.0927      0.6646     0.7371     78.8436
06800     1.0186      0.6624     0.7439     78.4326
06900     1.0401      0.6582     0.7439     77.4396
07000     0.9253      0.6878     0.7439     77.1294
07100     0.9173      0.7131     0.7439     78.2906
07200     1.0891      0.6308     0.7462     78.9782
07300     1.0008      0.6624     0.7532     76.6791
07400     0.9657      0.6920     0.7532     77.7089
07500     0.9196      0.7110     0.7532     77.8157
07600     1.0047      0.6603     0.7532     77.3305
07700     0.9377      0.6772     0.7532     77.0840
07800     0.9685      0.6772     0.7532     76.3561
07900     1.0896      0.6540     0.7532     79.0486
08000     0.9873      0.6772     0.7532     77.2755
08100     1.0096      0.6624     0.7532     78.1357
08200     0.9593      0.6878     0.7532     76.8376
08300     1.0272      0.6814     0.7532     78.8220
08400     0.8611      0.7152     0.7610     76.7001
08500     0.9145      0.6941     0.7610     74.9645
08600     0.9981      0.6878     0.7610     79.1630
08700     1.0136      0.6688     0.7610     75.7471
08800     0.9524      0.6878     0.7610     78.5295
08900     0.8863      0.7236     0.7610     78.6403
09000     1.0742      0.6646     0.7610     75.7870
09100     0.9237      0.7068     0.7610     77.8538
09200     0.9598      0.6835     0.7610     77.1105
09300     0.9856      0.6772     0.7610     76.3039
09400     0.9733      0.6582     0.7610     78.6603
09500     0.9571      0.7068     0.7610     77.8990
09600     1.0836      0.6667     0.7610     78.1592
09700     0.9628      0.7025     0.7610     78.3293
09800     1.0491      0.6540     0.7610     77.5800
09900     0.9808      0.7046     0.7610     78.8471
10000     1.0315      0.6941     0.7610     77.5159
10100     0.9319      0.6983     0.7688     78.1287
10200     0.9225      0.7236     0.7688     76.3001
10300     0.8805      0.7110     0.7688     78.3197
10400     0.8415      0.7257     0.7688     78.1979
10500     0.8990      0.7173     0.7688     77.8615
10600     1.0692      0.6751     0.7688     75.5786
10700     0.9203      0.7173     0.7718     77.0214
10800     0.8959      0.7131     0.7718     76.0897
10900     0.8666      0.7278     0.7718     76.8838
11000     0.9309      0.6899     0.7718     78.0052
11100     0.9479      0.6730     0.7718     78.0227
11200     1.0097      0.6751     0.7718     79.1061
11300     1.0257      0.6667     0.7718     77.3091
11400     0.8842      0.7131     0.7718     76.0670
11500     0.9397      0.6878     0.7718     76.7181
11600     0.9546      0.6899     0.7718     76.6389
11700     0.8588      0.7489     0.7718     76.8114
11800     0.9755      0.6772     0.7718     77.4066
11900     0.9800      0.6730     0.7718     76.2700
12000     0.9233      0.6983     0.7718     77.0964
12100     0.9184      0.6941     0.7718     75.7410
12200     0.9109      0.7046     0.7718     77.7879
12300     0.9236      0.7110     0.7744     79.1608
12400     0.9456      0.7194     0.7782     77.2498
12500     0.9487      0.6688     0.7782     77.3769
12600     0.9612      0.7194     0.7782     77.1419
12700     0.8762      0.6941     0.7782     78.3714
12800     0.7784      0.7384     0.7782     79.2148
12900     0.7945      0.7236     0.7782     77.0162
13000     0.9486      0.7152     0.7782     78.8416
13100     0.9093      0.7131     0.7791     77.5154
13200     0.9252      0.6899     0.7791     76.9149
13300     0.9036      0.7089     0.7791     77.4424
13400     0.9795      0.7131     0.7791     77.7503
13500     0.9699      0.6751     0.7791     75.0126
13600     0.9989      0.6646     0.7791     77.7534
13700     0.8563      0.7321     0.7791     76.2109
13800     0.8626      0.7173     0.7791     78.5334
13900     0.9180      0.7004     0.7791     76.2406
14000     0.8838      0.7152     0.7791     76.7327
14100     0.9003      0.7131     0.7791     76.2813
14200     0.8935      0.7194     0.7791     76.5479
14300     0.8704      0.7046     0.7791     77.0325
14400     0.9282      0.6772     0.7791     77.3683
14500     0.8599      0.7131     0.7791     76.4400
14600     0.9013      0.6983     0.7791     76.7427
14700     0.8909      0.7173     0.7791     77.5014
14800     0.9301      0.7131     0.7796     76.2230
14900     0.8308      0.7426     0.7796     78.0529
15000     0.8917      0.7173     0.7796     78.8910
15100     0.9728      0.6793     0.7796     77.0013
15200     0.9996      0.6857     0.7796     78.8588
15300     0.7800      0.7468     0.7796     78.6075
15400     0.8577      0.7489     0.7848     78.0023
15500     0.9443      0.6962     0.7848     76.2040
15600     1.0222      0.6582     0.7848     77.4474
15700     0.8883      0.7194     0.7848     77.6448
15800     0.8393      0.7194     0.7848     78.7439
15900     1.0198      0.6751     0.7848     77.3002
16000     0.8927      0.7110     0.7848     77.2413
16100     0.9793      0.6793     0.7848     76.4703
16200     0.9484      0.6962     0.7848     77.6989
16300     0.9022      0.7236     0.7848     76.9260
16400     0.8843      0.7046     0.7848     77.4737
16500     0.8925      0.7215     0.7848     78.0494
16600     0.8765      0.7257     0.7848     77.1896
16700     0.9395      0.6962     0.7848     76.6226
16800     1.0324      0.6962     0.7848     78.0778
16900     0.9312      0.7068     0.7848     78.8814
17000     0.9434      0.7004     0.7848     75.8851
17100     0.8359      0.7152     0.7848     77.5117
17200     0.9038      0.7110     0.7848     76.7854
17300     0.9120      0.7089     0.7848     76.5078
17400     0.9796      0.6962     0.7848     76.2031
17500     0.7956      0.7426     0.7858     77.9006
17600     0.9627      0.6793     0.7858     76.7321
17700     0.8301      0.7363     0.7858     78.3790
17800     0.9011      0.7152     0.7858     78.5966
17900     0.8770      0.7131     0.7858     76.8564
18000     0.8505      0.7089     0.7858     77.2548
18100     0.8503      0.7342     0.7858     76.4161
18200     0.8285      0.7426     0.7860     77.0973
18300     0.9296      0.7131     0.7860     77.3131
18400     0.8456      0.7173     0.7860     76.9106
18500     0.8586      0.7215     0.7860     77.4686
18600     0.7850      0.7700     0.7930     77.9142
18700     0.8082      0.7321     0.7930     79.0917
18800     0.7473      0.7595     0.7930     77.6033
18900     0.9742      0.6814     0.7930     76.8230
19000     0.9870      0.6772     0.7930     77.3374
19100     0.8102      0.7468     0.7930     77.4227
19200     0.8734      0.7068     0.7930     77.3494
19300     0.8143      0.7532     0.7930     78.1700
19400     1.0132      0.6709     0.7930     75.8268
19500     0.9333      0.7194     0.7930     77.0175
19600     0.9255      0.6983     0.7949     77.0252
19700     0.8438      0.7110     0.7949     76.6775
19800     0.8469      0.7215     0.7949     76.4667
19900     0.8287      0.7553     0.7949     78.0892
20000     0.9408      0.6941     0.7949     77.2167
20100     0.7660      0.7489     0.7949     75.5609
20200     0.7915      0.7574     0.7949     78.0753
20300     0.9716      0.6983     0.7949     77.7541
20400     0.8355      0.7215     0.7980     76.3168
20500     0.8388      0.7405     0.7980     76.6749
20600     0.8053      0.7532     0.7980     76.8548
20700     0.8197      0.7511     0.7980     77.3598
20800     0.8190      0.7447     0.7980     77.0149
20900     0.8404      0.7342     0.7980     77.7820
21000     0.8800      0.7574     0.7980     76.5738
21100     0.8261      0.7321     0.7980     77.7162
21200     0.7896      0.7468     0.7980     77.4485
21300     0.8707      0.7278     0.7980     78.2511
21400     0.7470      0.7257     0.7980     78.8196
21500     0.7756      0.7468     0.7980     79.4935
21600     0.8736      0.7278     0.7980     77.8510
21700     0.7754      0.7595     0.7980     78.9929
21800     0.8515      0.7131     0.7980     76.4087
21900     0.8727      0.7025     0.7980     77.5503
22000     0.7488      0.7363     0.7980     75.6246
22100     0.8363      0.7257     0.7980     76.7971
22200     0.9152      0.6941     0.7980     77.0479
22300     0.8320      0.7300     0.7980     75.7005
22400     0.9882      0.6899     0.7980     76.8367
22500     0.8647      0.7363     0.7980     75.5905
22600     0.8146      0.7110     0.7980     78.2190
22700     0.9143      0.7068     0.7980     76.4343
22800     0.8397      0.7068     0.7980     76.8900
22900     0.8235      0.7363     0.7980     78.4876
23000     0.8252      0.7532     0.7980     77.5168
23100     0.9104      0.7152     0.7980     77.4738
23200     0.8348      0.7489     0.7980     78.5443
23300     0.9238      0.7046     0.7980     75.5450
23400     0.8153      0.7110     0.7980     77.0698
23500     0.8594      0.6962     0.7980     78.7431
23600     0.9018      0.7025     0.7980     75.6689
23700     0.7963      0.7595     0.7980     77.8041
23800     0.8490      0.7215     0.7980     77.9549
23900     0.7879      0.7321     0.7980     76.7037
24000     0.8690      0.7173     0.7980     76.6561
24100     0.8784      0.7110     0.7980     77.3918
24200     0.8300      0.7342     0.7980     76.6187
24300     0.8614      0.7173     0.7980     77.9674
24400     0.8512      0.7110     0.7980     77.6155
24500     0.8383      0.7236     0.7980     76.5326
24600     0.8633      0.7236     0.7980     77.2281
24700     0.8155      0.7321     0.7980     76.9385
24800     0.8220      0.7595     0.7980     77.0709
24900     0.8491      0.7194     0.7980     77.8387
25000     0.8467      0.7532     0.7980     76.8997
25100     0.8440      0.7384     0.7980     77.1723
25200     0.8294      0.7532     0.7980     78.0794
25300     0.8617      0.7152     0.7980     76.1261
25400     0.8782      0.7321     0.7980     77.2975
25500     0.8994      0.7068     0.7980     77.2066
25600     0.8716      0.7152     0.7980     75.1923
25700     0.8127      0.7405     0.7980     76.4602
25800     0.8080      0.7405     0.7980     76.3090
25900     0.8227      0.7068     0.7980     77.4296
26000     0.7878      0.7595     0.7980     78.1119
26100     0.8044      0.7511     0.7980     76.2214
26200     0.9117      0.7426     0.7980     76.2332
26300     0.8418      0.7342     0.7980     77.8804
26400     0.8281      0.7447     0.7980     75.5957
26500     0.8945      0.7004     0.7980     76.2738
26600     0.7894      0.7511     0.7980     77.5720
26700     0.7808      0.7405     0.7980     77.2604
26800     0.8189      0.7468     0.7980     77.0966
26900     0.8331      0.7468     0.7980     77.5337
27000     0.8801      0.7278     0.7980     77.7295
27100     0.9307      0.7173     0.7980     77.8236
27200     0.8591      0.7236     0.7980     77.0947
27300     0.9345      0.7046     0.7980     76.5979
27400     0.9499      0.6920     0.7992     76.8747
27500     0.8526      0.7215     0.7992     78.2515
27600     0.8308      0.7405     0.7992     77.0804
27700     0.9760      0.6730     0.7992     76.2844
27800     0.8923      0.7173     0.7992     76.7049
27900     0.9248      0.7278     0.7992     75.6628
28000     0.7595      0.7511     0.7992     77.6016
28100     0.8490      0.7384     0.7992     77.7006
28200     0.7868      0.7468     0.7992     76.7156
28300     0.8345      0.7363     0.7992     80.2479
28400     0.7610      0.7700     0.7992     77.9750
28500     0.8617      0.7257     0.7992     76.5450
28600     0.8090      0.7532     0.7992     77.3993
28700     0.8147      0.7489     0.7992     77.1222
28800     0.8282      0.7468     0.7992     75.7588
28900     0.8292      0.7426     0.7992     76.6322
29000     0.9627      0.6835     0.7992     76.5773
29100     0.8509      0.7173     0.7992     76.9440
29200     0.8968      0.7068     0.7992     76.1715
29300     0.8566      0.7152     0.7992     76.1153
29400     0.7897      0.7489     0.7992     78.6909
29500     0.8276      0.7300     0.7992     72.7451
29600     0.8419      0.7468     0.7992     76.8046
29700     0.7952      0.7574     0.7992     77.3371
29800     0.8145      0.7300     0.7992     75.6692
29900     0.9317      0.7025     0.7992     78.1991
29999     0.9193      0.7025     0.7992     75.6499
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Start testing:
Test Accuracy: 0.7793
