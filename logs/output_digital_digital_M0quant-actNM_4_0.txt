Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 49, in __init__
    torch.nn.init.uniform_(self.cell.weight_ih, a = -limit2, b = limit2)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 49, in __init__
    torch.nn.init.uniform_(self.cell.weight_ih, a = -limit2, b = limit2)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 49, in __init__
    torch.nn.init.uniform_(self.cell.weight_ih, a = -limit2, b = limit2)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 49, in __init__
    torch.nn.init.uniform_(self.cell.weight_ih, a = -limit2, b = limit2)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 279, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.0, finetuning_epochs=0, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.0005,0.0001,0.00002', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,10000', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
5c3c470b-0e75-4505-8fbb-472d44340fe2
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.7903      0.1245     0.1179     14.5292
00100     2.4475      0.1055     0.1179     75.6115
00200     2.4847      0.1139     0.1683     75.1492
00300     2.1778      0.2278     0.2504     75.1740
00400     2.2108      0.1814     0.2504     75.9720
00500     2.0867      0.2637     0.2898     75.2790
00600     2.0631      0.2975     0.3340     76.1184
00700     1.9393      0.3143     0.3340     75.2594
00800     1.8903      0.3312     0.3590     74.9989
00900     1.8503      0.3966     0.3883     76.2656
01000     1.7299      0.4114     0.4235     75.3808
01100     1.7031      0.4156     0.4465     76.9790
01200     1.7268      0.4135     0.4634     76.3656
01300     1.5976      0.4810     0.4819     76.7005
01400     1.6345      0.4346     0.4819     76.9408
01500     1.5464      0.5105     0.5060     77.7587
01600     1.5640      0.5021     0.5060     77.0529
01700     1.5454      0.4768     0.5098     77.1118
01800     1.4254      0.5169     0.5374     77.1098
01900     1.4079      0.5127     0.5391     76.3867
02000     1.4848      0.4768     0.5391     76.0255
02100     1.3599      0.5591     0.5449     75.5125
02200     1.5186      0.4873     0.5661     77.2073
02300     1.4424      0.5063     0.5661     75.9358
02400     1.3017      0.5591     0.5661     77.7168
02500     1.3570      0.5359     0.5744     77.0122
02600     1.4771      0.4916     0.5744     76.7346
02700     1.3517      0.5464     0.6035     76.2484
02800     1.2203      0.5992     0.6050     76.2419
02900     1.3849      0.5506     0.6050     77.1155
03000     1.3416      0.5506     0.6193     77.2998
03100     1.3515      0.5696     0.6193     77.0211
03200     1.1883      0.5823     0.6193     77.0029
03300     1.3053      0.5549     0.6193     77.5649
03400     1.2257      0.6203     0.6234     77.0598
03500     1.2084      0.5823     0.6338     77.6344
03600     1.2184      0.6203     0.6685     77.1075
03700     1.1822      0.5865     0.6685     76.2012
03800     1.1278      0.6287     0.6879     80.3180
03900     1.0270      0.6498     0.7013     76.3782
04000     1.1383      0.6329     0.7013     78.5033
04100     1.0709      0.6371     0.7013     76.4263
04200     1.0755      0.6245     0.7067     76.5025
04300     1.0652      0.6392     0.7067     78.2409
04400     1.1207      0.6266     0.7146     77.9834
04500     1.1080      0.6266     0.7266     77.8451
04600     1.1687      0.6224     0.7266     79.4901
04700     1.1112      0.6435     0.7266     76.6778
04800     1.0539      0.6667     0.7266     77.8656
04900     1.1475      0.6308     0.7266     78.0293
05000     1.0948      0.6646     0.7266     76.0519
05100     1.2718      0.6181     0.7266     78.5549
05200     1.1351      0.6181     0.7266     76.4367
05300     1.0903      0.6456     0.7266     77.0975
05400     1.0802      0.6582     0.7266     78.7465
05500     1.0091      0.6603     0.7266     76.0073
05600     0.9692      0.6751     0.7266     78.2071
05700     1.0154      0.6498     0.7266     79.1000
05800     1.0920      0.6181     0.7266     76.2241
05900     1.0065      0.6941     0.7266     75.9700
06000     0.9482      0.7089     0.7266     76.9203
06100     1.0241      0.6878     0.7266     76.7108
06200     0.9449      0.6624     0.7371     76.8880
06300     0.9583      0.6624     0.7371     77.6268
06400     0.9895      0.6667     0.7371     78.0384
06500     1.1026      0.6624     0.7371     78.0407
06600     0.9687      0.6878     0.7371     78.1647
06700     1.0927      0.6646     0.7371     78.8436
06800     1.0186      0.6624     0.7439     78.4326
06900     1.0401      0.6582     0.7439     77.4396
07000     0.9253      0.6878     0.7439     77.1294
07100     0.9173      0.7131     0.7439     78.2906
07200     1.0891      0.6308     0.7462     78.9782
07300     1.0008      0.6624     0.7532     76.6791
07400     0.9657      0.6920     0.7532     77.7089
07500     0.9196      0.7110     0.7532     77.8157
07600     1.0047      0.6603     0.7532     77.3305
07700     0.9377      0.6772     0.7532     77.0840
07800     0.9685      0.6772     0.7532     76.3561
07900     1.0896      0.6540     0.7532     79.0486
08000     0.9873      0.6772     0.7532     77.2755
08100     1.0096      0.6624     0.7532     78.1357
08200     0.9593      0.6878     0.7532     76.8376
08300     1.0272      0.6814     0.7532     78.8220
08400     0.8611      0.7152     0.7610     76.7001
08500     0.9145      0.6941     0.7610     74.9645
08600     0.9981      0.6878     0.7610     79.1630
08700     1.0136      0.6688     0.7610     75.7471
08800     0.9524      0.6878     0.7610     78.5295
08900     0.8863      0.7236     0.7610     78.6403
09000     1.0742      0.6646     0.7610     75.7870
09100     0.9237      0.7068     0.7610     77.8538
09200     0.9598      0.6835     0.7610     77.1105
09300     0.9856      0.6772     0.7610     76.3039
09400     0.9733      0.6582     0.7610     78.6603
09500     0.9571      0.7068     0.7610     77.8990
09600     1.0836      0.6667     0.7610     78.1592
09700     0.9628      0.7025     0.7610     78.3293
09800     1.0491      0.6540     0.7610     77.5800
09900     0.9808      0.7046     0.7610     78.8471
10000     1.0315      0.6941     0.7610     77.5159
10100     0.9319      0.6983     0.7688     78.1287
10200     0.9225      0.7236     0.7688     76.3001
10300     0.8805      0.7110     0.7688     78.3197
10400     0.8415      0.7257     0.7688     78.1979
10500     0.8990      0.7173     0.7688     77.8615
10600     1.0692      0.6751     0.7688     75.5786
10700     0.9203      0.7173     0.7718     77.0214
10800     0.8959      0.7131     0.7718     76.0897
10900     0.8666      0.7278     0.7718     76.8838
11000     0.9309      0.6899     0.7718     78.0052
11100     0.9479      0.6730     0.7718     78.0227
11200     1.0097      0.6751     0.7718     79.1061
11300     1.0257      0.6667     0.7718     77.3091
11400     0.8842      0.7131     0.7718     76.0670
11500     0.9397      0.6878     0.7718     76.7181
11600     0.9546      0.6899     0.7718     76.6389
11700     0.8588      0.7489     0.7718     76.8114
11800     0.9755      0.6772     0.7718     77.4066
11900     0.9800      0.6730     0.7718     76.2700
12000     0.9233      0.6983     0.7718     77.0964
12100     0.9184      0.6941     0.7718     75.7410
12200     0.9109      0.7046     0.7718     77.7879
12300     0.9236      0.7110     0.7744     79.1608
12400     0.9456      0.7194     0.7782     77.2498
12500     0.9487      0.6688     0.7782     77.3769
12600     0.9612      0.7194     0.7782     77.1419
12700     0.8762      0.6941     0.7782     78.3714
12800     0.7784      0.7384     0.7782     79.2148
12900     0.7945      0.7236     0.7782     77.0162
13000     0.9486      0.7152     0.7782     78.8416
13100     0.9093      0.7131     0.7791     77.5154
13200     0.9252      0.6899     0.7791     76.9149
13300     0.9036      0.7089     0.7791     77.4424
13400     0.9795      0.7131     0.7791     77.7503
13500     0.9699      0.6751     0.7791     75.0126
13600     0.9989      0.6646     0.7791     77.7534
13700     0.8563      0.7321     0.7791     76.2109
13800     0.8626      0.7173     0.7791     78.5334
13900     0.9180      0.7004     0.7791     76.2406
14000     0.8838      0.7152     0.7791     76.7327
14100     0.9003      0.7131     0.7791     76.2813
14200     0.8935      0.7194     0.7791     76.5479
14300     0.8704      0.7046     0.7791     77.0325
14400     0.9282      0.6772     0.7791     77.3683
14500     0.8599      0.7131     0.7791     76.4400
14600     0.9013      0.6983     0.7791     76.7427
14700     0.8909      0.7173     0.7791     77.5014
14800     0.9301      0.7131     0.7796     76.2230
14900     0.8308      0.7426     0.7796     78.0529
15000     0.8917      0.7173     0.7796     78.8910
15100     0.9728      0.6793     0.7796     77.0013
15200     0.9996      0.6857     0.7796     78.8588
15300     0.7800      0.7468     0.7796     78.6075
15400     0.8577      0.7489     0.7848     78.0023
15500     0.9443      0.6962     0.7848     76.2040
15600     1.0222      0.6582     0.7848     77.4474
15700     0.8883      0.7194     0.7848     77.6448
15800     0.8393      0.7194     0.7848     78.7439
15900     1.0198      0.6751     0.7848     77.3002
16000     0.8927      0.7110     0.7848     77.2413
16100     0.9793      0.6793     0.7848     76.4703
16200     0.9484      0.6962     0.7848     77.6989
16300     0.9022      0.7236     0.7848     76.9260
16400     0.8843      0.7046     0.7848     77.4737
16500     0.8925      0.7215     0.7848     78.0494
16600     0.8765      0.7257     0.7848     77.1896
16700     0.9395      0.6962     0.7848     76.6226
16800     1.0324      0.6962     0.7848     78.0778
16900     0.9312      0.7068     0.7848     78.8814
17000     0.9434      0.7004     0.7848     75.8851
17100     0.8359      0.7152     0.7848     77.5117
17200     0.9038      0.7110     0.7848     76.7854
17300     0.9120      0.7089     0.7848     76.5078
17400     0.9796      0.6962     0.7848     76.2031
17500     0.7956      0.7426     0.7858     77.9006
17600     0.9627      0.6793     0.7858     76.7321
17700     0.8301      0.7363     0.7858     78.3790
17800     0.9011      0.7152     0.7858     78.5966
17900     0.8770      0.7131     0.7858     76.8564
18000     0.8505      0.7089     0.7858     77.2548
18100     0.8503      0.7342     0.7858     76.4161
18200     0.8285      0.7426     0.7860     77.0973
18300     0.9296      0.7131     0.7860     77.3131
18400     0.8456      0.7173     0.7860     76.9106
18500     0.8586      0.7215     0.7860     77.4686
18600     0.7850      0.7700     0.7930     77.9142
18700     0.8082      0.7321     0.7930     79.0917
18800     0.7473      0.7595     0.7930     77.6033
18900     0.9742      0.6814     0.7930     76.8230
19000     0.9870      0.6772     0.7930     77.3374
19100     0.8102      0.7468     0.7930     77.4227
19200     0.8734      0.7068     0.7930     77.3494
19300     0.8143      0.7532     0.7930     78.1700
19400     1.0132      0.6709     0.7930     75.8268
19500     0.9333      0.7194     0.7930     77.0175
19600     0.9255      0.6983     0.7949     77.0252
19700     0.8438      0.7110     0.7949     76.6775
19800     0.8469      0.7215     0.7949     76.4667
19900     0.8287      0.7553     0.7949     78.0892
20000     0.9408      0.6941     0.7949     77.2167
20100     0.7660      0.7489     0.7949     75.5609
20200     0.7915      0.7574     0.7949     78.0753
20300     0.9716      0.6983     0.7949     77.7541
20400     0.8355      0.7215     0.7980     76.3168
20500     0.8388      0.7405     0.7980     76.6749
20600     0.8053      0.7532     0.7980     76.8548
20700     0.8197      0.7511     0.7980     77.3598
20800     0.8190      0.7447     0.7980     77.0149
20900     0.8404      0.7342     0.7980     77.7820
21000     0.8800      0.7574     0.7980     76.5738
21100     0.8261      0.7321     0.7980     77.7162
21200     0.7896      0.7468     0.7980     77.4485
21300     0.8707      0.7278     0.7980     78.2511
21400     0.7470      0.7257     0.7980     78.8196
21500     0.7756      0.7468     0.7980     79.4935
21600     0.8736      0.7278     0.7980     77.8510
21700     0.7754      0.7595     0.7980     78.9929
21800     0.8515      0.7131     0.7980     76.4087
21900     0.8727      0.7025     0.7980     77.5503
22000     0.7488      0.7363     0.7980     75.6246
22100     0.8363      0.7257     0.7980     76.7971
22200     0.9152      0.6941     0.7980     77.0479
22300     0.8320      0.7300     0.7980     75.7005
22400     0.9882      0.6899     0.7980     76.8367
22500     0.8647      0.7363     0.7980     75.5905
22600     0.8146      0.7110     0.7980     78.2190
22700     0.9143      0.7068     0.7980     76.4343
22800     0.8397      0.7068     0.7980     76.8900
22900     0.8235      0.7363     0.7980     78.4876
23000     0.8252      0.7532     0.7980     77.5168
23100     0.9104      0.7152     0.7980     77.4738
23200     0.8348      0.7489     0.7980     78.5443
23300     0.9238      0.7046     0.7980     75.5450
23400     0.8153      0.7110     0.7980     77.0698
23500     0.8594      0.6962     0.7980     78.7431
23600     0.9018      0.7025     0.7980     75.6689
23700     0.7963      0.7595     0.7980     77.8041
23800     0.8490      0.7215     0.7980     77.9549
23900     0.7879      0.7321     0.7980     76.7037
24000     0.8690      0.7173     0.7980     76.6561
24100     0.8784      0.7110     0.7980     77.3918
24200     0.8300      0.7342     0.7980     76.6187
24300     0.8614      0.7173     0.7980     77.9674
24400     0.8512      0.7110     0.7980     77.6155
24500     0.8383      0.7236     0.7980     76.5326
24600     0.8633      0.7236     0.7980     77.2281
24700     0.8155      0.7321     0.7980     76.9385
24800     0.8220      0.7595     0.7980     77.0709
24900     0.8491      0.7194     0.7980     77.8387
25000     0.8467      0.7532     0.7980     76.8997
25100     0.8440      0.7384     0.7980     77.1723
25200     0.8294      0.7532     0.7980     78.0794
25300     0.8617      0.7152     0.7980     76.1261
25400     0.8782      0.7321     0.7980     77.2975
25500     0.8994      0.7068     0.7980     77.2066
25600     0.8716      0.7152     0.7980     75.1923
25700     0.8127      0.7405     0.7980     76.4602
25800     0.8080      0.7405     0.7980     76.3090
25900     0.8227      0.7068     0.7980     77.4296
26000     0.7878      0.7595     0.7980     78.1119
26100     0.8044      0.7511     0.7980     76.2214
26200     0.9117      0.7426     0.7980     76.2332
26300     0.8418      0.7342     0.7980     77.8804
26400     0.8281      0.7447     0.7980     75.5957
26500     0.8945      0.7004     0.7980     76.2738
26600     0.7894      0.7511     0.7980     77.5720
26700     0.7808      0.7405     0.7980     77.2604
26800     0.8189      0.7468     0.7980     77.0966
26900     0.8331      0.7468     0.7980     77.5337
27000     0.8801      0.7278     0.7980     77.7295
27100     0.9307      0.7173     0.7980     77.8236
27200     0.8591      0.7236     0.7980     77.0947
27300     0.9345      0.7046     0.7980     76.5979
27400     0.9499      0.6920     0.7992     76.8747
27500     0.8526      0.7215     0.7992     78.2515
27600     0.8308      0.7405     0.7992     77.0804
27700     0.9760      0.6730     0.7992     76.2844
27800     0.8923      0.7173     0.7992     76.7049
27900     0.9248      0.7278     0.7992     75.6628
28000     0.7595      0.7511     0.7992     77.6016
28100     0.8490      0.7384     0.7992     77.7006
28200     0.7868      0.7468     0.7992     76.7156
28300     0.8345      0.7363     0.7992     80.2479
28400     0.7610      0.7700     0.7992     77.9750
28500     0.8617      0.7257     0.7992     76.5450
28600     0.8090      0.7532     0.7992     77.3993
28700     0.8147      0.7489     0.7992     77.1222
28800     0.8282      0.7468     0.7992     75.7588
28900     0.8292      0.7426     0.7992     76.6322
29000     0.9627      0.6835     0.7992     76.5773
29100     0.8509      0.7173     0.7992     76.9440
29200     0.8968      0.7068     0.7992     76.1715
29300     0.8566      0.7152     0.7992     76.1153
29400     0.7897      0.7489     0.7992     78.6909
29500     0.8276      0.7300     0.7992     72.7451
29600     0.8419      0.7468     0.7992     76.8046
29700     0.7952      0.7574     0.7992     77.3371
29800     0.8145      0.7300     0.7992     75.6692
29900     0.9317      0.7025     0.7992     78.1991
29999     0.9193      0.7025     0.7992     75.6499
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Start testing:
Test Accuracy: 0.7793
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
d8d29015-a18b-4d91-a231-7c1cb90cf7fb
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.7957      0.0970     0.1010     12.1995
00100     2.3317      0.1793     0.1010     73.6919
00200     2.1265      0.2954     0.1010     71.4892
00300     2.1740      0.2300     0.1105     70.8869
00400     1.8418      0.3882     0.1161     70.7496
00500     1.6783      0.4219     0.1161     70.2457
00600     1.5974      0.4831     0.1303     71.9163
00700     1.4786      0.4916     0.1595     71.6749
00800     1.2894      0.5464     0.1595     70.5973
00900     1.2938      0.5549     0.1595     69.8174
01000     1.1921      0.5992     0.1602     70.4007
01100     1.2138      0.5844     0.1602     70.1103
01200     1.2147      0.6034     0.1602     69.8663
01300     1.2395      0.5886     0.1602     69.4600
01400     1.2236      0.6139     0.1604     69.6425
01500     1.1775      0.6076     0.1604     69.8621
01600     1.0667      0.6329     0.1765     70.0902
01700     0.9225      0.6962     0.1765     69.0657
01800     0.9631      0.6751     0.1765     71.2076
01900     1.0428      0.6540     0.1765     69.7099
02000     1.0030      0.6730     0.1765     70.2400
02100     0.9426      0.6857     0.1765     70.2644
02200     1.0328      0.6772     0.1765     70.3531
02300     1.0399      0.6835     0.1765     70.1299
02400     0.9680      0.7025     0.1824     69.5039
02500     1.0344      0.6435     0.1824     70.2186
02600     0.8739      0.7236     0.1876     69.8315
02700     0.9147      0.7300     0.1896     70.2761
02800     0.8645      0.7131     0.1896     69.6134
02900     0.9779      0.7110     0.1896     69.7344
03000     0.9928      0.7131     0.1912     69.5959
03100     0.8788      0.7152     0.1912     69.6735
03200     0.9341      0.7046     0.2033     70.3267
03300     0.9439      0.7004     0.2033     70.8272
03400     0.9402      0.7004     0.2033     69.2632
03500     0.9165      0.7173     0.2033     69.3219
03600     0.8386      0.7236     0.2137     69.7602
03700     0.8992      0.7068     0.2199     70.4023
03800     0.9289      0.7025     0.2199     70.2415
03900     0.9869      0.6962     0.2218     69.9130
04000     0.9112      0.7004     0.2280     70.9374
04100     0.7742      0.7553     0.2280     70.2534
04200     0.7911      0.7679     0.2309     70.5048
04300     0.9763      0.6624     0.2341     70.0862
04400     0.8644      0.7321     0.2413     70.5570
04500     0.8341      0.7300     0.2413     70.1940
04600     0.9833      0.6983     0.2413     69.2414
04700     0.6765      0.7911     0.2413     69.7512
04800     0.8577      0.7363     0.2413     70.1627
04900     0.8805      0.7257     0.2413     70.4960
05000     0.8119      0.7300     0.2413     70.9120
05100     0.7790      0.7405     0.2413     70.0950
05200     0.7829      0.7342     0.2899     69.1307
05300     0.9172      0.6983     0.2899     70.8836
05400     0.8391      0.7511     0.2899     72.9927
05500     0.9847      0.6899     0.2899     73.5865
05600     0.8209      0.7152     0.2899     74.3513
05700     0.7079      0.7722     0.2899     73.5508
05800     0.9511      0.7068     0.2899     74.5670
05900     0.8844      0.7278     0.2899     74.4911
06000     0.8751      0.7278     0.2899     72.8381
06100     0.9115      0.7089     0.2916     72.3060
06200     0.7775      0.7616     0.2916     72.7375
06300     0.8622      0.7215     0.2977     72.5143
06400     0.7849      0.7489     0.2977     73.0554
06500     0.7439      0.7743     0.3292     72.9736
06600     0.8347      0.7447     0.3292     72.7881
06700     0.8483      0.7426     0.3292     72.9237
06800     0.7419      0.7722     0.3530     73.0158
06900     0.8371      0.7595     0.3639     71.5732
07000     0.8086      0.7426     0.3639     72.1131
07100     0.8712      0.7426     0.3852     72.1591
07200     0.8547      0.7236     0.3852     73.0582
07300     0.7418      0.7700     0.3852     71.9756
07400     0.9110      0.7110     0.3852     72.2107
07500     0.7329      0.7700     0.3852     73.3048
07600     0.8012      0.7384     0.3852     74.1320
07700     0.7505      0.7869     0.3852     72.9583
07800     0.7557      0.7489     0.3852     72.6416
07900     0.7110      0.7700     0.3852     73.4911
08000     0.7925      0.7532     0.3852     72.5994
08100     0.7753      0.7532     0.3993     71.6282
08200     0.8574      0.7468     0.3993     71.5710
08300     0.7526      0.7869     0.3995     72.9275
08400     0.7784      0.7405     0.3995     72.6002
08500     0.6269      0.8059     0.3995     73.0933
08600     0.7490      0.7637     0.3995     71.9181
08700     0.7343      0.7616     0.3995     71.5981
08800     0.7073      0.7911     0.3995     73.7583
08900     0.7440      0.7511     0.3995     71.8214
09000     0.7360      0.7700     0.3995     72.5745
09100     0.6541      0.8101     0.3995     72.1843
09200     0.7948      0.7658     0.4001     71.8164
09300     0.7053      0.7785     0.4001     71.7304
09400     0.8940      0.7363     0.4347     72.4373
09500     0.7170      0.7827     0.4347     71.7188
09600     0.7990      0.7616     0.4347     73.7074
09700     0.7753      0.7447     0.4365     72.6245
09800     0.7457      0.7848     0.4365     72.3233
09900     0.6791      0.7996     0.4472     72.8521
10000     0.7197      0.7764     0.4486     72.8151
10100     0.6442      0.7848     0.4575     72.8993
10200     0.7670      0.7743     0.4584     73.1976
10300     0.6797      0.7827     0.4584     72.9396
10400     0.6194      0.8038     0.4584     73.8503
10500     0.6689      0.7806     0.4584     72.2756
10600     0.5661      0.8270     0.4584     73.7732
10700     0.6656      0.7869     0.4584     72.2345
10800     0.6958      0.7890     0.4584     72.3224
10900     0.6911      0.7954     0.4584     71.9338
11000     0.6599      0.7996     0.4584     73.8167
11100     0.6639      0.7848     0.4584     73.3878
11200     0.6736      0.8017     0.4584     72.7221
11300     0.6914      0.7996     0.4584     72.3828
11400     0.6921      0.7869     0.4586     72.2697
11500     0.6544      0.7869     0.4586     72.3799
11600     0.7642      0.7700     0.4586     72.0760
11700     0.6635      0.8101     0.4586     71.7543
11800     0.6075      0.7996     0.4586     73.0487
11900     0.6691      0.7954     0.4586     72.6106
12000     0.6936      0.7869     0.4684     73.1748
12100     0.6944      0.7679     0.4684     72.1989
12200     0.6892      0.7869     0.4684     73.6040
12300     0.7485      0.7595     0.4684     72.4474
12400     0.5903      0.8165     0.4684     73.9834
12500     0.6479      0.7806     0.4684     73.3799
12600     0.6319      0.8038     0.4684     73.3957
12700     0.5993      0.8228     0.4684     72.9052
12800     0.7322      0.7574     0.4684     72.5972
12900     0.7320      0.7869     0.4684     71.6885
13000     0.7104      0.7932     0.4684     73.0602
13100     0.6882      0.7700     0.4684     72.4840
13200     0.7151      0.7595     0.4684     73.4500
13300     0.6977      0.7785     0.4684     72.3714
13400     0.6545      0.8080     0.4684     72.6568
13500     0.5715      0.8165     0.4684     72.5794
13600     0.6895      0.7827     0.4684     72.8735
13700     0.6976      0.7932     0.4684     72.6799
13800     0.6984      0.7827     0.4684     72.5603
13900     0.6306      0.7890     0.4684     72.7026
14000     0.6952      0.7869     0.4684     71.8009
14100     0.5481      0.8354     0.4684     71.9556
14200     0.6443      0.8038     0.4684     71.9399
14300     0.7064      0.7679     0.4684     70.9619
14400     0.7028      0.7722     0.4684     72.6781
14500     0.6685      0.7911     0.4684     72.6420
14600     0.6351      0.7975     0.4684     73.3119
14700     0.6126      0.8101     0.4684     73.2509
14800     0.6715      0.7932     0.4684     71.7486
14900     0.6115      0.7911     0.4684     71.8644
15000     0.5454      0.8165     0.4684     72.9533
15100     0.6627      0.7890     0.4684     72.4339
15200     0.5754      0.8122     0.4684     73.3871
15300     0.6848      0.7785     0.4684     72.3911
15400     0.6456      0.8270     0.4684     71.9523
15500     0.7007      0.7700     0.4684     72.7427
15600     0.6038      0.8165     0.4684     72.5213
15700     0.6853      0.7932     0.4684     72.2329
15800     0.6685      0.7890     0.4684     72.0612
15900     0.6283      0.8207     0.4684     71.7653
16000     0.6891      0.7869     0.4684     73.8271
16100     0.6957      0.7806     0.4684     72.5273
16200     0.7030      0.7806     0.4684     72.9860
16300     0.6146      0.7954     0.4684     72.7952
16400     0.7103      0.7658     0.4684     72.6623
16500     0.6583      0.7954     0.4684     72.6855
16600     0.7830      0.7405     0.4684     72.6907
16700     0.7169      0.7827     0.4684     73.1349
16800     0.6741      0.7911     0.4684     72.3218
16900     0.7563      0.7574     0.4684     73.3658
17000     0.7368      0.7848     0.4684     72.0161
17100     0.6612      0.8080     0.4684     72.5429
17200     0.6373      0.7932     0.4684     72.7589
17300     0.6459      0.7764     0.4684     72.0880
17400     0.6673      0.8165     0.4684     71.5095
17500     0.6550      0.7890     0.4684     72.7105
17600     0.6868      0.7785     0.4684     71.6043
17700     0.6633      0.7848     0.4684     72.5296
17800     0.7063      0.7996     0.4684     72.5474
17900     0.6710      0.7827     0.4684     72.6776
18000     0.7342      0.7785     0.4684     73.1586
18100     0.7525      0.7806     0.4684     72.3813
18200     0.6543      0.7932     0.4684     72.6002
18300     0.7559      0.7447     0.4684     72.5007
18400     0.6667      0.8080     0.4684     72.8932
18500     0.6122      0.7996     0.4684     72.5439
18600     0.5879      0.8249     0.4684     72.6226
18700     0.6991      0.7911     0.4684     72.5222
18800     0.6961      0.7911     0.4684     72.0348
18900     0.7553      0.7679     0.4684     72.7888
19000     0.6282      0.7954     0.4684     73.4971
19100     0.6996      0.7722     0.4684     73.2408
19200     0.7109      0.7806     0.4684     73.6454
19300     0.6821      0.7722     0.4684     72.9987
19400     0.6661      0.7890     0.4684     72.6112
19500     0.6248      0.7932     0.4684     73.4360
19600     0.6759      0.7827     0.4684     72.1091
19700     0.6772      0.7954     0.4684     73.0143
19800     0.6331      0.8101     0.4684     72.2088
19900     0.6966      0.7806     0.4684     72.7563
20000     0.6915      0.7806     0.4684     72.8505
20100     0.5717      0.8186     0.4684     71.8475
20199     0.6243      0.7911     0.4684     72.4748
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.5442      0.3460     0.4333     10.3528
00100     1.5451      0.5169     0.5910     73.2129
00200     1.3825      0.5422     0.6427     72.4257
00300     1.3092      0.6181     0.6787     71.8348
00400     1.2774      0.6118     0.6906     71.7576
00500     1.1131      0.6603     0.7017     71.6735
00600     1.1977      0.6392     0.7017     72.5499
00700     1.2412      0.6160     0.7268     72.5390
00800     1.0946      0.6983     0.7268     71.9501
00900     1.2826      0.6477     0.7268     73.3078
01000     1.2813      0.6118     0.7278     73.4625
01100     1.0275      0.6920     0.7278     71.6973
01200     1.1364      0.6941     0.7319     72.7259
01300     1.2374      0.6519     0.7319     71.9367
01400     1.1843      0.6371     0.7319     72.8739
01500     1.1010      0.6561     0.7391     72.7552
01600     1.1101      0.6456     0.7391     73.0810
01700     1.1010      0.6646     0.7391     72.0328
01800     1.0444      0.6983     0.7391     73.7436
01900     1.0243      0.7025     0.7391     72.7876
02000     1.0413      0.6793     0.7397     71.9468
02100     1.1704      0.6667     0.7454     71.7118
02200     1.1887      0.6477     0.7454     73.3518
02300     1.1412      0.6667     0.7454     72.4832
02400     0.9719      0.7131     0.7454     71.2981
02500     1.0288      0.6814     0.7454     72.1523
02600     1.0209      0.6941     0.7562     72.8440
02700     0.9933      0.7089     0.7562     72.1689
02800     1.0104      0.6793     0.7562     72.2552
02900     1.0998      0.6751     0.7562     72.2521
03000     0.9638      0.7046     0.7562     72.6593
03100     1.0163      0.6857     0.7562     71.8511
03200     1.0325      0.7004     0.7562     73.9350
03300     1.0977      0.6730     0.7562     72.8444
03400     1.0426      0.6962     0.7562     72.4427
03500     1.2032      0.6624     0.7619     72.9795
03600     0.9541      0.7194     0.7619     73.6930
03700     1.0098      0.7236     0.7619     72.1325
03800     1.0462      0.6730     0.7619     72.0267
03900     1.0601      0.6814     0.7619     72.4720
04000     0.9981      0.7152     0.7619     73.0696
04100     1.0288      0.6899     0.7619     72.4555
04200     0.9625      0.7363     0.7619     71.8363
04300     1.0048      0.6962     0.7619     72.4730
04400     0.9307      0.7046     0.7619     72.3940
04500     0.9107      0.7447     0.7644     73.7759
04600     1.0033      0.7004     0.7644     72.4787
04700     1.0546      0.6899     0.7644     74.0916
04800     1.0435      0.6814     0.7644     72.6006
04900     1.0857      0.6899     0.7650     73.0679
05000     1.0121      0.6835     0.7650     72.6165
05100     1.0510      0.6561     0.7650     73.1197
05200     0.9065      0.7342     0.7650     71.9617
05300     1.0210      0.6983     0.7671     72.4912
05400     1.1849      0.6624     0.7760     73.8337
05500     0.9924      0.7068     0.7760     72.0782
05600     0.9512      0.6962     0.7760     71.8515
05700     1.0495      0.6772     0.7760     73.1037
05800     0.9816      0.6920     0.7760     71.5724
05900     0.9907      0.6688     0.7760     72.7898
06000     1.0327      0.6688     0.7760     72.1567
06100     1.0455      0.7046     0.7760     72.7268
06200     0.9941      0.6772     0.7760     73.0013
06300     0.9694      0.7004     0.7760     72.1156
06400     0.9325      0.7131     0.7760     72.2220
06500     0.9102      0.7257     0.7760     72.4367
06600     1.0779      0.6730     0.7760     72.8545
06700     0.9727      0.7257     0.7760     73.7442
06800     0.9888      0.7110     0.7760     71.8201
06900     1.0468      0.6624     0.7760     71.9425
07000     1.0251      0.6793     0.7760     72.0759
07100     1.1671      0.6835     0.7760     71.5207
07200     1.0582      0.6878     0.7760     73.0585
07300     0.8364      0.7363     0.7760     71.9568
07400     0.9267      0.7025     0.7760     72.1087
07500     0.9303      0.7068     0.7760     72.6862
07600     1.0427      0.6751     0.7760     72.4179
07700     0.9509      0.6920     0.7760     72.3609
07800     0.9268      0.7089     0.7760     73.4172
07900     1.0845      0.6878     0.7760     71.7824
08000     0.9644      0.7046     0.7760     72.6500
08100     1.0045      0.7300     0.7760     71.8328
08200     0.8560      0.7321     0.7760     72.1350
08300     0.9438      0.7089     0.7760     75.5596
08400     0.9119      0.7215     0.7760     73.6414
08500     1.0199      0.7152     0.7760     68.7879
08600     0.9637      0.6857     0.7760     76.1163
08700     0.8353      0.7215     0.7760     74.8448
08800     1.0537      0.6899     0.7760     72.9474
08900     0.9320      0.7110     0.7760     72.6882
09000     0.9763      0.6835     0.7760     71.0455
09100     0.8969      0.7236     0.7760     72.2179
09200     0.9987      0.6751     0.7812     73.9916
09300     0.9850      0.6814     0.7812     71.7590
09400     0.9716      0.6920     0.7812     72.0859
09500     0.9688      0.6962     0.7812     70.6788
09600     0.9631      0.7025     0.7812     71.4322
09700     0.8795      0.7131     0.7812     71.0578
09800     0.8993      0.7194     0.7812     70.9329
09900     0.9274      0.7321     0.7812     69.9423
Start testing:
Test Accuracy: 0.8062
