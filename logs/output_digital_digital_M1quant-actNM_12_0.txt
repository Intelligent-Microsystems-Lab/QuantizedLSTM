Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=12, quant_actNM=12, quant_inp=12, quant_w=12, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
6854e1a7-848b-42cc-8ee5-d9f4933e604c
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 373, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, 1.)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 82, in forward
    if len(x_range) > 1:
TypeError: object of type 'float' has no len()
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=12, quant_actNM=12, quant_inp=12, quant_w=12, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
d2ebdd4f-effe-459e-a096-f0c8e48bcb37
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 373, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, torch.tensor([1.]).device(input.device))
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 94, in forward
    x_scaled = x/x_range
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=12, quant_actNM=12, quant_inp=12, quant_w=12, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
ca2c684d-d807-4467-8280-a508b972ef0a
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     3.5373      0.0970     0.0902     11.0328
00100     2.2285      0.2194     0.2401     58.9724
00200     1.7543      0.4114     0.4383     57.8172
00300     1.5383      0.4852     0.5335     57.9866
00400     1.3226      0.5928     0.5936     57.4475
00500     1.2334      0.6076     0.6344     56.8669
00600     1.0354      0.6772     0.6683     57.7812
00700     1.1743      0.6498     0.6936     56.9426
00800     1.0687      0.6414     0.7064     57.5702
00900     0.9885      0.6646     0.7161     56.3890
01000     1.0217      0.6561     0.7289     56.3458
01100     0.8539      0.7236     0.7329     56.3828
01200     0.8922      0.7152     0.7329     55.8758
01300     0.9429      0.7004     0.7459     56.2411
01400     0.7896      0.7700     0.7574     60.6205
01500     0.9060      0.7110     0.7586     58.3652
01600     0.9003      0.7194     0.7586     58.5000
01700     0.8583      0.7278     0.7657     58.7243
01800     0.8104      0.7511     0.7764     58.7560
01900     0.8144      0.7426     0.7764     60.1631
02000     0.7460      0.7658     0.7781     58.5659
02100     0.7739      0.7405     0.7859     59.8694
02200     0.8323      0.7131     0.7859     61.5036
02300     0.7383      0.7743     0.7985     57.1343
02400     0.7812      0.7363     0.7985     59.2273
02500     0.6984      0.7954     0.7985     57.6860
02600     0.7968      0.7426     0.7985     58.4671
02700     0.7104      0.7743     0.7985     59.0636
02800     0.6686      0.7785     0.7988     59.0865
02900     0.7491      0.7595     0.7988     57.5241
03000     0.6975      0.7764     0.7988     58.0885
03100     0.6831      0.7764     0.8000     58.5869
03200     0.7601      0.7700     0.8033     58.9944
03300     0.6515      0.8038     0.8089     58.9237
03400     0.7648      0.7574     0.8118     60.2817
03500     0.7170      0.7553     0.8118     57.8525
03600     0.6560      0.7869     0.8118     57.9689
03700     0.6417      0.7996     0.8118     57.6112
03800     0.7628      0.7595     0.8134     60.2348
03900     0.5840      0.8207     0.8212     58.9161
04000     0.5444      0.8397     0.8212     58.5958
04100     0.5883      0.8207     0.8212     57.1523
04200     0.7268      0.7679     0.8212     57.4449
04300     0.6426      0.7827     0.8212     57.8491
04400     0.6385      0.8143     0.8212     57.1550
04500     0.6436      0.7890     0.8212     56.7862
04600     0.5786      0.8228     0.8212     58.4518
04700     0.6057      0.8207     0.8212     58.1864
04800     0.6547      0.7848     0.8212     58.1557
04900     0.6244      0.8143     0.8245     58.7973
05000     0.6822      0.7869     0.8245     58.0135
05100     0.6400      0.7954     0.8245     57.2068
05200     0.5854      0.8207     0.8245     57.9153
05300     0.6092      0.8122     0.8245     58.1108
05400     0.5500      0.8397     0.8245     59.9570
05500     0.6872      0.7890     0.8264     59.8295
05600     0.5544      0.8333     0.8264     58.4220
05700     0.6264      0.7975     0.8310     57.9689
05800     0.6917      0.7954     0.8351     59.6481
05900     0.5270      0.8333     0.8351     60.0916
06000     0.5879      0.8228     0.8351     57.4983
06100     0.6092      0.8038     0.8351     59.0320
06200     0.6051      0.8207     0.8351     59.4009
06300     0.5086      0.8354     0.8362     57.6138
06400     0.5871      0.8249     0.8362     59.3355
06500     0.6090      0.8101     0.8362     58.9018
06600     0.6087      0.8038     0.8362     59.7270
06700     0.6137      0.8080     0.8362     58.5493
06800     0.5019      0.8481     0.8362     58.9753
06900     0.5463      0.8397     0.8362     57.2503
07000     0.5592      0.8186     0.8362     58.9298
07100     0.5459      0.7975     0.8362     58.2479
07200     0.5902      0.8080     0.8362     58.8986
07300     0.6178      0.8017     0.8362     59.5811
07400     0.5980      0.8080     0.8362     57.5921
07500     0.6189      0.8017     0.8362     59.9158
07600     0.6085      0.8165     0.8362     58.7836
07700     0.5944      0.8080     0.8362     58.3637
07800     0.5723      0.8291     0.8362     59.0611
07900     0.6376      0.7932     0.8362     57.2817
08000     0.5794      0.8228     0.8362     60.0759
08100     0.6031      0.8186     0.8373     57.5193
08200     0.6102      0.8270     0.8397     57.7039
08300     0.6057      0.8101     0.8409     59.6588
08400     0.5772      0.8038     0.8409     59.4264
08500     0.5863      0.8207     0.8409     60.9404
08600     0.5541      0.8249     0.8409     60.2645
08700     0.4936      0.8397     0.8422     57.2180
08800     0.5946      0.8122     0.8422     57.1065
08900     0.4815      0.8502     0.8422     56.8114
09000     0.5663      0.8186     0.8422     56.3329
09100     0.5796      0.8080     0.8422     58.1389
09200     0.5791      0.8122     0.8456     57.3258
09300     0.5407      0.8460     0.8479     61.7771
09400     0.5310      0.8270     0.8479     56.6730
09500     0.5445      0.8101     0.8479     58.6065
09600     0.5112      0.8671     0.8479     57.9954
09700     0.5719      0.8207     0.8562     59.3774
09800     0.4848      0.8376     0.8562     57.2040
09900     0.5935      0.7975     0.8562     57.3913
10000     0.5594      0.8165     0.8562     58.4287
10100     0.5141      0.8565     0.8562     59.7789
10200     0.5266      0.8228     0.8562     60.5065
10300     0.4952      0.8608     0.8562     57.5010
10400     0.5210      0.8397     0.8562     59.5586
10500     0.5653      0.8017     0.8562     60.2099
10600     0.5124      0.8354     0.8562     59.2171
10700     0.5422      0.8270     0.8562     59.1335
10800     0.5196      0.8397     0.8562     59.5130
10900     0.6030      0.7996     0.8562     59.7203
11000     0.4922      0.8502     0.8562     59.2201
11100     0.5602      0.8312     0.8562     57.9367
11200     0.4455      0.8460     0.8562     59.1592
11300     0.4574      0.8608     0.8562     57.6808
11400     0.5789      0.8080     0.8562     58.3039
11500     0.4461      0.8671     0.8562     58.4947
11600     0.5077      0.8397     0.8562     60.5390
11700     0.4893      0.8439     0.8562     61.2442
11800     0.4898      0.8565     0.8562     57.9335
11900     0.5786      0.8270     0.8562     57.5912
12000     0.5014      0.8270     0.8562     59.1252
12100     0.5829      0.8059     0.8562     60.1701
12200     0.5398      0.8397     0.8562     59.0334
12300     0.5587      0.8312     0.8562     60.4990
12400     0.4721      0.8439     0.8562     57.3647
12500     0.5013      0.8565     0.8562     59.8246
12600     0.5514      0.8333     0.8562     57.7511
12700     0.5630      0.8291     0.8562     58.0470
12800     0.5152      0.8565     0.8562     60.6298
12900     0.4852      0.8354     0.8562     58.2309
13000     0.6007      0.8038     0.8562     59.5100
13100     0.4268      0.8629     0.8562     59.7866
13200     0.4655      0.8692     0.8562     59.2151
13300     0.4897      0.8439     0.8562     59.6278
13400     0.4655      0.8586     0.8562     59.7689
13500     0.4659      0.8629     0.8562     61.0422
13600     0.4053      0.8734     0.8562     57.7911
13700     0.4779      0.8397     0.8562     58.4135
13800     0.5232      0.8291     0.8562     58.3984
13900     0.4605      0.8481     0.8562     59.7431
14000     0.4649      0.8586     0.8562     57.9384
14100     0.4444      0.8481     0.8562     57.9259
14200     0.5046      0.8502     0.8562     58.4720
14300     0.5152      0.8418     0.8562     57.2664
14400     0.5057      0.8312     0.8562     59.1103
14500     0.4681      0.8502     0.8562     59.1823
14600     0.4388      0.8650     0.8562     58.6967
14700     0.4510      0.8671     0.8562     60.6506
14800     0.5466      0.8439     0.8562     58.8835
14900     0.5249      0.8249     0.8562     60.6351
15000     0.5851      0.8502     0.8562     58.9939
15100     0.4927      0.8397     0.8562     59.1346
15200     0.5002      0.8565     0.8562     61.1514
15300     0.5588      0.8270     0.8562     58.6005
15400     0.4806      0.8629     0.8562     58.7112
15500     0.5145      0.8312     0.8562     58.7042
15600     0.4813      0.8586     0.8562     60.3165
15700     0.5817      0.8143     0.8562     58.2791
15800     0.5109      0.8460     0.8583     60.3452
15900     0.5401      0.8270     0.8583     58.6805
16000     0.5120      0.8333     0.8583     58.6119
16100     0.5200      0.8460     0.8583     60.3818
16200     0.5147      0.8397     0.8583     60.2097
16300     0.5588      0.8207     0.8583     59.1359
16400     0.5145      0.8481     0.8583     60.6137
16500     0.6308      0.7954     0.8583     61.7152
16600     0.5654      0.8249     0.8583     60.9380
16700     0.5026      0.8439     0.8583     60.7806
16800     0.5119      0.8418     0.8583     60.5751
16900     0.4887      0.8333     0.8583     59.7492
17000     0.5372      0.8291     0.8583     60.4751
17100     0.5122      0.8650     0.8583     59.6673
17200     0.5042      0.8418     0.8583     58.3504
17300     0.5198      0.8207     0.8583     60.7493
17400     0.4607      0.8692     0.8583     59.2063
17500     0.5612      0.8418     0.8583     59.9146
17600     0.4847      0.8354     0.8583     59.6922
17700     0.4600      0.8439     0.8583     59.8446
17800     0.4851      0.8354     0.8583     59.1926
17900     0.5277      0.8291     0.8583     59.7480
18000     0.5035      0.8397     0.8583     59.4644
18100     0.5521      0.8122     0.8583     60.6612
18200     0.5884      0.8249     0.8583     59.4506
18300     0.4305      0.8713     0.8583     59.0020
18400     0.5138      0.8397     0.8583     59.3918
18500     0.4777      0.8629     0.8583     58.4900
18600     0.4950      0.8333     0.8583     59.5954
18700     0.5168      0.8397     0.8583     60.3443
18800     0.4541      0.8671     0.8583     58.3282
18900     0.4682      0.8523     0.8583     59.8643
19000     0.5049      0.8397     0.8583     58.9987
19100     0.5343      0.8291     0.8583     58.9739
19200     0.4877      0.8418     0.8583     59.3628
19300     0.5237      0.8228     0.8583     59.3125
19400     0.4667      0.8481     0.8583     61.0212
19500     0.5544      0.8143     0.8583     60.6190
19600     0.5109      0.8481     0.8583     58.8969
19700     0.4583      0.8460     0.8583     59.8971
19800     0.5546      0.8291     0.8583     59.5625
19900     0.5313      0.8249     0.8583     59.9227
20000     0.5441      0.8376     0.8583     60.3120
20100     0.4897      0.8481     0.8583     60.0372
20199     0.4454      0.8776     0.8583     59.5074
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     0.4457      0.8629     0.8513     10.0516
00100     0.4095      0.8671     0.8568     58.1512
00200     0.4060      0.8734     0.8585     58.9426
00300     0.4879      0.8629     0.8585     59.1974
00400     0.4145      0.8776     0.8585     58.3352
00500     0.3935      0.8797     0.8626     58.9622
00600     0.4173      0.8776     0.8626     59.7798
00700     0.4119      0.8650     0.8626     59.2347
00800     0.4429      0.8481     0.8626     60.8729
00900     0.4902      0.8481     0.8626     59.6297
01000     0.3765      0.8945     0.8626     59.8383
01100     0.3342      0.8987     0.8626     59.2047
01200     0.3887      0.8692     0.8626     58.6385
01300     0.4480      0.8544     0.8626     59.0339
01400     0.4251      0.8671     0.8626     59.6872
01500     0.5013      0.8333     0.8626     58.6936
01600     0.4657      0.8565     0.8626     59.7827
01700     0.3599      0.9051     0.8626     58.8253
01800     0.4565      0.8586     0.8626     59.7373
01900     0.4239      0.8776     0.8626     60.8643
02000     0.3719      0.8861     0.8634     58.8167
02100     0.4089      0.8692     0.8634     59.5980
02200     0.3829      0.8692     0.8634     59.4180
02300     0.4039      0.8797     0.8634     58.1537
02400     0.3885      0.8840     0.8634     60.4365
02500     0.4116      0.8692     0.8635     58.9605
02600     0.4214      0.8692     0.8635     59.5310
02700     0.4315      0.8608     0.8635     59.3456
02800     0.4742      0.8565     0.8635     60.0038
02900     0.4569      0.8544     0.8635     60.5798
03000     0.4326      0.8671     0.8635     59.5920
03100     0.4332      0.8671     0.8635     59.5328
03200     0.4135      0.8671     0.8635     59.6823
03300     0.4130      0.8650     0.8635     59.8119
03400     0.4269      0.8734     0.8635     60.9963
03500     0.3736      0.8882     0.8635     60.3031
03600     0.4033      0.8692     0.8635     58.7811
03700     0.4003      0.8692     0.8635     60.5656
03800     0.3858      0.8840     0.8635     60.3184
03900     0.4360      0.8481     0.8635     59.8922
04000     0.4242      0.8734     0.8635     60.2085
04100     0.4460      0.8481     0.8641     60.6661
04200     0.4313      0.8755     0.8641     59.5460
04300     0.3883      0.8903     0.8641     59.6866
04400     0.4220      0.8671     0.8641     60.2569
04500     0.4052      0.8671     0.8641     59.5936
04600     0.5099      0.8249     0.8641     59.8321
04700     0.4035      0.8840     0.8641     59.0231
04800     0.4705      0.8544     0.8652     59.6445
04900     0.4843      0.8481     0.8652     60.4020
05000     0.4125      0.8776     0.8652     59.7199
05100     0.3934      0.8797     0.8652     60.3613
05200     0.4278      0.8755     0.8652     59.1791
05300     0.4288      0.8692     0.8652     59.7620
05400     0.3909      0.8819     0.8652     60.2466
05500     0.4666      0.8397     0.8652     59.3300
05600     0.4225      0.8713     0.8652     61.3524
05700     0.3533      0.8924     0.8652     59.6114
05800     0.3950      0.8671     0.8652     59.2760
05900     0.4250      0.8629     0.8652     59.7575
06000     0.3455      0.8903     0.8652     60.1583
06100     0.3892      0.8734     0.8652     59.2165
06200     0.4900      0.8397     0.8652     59.6106
06300     0.4133      0.8650     0.8652     60.5879
06400     0.4640      0.8502     0.8652     60.3164
06500     0.4592      0.8418     0.8652     60.1210
06600     0.3829      0.8586     0.8652     59.7705
06700     0.4918      0.8460     0.8652     60.8618
06800     0.3711      0.8755     0.8652     59.9179
06900     0.3682      0.8966     0.8652     59.3981
07000     0.4308      0.8776     0.8652     60.1637
07100     0.4358      0.8734     0.8652     60.0732
07200     0.3911      0.8734     0.8652     59.5587
07300     0.4509      0.8460     0.8653     58.4131
07400     0.4463      0.8608     0.8653     59.4053
07500     0.4621      0.8671     0.8653     60.8770
07600     0.4810      0.8439     0.8653     57.1318
07700     0.3952      0.8797     0.8653     58.2729
07800     0.3821      0.8692     0.8653     60.2665
07900     0.3431      0.9072     0.8653     58.7284
08000     0.3846      0.8903     0.8653     60.5328
08100     0.4152      0.8776     0.8653     61.2212
08200     0.4067      0.8608     0.8653     59.7426
08300     0.3886      0.8713     0.8653     60.8111
08400     0.4116      0.8502     0.8653     59.6824
08500     0.4255      0.8734     0.8653     59.7157
08600     0.3991      0.8734     0.8653     60.0702
08700     0.5091      0.8397     0.8653     60.0615
08800     0.4111      0.8523     0.8653     60.9505
08900     0.4151      0.8713     0.8653     60.1659
09000     0.3708      0.8945     0.8653     59.8946
09100     0.3708      0.8819     0.8653     59.7014
09200     0.4681      0.8565     0.8653     59.6681
09300     0.4438      0.8586     0.8653     58.6802
09400     0.4262      0.8650     0.8653     60.0856
09500     0.4016      0.8755     0.8653     59.2842
09600     0.3948      0.8671     0.8653     60.3161
09700     0.3801      0.8629     0.8669     59.3193
09800     0.4422      0.8629     0.8669     59.6335
09900     0.4518      0.8354     0.8669     60.0973
Start testing:
Test Accuracy: 0.8452
