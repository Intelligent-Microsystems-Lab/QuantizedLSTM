Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=11, quant_actNM=11, quant_inp=11, quant_w=11, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
e8d0bf59-1fe3-49c5-bbaa-0ba9f4ac886f
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, 1.)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 82, in forward
    if len(x_range) > 1:
TypeError: object of type 'float' has no len()
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=11, quant_actNM=11, quant_inp=11, quant_w=11, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
7a836e3f-f07e-46a2-889a-3ebf36a156de
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, torch.tensor([1.]))
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 94, in forward
    x_scaled = x/x_range
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=11, quant_actNM=11, quant_inp=11, quant_w=11, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
456f8606-aaf3-481e-87f3-aec888102bc3
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, torch.tensor([1.]).to(input.device))
TypeError: 'torch.device' object is not callable
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=11, quant_actNM=11, quant_inp=11, quant_w=11, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
4530bbb2-e641-4539-b587-fe2e3a402ae5
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.8259      0.0675     0.0890     11.0863
00100     2.3457      0.2489     0.2918     55.3648
00200     1.8198      0.4219     0.4539     54.8554
00300     1.4509      0.5316     0.5436     54.8190
00400     1.3001      0.5992     0.6170     55.0090
00500     1.1765      0.6456     0.6605     54.2081
00600     1.0991      0.6667     0.6824     55.7965
00700     0.9632      0.7173     0.7214     54.1951
00800     0.9728      0.7110     0.7279     54.6910
00900     1.0147      0.6814     0.7408     54.7869
01000     0.9084      0.7278     0.7538     55.2589
01100     0.9521      0.7300     0.7538     55.4452
01200     0.9248      0.7342     0.7651     54.7152
01300     0.8151      0.7658     0.7740     57.7296
01400     0.9295      0.7131     0.7778     60.6913
01500     0.7769      0.7532     0.7785     60.1126
01600     0.8381      0.7532     0.7785     58.0964
01700     0.7491      0.7637     0.7875     57.1872
01800     0.8056      0.7532     0.7875     55.4492
01900     0.7213      0.7890     0.7989     55.7166
02000     0.7685      0.7764     0.7989     55.2080
02100     0.6860      0.7890     0.7989     55.6759
02200     0.6989      0.7932     0.7990     58.0425
02300     0.7793      0.7426     0.7990     54.9975
02400     0.6605      0.8249     0.7992     56.5164
02500     0.7697      0.7743     0.8053     54.8437
02600     0.7951      0.7722     0.8088     55.6220
02700     0.7111      0.7827     0.8111     54.9536
02800     0.6761      0.7890     0.8151     55.6844
02900     0.8101      0.7468     0.8151     55.6915
03000     0.7127      0.7890     0.8151     56.1198
03100     0.7414      0.7700     0.8222     55.4959
03200     0.6842      0.7954     0.8222     55.0822
03300     0.6146      0.8291     0.8222     55.2586
03400     0.7170      0.7764     0.8222     54.3432
03500     0.7206      0.7743     0.8239     56.2316
03600     0.6694      0.7890     0.8239     54.7992
03700     0.7065      0.7722     0.8239     55.3811
03800     0.6892      0.7890     0.8239     55.0900
03900     0.7338      0.7827     0.8239     55.7770
04000     0.6615      0.7911     0.8239     55.1064
04100     0.6149      0.8207     0.8239     54.7167
04200     0.7120      0.7616     0.8263     54.4358
04300     0.6517      0.8080     0.8263     55.2481
04400     0.6687      0.7869     0.8263     53.8559
04500     0.6248      0.8228     0.8263     55.1416
04600     0.6741      0.8080     0.8297     55.8152
04700     0.5553      0.8354     0.8297     54.3161
04800     0.6068      0.8165     0.8297     55.3622
04900     0.6090      0.8143     0.8354     54.9424
05000     0.7100      0.7869     0.8354     55.5645
05100     0.6635      0.8122     0.8354     56.0720
05200     0.6149      0.8186     0.8354     54.9914
05300     0.6598      0.8270     0.8354     54.5120
05400     0.6609      0.8038     0.8354     54.7304
05500     0.6254      0.8143     0.8354     55.1275
05600     0.5882      0.8249     0.8354     54.9710
05700     0.6782      0.7890     0.8354     54.2796
05800     0.6865      0.7996     0.8354     55.0064
05900     0.5624      0.8270     0.8354     56.0855
06000     0.6599      0.7869     0.8354     55.2458
06100     0.5873      0.8418     0.8354     55.7393
06200     0.5856      0.8228     0.8354     55.7261
06300     0.5998      0.8270     0.8354     56.2536
06400     0.5699      0.8270     0.8363     55.7221
06500     0.7126      0.7911     0.8363     55.1881
06600     0.6159      0.8186     0.8363     54.9298
06700     0.6122      0.8228     0.8363     55.0942
06800     0.5570      0.8186     0.8433     55.1443
06900     0.5850      0.8439     0.8433     55.2419
07000     0.6663      0.8249     0.8433     55.8350
07100     0.6723      0.7975     0.8433     54.5341
07200     0.5719      0.8354     0.8433     55.3569
07300     0.6726      0.7827     0.8433     55.1595
07400     0.5826      0.8207     0.8433     55.6819
07500     0.5611      0.8165     0.8433     55.6739
07600     0.6301      0.8207     0.8433     54.3847
07700     0.5317      0.8228     0.8438     54.6572
07800     0.5542      0.8312     0.8438     54.9830
07900     0.6663      0.7996     0.8438     54.6687
08000     0.6367      0.7954     0.8438     56.2935
08100     0.6479      0.8059     0.8438     55.4968
08200     0.5705      0.8228     0.8438     57.0250
08300     0.5328      0.8376     0.8438     54.9903
08400     0.6031      0.8270     0.8438     55.2376
08500     0.5940      0.8165     0.8438     54.7788
08600     0.6123      0.8038     0.8438     55.4805
08700     0.5714      0.8312     0.8438     55.4445
08800     0.6154      0.8059     0.8438     54.8399
08900     0.6235      0.8165     0.8438     54.9453
09000     0.5306      0.8586     0.8438     55.1556
09100     0.5223      0.8376     0.8438     54.9851
09200     0.6205      0.8207     0.8438     54.1926
09300     0.5154      0.8608     0.8438     54.1097
09400     0.5438      0.8376     0.8438     55.0089
09500     0.6483      0.7932     0.8438     55.7877
09600     0.5612      0.8249     0.8438     55.6625
09700     0.5506      0.8312     0.8438     56.6481
09800     0.5934      0.8186     0.8438     54.8536
09900     0.5531      0.8439     0.8438     55.7564
10000     0.5423      0.8481     0.8438     56.1065
10100     0.5832      0.8249     0.8438     56.6522
10200     0.5603      0.8376     0.8438     54.4915
10300     0.5295      0.8523     0.8438     54.8162
10400     0.5183      0.8376     0.8438     56.0649
10500     0.5318      0.8376     0.8449     56.5033
10600     0.5292      0.8418     0.8449     55.4971
10700     0.5398      0.8460     0.8449     55.8028
10800     0.5767      0.8460     0.8449     55.0775
10900     0.4900      0.8586     0.8449     54.5691
11000     0.4893      0.8565     0.8449     54.8713
11100     0.5811      0.8207     0.8454     54.7425
11200     0.4541      0.8608     0.8454     55.3841
11300     0.5662      0.8228     0.8454     56.1269
11400     0.5745      0.8291     0.8454     56.0877
11500     0.5656      0.8291     0.8454     56.2768
11600     0.5717      0.8249     0.8463     55.4044
11700     0.6053      0.8228     0.8463     54.6899
11800     0.5508      0.8439     0.8463     55.0919
11900     0.5277      0.8460     0.8463     54.4869
12000     0.5371      0.8418     0.8463     55.1808
12100     0.5229      0.8586     0.8463     54.6386
12200     0.5114      0.8249     0.8463     54.9617
12300     0.5722      0.8291     0.8463     55.9931
12400     0.5830      0.8228     0.8463     56.3761
12500     0.5574      0.8397     0.8463     56.4320
12600     0.5006      0.8502     0.8463     55.9275
12700     0.5878      0.8270     0.8463     56.4052
12800     0.5169      0.8502     0.8463     55.5451
12900     0.5255      0.8291     0.8463     55.1809
13000     0.5069      0.8397     0.8463     56.3248
13100     0.5440      0.8376     0.8463     55.7150
13200     0.5331      0.8481     0.8463     55.4336
13300     0.5526      0.8460     0.8463     54.9324
13400     0.6217      0.8101     0.8463     55.0262
13500     0.5282      0.8312     0.8463     55.7662
13600     0.5581      0.8418     0.8463     54.2876
13700     0.4879      0.8544     0.8474     54.6739
13800     0.4979      0.8544     0.8474     55.0667
13900     0.4988      0.8544     0.8474     55.0236
14000     0.5214      0.8586     0.8474     56.4416
14100     0.5754      0.8291     0.8474     52.5102
14200     0.5006      0.8502     0.8474     53.8069
14300     0.5062      0.8692     0.8474     54.5244
14400     0.5847      0.8270     0.8474     55.1577
14500     0.6208      0.8270     0.8474     54.0514
14600     0.5823      0.8270     0.8474     54.1453
14700     0.5105      0.8397     0.8474     55.2513
14800     0.5921      0.8249     0.8474     54.6625
14900     0.5414      0.8376     0.8474     54.6511
15000     0.5187      0.8586     0.8474     54.8333
15100     0.5229      0.8629     0.8474     54.4442
15200     0.5552      0.8312     0.8474     55.6773
15300     0.5376      0.8439     0.8496     54.9011
15400     0.5614      0.8228     0.8496     54.7319
15500     0.6124      0.8080     0.8496     55.0729
15600     0.5422      0.8502     0.8496     55.2752
15700     0.5391      0.8376     0.8496     54.2693
15800     0.6067      0.7911     0.8496     56.0603
15900     0.4941      0.8629     0.8523     54.6839
16000     0.5067      0.8397     0.8523     54.6753
16100     0.6211      0.7996     0.8523     55.7564
16200     0.5489      0.8333     0.8523     55.6233
16300     0.6135      0.7996     0.8523     56.0440
16400     0.5462      0.8228     0.8523     55.1861
16500     0.4969      0.8460     0.8523     54.9915
16600     0.5270      0.8397     0.8523     56.3789
16700     0.4896      0.8502     0.8523     56.0266
16800     0.5142      0.8460     0.8523     55.6163
16900     0.5920      0.8122     0.8523     55.2773
17000     0.5899      0.8143     0.8523     55.1905
17100     0.5544      0.8460     0.8523     55.1625
17200     0.6136      0.8122     0.8523     55.3525
17300     0.5675      0.8080     0.8523     54.6776
17400     0.5237      0.8460     0.8523     54.8590
17500     0.6028      0.8186     0.8523     55.2457
17600     0.5788      0.8354     0.8523     56.8844
17700     0.5221      0.8418     0.8523     55.5605
17800     0.5471      0.8291     0.8523     55.0584
17900     0.5162      0.8586     0.8523     55.4393
18000     0.5593      0.8397     0.8523     56.2466
18100     0.4965      0.8544     0.8523     56.7428
18200     0.5932      0.8165     0.8523     56.4726
18300     0.5100      0.8544     0.8523     54.8988
18400     0.5557      0.8333     0.8523     55.1212
18500     0.4756      0.8629     0.8523     54.7662
18600     0.6143      0.8059     0.8523     54.8851
18700     0.5346      0.8523     0.8523     55.3463
18800     0.5143      0.8418     0.8523     56.0163
18900     0.5510      0.8291     0.8523     56.4744
19000     0.4697      0.8755     0.8523     55.8330
19100     0.4970      0.8523     0.8523     55.4752
19200     0.4931      0.8481     0.8523     55.6193
19300     0.5934      0.8228     0.8523     54.9140
19400     0.5476      0.8397     0.8523     54.3217
19500     0.5153      0.8481     0.8523     54.8993
19600     0.5373      0.8502     0.8523     54.2999
19700     0.4896      0.8671     0.8523     55.9294
19800     0.5943      0.8059     0.8523     54.8168
19900     0.5540      0.8397     0.8523     54.7645
20000     0.6412      0.8122     0.8523     56.3265
20100     0.5278      0.8755     0.8523     56.4227
20199     0.5997      0.8228     0.8523     56.1187
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     0.4877      0.8650     0.8501     8.7192
00100     0.3432      0.9072     0.8550     57.4019
00200     0.4952      0.8460     0.8550     54.5843
00300     0.4868      0.8523     0.8550     54.5915
00400     0.4142      0.8861     0.8550     54.6339
00500     0.4099      0.8776     0.8564     54.5663
00600     0.4850      0.8713     0.8564     55.1097
00700     0.4204      0.8819     0.8564     54.7736
00800     0.3951      0.8966     0.8564     56.3455
00900     0.4889      0.8608     0.8564     55.8639
01000     0.3903      0.9008     0.8564     55.5737
01100     0.4743      0.8481     0.8564     54.1944
01200     0.5139      0.8502     0.8564     54.1835
01300     0.4407      0.8734     0.8564     55.1069
01400     0.4921      0.8565     0.8564     56.0103
01500     0.4544      0.8523     0.8564     54.5463
01600     0.4858      0.8650     0.8565     55.1766
01700     0.4833      0.8650     0.8565     55.1248
01800     0.3986      0.8882     0.8565     54.8796
01900     0.3965      0.8755     0.8565     55.6454
02000     0.4548      0.8565     0.8572     55.3582
02100     0.4212      0.8797     0.8572     55.0520
02200     0.3992      0.8903     0.8583     56.4407
02300     0.4033      0.8819     0.8583     55.9654
02400     0.3840      0.8903     0.8583     55.1291
02500     0.4785      0.8523     0.8583     54.8180
02600     0.4585      0.8608     0.8583     56.1144
02700     0.3744      0.8882     0.8592     55.9331
02800     0.4405      0.8629     0.8592     55.3675
02900     0.4101      0.8755     0.8592     54.9018
03000     0.4581      0.8565     0.8592     55.0923
03100     0.4817      0.8586     0.8592     55.6034
03200     0.4060      0.8797     0.8592     55.6257
03300     0.3918      0.8840     0.8592     56.2762
03400     0.4032      0.8861     0.8592     56.2328
03500     0.4878      0.8692     0.8601     55.5469
03600     0.4333      0.8692     0.8601     54.5378
03700     0.5389      0.8291     0.8607     55.1124
03800     0.4351      0.8882     0.8607     54.6860
03900     0.3891      0.8903     0.8607     55.5155
04000     0.4978      0.8608     0.8607     55.1643
04100     0.4258      0.8629     0.8607     54.6949
04200     0.4432      0.8692     0.8607     55.8663
04300     0.4125      0.8797     0.8607     55.1606
04400     0.4118      0.8755     0.8607     57.5083
04500     0.4446      0.8671     0.8607     56.5838
04600     0.4885      0.8544     0.8607     55.6030
04700     0.4820      0.8481     0.8607     54.7213
04800     0.4018      0.9051     0.8607     54.8260
04900     0.4852      0.8608     0.8607     55.9239
05000     0.4449      0.8565     0.8607     55.2286
05100     0.4081      0.8861     0.8607     55.4952
05200     0.4429      0.8586     0.8607     53.5665
05300     0.4078      0.8819     0.8607     55.9949
05400     0.4966      0.8629     0.8607     56.3170
05500     0.4027      0.8819     0.8607     54.9639
05600     0.4263      0.8776     0.8607     55.8355
05700     0.4578      0.8544     0.8607     57.0708
05800     0.4352      0.8840     0.8607     55.8296
05900     0.4433      0.8734     0.8607     56.3068
06000     0.4352      0.8840     0.8607     55.6668
06100     0.4735      0.8650     0.8607     55.4028
06200     0.3734      0.8966     0.8607     56.2153
06300     0.4439      0.8713     0.8607     54.5745
06400     0.4326      0.8713     0.8607     54.1842
06500     0.4646      0.8481     0.8607     55.5124
06600     0.4518      0.8692     0.8607     54.6803
06700     0.4895      0.8565     0.8607     55.8520
06800     0.4984      0.8523     0.8607     54.6019
06900     0.3544      0.8966     0.8607     54.5569
07000     0.4082      0.8776     0.8607     55.4507
07100     0.4242      0.8882     0.8607     55.1628
07200     0.4391      0.8692     0.8607     54.1203
07300     0.4836      0.8523     0.8607     55.6641
07400     0.4326      0.8819     0.8607     54.4404
07500     0.3791      0.8966     0.8607     55.3309
07600     0.4154      0.8755     0.8607     54.5633
07700     0.4446      0.8755     0.8607     54.5580
07800     0.4452      0.8755     0.8621     55.8879
07900     0.5207      0.8186     0.8621     53.8019
08000     0.4680      0.8797     0.8621     56.3900
08100     0.4268      0.8882     0.8621     55.9160
08200     0.4170      0.8840     0.8621     54.6089
08300     0.3979      0.8987     0.8621     55.6050
08400     0.3661      0.9072     0.8621     54.2685
08500     0.4031      0.8776     0.8621     54.8289
08600     0.5099      0.8523     0.8621     54.9771
08700     0.3975      0.8882     0.8621     54.9621
08800     0.4776      0.8671     0.8621     54.8952
08900     0.4057      0.8797     0.8621     55.2575
09000     0.4483      0.8671     0.8621     55.4516
09100     0.4416      0.8713     0.8621     54.7717
09200     0.4576      0.8586     0.8621     54.7045
09300     0.4617      0.8692     0.8621     55.2485
09400     0.4502      0.8650     0.8621     55.3996
09500     0.4621      0.8586     0.8621     54.2626
09600     0.4885      0.8481     0.8621     55.4853
09700     0.5244      0.8376     0.8621     55.5932
09800     0.4554      0.8755     0.8621     54.8739
09900     0.4455      0.8713     0.8621     55.4173
Start testing:
Test Accuracy: 0.8360
