Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 49, in __init__
    torch.nn.init.uniform_(self.cell.weight_ih, a = -limit2, b = limit2)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 49, in __init__
    torch.nn.init.uniform_(self.cell.weight_ih, a = -limit2, b = limit2)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 49, in __init__
    torch.nn.init.uniform_(self.cell.weight_ih, a = -limit2, b = limit2)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 362, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 362, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.0, finetuning_epochs=0, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.0005,0.0001,0.00002', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,10000', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
72047b21-57cd-4681-9409-756ee6894767
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.5520      0.0781     0.0808     12.4827
00100     2.5438      0.0844     0.0809     55.7737
00200     2.7826      0.1139     0.1875     55.6508
00300     2.7398      0.1899     0.2172     57.0456
00400     3.3193      0.1477     0.2172     55.8923
00500     4.0499      0.2342     0.2539     56.2790
00600     3.9848      0.2574     0.2650     56.8950
00700     3.8833      0.2722     0.2784     56.9034
00800     3.7778      0.2553     0.2863     56.6207
00900     3.9201      0.2827     0.3097     56.5530
01000     3.7805      0.2975     0.3249     56.6307
01100     3.5103      0.2911     0.3249     56.6024
01200     3.6830      0.2679     0.3249     55.8917
01300     4.0129      0.2869     0.3255     56.4744
01400     3.8137      0.2975     0.3255     57.1194
01500     3.4446      0.3523     0.3441     57.2367
01600     3.5744      0.2700     0.3441     55.0875
01700     3.7145      0.3228     0.3575     55.2476
01800     3.4060      0.3565     0.3759     55.9560
01900     3.1698      0.3439     0.3762     56.3429
02000     3.2876      0.3565     0.3762     55.7116
02100     3.2654      0.3755     0.3888     56.5354
02200     3.4182      0.3249     0.4060     56.4764
02300     3.1851      0.3418     0.4060     55.4730
02400     3.1333      0.3882     0.4293     56.9424
02500     3.2766      0.3671     0.4293     56.1694
02600     3.2024      0.3312     0.4293     56.9429
02700     2.9388      0.3692     0.4293     57.0969
02800     3.0026      0.3903     0.4293     56.1405
02900     2.5170      0.3903     0.4354     56.0957
03000     2.7615      0.4451     0.4354     57.9769
03100     2.8430      0.4409     0.4515     56.6805
03200     2.8205      0.4684     0.4737     56.1710
03300     2.6540      0.4262     0.4737     55.9217
03400     2.8237      0.4135     0.4737     56.6172
03500     2.7252      0.4283     0.4845     56.4690
03600     2.4555      0.4768     0.5153     56.0609
03700     2.6284      0.4557     0.5153     55.7078
03800     2.4591      0.4241     0.5153     57.2557
03900     2.4121      0.4789     0.5235     58.0821
04000     2.5034      0.4747     0.5425     57.4135
04100     2.3850      0.4684     0.5425     56.0151
04200     2.3006      0.4451     0.5425     55.5361
04300     2.3502      0.4641     0.5503     56.9910
04400     2.2532      0.4810     0.5503     57.2463
04500     2.2214      0.5084     0.5503     56.1195
04600     2.4720      0.5232     0.5503     56.2044
04700     2.3264      0.4873     0.5503     56.8854
04800     2.2298      0.5084     0.5503     57.3278
04900     2.2043      0.5105     0.5503     55.9007
05000     2.3520      0.4852     0.5503     56.5356
05100     2.4659      0.4852     0.5503     57.7333
05200     2.4188      0.5253     0.5620     57.0048
05300     2.0988      0.5316     0.5675     55.6992
05400     1.8484      0.5274     0.5762     58.1981
05500     2.0389      0.5338     0.5762     56.5084
05600     2.1666      0.4768     0.5762     57.3506
05700     2.2425      0.4852     0.5785     55.2779
05800     2.0111      0.5422     0.5836     55.6955
05900     2.0207      0.5232     0.5896     57.7234
06000     2.1333      0.5190     0.5896     55.6127
06100     2.1380      0.5105     0.5896     56.6444
06200     2.3818      0.4937     0.6030     56.9153
06300     2.0846      0.4726     0.6030     55.6691
06400     1.8509      0.5232     0.6039     57.1205
06500     2.2685      0.4557     0.6039     56.5132
06600     2.1614      0.5105     0.6039     56.1065
06700     2.1410      0.5148     0.6039     57.6395
06800     2.1124      0.5591     0.6189     55.3070
06900     1.8725      0.5380     0.6189     58.1281
07000     1.8368      0.5422     0.6189     56.4927
07100     1.7827      0.5675     0.6189     55.9771
07200     2.0017      0.5675     0.6189     56.8555
07300     1.6351      0.5527     0.6189     57.1295
07400     1.7019      0.5781     0.6189     56.4218
07500     1.9723      0.5380     0.6210     56.8275
07600     1.6620      0.5759     0.6210     56.8448
07700     1.6754      0.5781     0.6310     56.1567
07800     1.8272      0.5549     0.6310     56.6607
07900     1.7002      0.5949     0.6310     55.5245
08000     1.9660      0.5380     0.6310     57.4007
08100     1.9199      0.5612     0.6310     57.0587
08200     2.1859      0.5359     0.6313     56.5234
08300     2.1224      0.5253     0.6313     57.1072
08400     1.6366      0.5675     0.6359     55.8682
08500     1.9431      0.5506     0.6359     56.3086
08600     1.9744      0.5316     0.6359     57.5386
08700     1.8791      0.5506     0.6359     55.8541
08800     1.8887      0.5380     0.6419     56.2214
08900     1.7084      0.6055     0.6419     55.8510
09000     1.7716      0.5781     0.6419     57.7571
09100     1.9664      0.5570     0.6419     57.7061
09200     1.8280      0.5464     0.6419     56.8252
09300     1.4928      0.6308     0.6529     56.1424
09400     1.7072      0.5949     0.6529     56.6133
09500     1.6639      0.6097     0.6529     55.5635
09600     1.5236      0.6076     0.6529     56.3798
09700     1.8628      0.5148     0.6529     55.9828
09800     1.6070      0.6013     0.6529     55.9761
09900     1.6145      0.5844     0.6529     57.0045
10000     1.5733      0.6034     0.6529     56.3607
10100     1.6965      0.5738     0.6529     56.0130
10200     2.0309      0.5485     0.6708     56.6079
10300     1.8372      0.6097     0.6708     57.0376
10400     1.7905      0.5823     0.6708     57.2205
10500     1.5736      0.6118     0.6708     57.2086
10600     1.5986      0.6013     0.6708     55.5177
10700     1.7091      0.5970     0.6708     57.4673
10800     1.7244      0.5802     0.6708     56.9298
10900     1.7207      0.5907     0.6708     56.3135
11000     1.7762      0.5570     0.6708     56.1701
11100     1.6770      0.5591     0.6708     56.5163
11200     1.9579      0.5443     0.6708     57.1960
11300     1.7199      0.6013     0.6708     55.9155
11400     1.7942      0.5949     0.6708     57.5244
11500     1.7703      0.5696     0.6708     57.1610
11600     1.5350      0.5992     0.6708     56.8182
11700     1.7741      0.5717     0.6708     56.9394
11800     1.5225      0.6224     0.6708     56.3042
11900     1.8417      0.5907     0.6708     56.6030
12000     1.8340      0.5422     0.6708     55.9329
12100     1.6951      0.5464     0.6708     55.9783
12200     1.4589      0.6076     0.6708     57.4080
12300     1.8141      0.5338     0.6708     56.6777
12400     1.5848      0.6097     0.6708     56.2427
12500     1.7246      0.6329     0.6708     56.9162
12600     1.6129      0.5907     0.6708     57.3202
12700     1.9406      0.5612     0.6708     56.2822
12800     1.6009      0.6160     0.6708     56.4387
12900     1.8167      0.5549     0.6708     55.3213
13000     1.6968      0.5886     0.6708     55.7870
13100     1.8160      0.5675     0.6708     56.4629
13200     1.6949      0.5781     0.6708     57.6200
13300     1.6696      0.5802     0.6708     55.4467
13400     1.8197      0.5485     0.6708     56.7672
13500     1.6528      0.5717     0.6708     55.9691
13600     1.6940      0.6203     0.6708     56.8298
13700     1.8157      0.5823     0.6708     55.1302
13800     1.8490      0.5527     0.6708     55.2502
13900     1.6903      0.5401     0.6708     56.2617
14000     1.6638      0.6266     0.6708     56.5261
14100     1.6307      0.5992     0.6708     55.5065
14200     1.9146      0.5844     0.6708     57.1164
14300     1.5849      0.6013     0.6708     56.0227
14400     1.5710      0.5612     0.6708     57.4501
14500     1.7860      0.5823     0.6708     55.0286
14600     1.6749      0.5928     0.6708     56.3780
14700     1.6376      0.6308     0.6708     56.7486
14800     1.7531      0.6392     0.6708     55.9538
14900     1.7126      0.5211     0.6708     57.1237
15000     1.5856      0.5823     0.6708     56.8580
15100     1.6354      0.5591     0.6708     58.1602
15200     1.6657      0.5781     0.6708     56.1077
15300     1.6161      0.5865     0.6708     56.1503
15400     1.5969      0.5738     0.6708     55.1122
15500     1.4817      0.5591     0.6708     57.4254
15600     1.6752      0.5570     0.6708     56.3205
15700     1.5183      0.6097     0.6708     55.4615
15800     1.5323      0.5359     0.6708     56.7985
15900     1.5700      0.5675     0.6708     54.9907
16000     1.6187      0.5633     0.6708     55.8281
16100     1.8517      0.5633     0.6708     55.4611
16200     1.5004      0.6160     0.6708     56.0556
16300     1.6380      0.5886     0.6708     56.0247
16400     1.6846      0.5823     0.6708     56.9506
16500     1.6570      0.5886     0.6708     55.9948
16600     1.6307      0.6055     0.6708     56.8600
16700     1.6456      0.5907     0.6708     57.1122
16800     1.6187      0.5527     0.6708     56.4638
16900     1.8624      0.5527     0.6708     57.4035
17000     1.4398      0.6329     0.6708     55.9466
17100     1.4972      0.6034     0.6708     57.2285
17200     1.6783      0.6139     0.6708     55.2265
17300     1.6461      0.5844     0.6721     55.8295
17400     1.6154      0.5717     0.6721     56.3393
17500     1.7521      0.5549     0.6721     55.8373
17600     1.6863      0.5696     0.6721     56.4081
17700     1.5943      0.5970     0.6721     55.7269
17800     1.6279      0.5759     0.6721     56.6930
17900     1.5992      0.6076     0.6801     57.2433
18000     1.5178      0.6097     0.6801     57.1318
18100     1.8682      0.5717     0.6801     58.0232
18200     1.5436      0.5738     0.6801     56.1764
18300     1.5158      0.5949     0.6801     55.6139
18400     1.5636      0.6245     0.6801     56.8270
18500     1.4371      0.6245     0.6801     55.4428
18600     1.5599      0.6076     0.6801     55.9555
18700     1.6219      0.5802     0.6801     56.4571
18800     1.7262      0.6118     0.6801     56.1572
18900     1.6810      0.5823     0.6801     56.5819
19000     1.8905      0.5591     0.6801     57.0711
19100     1.7874      0.6076     0.6801     55.1873
19200     1.5692      0.5717     0.6801     56.3132
19300     1.6086      0.5865     0.6801     57.3116
19400     1.3907      0.6350     0.6801     57.0923
19500     1.4642      0.6055     0.6801     55.9914
19600     1.6050      0.6139     0.6801     56.2051
19700     1.6722      0.5759     0.6801     57.0431
19800     1.4380      0.6224     0.6801     57.3499
19900     1.5341      0.6055     0.6801     55.8251
20000     1.5324      0.6245     0.6801     56.5479
20100     1.6703      0.6181     0.6801     56.1730
20200     1.8201      0.5844     0.6801     57.6884
20300     1.7074      0.5823     0.6801     56.8043
20400     1.5570      0.6266     0.6801     56.2463
20500     1.5311      0.6287     0.6801     56.1718
20600     1.7435      0.6055     0.6801     55.4550
20700     1.5832      0.6118     0.6801     55.3578
20800     1.6436      0.6160     0.6801     56.3490
20900     1.5770      0.6118     0.6801     56.2818
21000     1.7199      0.5992     0.6801     56.5317
21100     1.7124      0.6055     0.6853     56.4572
21200     1.4071      0.6181     0.6853     55.1763
21300     1.4798      0.6097     0.6853     56.1434
21400     1.4867      0.6266     0.6853     55.7323
21500     1.4108      0.6055     0.6853     55.8022
21600     1.7561      0.5865     0.6853     57.8978
21700     1.6674      0.5928     0.6853     55.6061
21800     1.8179      0.5802     0.6853     55.8479
21900     1.6740      0.5633     0.6853     56.6423
22000     1.5674      0.5992     0.6853     56.0676
22100     1.2860      0.6624     0.6853     55.4805
22200     1.4747      0.6139     0.6853     56.9762
22300     1.5638      0.5696     0.6853     56.0718
22400     1.6559      0.5802     0.6853     57.5383
22500     1.5488      0.6139     0.6853     55.6453
22600     1.4448      0.6308     0.6853     55.1337
22700     1.5716      0.5759     0.6853     55.9986
22800     1.5291      0.6245     0.6853     56.1393
22900     1.6586      0.6181     0.6853     55.6685
23000     1.5503      0.6181     0.6853     56.4654
23100     1.6255      0.6160     0.6853     56.7428
23200     1.5574      0.5970     0.6853     58.3320
23300     1.5236      0.6055     0.6853     55.4398
23400     1.3887      0.6287     0.6853     56.5592
23500     1.5866      0.5928     0.6853     57.0808
23600     1.7003      0.6013     0.6853     55.8325
23700     1.4655      0.5738     0.6853     55.5295
23800     1.7961      0.5949     0.6853     56.0637
23900     1.4830      0.6287     0.6853     57.0097
24000     1.4878      0.6287     0.6853     55.9691
24100     1.4610      0.6034     0.6853     55.0752
24200     1.4250      0.6603     0.6853     57.4602
24300     1.5209      0.6139     0.6853     56.4572
24400     1.7544      0.5506     0.6853     55.9064
24500     1.6896      0.5970     0.6853     56.7202
24600     1.6119      0.6055     0.6853     56.7800
24700     1.6872      0.6034     0.6853     56.3454
24800     1.5477      0.5949     0.6853     55.9994
24900     1.8081      0.5781     0.6853     56.0835
25000     1.6809      0.5992     0.6853     55.7583
25100     1.6153      0.6203     0.6853     56.7022
25200     1.4761      0.6076     0.6853     56.1259
25300     1.4978      0.6287     0.6853     56.2803
25400     1.5540      0.6181     0.6853     55.5466
25500     1.4684      0.6097     0.6853     56.5044
25600     1.1631      0.6561     0.6853     56.8737
25700     1.7383      0.5907     0.6853     56.5673
25800     1.6194      0.6203     0.6853     55.2911
25900     1.4004      0.6224     0.6853     55.9997
26000     1.8458      0.5717     0.6853     55.5184
26100     1.3913      0.6181     0.6853     57.6674
26200     1.5399      0.6203     0.6853     56.9056
26300     1.6374      0.5886     0.6853     57.5860
26400     1.6480      0.5759     0.6853     57.4393
26500     1.6178      0.6245     0.6853     54.9362
26600     1.5879      0.5802     0.6853     55.1413
26700     1.4728      0.6055     0.6853     56.4507
26800     1.6104      0.5844     0.6853     55.6968
26900     1.4854      0.6329     0.6853     57.0177
27000     1.3853      0.5992     0.6853     57.7663
27100     1.7392      0.5802     0.6853     56.1618
27200     1.6003      0.6076     0.6853     56.5837
27300     1.6499      0.5928     0.6853     56.0488
27400     1.2830      0.6097     0.6853     56.2450
27500     1.5713      0.6203     0.6853     56.7682
27600     1.5988      0.5928     0.6853     56.7211
27700     1.6798      0.5907     0.6853     55.5815
27800     1.6360      0.5759     0.6853     57.2424
27900     1.7442      0.5781     0.6853     55.9114
28000     1.5140      0.5928     0.6858     56.9489
28100     1.5248      0.6013     0.6858     56.0235
28200     1.4734      0.5907     0.6858     55.2271
28300     1.6916      0.6308     0.6858     55.9857
28400     1.7049      0.6118     0.6858     57.4463
28500     1.7553      0.6034     0.6858     57.2907
28600     1.6387      0.5865     0.6858     56.8986
28700     1.6787      0.5781     0.6858     55.8649
28800     1.3504      0.6013     0.6858     56.6907
28900     1.6464      0.5738     0.6858     54.9337
29000     1.4745      0.5970     0.6858     55.4389
29100     1.4126      0.6371     0.6858     56.8318
29200     1.6262      0.5802     0.6858     56.1622
29300     1.4037      0.6245     0.6858     56.4857
29400     1.6829      0.5886     0.6858     56.6387
29500     1.4927      0.5949     0.6858     56.7660
29600     1.6340      0.6076     0.6858     56.1117
29700     1.3001      0.5949     0.6858     55.9220
29800     1.5008      0.6160     0.6858     54.8214
29900     1.6825      0.5738     0.6858     58.0102
29999     1.5066      0.5949     0.6858     55.4583
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Start testing:
Test Accuracy: 0.6732
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
6ca083b1-d90f-4b2c-ad15-ee2beacdc8c9
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.5687      0.0802     0.0814     10.7889
00100     4.0071      0.2025     0.0822     70.4005
00200     4.5023      0.2131     0.0905     71.9201
00300     4.0806      0.2405     0.0919     71.2985
00400     3.9679      0.2700     0.0919     69.9222
00500     3.5419      0.3017     0.0971     71.2627
00600     4.2377      0.2743     0.0971     70.0427
00700     3.6392      0.3249     0.1060     69.9653
00800     3.3273      0.3333     0.1060     70.1707
00900     3.0871      0.3671     0.1060     70.4793
01000     2.6373      0.4219     0.1060     70.4926
01100     2.8187      0.4093     0.1063     70.3600
01200     2.4239      0.4367     0.1125     69.6702
01300     2.5612      0.3882     0.1125     69.8962
01400     2.2329      0.4557     0.1224     69.3926
01500     2.3200      0.4684     0.1224     69.8902
01600     2.4093      0.4620     0.1224     69.7515
01700     2.2030      0.5274     0.1227     70.1995
01800     2.3418      0.4916     0.1338     69.4679
01900     2.0056      0.5295     0.1341     69.4447
02000     2.2074      0.4810     0.1341     70.0473
02100     2.2466      0.5211     0.1390     69.1967
02200     2.2507      0.4662     0.1444     69.2233
02300     1.9351      0.5485     0.1626     69.9337
02400     1.7853      0.5612     0.1626     69.3540
02500     1.8072      0.5549     0.1626     69.0192
02600     2.0755      0.5295     0.1626     69.9300
02700     1.9644      0.5591     0.1683     70.0201
02800     1.8780      0.5802     0.1683     69.1968
02900     1.5084      0.5844     0.1683     69.4540
03000     1.7922      0.6055     0.1683     70.1801
03100     1.7490      0.6181     0.1683     69.6399
03200     1.7746      0.5886     0.1823     69.7417
03300     1.8645      0.5844     0.1823     69.3744
03400     1.6503      0.5781     0.1823     69.2507
03500     1.9027      0.5591     0.1823     69.2518
03600     1.7305      0.5886     0.1843     69.9264
03700     1.5385      0.6245     0.1974     69.2047
03800     1.8361      0.6055     0.1974     70.7090
03900     1.8573      0.5759     0.1985     70.1073
04000     1.5486      0.6350     0.2016     70.5032
04100     1.4806      0.6287     0.2168     69.8607
04200     1.6167      0.6245     0.2296     69.5724
04300     1.6664      0.6392     0.2296     69.3435
04400     1.9241      0.5970     0.2296     69.1325
04500     1.5098      0.6540     0.2296     69.1576
04600     1.8332      0.6308     0.2296     69.5392
04700     1.5419      0.6435     0.2296     69.8833
04800     1.5891      0.6203     0.2296     69.9315
04900     1.5862      0.6224     0.2302     70.0276
05000     1.5202      0.6519     0.2313     69.3451
05100     1.5507      0.6245     0.2313     69.7109
05200     1.5422      0.6477     0.2427     70.7224
05300     1.4477      0.6329     0.2427     69.9547
05400     1.3785      0.6667     0.2471     69.4043
05500     1.5518      0.6350     0.2563     69.0548
05600     1.4118      0.6329     0.2587     69.8381
05700     1.5382      0.6709     0.2587     69.6618
05800     1.5419      0.6224     0.2587     69.7098
05900     1.2557      0.6751     0.2587     69.2894
06000     1.5007      0.6203     0.2587     70.3553
06100     1.6116      0.6519     0.2587     70.3042
06200     1.4764      0.6181     0.2587     69.7746
06300     1.3897      0.6540     0.2587     69.8727
06400     1.4756      0.6392     0.2592     70.0922
06500     1.6338      0.6371     0.2592     69.3531
06600     1.5311      0.6519     0.2592     69.5943
06700     1.6532      0.6709     0.2592     69.9525
06800     1.5995      0.6477     0.2592     69.9008
06900     1.5368      0.6688     0.2830     70.0884
07000     1.4262      0.6498     0.2830     69.6210
07100     1.5116      0.6793     0.2830     68.8378
07200     1.8208      0.6203     0.2830     70.2837
07300     1.3754      0.6456     0.2830     70.4189
07400     1.6249      0.6350     0.2838     69.9715
07500     1.9304      0.6118     0.2838     69.8336
07600     1.3975      0.7004     0.2838     69.5053
07700     1.4309      0.6266     0.2851     69.6836
07800     1.4542      0.6772     0.2851     69.9126
07900     1.3376      0.6561     0.2971     69.2739
08000     1.5452      0.6519     0.3028     69.9332
08100     1.6299      0.6160     0.3028     69.7763
08200     1.4995      0.6540     0.3229     69.5387
08300     1.4020      0.6624     0.3229     70.3767
08400     1.5352      0.6519     0.3229     69.0405
08500     1.4948      0.6624     0.3229     69.7923
08600     1.4018      0.6371     0.3229     69.6128
08700     1.5566      0.6751     0.3229     70.0589
08800     1.3376      0.6646     0.3229     69.4816
08900     1.4310      0.6688     0.3299     68.4509
09000     1.4650      0.6603     0.3299     70.1178
09100     1.4416      0.6582     0.3299     69.3486
09200     1.5458      0.6772     0.3299     69.1963
09300     1.4220      0.6266     0.3299     69.1004
09400     1.2919      0.6730     0.3299     69.3663
09500     1.4557      0.6456     0.3299     69.7239
09600     1.4956      0.6603     0.3302     68.9662
09700     1.4677      0.6371     0.3336     69.3504
09800     1.4838      0.6793     0.3384     69.5548
09900     1.5050      0.6624     0.3384     70.2597
10000     1.4422      0.6519     0.3384     70.0656
10100     1.3268      0.6561     0.3384     69.9703
10200     1.3418      0.6751     0.3474     69.8100
10300     1.2102      0.6899     0.3474     68.9230
10400     1.2127      0.6730     0.3474     70.2624
10500     1.4327      0.6941     0.3474     69.8980
10600     1.6733      0.6624     0.3474     69.3618
10700     1.2575      0.6751     0.3474     70.5515
10800     1.6232      0.6688     0.3474     70.3751
10900     1.4897      0.6540     0.3474     69.6019
11000     1.2613      0.6814     0.3474     70.8468
11100     1.2935      0.6878     0.3474     69.8709
11200     1.2868      0.7004     0.3474     70.0507
11300     1.3406      0.6793     0.3476     69.4904
11400     1.4123      0.6667     0.3476     70.2808
11500     1.3651      0.7278     0.3563     69.6500
11600     1.6000      0.6266     0.3563     69.1948
11700     1.1445      0.6857     0.3563     69.7723
11800     1.3253      0.6624     0.3563     69.1232
11900     1.4759      0.6371     0.3563     69.9430
12000     1.4049      0.6857     0.3563     70.5269
12100     1.1917      0.7046     0.3563     69.9212
12200     1.3227      0.6920     0.3563     68.8216
12300     1.4913      0.6709     0.3563     70.4469
12400     1.5361      0.6814     0.3563     69.9390
12500     1.2835      0.6540     0.3563     69.0775
12600     1.1660      0.6962     0.3563     69.7468
12700     1.5436      0.6793     0.3563     69.1580
12800     1.4114      0.6582     0.3563     69.8014
12900     1.5227      0.6646     0.3563     69.1202
13000     1.3246      0.7004     0.3563     70.0172
13100     1.3817      0.6624     0.3563     70.1801
13200     1.5362      0.6814     0.3563     69.3469
13300     1.2697      0.7025     0.3563     69.9141
13400     1.3688      0.6624     0.3722     70.3664
13500     1.4851      0.6962     0.3722     69.6218
13600     1.2067      0.6962     0.3722     69.0765
13700     1.3276      0.7089     0.3722     69.0836
13800     1.3696      0.6920     0.3722     69.3259
13900     1.3316      0.7131     0.3722     69.6410
14000     1.4057      0.6983     0.3722     69.1751
14100     1.3594      0.6835     0.3722     69.6663
14200     1.3695      0.6709     0.3722     69.7842
14300     1.1226      0.7278     0.3722     69.4550
14400     1.3532      0.6878     0.3722     69.5937
14500     1.4238      0.6688     0.3722     68.9298
14600     1.2606      0.7321     0.3722     69.0733
14700     1.4371      0.6540     0.3722     69.5219
14800     1.1675      0.7110     0.3722     69.7545
14900     1.4304      0.6477     0.3722     69.4413
15000     1.3507      0.6962     0.3722     69.6088
15100     1.4734      0.6688     0.3722     69.5005
15200     1.6030      0.6392     0.3722     69.6963
15300     1.3565      0.6920     0.3722     69.9714
15400     1.4302      0.6519     0.3722     69.3973
15500     1.2373      0.7152     0.3722     70.1735
15600     1.2422      0.7384     0.3722     70.0600
15700     1.5797      0.6118     0.3722     69.1789
15800     1.1921      0.7089     0.3722     69.2470
15900     1.4610      0.6878     0.3722     69.1588
16000     1.3186      0.6709     0.3722     69.7597
16100     1.5185      0.6709     0.3722     69.4514
16200     1.3768      0.6793     0.3722     69.6396
16300     1.3018      0.6835     0.3722     69.7197
16400     1.2246      0.6920     0.3722     69.5807
16500     1.3483      0.6857     0.3722     69.1381
16600     1.1362      0.7025     0.3722     69.8184
16700     1.4221      0.6751     0.3722     69.6217
16800     1.3321      0.6983     0.3722     70.6652
16900     1.2837      0.6857     0.3722     69.2951
17000     1.2850      0.6751     0.3722     69.1640
17100     1.2910      0.6920     0.3722     69.7303
17200     1.4615      0.6603     0.3722     68.8229
17300     1.4342      0.7004     0.3722     68.9376
17400     1.5632      0.6814     0.3722     69.6761
17500     1.3816      0.6456     0.3722     69.4096
17600     1.4070      0.6814     0.3722     69.9228
17700     1.2287      0.6835     0.3722     69.1207
17800     1.2121      0.6899     0.3722     69.6270
17900     1.3847      0.7194     0.3722     69.9386
18000     1.2892      0.7131     0.3722     69.9478
18100     1.3599      0.6793     0.3722     70.0309
18200     1.3757      0.7046     0.3722     70.1954
18300     1.4049      0.6962     0.3722     69.5930
18400     1.3146      0.7152     0.3815     70.1495
18500     1.3928      0.7025     0.3815     69.7177
18600     1.4631      0.6751     0.3815     69.7617
18700     1.1468      0.7300     0.3815     70.0901
18800     1.3830      0.6519     0.3815     69.5272
18900     1.3477      0.6435     0.3815     69.1583
19000     1.5282      0.6835     0.3815     69.0894
19100     1.2964      0.6793     0.3815     69.6338
19200     1.2847      0.6941     0.3815     69.3268
19300     1.4078      0.6962     0.3815     69.7039
19400     1.3495      0.6772     0.3815     69.8488
19500     1.2002      0.6667     0.3815     69.9642
19600     1.2518      0.6983     0.3815     69.0860
19700     1.2090      0.7004     0.3815     69.7178
19800     1.4250      0.6835     0.3815     69.9019
19900     1.1676      0.6983     0.3815     69.4762
20000     1.2790      0.6983     0.3815     69.9845
20100     1.5653      0.6350     0.3815     69.1771
20199     1.3643      0.7300     0.3815     68.6057
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     5.1290      0.3270     0.3706     9.8478
00100     2.8247      0.3333     0.4664     69.6885
00200     2.9126      0.4198     0.4799     69.3516
00300     2.5519      0.4241     0.5378     69.7348
00400     2.4484      0.3734     0.5609     69.1837
00500     2.0450      0.4979     0.5609     69.5216
00600     1.9337      0.5000     0.5732     69.6609
00700     2.4636      0.5169     0.6033     69.1540
00800     2.5094      0.4747     0.6033     69.2811
00900     2.4024      0.5127     0.6051     69.8298
01000     2.4239      0.4789     0.6331     69.3354
01100     2.1421      0.5549     0.6331     69.7243
01200     2.0686      0.5823     0.6331     69.3726
01300     2.2187      0.5591     0.6370     69.3089
01400     2.2342      0.5274     0.6370     69.6712
01500     2.2955      0.5169     0.6370     69.3396
01600     1.8663      0.5949     0.6370     69.7961
01700     1.9359      0.5316     0.6370     69.4523
01800     1.9547      0.6245     0.6370     69.1825
01900     2.4844      0.5274     0.6454     69.8675
02000     2.1419      0.6013     0.6454     68.7878
02100     1.8909      0.5570     0.6454     69.1730
02200     1.7289      0.5675     0.6454     69.9154
02300     1.8962      0.5612     0.6454     69.0850
02400     1.9966      0.5781     0.6454     68.8104
02500     2.3285      0.5063     0.6463     69.9067
02600     1.9668      0.5633     0.6518     69.3866
02700     2.0945      0.6034     0.6522     69.3937
02800     2.2060      0.5316     0.6522     68.9690
02900     1.8594      0.6076     0.6636     69.4974
03000     2.2481      0.5844     0.6636     69.7735
03100     2.0286      0.5802     0.6636     70.2014
03200     2.1147      0.6055     0.6636     69.8162
03300     2.3241      0.5084     0.6636     69.9711
03400     2.1208      0.5464     0.6636     69.5886
03500     2.0043      0.5169     0.6636     69.9333
03600     1.6842      0.5527     0.6636     69.1617
03700     1.8094      0.5949     0.6636     69.6772
03800     2.0171      0.6371     0.6636     70.4790
03900     2.1744      0.5949     0.6636     71.2124
04000     2.2000      0.5042     0.6636     71.2324
04100     2.0052      0.5781     0.6636     69.5947
04200     1.9429      0.5633     0.6636     68.5964
04300     1.8685      0.6224     0.6636     68.8212
04400     2.1506      0.5549     0.6636     67.9755
04500     2.2833      0.5401     0.6636     68.6965
04600     1.8613      0.5949     0.6661     68.6671
04700     2.0001      0.6181     0.6785     68.7214
04800     1.7562      0.6519     0.6785     69.0953
04900     1.9111      0.5844     0.6785     69.8699
05000     1.9103      0.6076     0.6785     69.0444
05100     1.7942      0.6350     0.6785     69.4344
05200     1.8124      0.6076     0.6785     70.1864
05300     1.7221      0.6266     0.6785     70.0062
05400     1.8718      0.5992     0.6785     69.6818
05500     1.9652      0.5949     0.6785     69.3781
05600     1.7333      0.5549     0.6785     68.8573
05700     1.8783      0.6308     0.6785     69.8498
05800     2.0075      0.5844     0.6785     72.0094
05900     1.6719      0.5992     0.6785     71.8161
06000     1.8187      0.6245     0.6785     69.0454
06100     1.7739      0.6392     0.6785     69.1646
06200     1.8138      0.6013     0.6785     69.3680
06300     2.1094      0.5802     0.6849     68.6025
06400     1.7772      0.6118     0.6849     69.4164
06500     1.8034      0.6224     0.6849     69.0859
06600     1.7470      0.6055     0.6849     69.5446
06700     1.8677      0.5907     0.6849     69.0282
06800     1.7418      0.5886     0.6849     69.4159
06900     1.7291      0.6181     0.6849     69.2323
07000     1.7644      0.6477     0.6849     69.2681
07100     2.2198      0.5717     0.6873     70.0046
07200     1.5237      0.6287     0.6873     69.0342
07300     2.1010      0.5907     0.6873     69.5347
07400     1.5883      0.6181     0.6873     69.0205
07500     1.6550      0.6308     0.6873     68.7602
07600     1.5542      0.6435     0.6873     68.9982
07700     1.6824      0.5570     0.6873     69.1495
07800     1.9804      0.5992     0.6873     68.9830
07900     1.7671      0.6139     0.6873     69.2687
08000     1.9015      0.6181     0.6873     69.2788
08100     2.0717      0.5823     0.6873     69.6049
08200     1.8739      0.5844     0.6873     68.5672
08300     1.9298      0.6203     0.6873     68.9392
08400     1.7508      0.6076     0.6873     69.4151
08500     2.0890      0.5738     0.6873     68.9672
08600     1.9991      0.5970     0.6873     68.9621
08700     2.0639      0.5696     0.6873     68.8399
08800     1.9520      0.5865     0.6873     68.2782
08900     2.0162      0.5802     0.6873     69.3222
09000     1.9859      0.5738     0.6998     69.2960
09100     1.7767      0.5485     0.6998     69.6780
09200     1.9731      0.5992     0.6998     69.3839
09300     1.9914      0.5696     0.6998     68.5764
09400     2.0187      0.5759     0.6998     69.0986
09500     1.7087      0.6371     0.6998     68.7920
09600     1.9649      0.6245     0.6998     69.4389
09700     2.1267      0.5781     0.6998     69.0782
09800     2.2376      0.5380     0.6998     68.5319
09900     2.0371      0.5717     0.6998     69.2566
Start testing:
Test Accuracy: 0.7453
