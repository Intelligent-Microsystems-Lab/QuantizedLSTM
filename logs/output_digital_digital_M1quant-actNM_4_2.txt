Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 49, in __init__
    torch.nn.init.uniform_(self.cell.weight_ih, a = -limit2, b = limit2)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 49, in __init__
    torch.nn.init.uniform_(self.cell.weight_ih, a = -limit2, b = limit2)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 49, in __init__
    torch.nn.init.uniform_(self.cell.weight_ih, a = -limit2, b = limit2)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 362, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 362, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.0, finetuning_epochs=0, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.0005,0.0001,0.00002', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,10000', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
72047b21-57cd-4681-9409-756ee6894767
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.5520      0.0781     0.0808     12.4827
00100     2.5438      0.0844     0.0809     55.7737
00200     2.7826      0.1139     0.1875     55.6508
00300     2.7398      0.1899     0.2172     57.0456
00400     3.3193      0.1477     0.2172     55.8923
00500     4.0499      0.2342     0.2539     56.2790
00600     3.9848      0.2574     0.2650     56.8950
00700     3.8833      0.2722     0.2784     56.9034
00800     3.7778      0.2553     0.2863     56.6207
00900     3.9201      0.2827     0.3097     56.5530
01000     3.7805      0.2975     0.3249     56.6307
01100     3.5103      0.2911     0.3249     56.6024
01200     3.6830      0.2679     0.3249     55.8917
01300     4.0129      0.2869     0.3255     56.4744
01400     3.8137      0.2975     0.3255     57.1194
01500     3.4446      0.3523     0.3441     57.2367
01600     3.5744      0.2700     0.3441     55.0875
01700     3.7145      0.3228     0.3575     55.2476
01800     3.4060      0.3565     0.3759     55.9560
01900     3.1698      0.3439     0.3762     56.3429
02000     3.2876      0.3565     0.3762     55.7116
02100     3.2654      0.3755     0.3888     56.5354
02200     3.4182      0.3249     0.4060     56.4764
02300     3.1851      0.3418     0.4060     55.4730
02400     3.1333      0.3882     0.4293     56.9424
02500     3.2766      0.3671     0.4293     56.1694
02600     3.2024      0.3312     0.4293     56.9429
02700     2.9388      0.3692     0.4293     57.0969
02800     3.0026      0.3903     0.4293     56.1405
02900     2.5170      0.3903     0.4354     56.0957
03000     2.7615      0.4451     0.4354     57.9769
03100     2.8430      0.4409     0.4515     56.6805
03200     2.8205      0.4684     0.4737     56.1710
03300     2.6540      0.4262     0.4737     55.9217
03400     2.8237      0.4135     0.4737     56.6172
03500     2.7252      0.4283     0.4845     56.4690
03600     2.4555      0.4768     0.5153     56.0609
03700     2.6284      0.4557     0.5153     55.7078
03800     2.4591      0.4241     0.5153     57.2557
03900     2.4121      0.4789     0.5235     58.0821
04000     2.5034      0.4747     0.5425     57.4135
04100     2.3850      0.4684     0.5425     56.0151
04200     2.3006      0.4451     0.5425     55.5361
04300     2.3502      0.4641     0.5503     56.9910
04400     2.2532      0.4810     0.5503     57.2463
04500     2.2214      0.5084     0.5503     56.1195
04600     2.4720      0.5232     0.5503     56.2044
04700     2.3264      0.4873     0.5503     56.8854
04800     2.2298      0.5084     0.5503     57.3278
04900     2.2043      0.5105     0.5503     55.9007
05000     2.3520      0.4852     0.5503     56.5356
05100     2.4659      0.4852     0.5503     57.7333
05200     2.4188      0.5253     0.5620     57.0048
05300     2.0988      0.5316     0.5675     55.6992
05400     1.8484      0.5274     0.5762     58.1981
05500     2.0389      0.5338     0.5762     56.5084
05600     2.1666      0.4768     0.5762     57.3506
05700     2.2425      0.4852     0.5785     55.2779
05800     2.0111      0.5422     0.5836     55.6955
05900     2.0207      0.5232     0.5896     57.7234
06000     2.1333      0.5190     0.5896     55.6127
06100     2.1380      0.5105     0.5896     56.6444
06200     2.3818      0.4937     0.6030     56.9153
06300     2.0846      0.4726     0.6030     55.6691
06400     1.8509      0.5232     0.6039     57.1205
06500     2.2685      0.4557     0.6039     56.5132
06600     2.1614      0.5105     0.6039     56.1065
06700     2.1410      0.5148     0.6039     57.6395
06800     2.1124      0.5591     0.6189     55.3070
06900     1.8725      0.5380     0.6189     58.1281
07000     1.8368      0.5422     0.6189     56.4927
07100     1.7827      0.5675     0.6189     55.9771
07200     2.0017      0.5675     0.6189     56.8555
07300     1.6351      0.5527     0.6189     57.1295
07400     1.7019      0.5781     0.6189     56.4218
07500     1.9723      0.5380     0.6210     56.8275
07600     1.6620      0.5759     0.6210     56.8448
07700     1.6754      0.5781     0.6310     56.1567
07800     1.8272      0.5549     0.6310     56.6607
07900     1.7002      0.5949     0.6310     55.5245
08000     1.9660      0.5380     0.6310     57.4007
08100     1.9199      0.5612     0.6310     57.0587
08200     2.1859      0.5359     0.6313     56.5234
08300     2.1224      0.5253     0.6313     57.1072
08400     1.6366      0.5675     0.6359     55.8682
08500     1.9431      0.5506     0.6359     56.3086
08600     1.9744      0.5316     0.6359     57.5386
08700     1.8791      0.5506     0.6359     55.8541
08800     1.8887      0.5380     0.6419     56.2214
08900     1.7084      0.6055     0.6419     55.8510
09000     1.7716      0.5781     0.6419     57.7571
09100     1.9664      0.5570     0.6419     57.7061
09200     1.8280      0.5464     0.6419     56.8252
09300     1.4928      0.6308     0.6529     56.1424
09400     1.7072      0.5949     0.6529     56.6133
09500     1.6639      0.6097     0.6529     55.5635
09600     1.5236      0.6076     0.6529     56.3798
09700     1.8628      0.5148     0.6529     55.9828
09800     1.6070      0.6013     0.6529     55.9761
09900     1.6145      0.5844     0.6529     57.0045
10000     1.5733      0.6034     0.6529     56.3607
10100     1.6965      0.5738     0.6529     56.0130
10200     2.0309      0.5485     0.6708     56.6079
10300     1.8372      0.6097     0.6708     57.0376
10400     1.7905      0.5823     0.6708     57.2205
10500     1.5736      0.6118     0.6708     57.2086
10600     1.5986      0.6013     0.6708     55.5177
10700     1.7091      0.5970     0.6708     57.4673
10800     1.7244      0.5802     0.6708     56.9298
10900     1.7207      0.5907     0.6708     56.3135
11000     1.7762      0.5570     0.6708     56.1701
11100     1.6770      0.5591     0.6708     56.5163
11200     1.9579      0.5443     0.6708     57.1960
11300     1.7199      0.6013     0.6708     55.9155
11400     1.7942      0.5949     0.6708     57.5244
11500     1.7703      0.5696     0.6708     57.1610
11600     1.5350      0.5992     0.6708     56.8182
11700     1.7741      0.5717     0.6708     56.9394
11800     1.5225      0.6224     0.6708     56.3042
11900     1.8417      0.5907     0.6708     56.6030
12000     1.8340      0.5422     0.6708     55.9329
12100     1.6951      0.5464     0.6708     55.9783
12200     1.4589      0.6076     0.6708     57.4080
12300     1.8141      0.5338     0.6708     56.6777
12400     1.5848      0.6097     0.6708     56.2427
12500     1.7246      0.6329     0.6708     56.9162
12600     1.6129      0.5907     0.6708     57.3202
12700     1.9406      0.5612     0.6708     56.2822
12800     1.6009      0.6160     0.6708     56.4387
12900     1.8167      0.5549     0.6708     55.3213
13000     1.6968      0.5886     0.6708     55.7870
13100     1.8160      0.5675     0.6708     56.4629
13200     1.6949      0.5781     0.6708     57.6200
13300     1.6696      0.5802     0.6708     55.4467
13400     1.8197      0.5485     0.6708     56.7672
13500     1.6528      0.5717     0.6708     55.9691
13600     1.6940      0.6203     0.6708     56.8298
13700     1.8157      0.5823     0.6708     55.1302
13800     1.8490      0.5527     0.6708     55.2502
13900     1.6903      0.5401     0.6708     56.2617
14000     1.6638      0.6266     0.6708     56.5261
14100     1.6307      0.5992     0.6708     55.5065
14200     1.9146      0.5844     0.6708     57.1164
14300     1.5849      0.6013     0.6708     56.0227
14400     1.5710      0.5612     0.6708     57.4501
14500     1.7860      0.5823     0.6708     55.0286
14600     1.6749      0.5928     0.6708     56.3780
14700     1.6376      0.6308     0.6708     56.7486
14800     1.7531      0.6392     0.6708     55.9538
14900     1.7126      0.5211     0.6708     57.1237
15000     1.5856      0.5823     0.6708     56.8580
15100     1.6354      0.5591     0.6708     58.1602
15200     1.6657      0.5781     0.6708     56.1077
15300     1.6161      0.5865     0.6708     56.1503
15400     1.5969      0.5738     0.6708     55.1122
15500     1.4817      0.5591     0.6708     57.4254
15600     1.6752      0.5570     0.6708     56.3205
15700     1.5183      0.6097     0.6708     55.4615
15800     1.5323      0.5359     0.6708     56.7985
15900     1.5700      0.5675     0.6708     54.9907
16000     1.6187      0.5633     0.6708     55.8281
16100     1.8517      0.5633     0.6708     55.4611
16200     1.5004      0.6160     0.6708     56.0556
16300     1.6380      0.5886     0.6708     56.0247
16400     1.6846      0.5823     0.6708     56.9506
16500     1.6570      0.5886     0.6708     55.9948
16600     1.6307      0.6055     0.6708     56.8600
16700     1.6456      0.5907     0.6708     57.1122
16800     1.6187      0.5527     0.6708     56.4638
16900     1.8624      0.5527     0.6708     57.4035
17000     1.4398      0.6329     0.6708     55.9466
17100     1.4972      0.6034     0.6708     57.2285
17200     1.6783      0.6139     0.6708     55.2265
17300     1.6461      0.5844     0.6721     55.8295
17400     1.6154      0.5717     0.6721     56.3393
17500     1.7521      0.5549     0.6721     55.8373
17600     1.6863      0.5696     0.6721     56.4081
17700     1.5943      0.5970     0.6721     55.7269
17800     1.6279      0.5759     0.6721     56.6930
17900     1.5992      0.6076     0.6801     57.2433
18000     1.5178      0.6097     0.6801     57.1318
18100     1.8682      0.5717     0.6801     58.0232
18200     1.5436      0.5738     0.6801     56.1764
18300     1.5158      0.5949     0.6801     55.6139
18400     1.5636      0.6245     0.6801     56.8270
18500     1.4371      0.6245     0.6801     55.4428
18600     1.5599      0.6076     0.6801     55.9555
18700     1.6219      0.5802     0.6801     56.4571
18800     1.7262      0.6118     0.6801     56.1572
18900     1.6810      0.5823     0.6801     56.5819
19000     1.8905      0.5591     0.6801     57.0711
19100     1.7874      0.6076     0.6801     55.1873
19200     1.5692      0.5717     0.6801     56.3132
19300     1.6086      0.5865     0.6801     57.3116
19400     1.3907      0.6350     0.6801     57.0923
19500     1.4642      0.6055     0.6801     55.9914
19600     1.6050      0.6139     0.6801     56.2051
19700     1.6722      0.5759     0.6801     57.0431
19800     1.4380      0.6224     0.6801     57.3499
19900     1.5341      0.6055     0.6801     55.8251
20000     1.5324      0.6245     0.6801     56.5479
20100     1.6703      0.6181     0.6801     56.1730
20200     1.8201      0.5844     0.6801     57.6884
20300     1.7074      0.5823     0.6801     56.8043
20400     1.5570      0.6266     0.6801     56.2463
20500     1.5311      0.6287     0.6801     56.1718
20600     1.7435      0.6055     0.6801     55.4550
20700     1.5832      0.6118     0.6801     55.3578
20800     1.6436      0.6160     0.6801     56.3490
20900     1.5770      0.6118     0.6801     56.2818
21000     1.7199      0.5992     0.6801     56.5317
21100     1.7124      0.6055     0.6853     56.4572
21200     1.4071      0.6181     0.6853     55.1763
21300     1.4798      0.6097     0.6853     56.1434
21400     1.4867      0.6266     0.6853     55.7323
21500     1.4108      0.6055     0.6853     55.8022
21600     1.7561      0.5865     0.6853     57.8978
21700     1.6674      0.5928     0.6853     55.6061
21800     1.8179      0.5802     0.6853     55.8479
21900     1.6740      0.5633     0.6853     56.6423
22000     1.5674      0.5992     0.6853     56.0676
22100     1.2860      0.6624     0.6853     55.4805
22200     1.4747      0.6139     0.6853     56.9762
22300     1.5638      0.5696     0.6853     56.0718
22400     1.6559      0.5802     0.6853     57.5383
22500     1.5488      0.6139     0.6853     55.6453
22600     1.4448      0.6308     0.6853     55.1337
22700     1.5716      0.5759     0.6853     55.9986
22800     1.5291      0.6245     0.6853     56.1393
22900     1.6586      0.6181     0.6853     55.6685
23000     1.5503      0.6181     0.6853     56.4654
23100     1.6255      0.6160     0.6853     56.7428
23200     1.5574      0.5970     0.6853     58.3320
23300     1.5236      0.6055     0.6853     55.4398
23400     1.3887      0.6287     0.6853     56.5592
23500     1.5866      0.5928     0.6853     57.0808
23600     1.7003      0.6013     0.6853     55.8325
23700     1.4655      0.5738     0.6853     55.5295
23800     1.7961      0.5949     0.6853     56.0637
23900     1.4830      0.6287     0.6853     57.0097
24000     1.4878      0.6287     0.6853     55.9691
24100     1.4610      0.6034     0.6853     55.0752
24200     1.4250      0.6603     0.6853     57.4602
24300     1.5209      0.6139     0.6853     56.4572
24400     1.7544      0.5506     0.6853     55.9064
24500     1.6896      0.5970     0.6853     56.7202
24600     1.6119      0.6055     0.6853     56.7800
24700     1.6872      0.6034     0.6853     56.3454
24800     1.5477      0.5949     0.6853     55.9994
24900     1.8081      0.5781     0.6853     56.0835
25000     1.6809      0.5992     0.6853     55.7583
25100     1.6153      0.6203     0.6853     56.7022
25200     1.4761      0.6076     0.6853     56.1259
25300     1.4978      0.6287     0.6853     56.2803
25400     1.5540      0.6181     0.6853     55.5466
25500     1.4684      0.6097     0.6853     56.5044
25600     1.1631      0.6561     0.6853     56.8737
25700     1.7383      0.5907     0.6853     56.5673
25800     1.6194      0.6203     0.6853     55.2911
25900     1.4004      0.6224     0.6853     55.9997
26000     1.8458      0.5717     0.6853     55.5184
26100     1.3913      0.6181     0.6853     57.6674
26200     1.5399      0.6203     0.6853     56.9056
26300     1.6374      0.5886     0.6853     57.5860
26400     1.6480      0.5759     0.6853     57.4393
26500     1.6178      0.6245     0.6853     54.9362
26600     1.5879      0.5802     0.6853     55.1413
26700     1.4728      0.6055     0.6853     56.4507
26800     1.6104      0.5844     0.6853     55.6968
26900     1.4854      0.6329     0.6853     57.0177
27000     1.3853      0.5992     0.6853     57.7663
27100     1.7392      0.5802     0.6853     56.1618
27200     1.6003      0.6076     0.6853     56.5837
27300     1.6499      0.5928     0.6853     56.0488
27400     1.2830      0.6097     0.6853     56.2450
27500     1.5713      0.6203     0.6853     56.7682
27600     1.5988      0.5928     0.6853     56.7211
27700     1.6798      0.5907     0.6853     55.5815
27800     1.6360      0.5759     0.6853     57.2424
27900     1.7442      0.5781     0.6853     55.9114
28000     1.5140      0.5928     0.6858     56.9489
28100     1.5248      0.6013     0.6858     56.0235
28200     1.4734      0.5907     0.6858     55.2271
28300     1.6916      0.6308     0.6858     55.9857
28400     1.7049      0.6118     0.6858     57.4463
28500     1.7553      0.6034     0.6858     57.2907
28600     1.6387      0.5865     0.6858     56.8986
28700     1.6787      0.5781     0.6858     55.8649
28800     1.3504      0.6013     0.6858     56.6907
28900     1.6464      0.5738     0.6858     54.9337
29000     1.4745      0.5970     0.6858     55.4389
29100     1.4126      0.6371     0.6858     56.8318
29200     1.6262      0.5802     0.6858     56.1622
29300     1.4037      0.6245     0.6858     56.4857
29400     1.6829      0.5886     0.6858     56.6387
29500     1.4927      0.5949     0.6858     56.7660
29600     1.6340      0.6076     0.6858     56.1117
29700     1.3001      0.5949     0.6858     55.9220
29800     1.5008      0.6160     0.6858     54.8214
29900     1.6825      0.5738     0.6858     58.0102
29999     1.5066      0.5949     0.6858     55.4583
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Start testing:
Test Accuracy: 0.6732
