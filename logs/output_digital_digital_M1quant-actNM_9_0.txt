Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=9, quant_actNM=9, quant_inp=9, quant_w=9, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
6aeaf613-03e5-47ee-8637-5ddf74856172
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 373, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, 1.)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 82, in forward
    if len(x_range) > 1:
TypeError: object of type 'float' has no len()
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=9, quant_actNM=9, quant_inp=9, quant_w=9, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
087e91ff-5d42-4397-a4e8-cf28368c76bd
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 373, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, torch.tensor([1.]).device(input.device))
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 94, in forward
    x_scaled = x/x_range
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=9, quant_actNM=9, quant_inp=9, quant_w=9, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
3b175ebe-717e-4ed2-9a9f-57e7f43333f7
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     3.5462      0.0907     0.0903     10.0320
00100     2.2433      0.2300     0.2369     54.7944
00200     1.7850      0.4114     0.4278     54.7508
00300     1.5440      0.4937     0.5214     56.5780
00400     1.3355      0.5823     0.6021     55.3200
00500     1.2712      0.5802     0.6241     55.5687
00600     1.0334      0.6814     0.6830     55.4067
00700     1.1784      0.6224     0.6975     54.3338
00800     1.0547      0.6456     0.7141     55.5396
00900     0.9752      0.6561     0.7162     55.4746
01000     1.0102      0.6688     0.7283     55.1236
01100     0.8631      0.7300     0.7283     55.5316
01200     0.8806      0.7110     0.7479     55.3482
01300     0.9404      0.6941     0.7546     55.4347
01400     0.7816      0.7553     0.7546     55.7373
01500     0.8862      0.6941     0.7636     55.7710
01600     0.8819      0.7257     0.7636     56.1592
01700     0.8067      0.7384     0.7712     55.8533
01800     0.7849      0.7658     0.7738     55.2001
01900     0.8082      0.7405     0.7814     55.7280
02000     0.7845      0.7532     0.7823     55.2243
02100     0.8170      0.7342     0.7823     54.8735
02200     0.7981      0.7511     0.7823     55.4829
02300     0.7576      0.7722     0.7886     55.2237
02400     0.7460      0.7574     0.7934     55.5393
02500     0.7087      0.7869     0.7943     55.0493
02600     0.8277      0.7342     0.7943     55.1264
02700     0.6993      0.7764     0.7943     55.1746
02800     0.7348      0.7658     0.8015     54.9783
02900     0.7455      0.7827     0.8015     55.0119
03000     0.6976      0.7785     0.8015     55.2911
03100     0.7120      0.7764     0.8015     54.7824
03200     0.8297      0.7384     0.8107     55.2723
03300     0.6685      0.7911     0.8107     54.8381
03400     0.7480      0.7658     0.8107     55.1517
03500     0.7484      0.7595     0.8121     55.4989
03600     0.6358      0.8122     0.8121     55.5782
03700     0.6547      0.8017     0.8148     55.3501
03800     0.7576      0.7426     0.8148     55.2540
03900     0.6305      0.8017     0.8148     55.0103
04000     0.5822      0.8249     0.8148     55.7312
04100     0.5995      0.8017     0.8148     55.0409
04200     0.6388      0.8059     0.8148     54.5298
04300     0.7276      0.7637     0.8148     55.4825
04400     0.5973      0.8165     0.8148     55.4947
04500     0.6513      0.7743     0.8227     55.3202
04600     0.6136      0.8017     0.8227     55.3417
04700     0.5944      0.8186     0.8227     55.3169
04800     0.6377      0.7806     0.8233     55.9155
04900     0.6011      0.7996     0.8233     54.9550
05000     0.6868      0.7911     0.8233     54.8429
05100     0.6805      0.7764     0.8233     55.3574
05200     0.5456      0.8291     0.8233     55.2965
05300     0.6306      0.7996     0.8233     55.2469
05400     0.5596      0.8186     0.8285     55.7763
05500     0.6773      0.7954     0.8285     55.9136
05600     0.5938      0.8291     0.8285     55.5865
05700     0.6554      0.7954     0.8285     55.0343
05800     0.6549      0.7722     0.8285     54.7665
05900     0.5300      0.8523     0.8285     54.9999
06000     0.5941      0.8165     0.8314     55.0148
06100     0.6166      0.8228     0.8314     54.9720
06200     0.6404      0.7911     0.8375     55.4909
06300     0.5351      0.8354     0.8375     55.2993
06400     0.6008      0.7932     0.8375     55.5434
06500     0.6475      0.8122     0.8375     55.0023
06600     0.6034      0.8143     0.8375     55.3978
06700     0.6417      0.7848     0.8375     55.4465
06800     0.5417      0.8312     0.8375     54.7740
06900     0.5327      0.8502     0.8375     55.2699
07000     0.5886      0.8186     0.8375     55.8251
07100     0.5001      0.8439     0.8375     55.0751
07200     0.6283      0.8080     0.8375     55.8408
07300     0.6427      0.7890     0.8375     55.3447
07400     0.5633      0.8207     0.8376     55.0075
07500     0.6775      0.7785     0.8402     55.6038
07600     0.6322      0.8165     0.8402     55.2485
07700     0.5376      0.8354     0.8402     55.3540
07800     0.5881      0.8017     0.8402     55.3035
07900     0.6340      0.7954     0.8402     54.9833
08000     0.6131      0.7975     0.8402     55.7158
08100     0.5285      0.8291     0.8402     55.7068
08200     0.5967      0.8101     0.8402     55.2092
08300     0.5913      0.8165     0.8402     55.3385
08400     0.5615      0.8291     0.8402     55.5365
08500     0.5563      0.8080     0.8402     55.5786
08600     0.5641      0.8376     0.8402     55.0987
08700     0.5546      0.8228     0.8402     55.0496
08800     0.5619      0.7975     0.8402     55.1769
08900     0.4882      0.8481     0.8402     55.0641
09000     0.5754      0.8207     0.8402     54.9080
09100     0.5128      0.8418     0.8402     55.5086
09200     0.6346      0.8101     0.8402     55.4362
09300     0.5685      0.8228     0.8402     55.6202
09400     0.5529      0.8249     0.8402     55.4668
09500     0.5797      0.8207     0.8402     55.1431
09600     0.5106      0.8544     0.8402     55.4311
09700     0.6187      0.8207     0.8402     54.8336
09800     0.4603      0.8608     0.8402     55.4770
09900     0.6158      0.8101     0.8402     55.6955
10000     0.5435      0.8312     0.8402     55.2336
10100     0.5178      0.8354     0.8440     56.2224
10200     0.5391      0.8376     0.8440     55.3559
10300     0.5313      0.8207     0.8440     55.1568
10400     0.5303      0.8418     0.8440     55.4892
10500     0.5979      0.8038     0.8440     54.8653
10600     0.5601      0.8186     0.8440     55.0109
10700     0.5383      0.8143     0.8440     55.7081
10800     0.5072      0.8333     0.8440     55.5663
10900     0.6402      0.8059     0.8440     55.3616
11000     0.5585      0.8186     0.8440     55.2132
11100     0.5630      0.7954     0.8440     54.9196
11200     0.4696      0.8565     0.8440     55.4242
11300     0.4801      0.8460     0.8440     55.3483
11400     0.5242      0.8397     0.8440     54.8814
11500     0.4264      0.8713     0.8440     55.3576
11600     0.5576      0.8165     0.8440     55.2495
11700     0.5465      0.8186     0.8440     55.3553
11800     0.4895      0.8418     0.8440     55.1343
11900     0.5708      0.8101     0.8440     55.3024
12000     0.4496      0.8481     0.8440     56.0172
12100     0.5149      0.8228     0.8440     55.5098
12200     0.5270      0.8312     0.8440     55.1559
12300     0.5397      0.8354     0.8440     55.4074
12400     0.5316      0.8228     0.8440     55.2510
12500     0.5007      0.8376     0.8440     55.6476
12600     0.4987      0.8481     0.8450     55.0884
12700     0.5398      0.8270     0.8450     55.0394
12800     0.5338      0.8439     0.8450     55.7944
12900     0.4669      0.8565     0.8450     55.1344
13000     0.5599      0.8333     0.8450     55.1919
13100     0.4086      0.8734     0.8450     55.2762
13200     0.4887      0.8439     0.8450     54.8462
13300     0.4742      0.8565     0.8450     55.4114
13400     0.5001      0.8397     0.8450     55.4253
13500     0.4649      0.8460     0.8450     55.1241
13600     0.4457      0.8608     0.8450     55.1855
13700     0.4534      0.8481     0.8450     54.9676
13800     0.5058      0.8439     0.8450     55.1728
13900     0.4795      0.8544     0.8450     55.5604
14000     0.4725      0.8502     0.8450     55.1584
14100     0.4693      0.8650     0.8450     55.1587
14200     0.5232      0.8397     0.8450     54.6969
14300     0.5034      0.8460     0.8450     54.9375
14400     0.5141      0.8376     0.8468     55.4081
14500     0.4662      0.8565     0.8468     55.1768
14600     0.4478      0.8734     0.8468     54.9590
14700     0.5009      0.8460     0.8468     55.8526
14800     0.4882      0.8523     0.8468     55.0049
14900     0.5302      0.8502     0.8468     55.5859
15000     0.5593      0.8439     0.8468     54.8827
15100     0.5500      0.8397     0.8468     55.3673
15200     0.5550      0.8354     0.8468     55.6465
15300     0.5846      0.8312     0.8468     55.1983
15400     0.5130      0.8460     0.8468     55.4488
15500     0.5080      0.8439     0.8468     55.3860
15600     0.4747      0.8565     0.8468     55.1639
15700     0.5563      0.8101     0.8468     55.6009
15800     0.5492      0.8291     0.8468     55.1097
15900     0.5061      0.8354     0.8474     55.4480
16000     0.5476      0.8376     0.8474     55.8071
16100     0.5256      0.8460     0.8474     55.2436
16200     0.5396      0.8270     0.8474     55.5908
16300     0.6016      0.8059     0.8474     55.2993
16400     0.5013      0.8460     0.8474     55.0165
16500     0.6282      0.7806     0.8474     55.6802
16600     0.5556      0.8270     0.8478     55.5317
16700     0.4808      0.8586     0.8478     55.1610
16800     0.5294      0.8418     0.8478     55.7180
16900     0.5052      0.8439     0.8478     54.9331
17000     0.5022      0.8502     0.8478     55.1719
17100     0.5045      0.8312     0.8478     55.1490
17200     0.5111      0.8354     0.8478     54.7335
17300     0.5466      0.8165     0.8478     55.3987
17400     0.5132      0.8544     0.8478     55.3257
17500     0.5875      0.8080     0.8478     55.8681
17600     0.5053      0.8460     0.8478     55.7970
17700     0.5075      0.8228     0.8478     55.0085
17800     0.4840      0.8565     0.8478     55.3776
17900     0.5448      0.8249     0.8478     55.7728
18000     0.5178      0.8397     0.8478     54.9155
18100     0.5381      0.8143     0.8478     55.0861
18200     0.5652      0.8186     0.8478     55.4345
18300     0.4733      0.8418     0.8478     55.7490
18400     0.5478      0.8165     0.8478     55.7671
18500     0.5581      0.8165     0.8478     55.3796
18600     0.5236      0.8333     0.8478     55.2745
18700     0.4966      0.8397     0.8478     55.3342
18800     0.4935      0.8397     0.8478     55.2635
18900     0.4773      0.8565     0.8478     55.7898
19000     0.5336      0.8228     0.8478     55.2836
19100     0.5326      0.8228     0.8478     55.1048
19200     0.5286      0.8249     0.8478     55.9957
19300     0.5297      0.8101     0.8478     55.1845
19400     0.5034      0.8523     0.8478     56.4564
19500     0.5733      0.8207     0.8478     55.9938
19600     0.5115      0.8608     0.8478     55.2388
19700     0.4334      0.8608     0.8478     56.7049
19800     0.5273      0.8270     0.8478     55.4634
19900     0.5071      0.8565     0.8478     55.3811
20000     0.5060      0.8460     0.8478     56.0732
20100     0.5324      0.8165     0.8478     55.3762
20199     0.4888      0.8481     0.8478     54.9165
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     0.4519      0.8650     0.8436     8.7284
00100     0.3959      0.8797     0.8437     54.7413
00200     0.4754      0.8544     0.8493     55.1198
00300     0.4687      0.8481     0.8493     56.0918
00400     0.4275      0.8608     0.8493     55.2092
00500     0.3940      0.8671     0.8493     55.4923
00600     0.4569      0.8565     0.8493     55.5191
00700     0.4577      0.8376     0.8493     56.1630
00800     0.4414      0.8650     0.8493     55.6970
00900     0.5193      0.8228     0.8503     55.7138
01000     0.3981      0.8776     0.8503     54.8214
01100     0.3939      0.8861     0.8503     55.4429
01200     0.3792      0.8924     0.8532     55.1104
01300     0.4783      0.8439     0.8532     55.8135
01400     0.4115      0.8776     0.8532     55.7554
01500     0.4808      0.8439     0.8532     55.3010
01600     0.4702      0.8481     0.8532     55.7911
01700     0.3666      0.8924     0.8532     55.2231
01800     0.4850      0.8565     0.8532     55.6838
01900     0.4940      0.8418     0.8532     55.7417
02000     0.4365      0.8565     0.8532     55.0343
02100     0.4483      0.8650     0.8532     55.2663
02200     0.4196      0.8671     0.8532     55.5125
02300     0.4094      0.8629     0.8532     55.4768
02400     0.4061      0.8776     0.8532     55.7197
02500     0.3796      0.8734     0.8532     55.2239
02600     0.4348      0.8586     0.8532     54.8449
02700     0.4547      0.8565     0.8532     55.9298
02800     0.5033      0.8333     0.8532     54.9995
02900     0.4842      0.8397     0.8532     55.7862
03000     0.4674      0.8586     0.8538     55.1673
03100     0.4488      0.8502     0.8556     55.3467
03200     0.4803      0.8460     0.8556     55.1729
03300     0.4469      0.8608     0.8556     54.8945
03400     0.4894      0.8376     0.8556     55.3923
03500     0.3821      0.8819     0.8556     55.4001
03600     0.4218      0.8608     0.8556     55.3824
03700     0.4971      0.8586     0.8556     55.2857
03800     0.3783      0.8755     0.8556     55.8312
03900     0.4827      0.8502     0.8556     55.2034
04000     0.3779      0.8819     0.8556     55.5329
04100     0.4444      0.8608     0.8556     55.5495
04200     0.4951      0.8460     0.8556     55.3894
04300     0.3951      0.8755     0.8556     55.7478
04400     0.4175      0.8671     0.8556     55.0979
04500     0.3866      0.8671     0.8558     55.6189
04600     0.5090      0.8502     0.8558     56.2234
04700     0.4575      0.8544     0.8558     55.5212
04800     0.4568      0.8544     0.8558     55.6849
04900     0.5076      0.8354     0.8558     55.7121
05000     0.4281      0.8671     0.8558     55.3906
05100     0.3980      0.8671     0.8558     56.4848
05200     0.4186      0.8797     0.8558     55.0309
05300     0.3962      0.8903     0.8558     55.2179
05400     0.3951      0.8840     0.8558     55.4814
05500     0.5115      0.8270     0.8567     55.2154
05600     0.4426      0.8376     0.8567     55.5037
05700     0.3907      0.8650     0.8567     55.2384
05800     0.4290      0.8608     0.8567     55.1896
05900     0.4340      0.8523     0.8567     56.1716
06000     0.3752      0.8945     0.8567     55.3535
06100     0.4195      0.8734     0.8567     55.4315
06200     0.4796      0.8376     0.8567     55.6254
06300     0.4316      0.8565     0.8567     54.9296
06400     0.4195      0.8586     0.8567     55.6775
06500     0.4687      0.8502     0.8567     55.4373
06600     0.4321      0.8565     0.8567     55.2292
06700     0.4680      0.8502     0.8567     55.4401
06800     0.3787      0.8713     0.8567     55.3410
06900     0.3982      0.8608     0.8567     55.4702
07000     0.4397      0.8713     0.8567     55.2897
07100     0.4531      0.8523     0.8567     55.0963
07200     0.4361      0.8650     0.8567     55.5781
07300     0.4555      0.8544     0.8567     55.2040
07400     0.4778      0.8523     0.8567     55.9769
07500     0.5005      0.8502     0.8567     56.1942
07600     0.4924      0.8502     0.8567     55.1727
07700     0.4103      0.8861     0.8567     55.4755
07800     0.3779      0.8882     0.8567     55.9936
07900     0.3921      0.8671     0.8567     55.2303
08000     0.4406      0.8629     0.8567     55.6976
08100     0.4088      0.8734     0.8567     55.8687
08200     0.4197      0.8671     0.8567     55.1823
08300     0.3886      0.8671     0.8567     56.0575
08400     0.4359      0.8776     0.8567     55.0148
08500     0.4178      0.8629     0.8567     55.7341
08600     0.4026      0.8734     0.8567     55.6546
08700     0.5010      0.8460     0.8567     55.1791
08800     0.4094      0.8608     0.8567     56.1871
08900     0.4644      0.8418     0.8567     55.7751
09000     0.3943      0.8755     0.8567     54.9716
09100     0.3883      0.8797     0.8567     55.7589
09200     0.4545      0.8439     0.8567     54.9164
09300     0.5052      0.8333     0.8567     55.6495
09400     0.4117      0.8586     0.8567     55.6559
09500     0.4116      0.8608     0.8567     55.2273
09600     0.4139      0.8481     0.8567     55.8270
09700     0.4076      0.8734     0.8575     55.8070
09800     0.4597      0.8692     0.8575     55.1761
09900     0.4519      0.8565     0.8575     55.4026
Start testing:
Test Accuracy: 0.8454
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=9, quant_actNM=9, quant_inp=9, quant_w=9, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
29f9b86f-1372-495d-b668-1c6e163ca6ce
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     3.5462      0.0907     0.0903     11.3243
00100     2.2453      0.2257     0.2373     74.1788
00200     1.7845      0.4177     0.4259     69.8964
00300     1.5576      0.4810     0.5163     70.3968
00400     1.3268      0.5823     0.5955     71.8266
00500     1.2385      0.5970     0.6327     70.3577
