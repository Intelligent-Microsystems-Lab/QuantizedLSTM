Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 49, in __init__
    torch.nn.init.uniform_(self.cell.weight_ih, a = -limit2, b = limit2)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 49, in __init__
    torch.nn.init.uniform_(self.cell.weight_ih, a = -limit2, b = limit2)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 49, in __init__
    torch.nn.init.uniform_(self.cell.weight_ih, a = -limit2, b = limit2)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 49, in __init__
    torch.nn.init.uniform_(self.cell.weight_ih, a = -limit2, b = limit2)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 279, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.0, finetuning_epochs=0, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.0005,0.0001,0.00002', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,10000', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
c601a6b9-da14-4bcc-b620-ae03ac10ee3e
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     3.1953      0.0612     0.0699     11.4945
00100     2.5097      0.1287     0.1236     55.2771
00200     2.4355      0.1857     0.1946     56.1132
00300     2.2448      0.2236     0.2570     56.6132
00400     2.1001      0.2511     0.2718     55.7871
00500     2.0784      0.2827     0.2718     56.2211
00600     1.9772      0.3059     0.3032     56.3923
00700     1.9502      0.3186     0.3433     55.8842
00800     1.8555      0.3523     0.3496     61.6204
00900     1.9244      0.3586     0.4000     60.0385
01000     1.8211      0.3819     0.4334     56.6792
01100     1.7459      0.4367     0.4334     57.1870
01200     1.7635      0.3861     0.4588     56.8699
01300     1.6128      0.4198     0.4681     56.0933
01400     1.6123      0.4473     0.4709     56.8077
01500     1.6136      0.4852     0.5050     57.6567
01600     1.5722      0.4873     0.5079     56.8505
01700     1.6245      0.4515     0.5134     55.8810
01800     1.5568      0.4578     0.5134     56.0576
01900     1.5564      0.4684     0.5227     55.9946
02000     1.4891      0.5338     0.5227     55.8449
02100     1.5614      0.5063     0.5492     56.4499
02200     1.4429      0.5063     0.5628     57.7813
02300     1.4045      0.5274     0.5728     57.0112
02400     1.4034      0.5105     0.5728     57.2506
02500     1.4652      0.5042     0.5754     56.1006
02600     1.3874      0.5506     0.5754     55.8192
02700     1.4962      0.4916     0.5891     56.3531
02800     1.3448      0.5570     0.5950     56.0793
02900     1.3880      0.5401     0.6025     55.4776
03000     1.2690      0.6097     0.6025     56.8810
03100     1.4536      0.5042     0.6079     56.5835
03200     1.3124      0.5802     0.6079     56.2320
03300     1.2939      0.5844     0.6145     57.4266
03400     1.3421      0.5591     0.6167     56.2126
03500     1.2758      0.5759     0.6502     56.2992
03600     1.1670      0.6245     0.6502     56.1008
03700     1.1774      0.6181     0.6642     56.9042
03800     1.1414      0.6329     0.6725     56.9464
03900     1.1366      0.6477     0.6847     57.8407
04000     1.0980      0.6160     0.6847     57.4166
04100     1.0805      0.6477     0.6847     56.0608
04200     1.1834      0.5970     0.6848     56.3492
04300     1.0808      0.6751     0.6848     58.5593
04400     1.1187      0.6371     0.7041     56.3861
04500     1.0856      0.6350     0.7041     55.6166
04600     1.0108      0.6582     0.7041     57.9803
04700     1.0878      0.6414     0.7041     56.9209
04800     1.1448      0.6308     0.7073     58.6164
04900     1.1276      0.6224     0.7073     57.3161
05000     1.1952      0.5949     0.7073     56.3416
05100     1.2140      0.6308     0.7073     58.2399
05200     1.0648      0.6498     0.7155     58.2626
05300     1.0092      0.6371     0.7155     56.4706
05400     1.0541      0.6540     0.7155     58.6675
05500     1.1095      0.6245     0.7155     56.4152
05600     1.0150      0.6667     0.7155     56.3443
05700     1.0516      0.6477     0.7155     57.3122
05800     0.9723      0.6561     0.7155     56.2325
05900     1.1081      0.6582     0.7155     57.8623
06000     1.0742      0.6498     0.7155     57.9286
06100     1.0626      0.6519     0.7201     58.6055
06200     1.0209      0.6624     0.7201     58.5023
06300     1.1323      0.6329     0.7201     57.6987
06400     1.1012      0.6498     0.7201     56.2884
06500     1.0489      0.6730     0.7201     55.6935
06600     1.0523      0.6667     0.7273     57.2225
06700     0.9391      0.6688     0.7286     57.7016
06800     1.0685      0.6477     0.7286     56.7886
06900     0.9937      0.6498     0.7286     56.3558
07000     1.0257      0.6730     0.7335     56.3164
07100     0.9771      0.6435     0.7335     58.3519
07200     0.9413      0.7025     0.7336     57.1203
07300     0.9493      0.7004     0.7410     55.8756
07400     0.9255      0.6667     0.7410     56.4391
07500     1.0962      0.6329     0.7410     57.4474
07600     0.9956      0.6646     0.7410     56.4395
07700     0.9945      0.6751     0.7421     55.8031
07800     0.9335      0.7068     0.7421     56.0482
07900     1.0027      0.6920     0.7421     55.8898
08000     1.0792      0.6498     0.7421     57.0079
08100     0.8596      0.7405     0.7421     57.2630
08200     1.0664      0.6540     0.7421     57.3632
08300     1.0026      0.6857     0.7421     57.7582
08400     1.0330      0.6498     0.7421     55.9548
08500     1.0081      0.6603     0.7474     56.2443
08600     1.0586      0.6329     0.7474     57.3783
08700     0.9829      0.6772     0.7474     57.9243
08800     1.0688      0.6435     0.7474     58.2802
08900     0.9277      0.6814     0.7474     55.8675
09000     0.9150      0.6709     0.7474     55.4004
09100     0.9573      0.6920     0.7474     56.9547
09200     1.0202      0.6709     0.7474     56.0257
09300     0.9625      0.6751     0.7474     55.9875
09400     0.9046      0.6983     0.7474     57.8178
09500     0.9796      0.6793     0.7474     57.5928
09600     0.9047      0.7046     0.7474     57.2040
09700     0.8705      0.7173     0.7478     55.8109
09800     0.8857      0.7300     0.7482     56.0858
09900     0.9758      0.6941     0.7482     58.9503
10000     0.9990      0.6793     0.7482     56.2181
10100     0.9656      0.6941     0.7491     56.2566
10200     0.8712      0.7215     0.7521     57.6354
10300     0.8565      0.7194     0.7521     56.0229
10400     0.9463      0.7068     0.7521     57.0068
10500     0.9106      0.6899     0.7521     56.2023
10600     0.9107      0.7405     0.7521     55.8877
10700     0.9500      0.7089     0.7521     56.1711
10800     0.9218      0.7089     0.7521     55.9665
10900     0.8994      0.6941     0.7521     57.7150
11000     0.8999      0.7321     0.7583     57.3003
11100     0.8007      0.7405     0.7583     56.1950
11200     0.8655      0.7173     0.7583     57.7596
11300     0.9484      0.6920     0.7583     56.3450
11400     0.8841      0.7004     0.7583     56.7654
11500     0.8812      0.6899     0.7583     56.8429
11600     0.9397      0.6751     0.7583     55.9168
11700     0.9194      0.7300     0.7583     56.1276
11800     0.9894      0.6793     0.7607     57.6421
11900     0.9414      0.6962     0.7607     56.8548
12000     0.9076      0.7215     0.7638     56.6719
12100     0.8625      0.7131     0.7638     57.5540
12200     0.9330      0.7004     0.7638     58.0262
12300     1.0179      0.6709     0.7638     57.9017
12400     0.9230      0.6835     0.7638     58.4863
12500     0.8899      0.7152     0.7638     56.2874
12600     0.9037      0.6983     0.7638     56.7911
12700     0.9051      0.7173     0.7638     56.8479
12800     0.8912      0.6962     0.7638     58.2603
12900     0.8999      0.6857     0.7638     55.8775
13000     0.9239      0.7004     0.7638     55.6522
13100     0.9093      0.7131     0.7638     58.3827
13200     1.0975      0.6603     0.7638     56.3242
13300     0.8727      0.6962     0.7638     56.3277
13400     0.9477      0.6667     0.7638     56.5614
13500     0.9517      0.6793     0.7638     56.3604
13600     0.9325      0.7110     0.7638     56.4531
13700     0.9371      0.6962     0.7638     57.0624
13800     0.8963      0.7089     0.7642     56.9234
13900     0.9103      0.6899     0.7642     56.9906
14000     0.8878      0.7046     0.7642     56.9233
14100     0.9631      0.7089     0.7642     55.5759
14200     0.9214      0.7025     0.7642     56.5888
14300     0.8318      0.7363     0.7642     57.4088
14400     0.8386      0.7257     0.7642     57.0883
14500     0.9603      0.6983     0.7642     57.4803
14600     0.9230      0.7004     0.7642     56.1383
14700     0.8716      0.7131     0.7642     57.3801
14800     0.8668      0.7025     0.7642     56.5545
14900     0.8486      0.7215     0.7642     56.3332
15000     0.8373      0.7089     0.7642     57.2314
15100     0.8076      0.7321     0.7642     56.6960
15200     0.9635      0.6835     0.7642     57.3981
15300     1.0392      0.6646     0.7642     56.6592
15400     0.9701      0.7068     0.7642     56.8828
15500     0.7948      0.7489     0.7642     57.1248
15600     0.8907      0.7025     0.7727     56.3419
15700     0.9342      0.7194     0.7727     56.6467
15800     0.8833      0.7236     0.7727     58.1440
15900     0.9493      0.7089     0.7727     57.2418
16000     0.9374      0.6983     0.7727     57.9772
16100     0.8108      0.7342     0.7727     56.7885
16200     0.8277      0.7426     0.7727     56.1680
16300     0.8670      0.7194     0.7727     56.6460
16400     0.8879      0.7426     0.7727     55.5489
16500     0.8707      0.7553     0.7727     55.5035
16600     0.9368      0.7025     0.7727     57.3940
16700     0.8436      0.7152     0.7727     56.0121
16800     0.9218      0.6814     0.7727     56.7623
16900     0.8759      0.7025     0.7727     56.3661
17000     0.7657      0.7447     0.7727     57.5471
17100     0.8741      0.7405     0.7727     57.3383
17200     0.8229      0.7426     0.7727     56.2971
17300     0.8031      0.7468     0.7727     56.8540
17400     0.8611      0.7300     0.7749     57.3071
17500     0.9225      0.7236     0.7801     56.1184
17600     0.9040      0.6941     0.7801     56.9686
17700     0.7119      0.7764     0.7801     56.1205
17800     0.8998      0.6983     0.7801     57.5740
17900     0.9100      0.6835     0.7801     56.9445
18000     0.8775      0.7046     0.7801     56.4773
18100     0.9567      0.6962     0.7801     55.8265
18200     0.8328      0.7321     0.7801     57.8878
18300     0.9226      0.7046     0.7801     57.2856
18400     0.7735      0.7574     0.7801     57.0105
18500     0.8063      0.7468     0.7801     57.5776
18600     0.9199      0.7089     0.7801     56.2838
18700     0.8049      0.7342     0.7801     56.6222
18800     0.8868      0.7110     0.7801     55.8231
18900     0.8359      0.7131     0.7821     57.1049
19000     0.8193      0.7131     0.7821     57.4460
19100     0.8505      0.7215     0.7821     56.8603
19200     0.9439      0.7131     0.7821     57.1364
19300     0.8478      0.7257     0.7821     57.5934
19400     0.8811      0.7131     0.7821     58.8206
19500     0.7860      0.7468     0.7821     56.8793
19600     0.9161      0.7068     0.7821     56.5591
19700     0.9021      0.6983     0.7821     56.7854
19800     0.9234      0.7046     0.7821     56.5658
19900     0.8219      0.7236     0.7821     56.2337
20000     1.0167      0.6878     0.7821     57.8174
20100     0.9650      0.6899     0.7821     55.9421
20200     0.8068      0.7089     0.7821     56.9749
20300     0.8018      0.7426     0.7821     57.6847
20400     0.8889      0.7046     0.7827     56.2795
20500     0.8855      0.6835     0.7827     57.5199
20600     0.8191      0.7489     0.7827     57.4598
20700     0.8742      0.7236     0.7827     57.5000
20800     0.8554      0.7278     0.7827     58.8880
20900     0.7898      0.7321     0.7827     56.8547
21000     0.7801      0.7489     0.7827     55.7859
21100     0.8103      0.7278     0.7827     56.4840
21200     0.8245      0.7257     0.7827     56.1698
21300     0.8061      0.7405     0.7827     57.4146
21400     0.7599      0.7616     0.7827     56.8433
21500     0.8703      0.7321     0.7827     56.2567
21600     0.8321      0.7278     0.7827     56.7147
21700     0.7754      0.7468     0.7827     56.5272
21800     0.8594      0.7068     0.7827     56.5403
21900     0.8846      0.7173     0.7868     56.9510
22000     0.9206      0.6983     0.7868     57.4661
22100     0.7966      0.7342     0.7868     55.7413
22200     0.7893      0.7405     0.7868     56.6703
22300     0.8218      0.7342     0.7868     58.0918
22400     0.8374      0.7068     0.7868     57.4574
22500     0.9367      0.6941     0.7868     56.3683
22600     0.8290      0.7532     0.7868     55.6030
22700     0.8759      0.7025     0.7868     56.8699
22800     0.9015      0.7068     0.7868     55.4083
22900     0.8188      0.7426     0.7868     55.7502
23000     0.9077      0.7468     0.7868     56.4418
23100     0.9355      0.7046     0.7868     55.9927
23200     0.9364      0.7131     0.7868     57.0270
23300     0.7918      0.7363     0.7868     55.9449
23400     0.8097      0.7300     0.7868     56.9871
23500     0.8890      0.7025     0.7868     57.5881
23600     0.8597      0.7173     0.7868     56.4181
23700     0.7743      0.7278     0.7868     57.0896
23800     0.9089      0.7046     0.7868     56.7706
23900     0.8267      0.7321     0.7868     56.7667
24000     0.8952      0.7257     0.7868     58.1527
24100     0.8109      0.7363     0.7868     56.3575
24200     0.8777      0.7068     0.7868     55.6652
24300     0.8133      0.7384     0.7868     58.0952
24400     0.9202      0.7004     0.7868     56.9318
24500     0.8244      0.7215     0.7868     56.2198
24600     0.9297      0.7300     0.7868     57.7471
24700     0.7623      0.7489     0.7868     57.2374
24800     0.8031      0.7553     0.7868     58.7317
24900     0.8493      0.7363     0.7868     56.3713
25000     0.8889      0.7173     0.7868     58.3261
25100     0.8050      0.7426     0.7868     59.0836
25200     0.9768      0.6920     0.7868     56.8531
25300     0.8805      0.7173     0.7868     56.9688
25400     0.9194      0.7173     0.7868     58.0569
25500     0.9196      0.7173     0.7868     57.4206
25600     0.8942      0.7257     0.7868     59.2458
25700     0.7967      0.7553     0.7868     55.7291
25800     0.9278      0.7110     0.7868     55.8791
25900     0.8185      0.7173     0.7868     56.5454
26000     0.8485      0.7278     0.7868     56.2062
26100     0.8992      0.6814     0.7868     56.2453
26200     0.7636      0.7489     0.7868     56.4144
26300     0.8541      0.7300     0.7868     56.9477
26400     0.8209      0.7363     0.7868     56.6463
26500     0.8447      0.7321     0.7868     56.7575
26600     0.9685      0.7004     0.7868     57.5644
26700     0.8831      0.7152     0.7868     57.2574
26800     0.8742      0.7068     0.7868     56.2014
26900     0.9390      0.6899     0.7868     56.6674
27000     0.8266      0.7173     0.7868     57.1738
27100     0.8443      0.7384     0.7868     56.2607
27200     0.8311      0.7363     0.7868     57.3991
27300     1.0195      0.6835     0.7868     56.4672
27400     0.8387      0.7342     0.7868     56.3418
27500     0.8862      0.7405     0.7868     57.6480
27600     0.9045      0.7278     0.7890     59.1784
27700     0.8058      0.7616     0.7890     59.7337
27800     0.8634      0.7089     0.7890     56.0231
27900     0.8098      0.7342     0.7890     55.6436
28000     0.8388      0.7342     0.7890     56.7342
28100     0.8065      0.7405     0.7890     55.8493
28200     0.7903      0.7574     0.7890     56.1340
28300     0.8992      0.7278     0.7890     57.2642
28400     0.9434      0.7110     0.7890     57.6997
28500     0.7467      0.7595     0.7890     57.6515
28600     0.8180      0.7236     0.7890     57.5435
28700     0.7739      0.7278     0.7890     57.0694
28800     0.9351      0.7025     0.7890     55.3499
28900     0.9007      0.7110     0.7890     54.5148
29000     0.8344      0.7236     0.7890     56.4881
29100     0.8343      0.7257     0.7890     56.4447
29200     0.8377      0.7426     0.7890     55.6590
29300     0.8509      0.7236     0.7890     55.6866
29400     0.8401      0.7363     0.7890     57.3684
29500     0.8180      0.7300     0.7890     57.0169
29600     0.8326      0.7384     0.7890     56.5741
29700     0.8602      0.6941     0.7890     56.1810
29800     0.8683      0.7131     0.7890     56.3797
29900     0.7359      0.7511     0.7890     57.1471
29999     0.7932      0.7342     0.7890     55.2319
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Start testing:
Test Accuracy: 0.7659
