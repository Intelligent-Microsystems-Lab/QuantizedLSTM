Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 49, in __init__
    torch.nn.init.uniform_(self.cell.weight_ih, a = -limit2, b = limit2)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 49, in __init__
    torch.nn.init.uniform_(self.cell.weight_ih, a = -limit2, b = limit2)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 49, in __init__
    torch.nn.init.uniform_(self.cell.weight_ih, a = -limit2, b = limit2)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 49, in __init__
    torch.nn.init.uniform_(self.cell.weight_ih, a = -limit2, b = limit2)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 279, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.0, finetuning_epochs=0, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.0005,0.0001,0.00002', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,10000', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
c601a6b9-da14-4bcc-b620-ae03ac10ee3e
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     3.1953      0.0612     0.0699     11.4945
00100     2.5097      0.1287     0.1236     55.2771
00200     2.4355      0.1857     0.1946     56.1132
00300     2.2448      0.2236     0.2570     56.6132
00400     2.1001      0.2511     0.2718     55.7871
00500     2.0784      0.2827     0.2718     56.2211
00600     1.9772      0.3059     0.3032     56.3923
00700     1.9502      0.3186     0.3433     55.8842
00800     1.8555      0.3523     0.3496     61.6204
00900     1.9244      0.3586     0.4000     60.0385
01000     1.8211      0.3819     0.4334     56.6792
01100     1.7459      0.4367     0.4334     57.1870
01200     1.7635      0.3861     0.4588     56.8699
01300     1.6128      0.4198     0.4681     56.0933
01400     1.6123      0.4473     0.4709     56.8077
01500     1.6136      0.4852     0.5050     57.6567
01600     1.5722      0.4873     0.5079     56.8505
01700     1.6245      0.4515     0.5134     55.8810
01800     1.5568      0.4578     0.5134     56.0576
01900     1.5564      0.4684     0.5227     55.9946
02000     1.4891      0.5338     0.5227     55.8449
02100     1.5614      0.5063     0.5492     56.4499
02200     1.4429      0.5063     0.5628     57.7813
02300     1.4045      0.5274     0.5728     57.0112
02400     1.4034      0.5105     0.5728     57.2506
02500     1.4652      0.5042     0.5754     56.1006
02600     1.3874      0.5506     0.5754     55.8192
02700     1.4962      0.4916     0.5891     56.3531
02800     1.3448      0.5570     0.5950     56.0793
02900     1.3880      0.5401     0.6025     55.4776
03000     1.2690      0.6097     0.6025     56.8810
03100     1.4536      0.5042     0.6079     56.5835
03200     1.3124      0.5802     0.6079     56.2320
03300     1.2939      0.5844     0.6145     57.4266
03400     1.3421      0.5591     0.6167     56.2126
03500     1.2758      0.5759     0.6502     56.2992
03600     1.1670      0.6245     0.6502     56.1008
03700     1.1774      0.6181     0.6642     56.9042
03800     1.1414      0.6329     0.6725     56.9464
03900     1.1366      0.6477     0.6847     57.8407
04000     1.0980      0.6160     0.6847     57.4166
04100     1.0805      0.6477     0.6847     56.0608
04200     1.1834      0.5970     0.6848     56.3492
04300     1.0808      0.6751     0.6848     58.5593
04400     1.1187      0.6371     0.7041     56.3861
04500     1.0856      0.6350     0.7041     55.6166
04600     1.0108      0.6582     0.7041     57.9803
04700     1.0878      0.6414     0.7041     56.9209
04800     1.1448      0.6308     0.7073     58.6164
04900     1.1276      0.6224     0.7073     57.3161
05000     1.1952      0.5949     0.7073     56.3416
05100     1.2140      0.6308     0.7073     58.2399
05200     1.0648      0.6498     0.7155     58.2626
05300     1.0092      0.6371     0.7155     56.4706
05400     1.0541      0.6540     0.7155     58.6675
05500     1.1095      0.6245     0.7155     56.4152
05600     1.0150      0.6667     0.7155     56.3443
05700     1.0516      0.6477     0.7155     57.3122
05800     0.9723      0.6561     0.7155     56.2325
05900     1.1081      0.6582     0.7155     57.8623
06000     1.0742      0.6498     0.7155     57.9286
06100     1.0626      0.6519     0.7201     58.6055
06200     1.0209      0.6624     0.7201     58.5023
06300     1.1323      0.6329     0.7201     57.6987
06400     1.1012      0.6498     0.7201     56.2884
06500     1.0489      0.6730     0.7201     55.6935
06600     1.0523      0.6667     0.7273     57.2225
06700     0.9391      0.6688     0.7286     57.7016
06800     1.0685      0.6477     0.7286     56.7886
06900     0.9937      0.6498     0.7286     56.3558
07000     1.0257      0.6730     0.7335     56.3164
07100     0.9771      0.6435     0.7335     58.3519
07200     0.9413      0.7025     0.7336     57.1203
07300     0.9493      0.7004     0.7410     55.8756
07400     0.9255      0.6667     0.7410     56.4391
07500     1.0962      0.6329     0.7410     57.4474
07600     0.9956      0.6646     0.7410     56.4395
07700     0.9945      0.6751     0.7421     55.8031
07800     0.9335      0.7068     0.7421     56.0482
07900     1.0027      0.6920     0.7421     55.8898
08000     1.0792      0.6498     0.7421     57.0079
08100     0.8596      0.7405     0.7421     57.2630
08200     1.0664      0.6540     0.7421     57.3632
08300     1.0026      0.6857     0.7421     57.7582
08400     1.0330      0.6498     0.7421     55.9548
08500     1.0081      0.6603     0.7474     56.2443
08600     1.0586      0.6329     0.7474     57.3783
08700     0.9829      0.6772     0.7474     57.9243
08800     1.0688      0.6435     0.7474     58.2802
08900     0.9277      0.6814     0.7474     55.8675
09000     0.9150      0.6709     0.7474     55.4004
09100     0.9573      0.6920     0.7474     56.9547
09200     1.0202      0.6709     0.7474     56.0257
09300     0.9625      0.6751     0.7474     55.9875
09400     0.9046      0.6983     0.7474     57.8178
09500     0.9796      0.6793     0.7474     57.5928
09600     0.9047      0.7046     0.7474     57.2040
09700     0.8705      0.7173     0.7478     55.8109
09800     0.8857      0.7300     0.7482     56.0858
09900     0.9758      0.6941     0.7482     58.9503
10000     0.9990      0.6793     0.7482     56.2181
10100     0.9656      0.6941     0.7491     56.2566
10200     0.8712      0.7215     0.7521     57.6354
10300     0.8565      0.7194     0.7521     56.0229
10400     0.9463      0.7068     0.7521     57.0068
10500     0.9106      0.6899     0.7521     56.2023
10600     0.9107      0.7405     0.7521     55.8877
10700     0.9500      0.7089     0.7521     56.1711
10800     0.9218      0.7089     0.7521     55.9665
10900     0.8994      0.6941     0.7521     57.7150
11000     0.8999      0.7321     0.7583     57.3003
11100     0.8007      0.7405     0.7583     56.1950
11200     0.8655      0.7173     0.7583     57.7596
11300     0.9484      0.6920     0.7583     56.3450
11400     0.8841      0.7004     0.7583     56.7654
11500     0.8812      0.6899     0.7583     56.8429
11600     0.9397      0.6751     0.7583     55.9168
11700     0.9194      0.7300     0.7583     56.1276
11800     0.9894      0.6793     0.7607     57.6421
11900     0.9414      0.6962     0.7607     56.8548
12000     0.9076      0.7215     0.7638     56.6719
12100     0.8625      0.7131     0.7638     57.5540
12200     0.9330      0.7004     0.7638     58.0262
12300     1.0179      0.6709     0.7638     57.9017
12400     0.9230      0.6835     0.7638     58.4863
12500     0.8899      0.7152     0.7638     56.2874
12600     0.9037      0.6983     0.7638     56.7911
12700     0.9051      0.7173     0.7638     56.8479
12800     0.8912      0.6962     0.7638     58.2603
12900     0.8999      0.6857     0.7638     55.8775
13000     0.9239      0.7004     0.7638     55.6522
13100     0.9093      0.7131     0.7638     58.3827
13200     1.0975      0.6603     0.7638     56.3242
13300     0.8727      0.6962     0.7638     56.3277
13400     0.9477      0.6667     0.7638     56.5614
13500     0.9517      0.6793     0.7638     56.3604
13600     0.9325      0.7110     0.7638     56.4531
13700     0.9371      0.6962     0.7638     57.0624
13800     0.8963      0.7089     0.7642     56.9234
13900     0.9103      0.6899     0.7642     56.9906
14000     0.8878      0.7046     0.7642     56.9233
14100     0.9631      0.7089     0.7642     55.5759
14200     0.9214      0.7025     0.7642     56.5888
14300     0.8318      0.7363     0.7642     57.4088
14400     0.8386      0.7257     0.7642     57.0883
14500     0.9603      0.6983     0.7642     57.4803
14600     0.9230      0.7004     0.7642     56.1383
14700     0.8716      0.7131     0.7642     57.3801
14800     0.8668      0.7025     0.7642     56.5545
14900     0.8486      0.7215     0.7642     56.3332
15000     0.8373      0.7089     0.7642     57.2314
15100     0.8076      0.7321     0.7642     56.6960
15200     0.9635      0.6835     0.7642     57.3981
15300     1.0392      0.6646     0.7642     56.6592
15400     0.9701      0.7068     0.7642     56.8828
15500     0.7948      0.7489     0.7642     57.1248
15600     0.8907      0.7025     0.7727     56.3419
15700     0.9342      0.7194     0.7727     56.6467
15800     0.8833      0.7236     0.7727     58.1440
15900     0.9493      0.7089     0.7727     57.2418
16000     0.9374      0.6983     0.7727     57.9772
16100     0.8108      0.7342     0.7727     56.7885
16200     0.8277      0.7426     0.7727     56.1680
16300     0.8670      0.7194     0.7727     56.6460
16400     0.8879      0.7426     0.7727     55.5489
16500     0.8707      0.7553     0.7727     55.5035
16600     0.9368      0.7025     0.7727     57.3940
16700     0.8436      0.7152     0.7727     56.0121
16800     0.9218      0.6814     0.7727     56.7623
16900     0.8759      0.7025     0.7727     56.3661
17000     0.7657      0.7447     0.7727     57.5471
17100     0.8741      0.7405     0.7727     57.3383
17200     0.8229      0.7426     0.7727     56.2971
17300     0.8031      0.7468     0.7727     56.8540
17400     0.8611      0.7300     0.7749     57.3071
17500     0.9225      0.7236     0.7801     56.1184
17600     0.9040      0.6941     0.7801     56.9686
17700     0.7119      0.7764     0.7801     56.1205
17800     0.8998      0.6983     0.7801     57.5740
17900     0.9100      0.6835     0.7801     56.9445
18000     0.8775      0.7046     0.7801     56.4773
18100     0.9567      0.6962     0.7801     55.8265
18200     0.8328      0.7321     0.7801     57.8878
18300     0.9226      0.7046     0.7801     57.2856
18400     0.7735      0.7574     0.7801     57.0105
18500     0.8063      0.7468     0.7801     57.5776
18600     0.9199      0.7089     0.7801     56.2838
18700     0.8049      0.7342     0.7801     56.6222
18800     0.8868      0.7110     0.7801     55.8231
18900     0.8359      0.7131     0.7821     57.1049
19000     0.8193      0.7131     0.7821     57.4460
19100     0.8505      0.7215     0.7821     56.8603
19200     0.9439      0.7131     0.7821     57.1364
19300     0.8478      0.7257     0.7821     57.5934
19400     0.8811      0.7131     0.7821     58.8206
19500     0.7860      0.7468     0.7821     56.8793
19600     0.9161      0.7068     0.7821     56.5591
19700     0.9021      0.6983     0.7821     56.7854
19800     0.9234      0.7046     0.7821     56.5658
19900     0.8219      0.7236     0.7821     56.2337
20000     1.0167      0.6878     0.7821     57.8174
20100     0.9650      0.6899     0.7821     55.9421
20200     0.8068      0.7089     0.7821     56.9749
20300     0.8018      0.7426     0.7821     57.6847
20400     0.8889      0.7046     0.7827     56.2795
20500     0.8855      0.6835     0.7827     57.5199
20600     0.8191      0.7489     0.7827     57.4598
20700     0.8742      0.7236     0.7827     57.5000
20800     0.8554      0.7278     0.7827     58.8880
20900     0.7898      0.7321     0.7827     56.8547
21000     0.7801      0.7489     0.7827     55.7859
21100     0.8103      0.7278     0.7827     56.4840
21200     0.8245      0.7257     0.7827     56.1698
21300     0.8061      0.7405     0.7827     57.4146
21400     0.7599      0.7616     0.7827     56.8433
21500     0.8703      0.7321     0.7827     56.2567
21600     0.8321      0.7278     0.7827     56.7147
21700     0.7754      0.7468     0.7827     56.5272
21800     0.8594      0.7068     0.7827     56.5403
21900     0.8846      0.7173     0.7868     56.9510
22000     0.9206      0.6983     0.7868     57.4661
22100     0.7966      0.7342     0.7868     55.7413
22200     0.7893      0.7405     0.7868     56.6703
22300     0.8218      0.7342     0.7868     58.0918
22400     0.8374      0.7068     0.7868     57.4574
22500     0.9367      0.6941     0.7868     56.3683
22600     0.8290      0.7532     0.7868     55.6030
22700     0.8759      0.7025     0.7868     56.8699
22800     0.9015      0.7068     0.7868     55.4083
22900     0.8188      0.7426     0.7868     55.7502
23000     0.9077      0.7468     0.7868     56.4418
23100     0.9355      0.7046     0.7868     55.9927
23200     0.9364      0.7131     0.7868     57.0270
23300     0.7918      0.7363     0.7868     55.9449
23400     0.8097      0.7300     0.7868     56.9871
23500     0.8890      0.7025     0.7868     57.5881
23600     0.8597      0.7173     0.7868     56.4181
23700     0.7743      0.7278     0.7868     57.0896
23800     0.9089      0.7046     0.7868     56.7706
23900     0.8267      0.7321     0.7868     56.7667
24000     0.8952      0.7257     0.7868     58.1527
24100     0.8109      0.7363     0.7868     56.3575
24200     0.8777      0.7068     0.7868     55.6652
24300     0.8133      0.7384     0.7868     58.0952
24400     0.9202      0.7004     0.7868     56.9318
24500     0.8244      0.7215     0.7868     56.2198
24600     0.9297      0.7300     0.7868     57.7471
24700     0.7623      0.7489     0.7868     57.2374
24800     0.8031      0.7553     0.7868     58.7317
24900     0.8493      0.7363     0.7868     56.3713
25000     0.8889      0.7173     0.7868     58.3261
25100     0.8050      0.7426     0.7868     59.0836
25200     0.9768      0.6920     0.7868     56.8531
25300     0.8805      0.7173     0.7868     56.9688
25400     0.9194      0.7173     0.7868     58.0569
25500     0.9196      0.7173     0.7868     57.4206
25600     0.8942      0.7257     0.7868     59.2458
25700     0.7967      0.7553     0.7868     55.7291
25800     0.9278      0.7110     0.7868     55.8791
25900     0.8185      0.7173     0.7868     56.5454
26000     0.8485      0.7278     0.7868     56.2062
26100     0.8992      0.6814     0.7868     56.2453
26200     0.7636      0.7489     0.7868     56.4144
26300     0.8541      0.7300     0.7868     56.9477
26400     0.8209      0.7363     0.7868     56.6463
26500     0.8447      0.7321     0.7868     56.7575
26600     0.9685      0.7004     0.7868     57.5644
26700     0.8831      0.7152     0.7868     57.2574
26800     0.8742      0.7068     0.7868     56.2014
26900     0.9390      0.6899     0.7868     56.6674
27000     0.8266      0.7173     0.7868     57.1738
27100     0.8443      0.7384     0.7868     56.2607
27200     0.8311      0.7363     0.7868     57.3991
27300     1.0195      0.6835     0.7868     56.4672
27400     0.8387      0.7342     0.7868     56.3418
27500     0.8862      0.7405     0.7868     57.6480
27600     0.9045      0.7278     0.7890     59.1784
27700     0.8058      0.7616     0.7890     59.7337
27800     0.8634      0.7089     0.7890     56.0231
27900     0.8098      0.7342     0.7890     55.6436
28000     0.8388      0.7342     0.7890     56.7342
28100     0.8065      0.7405     0.7890     55.8493
28200     0.7903      0.7574     0.7890     56.1340
28300     0.8992      0.7278     0.7890     57.2642
28400     0.9434      0.7110     0.7890     57.6997
28500     0.7467      0.7595     0.7890     57.6515
28600     0.8180      0.7236     0.7890     57.5435
28700     0.7739      0.7278     0.7890     57.0694
28800     0.9351      0.7025     0.7890     55.3499
28900     0.9007      0.7110     0.7890     54.5148
29000     0.8344      0.7236     0.7890     56.4881
29100     0.8343      0.7257     0.7890     56.4447
29200     0.8377      0.7426     0.7890     55.6590
29300     0.8509      0.7236     0.7890     55.6866
29400     0.8401      0.7363     0.7890     57.3684
29500     0.8180      0.7300     0.7890     57.0169
29600     0.8326      0.7384     0.7890     56.5741
29700     0.8602      0.6941     0.7890     56.1810
29800     0.8683      0.7131     0.7890     56.3797
29900     0.7359      0.7511     0.7890     57.1471
29999     0.7932      0.7342     0.7890     55.2319
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Start testing:
Test Accuracy: 0.7659
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
c643d9a4-9aaa-4695-a902-2b7cc9e2bbb4
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     3.1193      0.0612     0.0866     11.5604
00100     2.2482      0.2342     0.0930     55.4338
00200     2.1560      0.2722     0.0987     55.3954
00300     1.8983      0.3755     0.0987     55.2385
00400     1.8762      0.3945     0.0987     54.8223
00500     1.8800      0.3544     0.1294     55.2206
00600     1.6234      0.4620     0.1294     55.3677
00700     1.4395      0.5253     0.1369     55.1328
00800     1.4082      0.5253     0.1369     54.8391
00900     1.3215      0.5675     0.1398     55.2222
01000     1.4526      0.5422     0.1398     55.1504
01100     1.2478      0.5844     0.1407     55.9796
01200     1.2302      0.6034     0.1442     54.9223
01300     1.2594      0.6034     0.1789     55.8555
01400     1.0490      0.6477     0.1789     55.6170
01500     1.0607      0.6709     0.1789     55.1606
01600     1.0939      0.6055     0.1789     56.1622
01700     1.0681      0.6646     0.1789     55.0947
01800     1.1410      0.6245     0.1789     55.0377
01900     1.0429      0.6561     0.1789     55.4779
02000     1.0846      0.6371     0.1789     55.5279
02100     1.1344      0.6350     0.1789     55.2397
02200     1.0025      0.6793     0.1789     56.1240
02300     1.0688      0.6751     0.1789     55.1806
02400     1.0293      0.6561     0.1789     55.3394
02500     0.9071      0.7342     0.1904     55.1973
02600     1.0361      0.6667     0.1904     55.1303
02700     0.9927      0.6941     0.1904     55.6189
02800     0.9886      0.7004     0.2116     55.8001
02900     1.0107      0.6793     0.2116     54.6002
03000     0.9209      0.6899     0.2116     56.0290
03100     0.8455      0.7236     0.2116     56.0940
03200     0.8697      0.7068     0.2116     55.8481
03300     0.9256      0.7046     0.2116     55.9654
03400     0.9564      0.6835     0.2116     55.2698
03500     0.8608      0.7194     0.2116     55.7760
03600     0.8916      0.6983     0.2350     55.0966
03700     0.8693      0.7384     0.2350     55.5503
03800     0.9070      0.7089     0.2350     55.9603
03900     0.9072      0.7215     0.2350     55.0903
04000     0.8310      0.7236     0.2366     55.7520
04100     0.8477      0.7384     0.2366     54.9702
04200     0.8485      0.7468     0.2366     56.3203
04300     0.8483      0.7215     0.2442     55.4439
04400     0.8838      0.7131     0.2442     55.6378
04500     0.9091      0.6983     0.2522     55.7788
04600     0.8522      0.7468     0.2522     55.9097
04700     0.7820      0.7595     0.2532     55.3704
04800     0.9168      0.7131     0.2532     56.9062
04900     0.8511      0.7511     0.2532     55.2928
05000     0.8806      0.7004     0.2532     55.5754
05100     0.7495      0.7658     0.2683     55.8190
05200     0.7817      0.7426     0.2877     54.7267
05300     0.7994      0.7236     0.2877     55.4345
05400     0.7921      0.7321     0.2877     55.9885
05500     0.7929      0.7278     0.2877     54.9817
05600     0.8048      0.7384     0.2877     55.5574
05700     0.7812      0.7553     0.2877     55.2242
05800     0.8000      0.7658     0.2877     54.6006
05900     0.8166      0.7532     0.2877     55.7328
06000     0.7696      0.7595     0.2877     55.2382
06100     0.9970      0.6899     0.2877     55.9156
06200     0.8496      0.7278     0.2877     56.3980
06300     0.7558      0.7468     0.2877     55.1174
06400     0.8746      0.7342     0.2877     55.9513
06500     0.8531      0.7110     0.2877     54.3938
06600     0.7204      0.7595     0.2966     55.2495
06700     0.8249      0.7447     0.2966     56.0302
06800     0.8327      0.7278     0.2966     55.8122
06900     0.8384      0.7215     0.2966     56.5038
07000     0.7691      0.7722     0.3034     55.6399
07100     0.7887      0.7637     0.3034     55.3231
07200     0.8684      0.7447     0.3150     55.9752
07300     0.7980      0.7468     0.3220     55.1923
07400     0.7214      0.7743     0.3439     55.1886
07500     0.7487      0.7532     0.3608     56.0478
07600     0.7438      0.7806     0.3608     55.1175
07700     0.6782      0.7764     0.3608     55.0781
07800     0.7693      0.7468     0.3608     55.7238
07900     0.7373      0.7700     0.3634     55.1347
08000     0.7503      0.7679     0.3634     55.6890
08100     0.7809      0.7468     0.3666     55.4465
08200     0.7203      0.7785     0.3666     55.5399
08300     0.6872      0.7764     0.3719     55.9134
08400     0.6988      0.8017     0.3947     55.0087
08500     0.8299      0.7278     0.3947     55.3618
08600     0.7543      0.7405     0.3947     55.7673
08700     0.6444      0.7996     0.3947     55.4218
08800     0.7861      0.7743     0.3947     56.1775
08900     0.8022      0.7384     0.3947     55.8184
09000     0.7132      0.7827     0.3993     56.3085
09100     0.7699      0.7806     0.4079     56.7793
09200     0.7563      0.7743     0.4079     55.5769
09300     0.8500      0.7384     0.4198     55.9681
09400     0.7993      0.7532     0.4310     56.3548
09500     0.7387      0.7658     0.4310     55.4263
09600     0.7060      0.7637     0.4310     55.7907
09700     0.6954      0.7764     0.4310     55.7257
09800     0.8252      0.7574     0.4310     55.8549
09900     0.8036      0.7321     0.4310     55.4321
10000     0.7062      0.7932     0.4310     54.8032
10100     0.7953      0.7532     0.4310     55.4543
10200     0.6854      0.7785     0.4310     55.6411
10300     0.6550      0.7911     0.4377     55.2642
10400     0.7174      0.7722     0.4377     55.7085
10500     0.6481      0.8080     0.4377     54.6673
10600     0.7708      0.7595     0.4377     55.7123
10700     0.6560      0.7932     0.4377     55.6825
10800     0.6568      0.7996     0.4377     55.4061
10900     0.6245      0.8143     0.4377     55.4595
11000     0.6444      0.7679     0.4377     55.9237
11100     0.7979      0.7405     0.4421     55.0342
11200     0.8423      0.7468     0.4421     55.8144
11300     0.7457      0.7700     0.4421     56.0404
11400     0.6446      0.8038     0.4421     55.2901
11500     0.6597      0.7996     0.4421     56.0667
11600     0.6873      0.8017     0.4421     55.7026
11700     0.7451      0.7679     0.4421     55.4169
11800     0.7159      0.7785     0.4421     55.4898
11900     0.7338      0.7911     0.4421     55.0807
12000     0.7213      0.7743     0.4421     56.0038
12100     0.7032      0.7975     0.4421     54.8021
12200     0.6714      0.7848     0.4421     55.3008
12300     0.6446      0.7975     0.4421     55.7606
12400     0.7497      0.7785     0.4421     55.1636
12500     0.7232      0.7679     0.4421     55.5375
12600     0.6403      0.7975     0.4421     55.5801
12700     0.7279      0.7574     0.4421     55.0319
12800     0.6345      0.8143     0.4421     55.6165
12900     0.6511      0.7911     0.4421     55.1957
13000     0.7220      0.7869     0.4482     55.5356
13100     0.6623      0.7975     0.4482     55.9889
13200     0.6830      0.7869     0.4482     55.1957
13300     0.6626      0.7975     0.4482     55.8185
13400     0.6470      0.7932     0.4482     56.4710
13500     0.7355      0.7869     0.4482     55.6781
13600     0.7011      0.8038     0.4482     55.6624
13700     0.6589      0.8038     0.4482     54.9483
13800     0.7801      0.7595     0.4571     55.8079
13900     0.6199      0.7954     0.4571     55.8414
14000     0.5911      0.8080     0.4571     55.1655
14100     0.7545      0.7489     0.4571     55.2577
14200     0.6746      0.7827     0.4571     56.2828
14300     0.6503      0.7932     0.4571     55.7420
14400     0.7415      0.7827     0.4571     56.3496
14500     0.5929      0.8080     0.4571     54.6981
14600     0.6136      0.7890     0.4590     55.4158
14700     0.6075      0.8101     0.4590     55.4266
14800     0.6259      0.8122     0.4590     55.2542
14900     0.5587      0.8228     0.4590     55.0587
15000     0.6415      0.8122     0.4590     55.5837
15100     0.6348      0.7785     0.4590     55.6274
15200     0.6797      0.8059     0.4590     56.2586
15300     0.7591      0.7743     0.4590     55.5089
15400     0.6924      0.7785     0.4590     54.9541
15500     0.5941      0.8228     0.4590     55.7455
15600     0.5496      0.8354     0.4613     55.3620
15700     0.7327      0.7679     0.4613     55.2802
15800     0.5868      0.8080     0.4613     55.3464
15900     0.5895      0.8165     0.4613     54.9469
16000     0.6724      0.7975     0.4613     55.9493
16100     0.6920      0.7595     0.4613     55.9490
16200     0.6125      0.8017     0.4613     55.7339
16300     0.6645      0.7722     0.4613     56.1215
16400     0.7453      0.7468     0.4613     56.0193
16500     0.6080      0.7996     0.4613     55.2011
16600     0.6133      0.7975     0.4613     55.6674
16700     0.6263      0.7806     0.4613     55.6864
16800     0.7270      0.7637     0.4613     55.8263
16900     0.7568      0.7637     0.4613     54.9586
17000     0.6001      0.8122     0.4613     55.6928
17100     0.6968      0.7848     0.4613     55.7628
17200     0.6189      0.7996     0.4613     55.1925
17300     0.6583      0.7954     0.4823     55.1909
17400     0.6780      0.7848     0.4823     56.0138
17500     0.6626      0.8038     0.4823     55.3295
17600     0.7269      0.7679     0.4823     55.0825
17700     0.6610      0.8059     0.4823     54.6760
17800     0.6230      0.8038     0.4823     55.9857
17900     0.6637      0.7827     0.4823     56.3747
18000     0.5589      0.8312     0.4823     55.5118
18100     0.6518      0.7975     0.4823     55.2422
18200     0.7007      0.8080     0.4823     56.1018
18300     0.7170      0.7700     0.4823     54.9964
18400     0.7108      0.7954     0.4823     55.2485
18500     0.6501      0.7932     0.4823     55.7526
18600     0.6877      0.7827     0.4823     55.0931
18700     0.6812      0.7764     0.4923     56.0320
18800     0.6301      0.8122     0.4923     54.8430
18900     0.7311      0.7827     0.4923     55.5097
19000     0.6461      0.7869     0.4923     55.3734
19100     0.6045      0.8228     0.4923     55.2219
19200     0.7067      0.7890     0.4923     55.7140
19300     0.6397      0.8080     0.4923     55.0811
19400     0.6651      0.7932     0.4923     55.7210
19500     0.7954      0.7468     0.4923     55.8980
19600     0.6673      0.7911     0.4923     55.4571
19700     0.6946      0.7764     0.4923     54.9802
19800     0.6572      0.8038     0.4923     56.0189
19900     0.6746      0.7932     0.4923     55.6724
20000     0.5823      0.8249     0.4923     55.4500
20100     0.6420      0.7848     0.4923     55.2010
20199     0.6615      0.7932     0.4923     54.4778
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.1859      0.4072     0.4700     9.0997
00100     1.7773      0.4873     0.5987     55.2130
00200     1.2559      0.6540     0.6481     54.3972
00300     1.1986      0.6603     0.6816     55.4643
00400     1.1561      0.6392     0.6877     54.2289
00500     1.2807      0.6287     0.7104     54.3370
00600     1.3164      0.6055     0.7188     55.0252
00700     1.2902      0.6034     0.7188     54.7496
00800     1.0481      0.6941     0.7188     54.7643
00900     1.0911      0.6793     0.7302     54.8484
01000     1.1875      0.6435     0.7302     55.6737
01100     1.1402      0.6857     0.7302     54.9554
01200     1.1056      0.6857     0.7302     54.3615
01300     0.9798      0.7068     0.7302     54.2541
01400     1.1730      0.6878     0.7347     55.3266
01500     1.0827      0.6814     0.7347     53.4946
01600     1.1082      0.6603     0.7409     54.1505
01700     1.0791      0.6962     0.7423     54.5313
01800     1.0180      0.7068     0.7515     54.2340
01900     1.0882      0.6835     0.7515     54.8722
02000     1.0141      0.6688     0.7515     54.1901
02100     1.0353      0.7131     0.7515     54.5001
02200     1.1445      0.6646     0.7515     54.1351
02300     1.2454      0.6371     0.7515     54.3645
02400     0.9919      0.6709     0.7515     53.9624
02500     1.0583      0.6624     0.7571     54.5762
02600     1.1405      0.6477     0.7571     53.6542
02700     1.1425      0.6540     0.7571     54.5016
02800     1.1174      0.6793     0.7571     54.0808
02900     1.0560      0.6667     0.7571     54.1114
03000     1.1213      0.6561     0.7604     54.3062
03100     0.9534      0.7215     0.7604     54.0945
03200     1.0730      0.6793     0.7604     53.7219
03300     1.1060      0.6519     0.7604     54.6035
03400     0.9641      0.7089     0.7604     53.7510
03500     1.0299      0.6793     0.7604     54.6352
03600     1.0324      0.6772     0.7604     54.2699
03700     1.0013      0.6814     0.7604     54.2445
03800     1.0151      0.7194     0.7604     54.2601
03900     1.0722      0.6709     0.7604     53.7197
04000     1.0263      0.6920     0.7604     54.0380
04100     1.1283      0.6540     0.7604     54.0199
04200     1.1090      0.6582     0.7604     54.1463
04300     0.9773      0.7152     0.7604     54.5978
04400     1.0709      0.6941     0.7604     54.3707
04500     1.0452      0.6772     0.7604     54.5861
04600     1.0396      0.6878     0.7604     55.4106
04700     1.0573      0.6793     0.7604     54.2331
04800     1.0649      0.6709     0.7637     54.4648
04900     1.0451      0.6962     0.7637     54.6232
05000     1.0162      0.6983     0.7637     54.2116
05100     0.9478      0.6983     0.7637     54.7272
05200     1.2247      0.6055     0.7637     53.9909
05300     1.0037      0.7068     0.7637     54.1672
05400     0.9968      0.7004     0.7683     54.8912
05500     0.8990      0.7257     0.7683     54.7663
05600     1.0042      0.7131     0.7683     54.3598
05700     0.9640      0.7110     0.7683     54.7448
05800     1.0060      0.7194     0.7683     53.7331
05900     1.0129      0.6772     0.7683     54.8508
06000     0.9733      0.7131     0.7683     54.1659
06100     1.0553      0.6899     0.7683     54.4653
06200     0.9782      0.6899     0.7683     54.8558
06300     0.9511      0.7278     0.7683     54.0911
06400     0.9733      0.7152     0.7683     54.2065
06500     1.1252      0.6561     0.7683     55.1365
06600     0.8761      0.7426     0.7683     54.2991
06700     1.0117      0.7068     0.7686     55.1074
06800     0.9670      0.6962     0.7686     54.3101
06900     0.9640      0.7089     0.7713     53.9319
07000     0.9578      0.7236     0.7713     54.1752
07100     1.0393      0.7046     0.7713     54.1324
07200     0.8855      0.7658     0.7713     53.6781
07300     0.9829      0.7089     0.7713     54.7662
07400     0.9983      0.6835     0.7713     54.1232
07500     1.0434      0.7068     0.7713     54.5359
07600     0.9006      0.7173     0.7713     53.9460
07700     0.9998      0.7004     0.7713     54.0900
07800     0.9576      0.7405     0.7730     55.0573
07900     1.0478      0.6772     0.7730     54.1715
08000     0.9812      0.7025     0.7730     55.0455
08100     0.9379      0.7321     0.7730     54.0188
08200     0.9223      0.7321     0.7730     53.9258
08300     0.9908      0.7089     0.7730     54.4740
08400     0.8812      0.7468     0.7730     54.1737
08500     1.0614      0.6941     0.7730     54.5208
08600     0.9809      0.7278     0.7730     54.5573
08700     0.9379      0.7025     0.7730     54.1801
08800     1.0390      0.6962     0.7730     53.9228
08900     1.0139      0.7025     0.7730     54.1097
09000     0.8563      0.7447     0.7774     53.9963
09100     0.9048      0.7300     0.7774     54.3322
09200     1.0040      0.7068     0.7774     54.0848
09300     1.0116      0.6899     0.7800     54.0168
09400     0.9979      0.7046     0.7800     54.5490
09500     0.9639      0.6920     0.7800     53.7963
09600     0.9472      0.6920     0.7800     53.9857
09700     0.9742      0.6962     0.7800     55.0560
09800     0.9660      0.6962     0.7800     54.3013
09900     0.8989      0.7004     0.7800     54.7189
Start testing:
Test Accuracy: 0.8050
