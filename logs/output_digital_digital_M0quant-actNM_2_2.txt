Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 279, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.0, finetuning_epochs=0, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.0005,0.0001,0.00002', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,10000', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
b4b4174b-d617-4a90-9ab7-ea1d02f8b2a7
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.5257      0.1160     0.1004     11.7788
00100     2.5256      0.1013     0.1004     55.2087
00200     2.5256      0.1076     0.1004     54.9891
00300     2.5256      0.0949     0.1004     55.1360
00400     2.5256      0.1055     0.1006     54.3815
00500     2.5256      0.1055     0.1006     54.9678
00600     2.5255      0.1160     0.1006     55.1698
00700     2.5255      0.1266     0.1008     54.9820
00800     2.5255      0.1139     0.1008     56.3445
00900     2.5255      0.0865     0.1008     55.4302
01000     2.5254      0.0886     0.1008     55.0902
01100     2.5254      0.0823     0.1008     55.6039
01200     2.5254      0.0865     0.1008     55.0608
01300     2.5254      0.1160     0.1008     55.0913
01400     2.5254      0.0992     0.1008     55.7326
01500     2.5253      0.0992     0.1008     55.3670
01600     2.5253      0.1034     0.1008     55.7141
01700     2.5253      0.0886     0.1008     54.8763
01800     2.5253      0.1160     0.1008     54.5954
01900     2.5252      0.0633     0.1008     55.5360
02000     2.5252      0.0970     0.1008     55.2624
02100     2.5252      0.1013     0.1008     55.3588
02200     2.5252      0.0949     0.1008     55.4923
02300     2.5252      0.0949     0.1008     55.4095
02400     2.5251      0.1266     0.1008     55.7287
02500     2.5251      0.0675     0.1008     55.1317
02600     2.5251      0.1076     0.1008     55.1626
02700     2.5251      0.0949     0.1008     55.8566
02800     2.5251      0.1013     0.1008     55.2878
02900     2.5250      0.0949     0.1008     55.2645
03000     2.5250      0.1118     0.1008     55.4396
03100     2.5250      0.0949     0.1008     55.0249
03200     2.5250      0.0970     0.1008     55.5863
03300     2.5249      0.0886     0.1008     55.1208
03400     2.5249      0.0928     0.1008     55.1890
03500     2.5249      0.0928     0.1008     55.9335
03600     2.5249      0.1350     0.1008     55.1475
03700     2.5249      0.1034     0.1008     55.3084
03800     2.5248      0.1287     0.1008     55.7193
03900     2.5248      0.0844     0.1008     55.1956
04000     2.5248      0.0781     0.1008     55.8254
04100     2.5248      0.1034     0.1008     55.0648
04200     2.5248      0.1118     0.1008     55.1001
04300     2.5247      0.0823     0.1008     55.8091
04400     2.5247      0.0886     0.1008     55.1779
04500     2.5247      0.1076     0.1008     55.3651
04600     2.5247      0.1076     0.1008     56.0731
04700     2.5247      0.0992     0.1008     55.3295
04800     2.5246      0.1160     0.1008     55.5896
04900     2.5246      0.0970     0.1008     55.0950
05000     2.5246      0.0907     0.1008     55.4296
05100     2.5246      0.0928     0.1008     55.5342
05200     2.5246      0.0886     0.1008     55.0598
05300     2.5245      0.0949     0.1008     54.9908
05400     2.5245      0.0738     0.1008     55.6872
05500     2.5245      0.0844     0.1008     55.0822
05600     2.5245      0.0907     0.1008     56.1445
05700     2.5245      0.1181     0.1008     55.0383
05800     2.5244      0.0865     0.1008     54.9300
05900     2.5244      0.1055     0.1008     55.8354
06000     2.5244      0.0992     0.1008     54.9324
06100     2.5244      0.1287     0.1008     55.2827
06200     2.5244      0.0970     0.1008     55.8516
06300     2.5243      0.0865     0.1008     55.2818
06400     2.5243      0.0907     0.1008     56.2086
06500     2.5243      0.1245     0.1008     55.3313
06600     2.5243      0.0844     0.1008     55.3389
06700     2.5243      0.0992     0.1008     55.6810
06800     2.5242      0.1118     0.1008     55.2087
06900     2.5242      0.0907     0.1008     55.5677
07000     2.5242      0.0823     0.1008     56.0942
07100     2.5242      0.0970     0.1008     55.1351
07200     2.5242      0.0949     0.1008     56.0155
07300     2.5242      0.0928     0.1008     55.1958
07400     2.5241      0.0949     0.1008     55.2757
07500     2.5241      0.1097     0.1008     55.8899
07600     2.5241      0.0759     0.1008     55.0527
07700     2.5241      0.0802     0.1008     55.3241
07800     2.5241      0.1076     0.1008     55.8349
07900     2.5240      0.0844     0.1008     55.2531
08000     2.5240      0.1034     0.1008     55.8051
08100     2.5240      0.0992     0.1008     55.1333
08200     2.5240      0.1055     0.1008     55.1348
08300     2.5240      0.1203     0.1008     55.6219
08400     2.5239      0.0992     0.1008     55.1518
08500     2.5239      0.0928     0.1008     55.2930
08600     2.5239      0.0907     0.1008     55.4733
08700     2.5239      0.1055     0.1008     55.1324
08800     2.5239      0.1034     0.1008     55.9576
08900     2.5239      0.0970     0.1008     55.6261
09000     2.5238      0.1076     0.1008     55.4010
09100     2.5238      0.1076     0.1008     55.8400
09200     2.5238      0.1118     0.1008     54.9689
09300     2.5238      0.0992     0.1008     55.9786
09400     2.5238      0.1055     0.1008     55.8172
09500     2.5237      0.1055     0.1008     55.4242
09600     2.5237      0.1076     0.1008     56.2716
09700     2.5237      0.1181     0.1008     55.2507
09800     2.5237      0.1203     0.1008     55.1362
09900     2.5237      0.0802     0.1008     56.2258
10000     2.5236      0.0992     0.1008     55.4892
10100     2.5236      0.0949     0.1008     55.3860
10200     2.5236      0.1034     0.1008     55.7562
10300     2.5236      0.0992     0.1008     55.3808
10400     2.5236      0.0992     0.1008     55.7540
10500     2.5236      0.1329     0.1008     55.3220
10600     2.5236      0.1266     0.1008     55.0736
10700     2.5236      0.0886     0.1008     55.5987
10800     2.5236      0.1245     0.1008     55.2006
10900     2.5236      0.1160     0.1008     55.4263
11000     2.5236      0.1013     0.1008     55.8629
11100     2.5236      0.1118     0.1008     55.0344
11200     2.5236      0.1013     0.1008     55.9147
11300     2.5236      0.1013     0.1008     55.1421
11400     2.5236      0.1013     0.1008     55.2169
11500     2.5236      0.0865     0.1008     55.5667
11600     2.5236      0.0654     0.1008     54.8996
11700     2.5236      0.1097     0.1008     54.9986
11800     2.5236      0.1160     0.1008     56.1336
11900     2.5236      0.0865     0.1008     55.3464
12000     2.5236      0.1076     0.1008     56.1546
12100     2.5236      0.0992     0.1008     55.1938
12200     2.5236      0.0907     0.1008     55.0901
12300     2.5236      0.0949     0.1008     55.4771
12400     2.5236      0.0886     0.1008     54.9173
12500     2.5236      0.1160     0.1008     54.8080
12600     2.5236      0.0928     0.1008     55.2635
12700     2.5235      0.1055     0.1008     54.9175
12800     2.5235      0.0928     0.1008     55.4574
12900     2.5235      0.0970     0.1008     55.0109
13000     2.5235      0.1055     0.1008     54.9570
13100     2.5235      0.1181     0.1008     55.7752
13200     2.5235      0.0675     0.1008     54.9243
13300     2.5235      0.1139     0.1008     55.0959
13400     2.5235      0.1076     0.1008     55.8067
13500     2.5235      0.0928     0.1008     55.1193
13600     2.5235      0.0907     0.1008     56.0136
13700     2.5235      0.1055     0.1008     55.3405
13800     2.5235      0.0865     0.1008     55.0120
13900     2.5235      0.0717     0.1008     55.8179
14000     2.5235      0.0907     0.1008     55.0489
14100     2.5235      0.1076     0.1008     55.0751
14200     2.5235      0.1055     0.1008     55.6594
14300     2.5235      0.1287     0.1008     54.9264
14400     2.5235      0.1097     0.1008     55.5556
14500     2.5235      0.1224     0.1008     55.2574
14600     2.5235      0.0759     0.1008     55.4169
14700     2.5235      0.1139     0.1008     55.9107
14800     2.5235      0.1055     0.1008     55.2372
14900     2.5235      0.1139     0.1008     55.3099
15000     2.5235      0.0992     0.1008     55.5879
15100     2.5235      0.1266     0.1008     55.0385
15200     2.5235      0.0886     0.1008     55.7206
15300     2.5235      0.1034     0.1008     55.0683
15400     2.5235      0.0823     0.1008     55.3500
15500     2.5234      0.1181     0.1008     55.9609
15600     2.5234      0.1013     0.1008     55.1505
15700     2.5234      0.0928     0.1008     54.9653
15800     2.5234      0.0886     0.1008     55.4209
15900     2.5234      0.1055     0.1008     55.0628
16000     2.5234      0.0949     0.1008     56.1415
16100     2.5234      0.0844     0.1008     55.3136
16200     2.5234      0.1203     0.1008     55.4272
16300     2.5234      0.1055     0.1008     55.8837
16400     2.5234      0.1160     0.1008     55.0990
16500     2.5234      0.1097     0.1008     55.0918
16600     2.5234      0.0992     0.1008     55.6033
16700     2.5234      0.1034     0.1008     55.3625
16800     2.5234      0.0907     0.1008     56.2235
16900     2.5234      0.0970     0.1008     55.3668
17000     2.5234      0.0907     0.1008     55.6116
17100     2.5234      0.1034     0.1008     56.3233
17200     2.5234      0.1055     0.1008     55.3862
17300     2.5234      0.1055     0.1008     55.3518
17400     2.5234      0.1160     0.1008     56.4733
17500     2.5234      0.1203     0.1008     55.6407
17600     2.5234      0.1097     0.1008     55.7560
17700     2.5234      0.1055     0.1008     55.2553
17800     2.5234      0.0970     0.1008     55.5164
17900     2.5234      0.0992     0.1008     56.2082
18000     2.5234      0.1034     0.1008     55.6268
18100     2.5234      0.1013     0.1008     55.9185
18200     2.5234      0.0949     0.1008     56.5270
18300     2.5233      0.0865     0.1008     55.7455
18400     2.5233      0.1097     0.1009     56.0300
18500     2.5233      0.1266     0.1009     55.4978
18600     2.5233      0.1203     0.1009     55.2701
18700     2.5233      0.0949     0.1009     55.6339
18800     2.5233      0.0928     0.1009     55.5001
18900     2.5233      0.0928     0.1009     55.3473
19000     2.5233      0.1013     0.1009     55.6299
19100     2.5233      0.1392     0.1009     55.3128
19200     2.5233      0.0928     0.1009     55.6748
19300     2.5233      0.1034     0.1009     55.4256
19400     2.5233      0.0970     0.1009     55.4741
19500     2.5233      0.0865     0.1009     55.8860
19600     2.5233      0.0907     0.1009     55.1854
19700     2.5233      0.0949     0.1009     55.1525
19800     2.5233      0.0886     0.1009     55.6549
19900     2.5233      0.0844     0.1009     54.8887
20000     2.5233      0.1097     0.1009     55.9864
20100     2.5233      0.0970     0.1009     55.5305
20200     2.5233      0.1118     0.1009     55.3593
20300     2.5233      0.0781     0.1009     56.3001
20400     2.5233      0.0992     0.1009     55.7885
20500     2.5233      0.1097     0.1009     55.5847
20600     2.5233      0.1118     0.1009     56.2037
20700     2.5233      0.1055     0.1009     55.8258
20800     2.5233      0.1245     0.1009     56.7422
20900     2.5233      0.1013     0.1009     56.2186
21000     2.5233      0.0992     0.1009     55.5642
21100     2.5233      0.1139     0.1009     56.5332
21200     2.5233      0.1013     0.1009     56.0709
21300     2.5233      0.0949     0.1009     55.9472
21400     2.5233      0.0907     0.1009     57.7129
21500     2.5233      0.0949     0.1009     56.3964
21600     2.5233      0.1013     0.1009     56.9334
21700     2.5233      0.1139     0.1009     56.1660
21800     2.5233      0.1181     0.1009     55.7776
21900     2.5233      0.1076     0.1009     56.1200
22000     2.5233      0.0949     0.1009     55.5413
22100     2.5233      0.1139     0.1009     55.6984
22200     2.5233      0.1034     0.1009     56.6571
22300     2.5233      0.0865     0.1009     56.7691
22400     2.5233      0.0949     0.1009     57.3157
22500     2.5233      0.1034     0.1009     56.6251
22600     2.5233      0.1097     0.1011     56.0868
22700     2.5233      0.0907     0.1011     56.9498
22800     2.5233      0.1097     0.1011     55.5599
22900     2.5233      0.1139     0.1011     55.2900
23000     2.5233      0.1160     0.1011     56.4080
23100     2.5233      0.0886     0.1011     55.9073
23200     2.5233      0.0992     0.1011     56.4705
23300     2.5233      0.1076     0.1011     55.8117
23400     2.5233      0.1097     0.1011     55.3687
23500     2.5233      0.1013     0.1011     56.2165
23600     2.5233      0.0992     0.1011     55.8561
23700     2.5233      0.1055     0.1011     55.7586
23800     2.5233      0.0886     0.1011     56.5151
23900     2.5233      0.1266     0.1011     56.1939
24000     2.5233      0.0717     0.1011     57.4427
24100     2.5233      0.0992     0.1011     55.7140
24200     2.5233      0.0928     0.1011     56.0290
24300     2.5233      0.0907     0.1011     56.9064
24400     2.5233      0.0781     0.1011     56.0432
24500     2.5232      0.0928     0.1011     56.2608
24600     2.5232      0.0802     0.1011     56.3567
24700     2.5232      0.1118     0.1011     56.2575
24800     2.5232      0.1329     0.1011     56.6685
24900     2.5232      0.0992     0.1011     56.0684
25000     2.5232      0.0949     0.1011     56.2649
25100     2.5232      0.1245     0.1011     56.3234
25200     2.5232      0.0928     0.1011     55.9850
25300     2.5232      0.1139     0.1011     55.9315
25400     2.5232      0.0970     0.1011     55.9551
25500     2.5232      0.1034     0.1011     56.4852
25600     2.5232      0.0992     0.1011     57.0265
25700     2.5232      0.0907     0.1011     55.8814
25800     2.5232      0.0907     0.1011     55.7195
25900     2.5232      0.1160     0.1011     56.8813
26000     2.5232      0.1034     0.1011     55.7295
26100     2.5232      0.1034     0.1011     55.7572
26200     2.5232      0.1055     0.1011     56.4390
26300     2.5232      0.0970     0.1011     56.2008
26400     2.5232      0.1097     0.1011     56.8978
26500     2.5232      0.0844     0.1011     55.9983
26600     2.5232      0.0759     0.1011     56.1877
26700     2.5232      0.1055     0.1011     56.7276
26800     2.5232      0.1224     0.1011     55.9755
26900     2.5232      0.0928     0.1011     56.7108
27000     2.5232      0.1266     0.1011     57.0222
27100     2.5232      0.1097     0.1011     56.3376
27200     2.5232      0.1287     0.1011     57.1940
27300     2.5232      0.1245     0.1011     55.8830
27400     2.5232      0.1013     0.1011     55.6743
27500     2.5232      0.0717     0.1011     56.5665
27600     2.5232      0.0992     0.1011     56.0552
27700     2.5232      0.1055     0.1011     56.3541
27800     2.5232      0.1181     0.1011     57.0112
27900     2.5232      0.1435     0.1011     56.1323
28000     2.5232      0.0738     0.1011     56.8940
28100     2.5232      0.0970     0.1011     56.0388
28200     2.5232      0.1055     0.1011     56.5048
28300     2.5232      0.0844     0.1011     56.1284
28400     2.5232      0.0928     0.1011     55.7554
28500     2.5232      0.1034     0.1011     56.0138
28600     2.5232      0.1076     0.1011     56.3945
28700     2.5232      0.0781     0.1011     55.7807
28800     2.5232      0.1097     0.1011     56.9515
28900     2.5232      0.1055     0.1011     55.9726
29000     2.5232      0.0907     0.1011     56.4868
29100     2.5232      0.1139     0.1011     56.6557
29200     2.5232      0.1160     0.1011     56.3730
29300     2.5232      0.1055     0.1011     55.9767
29400     2.5232      0.0886     0.1011     56.3420
29500     2.5232      0.1224     0.1011     56.4100
29600     2.5232      0.0949     0.1011     56.6666
29700     2.5232      0.0992     0.1011     55.9126
29800     2.5232      0.1034     0.1011     55.9655
29900     2.5232      0.1118     0.1011     56.3658
29999     2.5232      0.1139     0.1011     55.9626
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Start testing:
Test Accuracy: 0.1002
