Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 279, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.0, finetuning_epochs=0, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.0005,0.0001,0.00002', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,10000', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
b4b4174b-d617-4a90-9ab7-ea1d02f8b2a7
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.5257      0.1160     0.1004     11.7788
00100     2.5256      0.1013     0.1004     55.2087
00200     2.5256      0.1076     0.1004     54.9891
00300     2.5256      0.0949     0.1004     55.1360
00400     2.5256      0.1055     0.1006     54.3815
00500     2.5256      0.1055     0.1006     54.9678
00600     2.5255      0.1160     0.1006     55.1698
00700     2.5255      0.1266     0.1008     54.9820
00800     2.5255      0.1139     0.1008     56.3445
00900     2.5255      0.0865     0.1008     55.4302
01000     2.5254      0.0886     0.1008     55.0902
01100     2.5254      0.0823     0.1008     55.6039
01200     2.5254      0.0865     0.1008     55.0608
01300     2.5254      0.1160     0.1008     55.0913
01400     2.5254      0.0992     0.1008     55.7326
01500     2.5253      0.0992     0.1008     55.3670
01600     2.5253      0.1034     0.1008     55.7141
01700     2.5253      0.0886     0.1008     54.8763
01800     2.5253      0.1160     0.1008     54.5954
01900     2.5252      0.0633     0.1008     55.5360
02000     2.5252      0.0970     0.1008     55.2624
02100     2.5252      0.1013     0.1008     55.3588
02200     2.5252      0.0949     0.1008     55.4923
02300     2.5252      0.0949     0.1008     55.4095
02400     2.5251      0.1266     0.1008     55.7287
02500     2.5251      0.0675     0.1008     55.1317
02600     2.5251      0.1076     0.1008     55.1626
02700     2.5251      0.0949     0.1008     55.8566
02800     2.5251      0.1013     0.1008     55.2878
02900     2.5250      0.0949     0.1008     55.2645
03000     2.5250      0.1118     0.1008     55.4396
03100     2.5250      0.0949     0.1008     55.0249
03200     2.5250      0.0970     0.1008     55.5863
03300     2.5249      0.0886     0.1008     55.1208
03400     2.5249      0.0928     0.1008     55.1890
03500     2.5249      0.0928     0.1008     55.9335
03600     2.5249      0.1350     0.1008     55.1475
03700     2.5249      0.1034     0.1008     55.3084
03800     2.5248      0.1287     0.1008     55.7193
03900     2.5248      0.0844     0.1008     55.1956
04000     2.5248      0.0781     0.1008     55.8254
04100     2.5248      0.1034     0.1008     55.0648
04200     2.5248      0.1118     0.1008     55.1001
04300     2.5247      0.0823     0.1008     55.8091
04400     2.5247      0.0886     0.1008     55.1779
04500     2.5247      0.1076     0.1008     55.3651
04600     2.5247      0.1076     0.1008     56.0731
04700     2.5247      0.0992     0.1008     55.3295
04800     2.5246      0.1160     0.1008     55.5896
04900     2.5246      0.0970     0.1008     55.0950
05000     2.5246      0.0907     0.1008     55.4296
05100     2.5246      0.0928     0.1008     55.5342
05200     2.5246      0.0886     0.1008     55.0598
05300     2.5245      0.0949     0.1008     54.9908
05400     2.5245      0.0738     0.1008     55.6872
05500     2.5245      0.0844     0.1008     55.0822
05600     2.5245      0.0907     0.1008     56.1445
05700     2.5245      0.1181     0.1008     55.0383
05800     2.5244      0.0865     0.1008     54.9300
05900     2.5244      0.1055     0.1008     55.8354
06000     2.5244      0.0992     0.1008     54.9324
06100     2.5244      0.1287     0.1008     55.2827
06200     2.5244      0.0970     0.1008     55.8516
06300     2.5243      0.0865     0.1008     55.2818
06400     2.5243      0.0907     0.1008     56.2086
06500     2.5243      0.1245     0.1008     55.3313
06600     2.5243      0.0844     0.1008     55.3389
06700     2.5243      0.0992     0.1008     55.6810
06800     2.5242      0.1118     0.1008     55.2087
06900     2.5242      0.0907     0.1008     55.5677
07000     2.5242      0.0823     0.1008     56.0942
07100     2.5242      0.0970     0.1008     55.1351
07200     2.5242      0.0949     0.1008     56.0155
07300     2.5242      0.0928     0.1008     55.1958
07400     2.5241      0.0949     0.1008     55.2757
07500     2.5241      0.1097     0.1008     55.8899
07600     2.5241      0.0759     0.1008     55.0527
07700     2.5241      0.0802     0.1008     55.3241
07800     2.5241      0.1076     0.1008     55.8349
07900     2.5240      0.0844     0.1008     55.2531
08000     2.5240      0.1034     0.1008     55.8051
08100     2.5240      0.0992     0.1008     55.1333
08200     2.5240      0.1055     0.1008     55.1348
08300     2.5240      0.1203     0.1008     55.6219
08400     2.5239      0.0992     0.1008     55.1518
08500     2.5239      0.0928     0.1008     55.2930
08600     2.5239      0.0907     0.1008     55.4733
08700     2.5239      0.1055     0.1008     55.1324
08800     2.5239      0.1034     0.1008     55.9576
08900     2.5239      0.0970     0.1008     55.6261
09000     2.5238      0.1076     0.1008     55.4010
09100     2.5238      0.1076     0.1008     55.8400
09200     2.5238      0.1118     0.1008     54.9689
09300     2.5238      0.0992     0.1008     55.9786
09400     2.5238      0.1055     0.1008     55.8172
09500     2.5237      0.1055     0.1008     55.4242
09600     2.5237      0.1076     0.1008     56.2716
09700     2.5237      0.1181     0.1008     55.2507
09800     2.5237      0.1203     0.1008     55.1362
09900     2.5237      0.0802     0.1008     56.2258
10000     2.5236      0.0992     0.1008     55.4892
10100     2.5236      0.0949     0.1008     55.3860
10200     2.5236      0.1034     0.1008     55.7562
10300     2.5236      0.0992     0.1008     55.3808
10400     2.5236      0.0992     0.1008     55.7540
10500     2.5236      0.1329     0.1008     55.3220
10600     2.5236      0.1266     0.1008     55.0736
10700     2.5236      0.0886     0.1008     55.5987
10800     2.5236      0.1245     0.1008     55.2006
10900     2.5236      0.1160     0.1008     55.4263
11000     2.5236      0.1013     0.1008     55.8629
11100     2.5236      0.1118     0.1008     55.0344
11200     2.5236      0.1013     0.1008     55.9147
11300     2.5236      0.1013     0.1008     55.1421
11400     2.5236      0.1013     0.1008     55.2169
11500     2.5236      0.0865     0.1008     55.5667
11600     2.5236      0.0654     0.1008     54.8996
11700     2.5236      0.1097     0.1008     54.9986
11800     2.5236      0.1160     0.1008     56.1336
11900     2.5236      0.0865     0.1008     55.3464
12000     2.5236      0.1076     0.1008     56.1546
12100     2.5236      0.0992     0.1008     55.1938
12200     2.5236      0.0907     0.1008     55.0901
12300     2.5236      0.0949     0.1008     55.4771
12400     2.5236      0.0886     0.1008     54.9173
12500     2.5236      0.1160     0.1008     54.8080
12600     2.5236      0.0928     0.1008     55.2635
12700     2.5235      0.1055     0.1008     54.9175
12800     2.5235      0.0928     0.1008     55.4574
12900     2.5235      0.0970     0.1008     55.0109
13000     2.5235      0.1055     0.1008     54.9570
13100     2.5235      0.1181     0.1008     55.7752
13200     2.5235      0.0675     0.1008     54.9243
13300     2.5235      0.1139     0.1008     55.0959
13400     2.5235      0.1076     0.1008     55.8067
13500     2.5235      0.0928     0.1008     55.1193
13600     2.5235      0.0907     0.1008     56.0136
13700     2.5235      0.1055     0.1008     55.3405
13800     2.5235      0.0865     0.1008     55.0120
13900     2.5235      0.0717     0.1008     55.8179
14000     2.5235      0.0907     0.1008     55.0489
14100     2.5235      0.1076     0.1008     55.0751
14200     2.5235      0.1055     0.1008     55.6594
14300     2.5235      0.1287     0.1008     54.9264
14400     2.5235      0.1097     0.1008     55.5556
14500     2.5235      0.1224     0.1008     55.2574
14600     2.5235      0.0759     0.1008     55.4169
14700     2.5235      0.1139     0.1008     55.9107
14800     2.5235      0.1055     0.1008     55.2372
14900     2.5235      0.1139     0.1008     55.3099
15000     2.5235      0.0992     0.1008     55.5879
15100     2.5235      0.1266     0.1008     55.0385
15200     2.5235      0.0886     0.1008     55.7206
15300     2.5235      0.1034     0.1008     55.0683
15400     2.5235      0.0823     0.1008     55.3500
15500     2.5234      0.1181     0.1008     55.9609
15600     2.5234      0.1013     0.1008     55.1505
15700     2.5234      0.0928     0.1008     54.9653
15800     2.5234      0.0886     0.1008     55.4209
15900     2.5234      0.1055     0.1008     55.0628
16000     2.5234      0.0949     0.1008     56.1415
16100     2.5234      0.0844     0.1008     55.3136
16200     2.5234      0.1203     0.1008     55.4272
16300     2.5234      0.1055     0.1008     55.8837
16400     2.5234      0.1160     0.1008     55.0990
16500     2.5234      0.1097     0.1008     55.0918
16600     2.5234      0.0992     0.1008     55.6033
16700     2.5234      0.1034     0.1008     55.3625
16800     2.5234      0.0907     0.1008     56.2235
16900     2.5234      0.0970     0.1008     55.3668
17000     2.5234      0.0907     0.1008     55.6116
17100     2.5234      0.1034     0.1008     56.3233
17200     2.5234      0.1055     0.1008     55.3862
17300     2.5234      0.1055     0.1008     55.3518
17400     2.5234      0.1160     0.1008     56.4733
17500     2.5234      0.1203     0.1008     55.6407
17600     2.5234      0.1097     0.1008     55.7560
17700     2.5234      0.1055     0.1008     55.2553
17800     2.5234      0.0970     0.1008     55.5164
17900     2.5234      0.0992     0.1008     56.2082
18000     2.5234      0.1034     0.1008     55.6268
18100     2.5234      0.1013     0.1008     55.9185
18200     2.5234      0.0949     0.1008     56.5270
18300     2.5233      0.0865     0.1008     55.7455
18400     2.5233      0.1097     0.1009     56.0300
18500     2.5233      0.1266     0.1009     55.4978
18600     2.5233      0.1203     0.1009     55.2701
18700     2.5233      0.0949     0.1009     55.6339
18800     2.5233      0.0928     0.1009     55.5001
18900     2.5233      0.0928     0.1009     55.3473
19000     2.5233      0.1013     0.1009     55.6299
19100     2.5233      0.1392     0.1009     55.3128
19200     2.5233      0.0928     0.1009     55.6748
19300     2.5233      0.1034     0.1009     55.4256
19400     2.5233      0.0970     0.1009     55.4741
19500     2.5233      0.0865     0.1009     55.8860
19600     2.5233      0.0907     0.1009     55.1854
19700     2.5233      0.0949     0.1009     55.1525
19800     2.5233      0.0886     0.1009     55.6549
19900     2.5233      0.0844     0.1009     54.8887
20000     2.5233      0.1097     0.1009     55.9864
20100     2.5233      0.0970     0.1009     55.5305
20200     2.5233      0.1118     0.1009     55.3593
20300     2.5233      0.0781     0.1009     56.3001
20400     2.5233      0.0992     0.1009     55.7885
20500     2.5233      0.1097     0.1009     55.5847
20600     2.5233      0.1118     0.1009     56.2037
20700     2.5233      0.1055     0.1009     55.8258
20800     2.5233      0.1245     0.1009     56.7422
20900     2.5233      0.1013     0.1009     56.2186
21000     2.5233      0.0992     0.1009     55.5642
21100     2.5233      0.1139     0.1009     56.5332
21200     2.5233      0.1013     0.1009     56.0709
21300     2.5233      0.0949     0.1009     55.9472
21400     2.5233      0.0907     0.1009     57.7129
21500     2.5233      0.0949     0.1009     56.3964
21600     2.5233      0.1013     0.1009     56.9334
21700     2.5233      0.1139     0.1009     56.1660
21800     2.5233      0.1181     0.1009     55.7776
21900     2.5233      0.1076     0.1009     56.1200
22000     2.5233      0.0949     0.1009     55.5413
22100     2.5233      0.1139     0.1009     55.6984
22200     2.5233      0.1034     0.1009     56.6571
22300     2.5233      0.0865     0.1009     56.7691
22400     2.5233      0.0949     0.1009     57.3157
22500     2.5233      0.1034     0.1009     56.6251
22600     2.5233      0.1097     0.1011     56.0868
22700     2.5233      0.0907     0.1011     56.9498
22800     2.5233      0.1097     0.1011     55.5599
22900     2.5233      0.1139     0.1011     55.2900
23000     2.5233      0.1160     0.1011     56.4080
23100     2.5233      0.0886     0.1011     55.9073
23200     2.5233      0.0992     0.1011     56.4705
23300     2.5233      0.1076     0.1011     55.8117
23400     2.5233      0.1097     0.1011     55.3687
23500     2.5233      0.1013     0.1011     56.2165
23600     2.5233      0.0992     0.1011     55.8561
23700     2.5233      0.1055     0.1011     55.7586
23800     2.5233      0.0886     0.1011     56.5151
23900     2.5233      0.1266     0.1011     56.1939
24000     2.5233      0.0717     0.1011     57.4427
24100     2.5233      0.0992     0.1011     55.7140
24200     2.5233      0.0928     0.1011     56.0290
24300     2.5233      0.0907     0.1011     56.9064
24400     2.5233      0.0781     0.1011     56.0432
24500     2.5232      0.0928     0.1011     56.2608
24600     2.5232      0.0802     0.1011     56.3567
24700     2.5232      0.1118     0.1011     56.2575
24800     2.5232      0.1329     0.1011     56.6685
24900     2.5232      0.0992     0.1011     56.0684
25000     2.5232      0.0949     0.1011     56.2649
25100     2.5232      0.1245     0.1011     56.3234
25200     2.5232      0.0928     0.1011     55.9850
25300     2.5232      0.1139     0.1011     55.9315
25400     2.5232      0.0970     0.1011     55.9551
25500     2.5232      0.1034     0.1011     56.4852
25600     2.5232      0.0992     0.1011     57.0265
25700     2.5232      0.0907     0.1011     55.8814
25800     2.5232      0.0907     0.1011     55.7195
25900     2.5232      0.1160     0.1011     56.8813
26000     2.5232      0.1034     0.1011     55.7295
26100     2.5232      0.1034     0.1011     55.7572
26200     2.5232      0.1055     0.1011     56.4390
26300     2.5232      0.0970     0.1011     56.2008
26400     2.5232      0.1097     0.1011     56.8978
26500     2.5232      0.0844     0.1011     55.9983
26600     2.5232      0.0759     0.1011     56.1877
26700     2.5232      0.1055     0.1011     56.7276
26800     2.5232      0.1224     0.1011     55.9755
26900     2.5232      0.0928     0.1011     56.7108
27000     2.5232      0.1266     0.1011     57.0222
27100     2.5232      0.1097     0.1011     56.3376
27200     2.5232      0.1287     0.1011     57.1940
27300     2.5232      0.1245     0.1011     55.8830
27400     2.5232      0.1013     0.1011     55.6743
27500     2.5232      0.0717     0.1011     56.5665
27600     2.5232      0.0992     0.1011     56.0552
27700     2.5232      0.1055     0.1011     56.3541
27800     2.5232      0.1181     0.1011     57.0112
27900     2.5232      0.1435     0.1011     56.1323
28000     2.5232      0.0738     0.1011     56.8940
28100     2.5232      0.0970     0.1011     56.0388
28200     2.5232      0.1055     0.1011     56.5048
28300     2.5232      0.0844     0.1011     56.1284
28400     2.5232      0.0928     0.1011     55.7554
28500     2.5232      0.1034     0.1011     56.0138
28600     2.5232      0.1076     0.1011     56.3945
28700     2.5232      0.0781     0.1011     55.7807
28800     2.5232      0.1097     0.1011     56.9515
28900     2.5232      0.1055     0.1011     55.9726
29000     2.5232      0.0907     0.1011     56.4868
29100     2.5232      0.1139     0.1011     56.6557
29200     2.5232      0.1160     0.1011     56.3730
29300     2.5232      0.1055     0.1011     55.9767
29400     2.5232      0.0886     0.1011     56.3420
29500     2.5232      0.1224     0.1011     56.4100
29600     2.5232      0.0949     0.1011     56.6666
29700     2.5232      0.0992     0.1011     55.9126
29800     2.5232      0.1034     0.1011     55.9655
29900     2.5232      0.1118     0.1011     56.3658
29999     2.5232      0.1139     0.1011     55.9626
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Start testing:
Test Accuracy: 0.1002
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
ed96e487-8dd1-433f-86bb-05c4d3cdd7c2
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.5257      0.1055     0.1005     13.2520
00100     2.5256      0.0907     0.1005     78.0736
00200     2.5255      0.1076     0.1005     73.6761
00300     2.5254      0.0970     0.1005     72.8059
00400     2.5253      0.0886     0.1005     73.7608
00500     2.5252      0.0949     0.1005     74.0221
00600     2.5251      0.0844     0.1005     73.4971
00700     2.5251      0.1139     0.1005     72.5494
00800     2.5250      0.1097     0.1005     73.8167
00900     2.5249      0.1160     0.1011     72.9208
01000     2.5248      0.0949     0.1011     72.2179
01100     2.5247      0.0886     0.1011     74.1016
01200     2.5247      0.0992     0.1011     73.9665
01300     2.5246      0.1097     0.1011     73.6866
01400     2.5245      0.1371     0.1011     72.4015
01500     2.5244      0.0844     0.1011     73.2795
01600     2.5243      0.0907     0.1011     73.1670
01700     2.5243      0.0992     0.1011     72.8241
01800     2.5242      0.0907     0.1011     73.8963
01900     2.5241      0.0907     0.1011     72.6541
02000     2.5240      0.0886     0.1011     72.9147
02100     2.5240      0.0992     0.1011     74.1477
02200     2.5239      0.0992     0.1011     73.7895
02300     2.5238      0.0928     0.1011     74.7298
02400     2.5237      0.0865     0.1011     73.1166
02500     2.5237      0.0992     0.1011     73.1016
02600     2.5236      0.1097     0.1011     72.6697
02700     2.5235      0.0992     0.1011     74.0221
02800     2.5234      0.1013     0.1011     72.8527
02900     2.5234      0.0886     0.1011     72.2800
03000     2.5233      0.0949     0.1011     73.6349
03100     2.5232      0.1013     0.1011     75.4135
03200     2.5232      0.1097     0.1011     75.2875
03300     2.5231      0.1097     0.1011     73.3588
03400     2.5230      0.1013     0.1011     72.3863
03500     2.5229      0.0738     0.1011     73.6129
03600     2.5229      0.0907     0.1011     73.1498
03700     2.5228      0.0886     0.1011     73.3771
03800     2.5227      0.0886     0.1011     74.2930
03900     2.5227      0.1245     0.1011     75.2941
04000     2.5226      0.0970     0.1011     74.0224
04100     2.5225      0.1118     0.1011     73.8316
04200     2.5225      0.1076     0.1011     77.4345
04300     2.5224      0.0949     0.1011     75.2407
04400     2.5223      0.0907     0.1011     74.7044
04500     2.5222      0.1034     0.1011     76.6896
04600     2.5222      0.0844     0.1011     75.6845
04700     2.5221      0.0949     0.1011     74.3801
04800     2.5220      0.0886     0.1011     74.6421
04900     2.5220      0.1329     0.1011     74.0319
05000     2.5219      0.0633     0.1011     75.1053
05100     2.5218      0.0865     0.1011     76.9035
05200     2.5218      0.0949     0.1011     74.4900
05300     2.5217      0.0970     0.1011     76.2434
05400     2.5216      0.0886     0.1011     74.8496
05500     2.5216      0.0907     0.1011     76.9056
05600     2.5215      0.1097     0.1011     76.1247
05700     2.5214      0.0886     0.1011     75.6360
05800     2.5214      0.0823     0.1011     75.1634
05900     2.5213      0.0949     0.1011     74.2757
06000     2.5212      0.0949     0.1011     75.0873
06100     2.5212      0.0844     0.1011     73.4665
06200     2.5211      0.1034     0.1011     76.1301
06300     2.5210      0.0970     0.1011     75.2208
06400     2.5210      0.1160     0.1011     74.3401
06500     2.5209      0.0696     0.1011     73.9954
06600     2.5208      0.1013     0.1011     72.9420
06700     2.5208      0.1181     0.1011     73.5211
06800     2.5207      0.1034     0.1011     74.1216
06900     2.5206      0.0970     0.1011     74.7684
07000     2.5206      0.1034     0.1011     74.2530
07100     2.5205      0.1055     0.1011     73.1812
07200     2.5204      0.1055     0.1011     75.2753
07300     2.5204      0.0992     0.1011     77.0808
07400     2.5203      0.1245     0.1011     76.4149
07500     2.5202      0.0992     0.1011     74.9421
07600     2.5202      0.1224     0.1011     74.1507
07700     2.5201      0.0992     0.1011     75.6356
07800     2.5200      0.0949     0.1011     76.2918
07900     2.5200      0.0992     0.1011     74.9486
08000     2.5199      0.0759     0.1011     74.6820
08100     2.5199      0.1118     0.1011     74.9901
08200     2.5198      0.0949     0.1011     76.5022
08300     2.5197      0.0865     0.1011     74.4720
08400     2.5197      0.1097     0.1011     74.8915
08500     2.5196      0.0992     0.1011     74.5967
08600     2.5195      0.1076     0.1011     73.7866
08700     2.5195      0.1055     0.1011     75.0603
08800     2.5194      0.0949     0.1011     75.7085
08900     2.5193      0.0949     0.1011     76.2838
09000     2.5193      0.1203     0.1011     74.2584
09100     2.5192      0.1097     0.1011     76.7023
09200     2.5192      0.0970     0.1011     75.9211
09300     2.5191      0.0844     0.1011     75.0288
09400     2.5190      0.0907     0.1011     75.9481
09500     2.5190      0.0865     0.1011     74.0277
09600     2.5189      0.1160     0.1011     75.4244
09700     2.5188      0.0907     0.1011     74.6423
09800     2.5188      0.0970     0.1011     73.9875
09900     2.5187      0.1034     0.1011     74.1977
10000     2.5186      0.0970     0.1011     74.9721
10100     2.5186      0.0907     0.1011     76.1215
10200     2.5186      0.0949     0.1011     75.4214
10300     2.5186      0.1118     0.1011     74.8152
10400     2.5186      0.0717     0.1011     75.6897
10500     2.5186      0.0823     0.1011     75.2373
10600     2.5186      0.0886     0.1011     74.4515
10700     2.5185      0.1097     0.1011     77.1339
10800     2.5185      0.1139     0.1011     77.2337
10900     2.5185      0.1055     0.1011     76.4293
11000     2.5185      0.0802     0.1011     75.7616
11100     2.5185      0.0886     0.1011     75.0339
11200     2.5185      0.0970     0.1011     74.8761
11300     2.5184      0.0844     0.1011     76.8224
11400     2.5184      0.0992     0.1011     75.2878
11500     2.5184      0.0949     0.1011     76.2296
11600     2.5184      0.0865     0.1011     75.6951
11700     2.5184      0.1055     0.1011     74.8439
11800     2.5184      0.0907     0.1011     76.7654
11900     2.5183      0.0970     0.1011     74.8409
12000     2.5183      0.1139     0.1011     74.9509
12100     2.5183      0.1477     0.1011     78.0505
12200     2.5183      0.0928     0.1011     74.2477
12300     2.5183      0.1308     0.1011     75.3967
12400     2.5183      0.0907     0.1011     74.7034
12500     2.5183      0.0886     0.1011     76.7103
12600     2.5182      0.0844     0.1011     74.7441
12700     2.5182      0.0886     0.1011     76.4072
12800     2.5182      0.1076     0.1011     75.4193
12900     2.5182      0.1181     0.1011     76.4861
13000     2.5182      0.0928     0.1011     76.7402
13100     2.5182      0.1076     0.1011     74.7459
13200     2.5181      0.0970     0.1011     74.8132
13300     2.5181      0.0970     0.1011     76.0699
13400     2.5181      0.0928     0.1011     76.8044
13500     2.5181      0.1097     0.1011     75.2649
13600     2.5181      0.0970     0.1011     76.4774
13700     2.5181      0.1034     0.1011     77.0947
13800     2.5180      0.1034     0.1011     76.4111
13900     2.5180      0.0992     0.1011     76.9372
14000     2.5180      0.0886     0.1011     75.0918
14100     2.5180      0.0949     0.1011     75.5748
14200     2.5180      0.1034     0.1011     75.0898
14300     2.5180      0.0865     0.1011     75.1848
14400     2.5180      0.1076     0.1011     75.3825
14500     2.5179      0.1076     0.1011     75.2871
14600     2.5179      0.1013     0.1011     74.5557
14700     2.5179      0.1118     0.1011     76.4862
14800     2.5179      0.0865     0.1011     75.6873
14900     2.5179      0.1371     0.1011     74.9246
15000     2.5179      0.1034     0.1011     76.0459
15100     2.5178      0.1055     0.1011     76.4710
15200     2.5178      0.1308     0.1011     74.9368
15300     2.5178      0.0992     0.1011     74.7441
15400     2.5178      0.1076     0.1011     77.3304
15500     2.5178      0.1055     0.1011     75.1912
15600     2.5178      0.1013     0.1011     74.7048
15700     2.5178      0.1118     0.1011     75.4678
15800     2.5177      0.1203     0.1011     76.3598
15900     2.5177      0.0844     0.1011     75.0469
16000     2.5177      0.1055     0.1011     77.8813
16100     2.5177      0.0738     0.1011     76.6520
16200     2.5177      0.1203     0.1011     76.2371
16300     2.5177      0.1266     0.1011     75.8238
16400     2.5176      0.0970     0.1011     75.2434
16500     2.5176      0.0844     0.1011     74.9089
16600     2.5176      0.1203     0.1011     76.1326
16700     2.5176      0.0928     0.1011     75.6598
16800     2.5176      0.0781     0.1011     77.4667
16900     2.5176      0.0907     0.1011     77.2660
17000     2.5175      0.0992     0.1011     73.9797
17100     2.5175      0.1076     0.1011     75.2676
17200     2.5175      0.1181     0.1011     75.9634
17300     2.5175      0.1118     0.1011     75.8617
17400     2.5175      0.0970     0.1011     76.8512
17500     2.5175      0.1498     0.1011     76.8603
17600     2.5175      0.0844     0.1011     76.2147
17700     2.5174      0.1160     0.1011     74.7718
17800     2.5174      0.0865     0.1020     74.8341
17900     2.5174      0.1118     0.1020     76.3795
18000     2.5174      0.1076     0.1020     76.7375
18100     2.5174      0.0802     0.1020     76.3254
18200     2.5174      0.0823     0.1020     76.4631
18300     2.5173      0.1013     0.1020     76.1533
18400     2.5173      0.1076     0.1020     77.2133
18500     2.5173      0.1013     0.1020     76.2887
18600     2.5173      0.1181     0.1020     74.8192
18700     2.5173      0.1266     0.1020     76.7842
18800     2.5173      0.0886     0.1020     75.4278
18900     2.5172      0.1034     0.1020     77.3014
19000     2.5172      0.1139     0.1020     75.8133
19100     2.5172      0.0907     0.1020     76.1900
19200     2.5172      0.0970     0.1020     75.8562
19300     2.5172      0.1224     0.1020     76.4812
19400     2.5172      0.0970     0.1020     74.8224
19500     2.5172      0.0823     0.1020     76.7794
19600     2.5171      0.1034     0.1020     78.2809
19700     2.5171      0.1034     0.1020     78.1646
19800     2.5171      0.1329     0.1020     75.4952
19900     2.5171      0.0949     0.1020     75.5672
20000     2.5171      0.1118     0.1020     77.4773
20100     2.5171      0.1118     0.1020     74.1295
20199     2.5171      0.0717     0.1020     74.1539
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.5171      0.0949     0.0997     11.4023
00100     2.5171      0.0949     0.0997     74.8276
00200     2.5170      0.0759     0.0997     75.1151
00300     2.5172      0.0717     0.0997     75.2670
00400     2.5171      0.0844     0.0997     75.9110
00500     2.5171      0.0802     0.0997     76.1205
00600     2.5171      0.0591     0.0997     74.0997
00700     2.5170      0.1118     0.0997     76.2637
00800     2.5171      0.0781     0.0997     75.1349
00900     2.5171      0.0759     0.0997     76.0428
01000     2.5171      0.0759     0.0997     75.3232
01100     2.5169      0.0844     0.0997     76.4953
01200     2.5170      0.0865     0.0997     75.6488
01300     2.5169      0.1055     0.0997     76.0821
01400     2.5170      0.0823     0.0997     74.1274
01500     2.5170      0.0823     0.0997     75.0152
01600     2.5171      0.0717     0.0997     76.1603
01700     2.5171      0.0865     0.0997     75.9038
01800     2.5170      0.0802     0.0997     74.9450
01900     2.5171      0.0802     0.0997     75.7355
02000     2.5169      0.0802     0.0997     75.6587
02100     2.5171      0.0844     0.0997     74.4309
02200     2.5171      0.0612     0.0997     75.6082
02300     2.5171      0.0781     0.0997     75.3937
02400     2.5170      0.0717     0.0997     76.6069
02500     2.5171      0.0759     0.0997     76.1000
02600     2.5170      0.0717     0.0997     76.4054
02700     2.5169      0.0907     0.0997     76.4533
02800     2.5170      0.0907     0.0997     76.1536
02900     2.5170      0.0907     0.0997     74.0807
03000     2.5170      0.0802     0.0997     75.4091
03100     2.5171      0.0717     0.0997     75.8278
03200     2.5169      0.0844     0.0997     76.0488
03300     2.5169      0.1034     0.0997     76.5494
03400     2.5170      0.0844     0.0997     75.6745
03500     2.5171      0.0654     0.0997     77.7191
03600     2.5170      0.0696     0.0997     75.8551
03700     2.5169      0.0928     0.0997     76.1939
03800     2.5170      0.0781     0.0997     75.7264
03900     2.5169      0.0696     0.0997     77.4393
04000     2.5170      0.0738     0.0997     77.5507
04100     2.5168      0.0865     0.0997     74.6672
04200     2.5169      0.0717     0.0997     76.7537
04300     2.5170      0.0759     0.0997     75.4588
04400     2.5170      0.0781     0.0997     76.6651
04500     2.5169      0.0844     0.0997     74.1776
04600     2.5169      0.0907     0.0997     76.9370
04700     2.5170      0.0696     0.0997     76.4324
04800     2.5170      0.0633     0.0997     76.3408
04900     2.5170      0.0802     0.0997     76.4659
05000     2.5169      0.1076     0.1008     75.5069
05100     2.5169      0.1076     0.1008     75.6246
05200     2.5169      0.0696     0.1008     75.3936
05300     2.5168      0.1013     0.1008     76.3525
05400     2.5169      0.0823     0.1008     76.4834
05500     2.5169      0.0907     0.1008     76.2277
05600     2.5169      0.1055     0.1008     76.3624
05700     2.5170      0.0675     0.1008     77.1904
05800     2.5169      0.0970     0.1008     77.2236
05900     2.5169      0.1118     0.1008     76.6127
06000     2.5169      0.0823     0.1008     76.2957
06100     2.5169      0.0612     0.1008     75.4607
06200     2.5170      0.0802     0.1008     76.1464
06300     2.5169      0.0717     0.1008     75.1551
06400     2.5169      0.0675     0.1008     75.4486
06500     2.5169      0.1055     0.1008     74.8172
06600     2.5169      0.1076     0.1008     75.6244
06700     2.5169      0.0928     0.1008     75.3784
06800     2.5169      0.0949     0.1008     76.6775
06900     2.5169      0.0844     0.1008     76.3873
07000     2.5169      0.0823     0.1008     76.4602
07100     2.5168      0.0970     0.1008     75.3121
07200     2.5169      0.0928     0.1008     75.0100
07300     2.5171      0.0675     0.1008     75.7045
07400     2.5169      0.0654     0.1008     75.5577
07500     2.5168      0.0992     0.1008     75.9754
07600     2.5169      0.0844     0.1008     76.8764
07700     2.5169      0.1013     0.1008     76.2606
07800     2.5172      0.0612     0.1008     76.1510
07900     2.5168      0.1139     0.1008     74.9627
08000     2.5170      0.0696     0.1008     75.9921
08100     2.5167      0.1055     0.1008     74.9581
08200     2.5168      0.0949     0.1008     76.4277
08300     2.5169      0.0865     0.1008     75.2633
08400     2.5169      0.0928     0.1008     75.6021
08500     2.5169      0.0844     0.1008     77.2126
08600     2.5168      0.0970     0.1008     75.5347
08700     2.5169      0.0886     0.1008     75.2972
08800     2.5168      0.0717     0.1008     75.6390
08900     2.5168      0.0907     0.1008     76.3424
09000     2.5169      0.0823     0.1008     76.6969
09100     2.5168      0.0970     0.1008     74.9237
09200     2.5168      0.0907     0.1008     76.3448
09300     2.5169      0.0907     0.1008     74.9968
09400     2.5169      0.0907     0.1008     75.4695
09500     2.5169      0.0738     0.1008     77.5805
09600     2.5168      0.1013     0.1008     75.1259
09700     2.5168      0.0844     0.1008     76.4089
09800     2.5168      0.0823     0.1008     71.7659
09900     2.5168      0.0949     0.1008     75.0156
Start testing:
Test Accuracy: 0.0921
