Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 49, in __init__
    torch.nn.init.uniform_(self.cell.weight_ih, a = -limit2, b = limit2)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 49, in __init__
    torch.nn.init.uniform_(self.cell.weight_ih, a = -limit2, b = limit2)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 49, in __init__
    torch.nn.init.uniform_(self.cell.weight_ih, a = -limit2, b = limit2)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 49, in __init__
    torch.nn.init.uniform_(self.cell.weight_ih, a = -limit2, b = limit2)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=235899598, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=235899598, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 279, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.0, finetuning_epochs=0, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.0005,0.0001,0.00002', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=235899598, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,10000', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
d5f8e45c-7d5c-4274-b565-193cf6873dad
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     3.1123      0.0928     0.0879     14.2815
00100     2.5391      0.0886     0.1028     72.5087
00200     2.3941      0.1814     0.1844     71.3902
00300     2.1565      0.2489     0.2697     74.0672
00400     2.3993      0.1983     0.2996     71.0949
00500     2.0688      0.3122     0.2996     72.9241
00600     2.0902      0.3122     0.3455     73.1136
00700     1.9355      0.3122     0.3570     73.4012
00800     1.7727      0.3608     0.4153     72.5824
00900     1.7494      0.4030     0.4153     72.6073
01000     1.7187      0.3882     0.4528     73.8250
01100     1.6895      0.4430     0.4656     73.8140
01200     1.6794      0.4536     0.4901     71.7740
01300     1.5667      0.4494     0.4945     73.5102
01400     1.5869      0.4304     0.4945     73.0144
01500     1.5693      0.4536     0.5177     72.8630
01600     1.7285      0.4620     0.5343     73.5267
01700     1.5523      0.4684     0.5343     72.2977
01800     1.4950      0.4768     0.5343     70.6430
01900     1.4221      0.5105     0.5564     72.7259
02000     1.5299      0.4789     0.5576     71.1061
02100     1.4021      0.5127     0.5576     71.7375
02200     1.5109      0.4831     0.5741     71.3389
02300     1.5350      0.5105     0.5741     71.8135
02400     1.4601      0.5316     0.5741     71.6797
02500     1.3509      0.5485     0.5851     72.4904
02600     1.4854      0.5021     0.5851     72.4258
02700     1.2950      0.5886     0.5924     71.0968
02800     1.4239      0.5190     0.5924     72.0585
02900     1.3682      0.5570     0.6047     70.5202
03000     1.4099      0.5591     0.6162     72.4090
03100     1.3102      0.5612     0.6271     72.0262
03200     1.2848      0.5612     0.6271     72.7634
03300     1.2652      0.5992     0.6371     71.8244
03400     1.5273      0.5612     0.6476     71.2671
03500     1.2527      0.5886     0.6952     74.2191
03600     1.1026      0.6139     0.6952     71.3629
03700     1.2741      0.5823     0.6952     71.6487
03800     1.1962      0.6266     0.6952     71.8450
03900     1.1444      0.6076     0.6989     71.0029
04000     0.9958      0.6603     0.7020     72.4163
04100     1.1342      0.6097     0.7020     72.6362
04200     1.1914      0.6203     0.7068     72.4604
04300     1.0849      0.6203     0.7085     71.0455
04400     1.0697      0.6392     0.7085     73.0001
04500     1.0946      0.6414     0.7085     71.7593
04600     1.0327      0.6878     0.7085     72.2411
04700     1.1313      0.6350     0.7085     72.1800
04800     1.0447      0.6498     0.7085     72.8682
04900     1.1296      0.6329     0.7085     73.3221
05000     1.0639      0.6561     0.7085     73.5198
05100     1.1236      0.6561     0.7169     72.4405
05200     1.0833      0.6371     0.7169     72.3020
05300     1.0806      0.6308     0.7169     72.8969
05400     1.1212      0.6287     0.7169     71.6508
05500     1.0067      0.6477     0.7169     70.9704
05600     1.0857      0.6667     0.7169     72.5106
05700     1.0380      0.6646     0.7169     72.1608
05800     1.0479      0.6730     0.7327     72.0240
05900     1.0265      0.6561     0.7327     71.1804
06000     1.1984      0.5865     0.7327     72.2771
06100     0.9762      0.6920     0.7327     73.0817
06200     0.9956      0.6878     0.7327     71.7505
06300     0.9512      0.6878     0.7327     70.9990
06400     1.0536      0.6498     0.7327     72.3169
06500     0.9785      0.6498     0.7327     71.4050
06600     0.9770      0.6751     0.7327     72.6256
06700     1.1074      0.6329     0.7327     71.4950
06800     1.0050      0.6730     0.7412     72.4539
06900     1.0322      0.6624     0.7442     72.5841
07000     1.0491      0.6814     0.7442     71.5194
07100     1.0869      0.6392     0.7442     73.5545
07200     1.0944      0.6224     0.7442     71.6920
07300     0.9581      0.6603     0.7442     71.4641
07400     0.9559      0.6730     0.7442     73.0326
07500     0.9161      0.7131     0.7442     74.4357
07600     1.0343      0.6646     0.7442     73.1575
07700     0.9707      0.6814     0.7442     71.7970
07800     0.9904      0.6814     0.7442     72.1171
07900     0.9952      0.6245     0.7442     71.4706
08000     1.0051      0.6646     0.7487     74.6466
08100     0.9094      0.6899     0.7487     72.3348
08200     0.9483      0.6878     0.7487     73.9230
08300     1.0273      0.6540     0.7487     73.8205
08400     0.9100      0.6878     0.7541     72.5798
08500     1.0468      0.6477     0.7541     72.5342
08600     0.9702      0.6835     0.7676     72.3068
08700     0.9657      0.6793     0.7676     71.5703
08800     0.9842      0.6857     0.7676     72.2630
08900     0.9923      0.6624     0.7676     71.2209
09000     0.9868      0.6624     0.7676     72.2697
09100     0.9433      0.6857     0.7676     73.1064
09200     0.9887      0.6667     0.7676     71.9451
09300     0.9612      0.6857     0.7676     72.0675
09400     1.0223      0.6498     0.7676     72.3125
09500     0.9746      0.6941     0.7676     73.5532
09600     1.0086      0.6667     0.7676     71.6889
09700     0.8439      0.7089     0.7676     72.9311
09800     0.9715      0.6983     0.7676     75.5619
09900     0.9415      0.6941     0.7768     74.2673
10000     0.9842      0.6624     0.7768     71.7958
10100     0.8836      0.7321     0.7768     72.4723
10200     0.9400      0.6962     0.7768     73.3318
10300     0.9470      0.6835     0.7768     72.2577
10400     0.8602      0.7300     0.7768     72.8845
10500     0.8151      0.7131     0.7922     74.5543
10600     0.8980      0.6941     0.7922     73.0185
10700     0.8569      0.7342     0.7922     74.0179
10800     0.9387      0.7068     0.7922     73.2509
10900     0.9419      0.7152     0.7922     71.7158
11000     1.0017      0.6667     0.7922     73.8449
11100     0.9018      0.7110     0.7922     72.5592
11200     1.0065      0.6667     0.7922     72.4023
11300     0.9287      0.6899     0.7922     73.2784
11400     0.9161      0.7257     0.7922     74.0913
11500     0.9458      0.6730     0.7922     72.1835
11600     0.9696      0.6920     0.7922     73.3169
11700     1.0053      0.6983     0.7922     72.5872
11800     0.8726      0.7236     0.7922     72.9489
11900     0.9997      0.6709     0.7922     73.2098
12000     0.8920      0.7131     0.7922     73.9182
12100     0.8971      0.7342     0.7922     71.9334
12200     0.8760      0.7004     0.7922     73.1265
12300     0.8726      0.7194     0.7922     73.9011
12400     0.7679      0.7300     0.7922     74.1717
12500     0.8876      0.7257     0.7922     72.0024
12600     0.8760      0.7194     0.7922     72.3939
12700     0.9753      0.6751     0.7922     73.1486
12800     1.0124      0.6646     0.7922     73.2111
12900     0.9410      0.6920     0.7922     73.2804
13000     0.8454      0.7173     0.7922     70.6899
13100     0.9584      0.7046     0.7922     73.5366
13200     0.9090      0.7089     0.7922     74.4191
13300     0.9140      0.7215     0.7922     73.7131
13400     0.9646      0.6835     0.7922     72.8581
13500     0.9381      0.6899     0.7922     71.9504
13600     0.9303      0.6920     0.7922     73.3379
13700     0.8667      0.7131     0.7922     71.5656
13800     0.8615      0.7110     0.7922     71.2413
13900     0.8489      0.7489     0.7922     73.0244
14000     0.9682      0.6772     0.7922     70.6818
14100     0.8326      0.7257     0.7922     71.5193
14200     0.9038      0.7068     0.7922     73.8544
14300     0.9076      0.7257     0.7922     71.8065
14400     0.9313      0.7152     0.7922     73.1262
14500     0.9296      0.7004     0.7922     73.0314
14600     0.9451      0.7046     0.7922     73.3105
14700     0.9588      0.7068     0.7922     71.4437
14800     0.8836      0.7363     0.7922     72.7055
14900     0.8581      0.7342     0.7922     72.9466
15000     0.7455      0.7574     0.7922     71.3888
15100     0.9260      0.6899     0.7922     72.8945
15200     0.9142      0.7004     0.7922     73.2186
15300     0.9550      0.6835     0.7922     72.4866
15400     0.9289      0.6983     0.7922     71.4738
15500     0.7720      0.7342     0.7922     71.7969
15600     0.9226      0.6709     0.7922     72.9330
15700     1.0747      0.6582     0.7922     72.1999
15800     0.6943      0.7700     0.7922     72.9190
15900     0.8589      0.7215     0.7922     72.2801
16000     0.9140      0.7089     0.7922     74.4353
16100     0.8836      0.7025     0.7922     73.9739
16200     0.8545      0.7384     0.7922     72.6343
16300     0.8868      0.6941     0.7922     73.0402
16400     0.8973      0.7089     0.7922     72.5598
16500     0.8853      0.7131     0.7922     74.3133
16600     0.8216      0.7426     0.7922     74.1124
16700     0.9020      0.7236     0.7922     72.2244
16800     0.8795      0.7173     0.7922     73.3082
16900     0.8861      0.7068     0.7922     73.5475
17000     0.9144      0.7004     0.7922     72.2328
17100     0.7534      0.7616     0.7922     74.0822
17200     0.8009      0.7384     0.7922     71.8162
17300     0.8428      0.7426     0.7922     71.8242
17400     0.8572      0.7152     0.7922     73.8203
17500     0.7741      0.7468     0.7922     74.2842
17600     0.8721      0.7194     0.7922     74.0535
17700     0.9299      0.6941     0.7964     71.9339
17800     0.7870      0.7468     0.7964     71.5982
17900     0.7619      0.7637     0.7964     72.0324
18000     0.9696      0.7046     0.7964     71.0228
18100     0.9072      0.7321     0.7964     71.5681
18200     0.9135      0.7089     0.7964     74.1362
18300     0.8623      0.7257     0.7964     71.7329
18400     0.8112      0.7173     0.7964     72.4131
18500     0.8674      0.7152     0.7964     72.1035
18600     0.8351      0.7215     0.7964     71.8113
18700     0.9239      0.7152     0.7964     72.9961
18800     0.9100      0.7215     0.7964     73.8090
18900     0.9298      0.7236     0.7964     72.5524
19000     0.8451      0.7300     0.7964     73.7334
19100     0.8722      0.7089     0.7964     71.5974
19200     0.8451      0.7342     0.7964     73.7814
19300     0.8378      0.7489     0.7964     72.4659
19400     0.9099      0.7110     0.7964     71.5728
19500     0.8995      0.6857     0.7964     72.7635
19600     0.9085      0.7089     0.7964     72.7598
19700     0.9340      0.7046     0.7964     74.3807
19800     0.8971      0.7236     0.7964     73.5860
19900     0.8907      0.7300     0.7964     72.6568
20000     0.8769      0.7215     0.7964     74.4840
20100     0.7698      0.7468     0.7964     74.7685
20200     0.8794      0.7131     0.7964     72.3006
20300     0.8423      0.7342     0.7964     74.3749
20400     0.8910      0.6983     0.7964     74.0067
20500     0.8101      0.7447     0.7964     72.4959
20600     0.7908      0.7574     0.7964     72.4286
20700     0.8511      0.7173     0.7964     72.2251
20800     0.8066      0.7405     0.7964     72.5353
20900     0.8958      0.7152     0.7964     71.6304
21000     0.8629      0.6941     0.7964     71.9457
21100     0.9212      0.7110     0.7964     73.0684
21200     0.7960      0.7257     0.7964     73.0320
21300     0.8521      0.7131     0.7964     72.7056
21400     0.9187      0.7068     0.7964     71.8092
21500     0.8232      0.7405     0.7964     72.8694
21600     0.9164      0.7321     0.7964     73.5144
21700     0.7572      0.7426     0.7964     71.9740
21800     0.8677      0.7068     0.8011     72.6431
21900     0.9087      0.7257     0.8011     71.4950
22000     0.8273      0.7300     0.8011     75.0660
22100     0.8519      0.7321     0.8011     72.4575
22200     0.8960      0.7004     0.8011     73.1087
22300     0.9080      0.7194     0.8011     70.9940
22400     0.8116      0.7300     0.8011     72.5423
22500     0.8947      0.7004     0.8011     72.8697
22600     0.8862      0.7152     0.8011     74.2758
22700     0.9277      0.7131     0.8011     76.2971
22800     0.8001      0.7447     0.8011     72.1376
22900     0.8771      0.7257     0.8011     73.0363
23000     0.8090      0.7215     0.8011     72.7419
23100     0.8935      0.7173     0.8011     71.9401
23200     0.8243      0.7363     0.8011     73.0559
23300     0.7628      0.7616     0.8011     71.1829
23400     0.8579      0.7321     0.8011     72.4743
23500     0.9099      0.7025     0.8011     73.7911
23600     0.9114      0.7131     0.8011     73.4456
23700     1.0117      0.6983     0.8011     72.7239
23800     0.8200      0.7447     0.8011     72.4806
23900     0.8351      0.7173     0.8011     70.8807
24000     0.9032      0.7278     0.8011     70.3119
24100     0.8628      0.7321     0.8011     73.8675
24200     0.8023      0.7405     0.8011     71.0944
24300     0.8290      0.7532     0.8011     74.6826
24400     0.8398      0.7384     0.8011     72.2812
24500     0.8106      0.7257     0.8011     75.0914
24600     0.9238      0.6941     0.8011     71.4366
24700     0.7908      0.7553     0.8011     70.8844
24800     0.8569      0.7046     0.8011     70.8251
24900     0.7846      0.7426     0.8011     70.7396
25000     0.7778      0.7806     0.8011     72.3484
25100     0.7810      0.7468     0.8011     73.3669
25200     0.9208      0.7278     0.8011     71.5327
25300     0.8056      0.7489     0.8011     70.3470
25400     0.8063      0.7257     0.8011     71.9683
25500     0.8124      0.7384     0.8011     70.8942
25600     0.7866      0.7342     0.8011     71.0935
25700     0.9223      0.7004     0.8011     72.7582
25800     0.8603      0.7110     0.8011     70.7463
25900     0.8127      0.7489     0.8011     75.1452
26000     0.8065      0.7363     0.8011     71.3608
26100     0.8344      0.7405     0.8011     70.7795
26200     0.8673      0.7215     0.8011     72.0855
26300     0.8253      0.7468     0.8011     71.6746
26400     0.8206      0.7489     0.8011     73.2577
26500     0.7596      0.7468     0.8011     73.1873
26600     0.8325      0.7236     0.8011     71.3453
26700     0.8962      0.7025     0.8011     72.3593
26800     0.6951      0.7975     0.8011     72.0538
26900     0.7989      0.7658     0.8011     70.9331
27000     0.8942      0.7004     0.8011     71.4919
27100     0.7413      0.7511     0.8011     70.8235
27200     1.0278      0.6772     0.8011     71.0096
27300     0.9557      0.7046     0.8011     72.3065
27400     0.8574      0.7342     0.8011     71.7788
27500     0.7855      0.7553     0.8011     71.7481
27600     0.7891      0.7553     0.8011     71.9748
27700     0.8333      0.7131     0.8011     71.2385
27800     0.8636      0.7152     0.8011     71.7799
27900     0.9066      0.7025     0.8011     70.6568
28000     0.9365      0.6920     0.8011     71.4221
28100     0.9204      0.7046     0.8011     73.1541
28200     0.8999      0.7025     0.8011     71.0639
28300     0.9243      0.7194     0.8011     71.8605
28400     0.8701      0.7278     0.8011     72.0948
28500     0.8330      0.7089     0.8011     70.9290
28600     0.8103      0.7363     0.8011     72.0968
28700     0.8384      0.7426     0.8011     70.9721
28800     0.9383      0.6962     0.8011     72.0569
28900     0.8319      0.7658     0.8011     73.4570
29000     0.7877      0.7426     0.8011     70.8044
29100     0.6971      0.8186     0.8011     73.2202
29200     0.8636      0.7300     0.8011     70.5250
29300     0.8520      0.7152     0.8011     71.1319
29400     0.8616      0.7194     0.8011     71.1268
29500     0.8472      0.7236     0.8011     71.9205
29600     0.8576      0.7215     0.8011     70.5871
29700     0.9775      0.6983     0.8011     71.6947
29800     0.8940      0.7194     0.8011     73.7638
29900     0.7412      0.7532     0.8011     71.9329
29999     0.8068      0.7152     0.8011     69.3321
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Start testing:
Test Accuracy: 0.7598
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=235899598, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
528c4066-a7f6-4db8-9f5a-df4a518e5815
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     3.0259      0.0865     0.0944     15.3275
00100     2.2228      0.2194     0.1043     73.9041
00200     2.0629      0.3228     0.1043     72.3447
00300     1.9247      0.3608     0.1200     72.1185
00400     1.8920      0.3882     0.1200     71.9566
00500     1.6974      0.4114     0.1200     71.4485
00600     1.5775      0.4873     0.1339     71.7673
00700     1.4549      0.4979     0.1339     72.3389
00800     1.4451      0.4895     0.1505     72.2262
00900     1.2605      0.5928     0.1505     71.8666
01000     1.1713      0.5844     0.1588     71.7800
01100     1.0745      0.6329     0.1588     72.2671
01200     1.2442      0.5886     0.1588     71.7901
01300     1.1431      0.5970     0.1625     71.5648
01400     1.0686      0.6709     0.1753     72.1422
01500     1.1175      0.6245     0.1753     71.5356
01600     1.1307      0.6308     0.1753     71.9647
01700     1.0510      0.6477     0.1753     71.9456
01800     1.0689      0.6456     0.1753     72.3609
01900     0.9652      0.6793     0.1753     72.1168
02000     1.0045      0.6920     0.1753     72.2460
02100     1.0730      0.6519     0.1753     72.8550
02200     1.0492      0.6603     0.1814     72.0817
02300     0.9454      0.7110     0.1814     71.8750
02400     0.9379      0.7068     0.1824     72.3817
02500     0.8458      0.7363     0.1856     72.0843
02600     1.0415      0.6477     0.2063     72.1656
02700     0.9885      0.6814     0.2063     71.4929
02800     0.8718      0.7321     0.2078     72.0315
02900     1.0714      0.6456     0.2078     71.6897
03000     0.9949      0.6772     0.2172     71.2352
03100     0.9821      0.6793     0.2439     71.9784
03200     0.9176      0.7046     0.2439     72.6847
03300     0.9422      0.6561     0.2439     72.4928
03400     0.9382      0.7110     0.2439     72.2787
03500     0.8265      0.7447     0.2439     72.2821
03600     0.8902      0.7257     0.2439     71.4580
03700     0.8575      0.7194     0.2439     71.4583
03800     0.8148      0.7447     0.2439     72.3052
03900     0.9220      0.7405     0.2439     72.1561
04000     0.8619      0.7426     0.2678     72.5893
04100     0.8138      0.7426     0.2678     72.4734
04200     0.8516      0.7300     0.2678     72.2864
04300     0.8379      0.7321     0.2678     71.8955
04400     0.8990      0.7152     0.2678     71.3301
04500     0.8505      0.7532     0.2678     71.5544
04600     0.8007      0.7278     0.2678     72.4115
04700     0.8798      0.7152     0.2678     72.1587
04800     0.9235      0.7215     0.2678     72.0011
04900     0.8676      0.7321     0.2678     72.2588
05000     0.8125      0.7468     0.2702     71.5613
05100     0.8370      0.7489     0.2742     71.3455
05200     0.8407      0.7321     0.2854     71.6137
05300     0.7731      0.7595     0.2854     71.9170
05400     0.8883      0.7004     0.2854     72.3549
05500     0.8626      0.7405     0.2883     72.0292
05600     0.7759      0.7447     0.2908     72.2101
05700     0.7526      0.7722     0.2908     71.6677
05800     0.7100      0.7785     0.3014     72.9127
05900     0.7964      0.7511     0.3014     72.0088
06000     0.8931      0.7194     0.3074     72.5999
06100     0.7435      0.7679     0.3074     72.4760
06200     0.8419      0.7278     0.3176     72.2199
06300     0.8324      0.7426     0.3176     72.3034
06400     0.9292      0.7342     0.3176     71.6243
06500     0.7945      0.7405     0.3264     71.4127
06600     0.7738      0.7574     0.3264     71.2432
06700     0.8405      0.7384     0.3264     71.8948
06800     0.6821      0.7679     0.3273     72.1031
06900     0.8230      0.7300     0.3273     71.0756
07000     0.7826      0.7616     0.3451     71.6426
07100     0.8593      0.7131     0.3499     72.1813
07200     0.8787      0.7194     0.3525     72.4258
07300     0.7821      0.7447     0.3525     71.7011
07400     0.7589      0.7426     0.3525     71.4798
07500     0.7041      0.7700     0.3525     72.4609
07600     0.7876      0.7532     0.3639     71.7374
07700     0.6992      0.7658     0.3639     71.6448
07800     0.7505      0.7658     0.3860     72.7018
07900     0.7914      0.7511     0.3860     71.4738
08000     0.7670      0.7511     0.3860     72.2053
08100     0.6549      0.7996     0.3860     71.3738
08200     0.7383      0.7700     0.3908     71.7851
08300     0.7965      0.7447     0.3908     72.4891
08400     0.6554      0.7911     0.4230     72.1285
08500     0.7774      0.7447     0.4230     71.2773
08600     0.7016      0.7764     0.4230     71.8578
08700     0.7727      0.7363     0.4230     71.4356
08800     0.7407      0.7637     0.4230     72.6789
08900     0.7741      0.7321     0.4230     72.1713
09000     0.7235      0.7658     0.4230     71.7427
09100     0.7749      0.7574     0.4230     73.0866
09200     0.6906      0.7996     0.4230     71.8104
09300     0.6588      0.8207     0.4230     72.0547
09400     0.6234      0.8017     0.4230     72.0184
09500     0.8175      0.7363     0.4230     71.5517
09600     0.6626      0.7658     0.4230     72.5243
09700     0.6689      0.7637     0.4418     72.6060
09800     0.7220      0.7764     0.4685     72.9210
09900     0.8010      0.7511     0.4685     71.9713
10000     0.7317      0.7574     0.4685     71.9628
10100     0.7258      0.7637     0.4685     72.1406
10200     0.7468      0.7511     0.4685     72.7279
10300     0.6881      0.7806     0.4685     73.1391
10400     0.7079      0.7700     0.4685     71.9582
10500     0.7894      0.7616     0.4685     71.7026
10600     0.6920      0.7890     0.4685     72.3411
10700     0.8013      0.7342     0.4685     72.2676
10800     0.7063      0.7658     0.4685     71.9669
10900     0.7600      0.7785     0.4685     72.3937
11000     0.6507      0.8165     0.4685     72.5985
11100     0.7233      0.7869     0.4685     72.1628
11200     0.6779      0.7764     0.4685     72.3596
11300     0.7026      0.7764     0.4685     72.2129
11400     0.6771      0.7932     0.4685     72.3129
11500     0.7039      0.7869     0.4685     72.2806
11600     0.5908      0.8080     0.4685     72.1900
11700     0.6836      0.7911     0.4685     72.6107
11800     0.5824      0.8333     0.4685     72.3418
11900     0.6268      0.8101     0.4685     71.7372
12000     0.6434      0.7806     0.4685     72.1937
12100     0.5738      0.8376     0.4685     71.7285
12200     0.7374      0.7637     0.4685     71.7652
12300     0.6419      0.7890     0.4685     71.8956
12400     0.7305      0.7574     0.4685     72.2578
12500     0.6426      0.7827     0.4807     72.1123
12600     0.6264      0.8017     0.4807     71.8059
12700     0.5925      0.8080     0.4807     72.3753
12800     0.7386      0.7764     0.4807     72.4304
12900     0.6607      0.7869     0.4807     71.8121
13000     0.6020      0.8059     0.4807     71.8645
13100     0.6749      0.7764     0.4807     72.1716
13200     0.6630      0.8017     0.4807     71.8268
13300     0.6632      0.8101     0.4923     71.7860
13400     0.7591      0.7827     0.4923     72.3459
13500     0.6516      0.8143     0.4923     72.1246
13600     0.6460      0.7975     0.4923     72.8059
13700     0.6910      0.7827     0.4923     71.9014
13800     0.6337      0.8080     0.4923     72.6000
13900     0.6587      0.8122     0.4923     72.0973
14000     0.6714      0.8017     0.4923     71.9010
14100     0.6919      0.7700     0.4923     71.7519
14200     0.6533      0.7911     0.4923     72.5163
14300     0.6939      0.7890     0.4923     72.1331
14400     0.6343      0.7890     0.4923     72.4222
14500     0.6395      0.8122     0.4923     71.7612
14600     0.6141      0.7975     0.4923     71.7354
14700     0.7132      0.7848     0.4923     72.0421
14800     0.5478      0.8376     0.4923     71.3428
14900     0.6684      0.7911     0.4923     71.3792
15000     0.6065      0.8186     0.4923     71.3220
15100     0.6568      0.8101     0.4923     71.6691
15200     0.6924      0.7700     0.4923     72.7419
15300     0.6388      0.7890     0.4923     72.3819
15400     0.5979      0.8143     0.4923     71.8060
15500     0.5559      0.8165     0.4923     71.8676
15600     0.6107      0.8101     0.4923     72.1773
15700     0.6684      0.7827     0.4923     72.1659
15800     0.6727      0.7848     0.4923     72.1405
15900     0.6174      0.7975     0.4988     71.8533
16000     0.6553      0.7932     0.4988     71.2132
16100     0.6523      0.8207     0.4988     71.6629
16200     0.6686      0.7869     0.4988     71.6613
16300     0.7884      0.7532     0.4988     72.0634
16400     0.6276      0.8291     0.4988     71.8756
16500     0.6005      0.8291     0.4988     71.6478
16600     0.6415      0.7911     0.4988     71.5692
16700     0.6455      0.7954     0.4988     71.7898
16800     0.6424      0.7975     0.4988     72.4446
16900     0.6996      0.7911     0.5023     71.5333
17000     0.7236      0.7932     0.5023     71.9804
17100     0.6394      0.7975     0.5023     72.2483
17200     0.6642      0.8143     0.5023     71.6686
17300     0.6370      0.7869     0.5023     71.8656
17400     0.6111      0.7954     0.5023     72.0541
17500     0.6782      0.8059     0.5023     72.5378
17600     0.7681      0.7679     0.5023     72.2326
17700     0.6037      0.7975     0.5023     71.6782
17800     0.7427      0.7553     0.5023     71.9685
17900     0.6357      0.7975     0.5023     72.6623
18000     0.5881      0.8017     0.5023     72.7393
18100     0.6489      0.8038     0.5023     71.8543
18200     0.6479      0.8017     0.5023     72.4644
18300     0.6140      0.8122     0.5023     72.5674
18400     0.7322      0.7700     0.5023     72.5834
18500     0.6913      0.7932     0.5023     72.3108
18600     0.6485      0.7954     0.5023     71.9066
18700     0.6805      0.7932     0.5023     72.0457
18800     0.5855      0.8165     0.5023     72.2794
18900     0.6729      0.7996     0.5023     72.7211
19000     0.6890      0.7827     0.5023     72.2470
19100     0.6348      0.8059     0.5023     72.0447
19200     0.6392      0.7954     0.5023     72.5953
19300     0.5588      0.8354     0.5023     71.6109
19400     0.5923      0.7996     0.5023     72.0527
19500     0.6700      0.7954     0.5023     72.7887
19600     0.6280      0.8165     0.5023     71.6939
19700     0.7333      0.7954     0.5023     72.0566
19800     0.7523      0.7806     0.5023     72.5627
19900     0.5874      0.8059     0.5023     72.0382
20000     0.6674      0.7932     0.5023     72.3518
20100     0.5561      0.8165     0.5023     71.2214
20199     0.6554      0.8017     0.5023     71.3102
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.3338      0.4430     0.4343     10.0599
00100     1.5804      0.4916     0.5787     71.7754
00200     1.2733      0.6139     0.6215     71.7150
00300     1.2893      0.6203     0.6801     71.5640
00400     1.3629      0.5970     0.6960     71.0815
00500     1.2831      0.6224     0.6960     71.2196
00600     1.1584      0.6646     0.6960     72.2843
00700     1.1841      0.6498     0.7013     71.0301
00800     1.1686      0.6814     0.7112     71.3326
00900     1.1260      0.6688     0.7270     71.6642
01000     1.2756      0.6350     0.7270     71.6270
01100     1.0878      0.6624     0.7270     71.9791
01200     1.2501      0.6540     0.7270     71.3036
01300     1.1235      0.6793     0.7270     71.9278
01400     1.0344      0.6814     0.7383     72.4396
01500     1.2336      0.6435     0.7383     72.1434
01600     1.0639      0.6688     0.7383     72.5667
01700     1.1383      0.6456     0.7383     71.8808
01800     1.1233      0.6561     0.7383     71.5281
01900     1.1668      0.6498     0.7383     71.7620
02000     1.1314      0.6772     0.7383     71.7501
02100     1.0960      0.6498     0.7383     71.0194
02200     1.2373      0.6203     0.7383     71.6817
02300     1.1855      0.6709     0.7383     72.0264
02400     1.1145      0.6857     0.7383     71.6450
02500     0.9826      0.7110     0.7383     71.4688
02600     1.1498      0.6646     0.7451     72.1135
02700     1.1548      0.6751     0.7451     71.8622
02800     1.0907      0.6941     0.7451     72.1372
02900     0.9263      0.7384     0.7451     71.4109
03000     1.0455      0.6646     0.7456     71.4311
03100     1.0533      0.6857     0.7456     70.9722
03200     1.0191      0.7046     0.7456     71.5180
03300     1.0579      0.6857     0.7476     71.7301
03400     1.0672      0.6899     0.7489     71.0388
03500     1.0951      0.6772     0.7489     71.4484
03600     1.1115      0.6857     0.7520     71.5971
03700     1.0154      0.6857     0.7520     71.1335
03800     1.0322      0.6835     0.7520     71.7879
03900     1.2215      0.6477     0.7520     71.5414
04000     1.0246      0.6857     0.7571     71.8878
04100     1.0784      0.7025     0.7571     71.1216
04200     1.0309      0.7089     0.7571     71.5310
04300     0.9322      0.7152     0.7571     71.6048
04400     1.0737      0.6899     0.7571     71.7384
04500     1.0775      0.6667     0.7571     71.8907
04600     1.1043      0.6603     0.7571     72.1075
04700     1.0098      0.7089     0.7597     71.4215
04800     1.0429      0.6793     0.7597     71.8423
04900     1.2392      0.6266     0.7661     71.4726
05000     1.1219      0.6709     0.7682     71.7656
05100     1.0891      0.6793     0.7682     72.3064
05200     0.9992      0.6962     0.7682     71.9124
05300     1.0880      0.6920     0.7682     71.6885
05400     1.0369      0.6941     0.7682     71.7874
05500     1.1299      0.6582     0.7682     71.2567
05600     1.1126      0.6603     0.7682     71.5821
05700     1.0091      0.6983     0.7682     71.0193
05800     0.9619      0.7194     0.7682     71.3266
05900     1.1647      0.6667     0.7682     71.8865
06000     1.0916      0.6688     0.7682     71.8847
06100     1.0013      0.6730     0.7682     71.3496
06200     1.0982      0.6371     0.7682     71.7932
06300     1.1197      0.6540     0.7682     70.8574
06400     1.0966      0.6920     0.7682     72.1820
06500     1.1589      0.6498     0.7682     71.1002
06600     0.9725      0.7257     0.7682     70.5596
06700     1.1267      0.6603     0.7682     71.2594
06800     1.0527      0.6814     0.7682     70.4104
06900     0.9596      0.7152     0.7682     71.3873
07000     1.0233      0.6730     0.7682     71.7487
07100     1.0853      0.6772     0.7682     71.5215
07200     0.9754      0.6899     0.7682     71.4206
07300     1.0666      0.6772     0.7682     71.0724
07400     1.1102      0.6793     0.7682     71.2097
07500     1.0792      0.6835     0.7682     72.0946
07600     0.9981      0.6646     0.7682     71.2199
07700     1.1633      0.6498     0.7682     70.7655
07800     1.0282      0.6983     0.7682     71.1033
07900     1.0715      0.7236     0.7688     72.2064
08000     1.1026      0.6519     0.7688     71.0939
08100     1.0704      0.6561     0.7688     71.6737
08200     0.9095      0.7426     0.7688     71.3366
08300     1.0167      0.6730     0.7688     71.6826
08400     0.9257      0.7321     0.7688     71.2859
08500     1.0603      0.6814     0.7688     71.4296
08600     0.9804      0.7321     0.7688     71.1512
08700     1.0600      0.6751     0.7688     71.8531
08800     0.9980      0.6835     0.7688     71.7253
08900     0.9210      0.7068     0.7688     71.5770
09000     0.8529      0.7426     0.7688     71.2828
09100     0.9377      0.6941     0.7688     72.2866
09200     1.1173      0.6646     0.7688     71.8595
09300     1.0398      0.6688     0.7688     71.8791
09400     1.0987      0.6667     0.7688     71.8036
09500     0.9332      0.7046     0.7688     71.7790
09600     1.0348      0.6962     0.7688     73.3538
09700     1.0032      0.7004     0.7688     71.4244
09800     0.9663      0.7131     0.7688     71.1169
09900     1.0363      0.6793     0.7688     71.9709
Start testing:
Test Accuracy: 0.7942
