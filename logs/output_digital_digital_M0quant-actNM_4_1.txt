Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 49, in __init__
    torch.nn.init.uniform_(self.cell.weight_ih, a = -limit2, b = limit2)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 49, in __init__
    torch.nn.init.uniform_(self.cell.weight_ih, a = -limit2, b = limit2)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 49, in __init__
    torch.nn.init.uniform_(self.cell.weight_ih, a = -limit2, b = limit2)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 49, in __init__
    torch.nn.init.uniform_(self.cell.weight_ih, a = -limit2, b = limit2)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=235899598, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=235899598, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 279, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.0, finetuning_epochs=0, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.0005,0.0001,0.00002', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=235899598, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,10000', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
d5f8e45c-7d5c-4274-b565-193cf6873dad
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     3.1123      0.0928     0.0879     14.2815
00100     2.5391      0.0886     0.1028     72.5087
00200     2.3941      0.1814     0.1844     71.3902
00300     2.1565      0.2489     0.2697     74.0672
00400     2.3993      0.1983     0.2996     71.0949
00500     2.0688      0.3122     0.2996     72.9241
00600     2.0902      0.3122     0.3455     73.1136
00700     1.9355      0.3122     0.3570     73.4012
00800     1.7727      0.3608     0.4153     72.5824
00900     1.7494      0.4030     0.4153     72.6073
01000     1.7187      0.3882     0.4528     73.8250
01100     1.6895      0.4430     0.4656     73.8140
01200     1.6794      0.4536     0.4901     71.7740
01300     1.5667      0.4494     0.4945     73.5102
01400     1.5869      0.4304     0.4945     73.0144
01500     1.5693      0.4536     0.5177     72.8630
01600     1.7285      0.4620     0.5343     73.5267
01700     1.5523      0.4684     0.5343     72.2977
01800     1.4950      0.4768     0.5343     70.6430
01900     1.4221      0.5105     0.5564     72.7259
02000     1.5299      0.4789     0.5576     71.1061
02100     1.4021      0.5127     0.5576     71.7375
02200     1.5109      0.4831     0.5741     71.3389
02300     1.5350      0.5105     0.5741     71.8135
02400     1.4601      0.5316     0.5741     71.6797
02500     1.3509      0.5485     0.5851     72.4904
02600     1.4854      0.5021     0.5851     72.4258
02700     1.2950      0.5886     0.5924     71.0968
02800     1.4239      0.5190     0.5924     72.0585
02900     1.3682      0.5570     0.6047     70.5202
03000     1.4099      0.5591     0.6162     72.4090
03100     1.3102      0.5612     0.6271     72.0262
03200     1.2848      0.5612     0.6271     72.7634
03300     1.2652      0.5992     0.6371     71.8244
03400     1.5273      0.5612     0.6476     71.2671
03500     1.2527      0.5886     0.6952     74.2191
03600     1.1026      0.6139     0.6952     71.3629
03700     1.2741      0.5823     0.6952     71.6487
03800     1.1962      0.6266     0.6952     71.8450
03900     1.1444      0.6076     0.6989     71.0029
04000     0.9958      0.6603     0.7020     72.4163
04100     1.1342      0.6097     0.7020     72.6362
04200     1.1914      0.6203     0.7068     72.4604
04300     1.0849      0.6203     0.7085     71.0455
04400     1.0697      0.6392     0.7085     73.0001
04500     1.0946      0.6414     0.7085     71.7593
04600     1.0327      0.6878     0.7085     72.2411
04700     1.1313      0.6350     0.7085     72.1800
04800     1.0447      0.6498     0.7085     72.8682
04900     1.1296      0.6329     0.7085     73.3221
05000     1.0639      0.6561     0.7085     73.5198
05100     1.1236      0.6561     0.7169     72.4405
05200     1.0833      0.6371     0.7169     72.3020
05300     1.0806      0.6308     0.7169     72.8969
05400     1.1212      0.6287     0.7169     71.6508
05500     1.0067      0.6477     0.7169     70.9704
05600     1.0857      0.6667     0.7169     72.5106
05700     1.0380      0.6646     0.7169     72.1608
05800     1.0479      0.6730     0.7327     72.0240
05900     1.0265      0.6561     0.7327     71.1804
06000     1.1984      0.5865     0.7327     72.2771
06100     0.9762      0.6920     0.7327     73.0817
06200     0.9956      0.6878     0.7327     71.7505
06300     0.9512      0.6878     0.7327     70.9990
06400     1.0536      0.6498     0.7327     72.3169
06500     0.9785      0.6498     0.7327     71.4050
06600     0.9770      0.6751     0.7327     72.6256
06700     1.1074      0.6329     0.7327     71.4950
06800     1.0050      0.6730     0.7412     72.4539
06900     1.0322      0.6624     0.7442     72.5841
07000     1.0491      0.6814     0.7442     71.5194
07100     1.0869      0.6392     0.7442     73.5545
07200     1.0944      0.6224     0.7442     71.6920
07300     0.9581      0.6603     0.7442     71.4641
07400     0.9559      0.6730     0.7442     73.0326
07500     0.9161      0.7131     0.7442     74.4357
07600     1.0343      0.6646     0.7442     73.1575
07700     0.9707      0.6814     0.7442     71.7970
07800     0.9904      0.6814     0.7442     72.1171
07900     0.9952      0.6245     0.7442     71.4706
08000     1.0051      0.6646     0.7487     74.6466
08100     0.9094      0.6899     0.7487     72.3348
08200     0.9483      0.6878     0.7487     73.9230
08300     1.0273      0.6540     0.7487     73.8205
08400     0.9100      0.6878     0.7541     72.5798
08500     1.0468      0.6477     0.7541     72.5342
08600     0.9702      0.6835     0.7676     72.3068
08700     0.9657      0.6793     0.7676     71.5703
08800     0.9842      0.6857     0.7676     72.2630
08900     0.9923      0.6624     0.7676     71.2209
09000     0.9868      0.6624     0.7676     72.2697
09100     0.9433      0.6857     0.7676     73.1064
09200     0.9887      0.6667     0.7676     71.9451
09300     0.9612      0.6857     0.7676     72.0675
09400     1.0223      0.6498     0.7676     72.3125
09500     0.9746      0.6941     0.7676     73.5532
09600     1.0086      0.6667     0.7676     71.6889
09700     0.8439      0.7089     0.7676     72.9311
09800     0.9715      0.6983     0.7676     75.5619
09900     0.9415      0.6941     0.7768     74.2673
10000     0.9842      0.6624     0.7768     71.7958
10100     0.8836      0.7321     0.7768     72.4723
10200     0.9400      0.6962     0.7768     73.3318
10300     0.9470      0.6835     0.7768     72.2577
10400     0.8602      0.7300     0.7768     72.8845
10500     0.8151      0.7131     0.7922     74.5543
10600     0.8980      0.6941     0.7922     73.0185
10700     0.8569      0.7342     0.7922     74.0179
10800     0.9387      0.7068     0.7922     73.2509
10900     0.9419      0.7152     0.7922     71.7158
11000     1.0017      0.6667     0.7922     73.8449
11100     0.9018      0.7110     0.7922     72.5592
11200     1.0065      0.6667     0.7922     72.4023
11300     0.9287      0.6899     0.7922     73.2784
11400     0.9161      0.7257     0.7922     74.0913
11500     0.9458      0.6730     0.7922     72.1835
11600     0.9696      0.6920     0.7922     73.3169
11700     1.0053      0.6983     0.7922     72.5872
11800     0.8726      0.7236     0.7922     72.9489
11900     0.9997      0.6709     0.7922     73.2098
12000     0.8920      0.7131     0.7922     73.9182
12100     0.8971      0.7342     0.7922     71.9334
12200     0.8760      0.7004     0.7922     73.1265
12300     0.8726      0.7194     0.7922     73.9011
12400     0.7679      0.7300     0.7922     74.1717
12500     0.8876      0.7257     0.7922     72.0024
12600     0.8760      0.7194     0.7922     72.3939
12700     0.9753      0.6751     0.7922     73.1486
12800     1.0124      0.6646     0.7922     73.2111
12900     0.9410      0.6920     0.7922     73.2804
13000     0.8454      0.7173     0.7922     70.6899
13100     0.9584      0.7046     0.7922     73.5366
13200     0.9090      0.7089     0.7922     74.4191
13300     0.9140      0.7215     0.7922     73.7131
13400     0.9646      0.6835     0.7922     72.8581
13500     0.9381      0.6899     0.7922     71.9504
13600     0.9303      0.6920     0.7922     73.3379
13700     0.8667      0.7131     0.7922     71.5656
13800     0.8615      0.7110     0.7922     71.2413
13900     0.8489      0.7489     0.7922     73.0244
14000     0.9682      0.6772     0.7922     70.6818
14100     0.8326      0.7257     0.7922     71.5193
14200     0.9038      0.7068     0.7922     73.8544
14300     0.9076      0.7257     0.7922     71.8065
14400     0.9313      0.7152     0.7922     73.1262
14500     0.9296      0.7004     0.7922     73.0314
14600     0.9451      0.7046     0.7922     73.3105
14700     0.9588      0.7068     0.7922     71.4437
14800     0.8836      0.7363     0.7922     72.7055
14900     0.8581      0.7342     0.7922     72.9466
15000     0.7455      0.7574     0.7922     71.3888
15100     0.9260      0.6899     0.7922     72.8945
15200     0.9142      0.7004     0.7922     73.2186
15300     0.9550      0.6835     0.7922     72.4866
15400     0.9289      0.6983     0.7922     71.4738
15500     0.7720      0.7342     0.7922     71.7969
15600     0.9226      0.6709     0.7922     72.9330
15700     1.0747      0.6582     0.7922     72.1999
15800     0.6943      0.7700     0.7922     72.9190
15900     0.8589      0.7215     0.7922     72.2801
16000     0.9140      0.7089     0.7922     74.4353
16100     0.8836      0.7025     0.7922     73.9739
16200     0.8545      0.7384     0.7922     72.6343
16300     0.8868      0.6941     0.7922     73.0402
16400     0.8973      0.7089     0.7922     72.5598
16500     0.8853      0.7131     0.7922     74.3133
16600     0.8216      0.7426     0.7922     74.1124
16700     0.9020      0.7236     0.7922     72.2244
16800     0.8795      0.7173     0.7922     73.3082
16900     0.8861      0.7068     0.7922     73.5475
17000     0.9144      0.7004     0.7922     72.2328
17100     0.7534      0.7616     0.7922     74.0822
17200     0.8009      0.7384     0.7922     71.8162
17300     0.8428      0.7426     0.7922     71.8242
17400     0.8572      0.7152     0.7922     73.8203
17500     0.7741      0.7468     0.7922     74.2842
17600     0.8721      0.7194     0.7922     74.0535
17700     0.9299      0.6941     0.7964     71.9339
17800     0.7870      0.7468     0.7964     71.5982
17900     0.7619      0.7637     0.7964     72.0324
18000     0.9696      0.7046     0.7964     71.0228
18100     0.9072      0.7321     0.7964     71.5681
18200     0.9135      0.7089     0.7964     74.1362
18300     0.8623      0.7257     0.7964     71.7329
18400     0.8112      0.7173     0.7964     72.4131
18500     0.8674      0.7152     0.7964     72.1035
18600     0.8351      0.7215     0.7964     71.8113
18700     0.9239      0.7152     0.7964     72.9961
18800     0.9100      0.7215     0.7964     73.8090
18900     0.9298      0.7236     0.7964     72.5524
19000     0.8451      0.7300     0.7964     73.7334
19100     0.8722      0.7089     0.7964     71.5974
19200     0.8451      0.7342     0.7964     73.7814
19300     0.8378      0.7489     0.7964     72.4659
19400     0.9099      0.7110     0.7964     71.5728
19500     0.8995      0.6857     0.7964     72.7635
19600     0.9085      0.7089     0.7964     72.7598
19700     0.9340      0.7046     0.7964     74.3807
19800     0.8971      0.7236     0.7964     73.5860
19900     0.8907      0.7300     0.7964     72.6568
20000     0.8769      0.7215     0.7964     74.4840
20100     0.7698      0.7468     0.7964     74.7685
20200     0.8794      0.7131     0.7964     72.3006
20300     0.8423      0.7342     0.7964     74.3749
20400     0.8910      0.6983     0.7964     74.0067
20500     0.8101      0.7447     0.7964     72.4959
20600     0.7908      0.7574     0.7964     72.4286
20700     0.8511      0.7173     0.7964     72.2251
20800     0.8066      0.7405     0.7964     72.5353
20900     0.8958      0.7152     0.7964     71.6304
21000     0.8629      0.6941     0.7964     71.9457
21100     0.9212      0.7110     0.7964     73.0684
21200     0.7960      0.7257     0.7964     73.0320
21300     0.8521      0.7131     0.7964     72.7056
21400     0.9187      0.7068     0.7964     71.8092
21500     0.8232      0.7405     0.7964     72.8694
21600     0.9164      0.7321     0.7964     73.5144
21700     0.7572      0.7426     0.7964     71.9740
21800     0.8677      0.7068     0.8011     72.6431
21900     0.9087      0.7257     0.8011     71.4950
22000     0.8273      0.7300     0.8011     75.0660
22100     0.8519      0.7321     0.8011     72.4575
22200     0.8960      0.7004     0.8011     73.1087
22300     0.9080      0.7194     0.8011     70.9940
22400     0.8116      0.7300     0.8011     72.5423
22500     0.8947      0.7004     0.8011     72.8697
22600     0.8862      0.7152     0.8011     74.2758
22700     0.9277      0.7131     0.8011     76.2971
22800     0.8001      0.7447     0.8011     72.1376
22900     0.8771      0.7257     0.8011     73.0363
23000     0.8090      0.7215     0.8011     72.7419
23100     0.8935      0.7173     0.8011     71.9401
23200     0.8243      0.7363     0.8011     73.0559
23300     0.7628      0.7616     0.8011     71.1829
23400     0.8579      0.7321     0.8011     72.4743
23500     0.9099      0.7025     0.8011     73.7911
23600     0.9114      0.7131     0.8011     73.4456
23700     1.0117      0.6983     0.8011     72.7239
23800     0.8200      0.7447     0.8011     72.4806
23900     0.8351      0.7173     0.8011     70.8807
24000     0.9032      0.7278     0.8011     70.3119
24100     0.8628      0.7321     0.8011     73.8675
24200     0.8023      0.7405     0.8011     71.0944
24300     0.8290      0.7532     0.8011     74.6826
24400     0.8398      0.7384     0.8011     72.2812
24500     0.8106      0.7257     0.8011     75.0914
24600     0.9238      0.6941     0.8011     71.4366
24700     0.7908      0.7553     0.8011     70.8844
24800     0.8569      0.7046     0.8011     70.8251
24900     0.7846      0.7426     0.8011     70.7396
25000     0.7778      0.7806     0.8011     72.3484
25100     0.7810      0.7468     0.8011     73.3669
25200     0.9208      0.7278     0.8011     71.5327
25300     0.8056      0.7489     0.8011     70.3470
25400     0.8063      0.7257     0.8011     71.9683
25500     0.8124      0.7384     0.8011     70.8942
25600     0.7866      0.7342     0.8011     71.0935
25700     0.9223      0.7004     0.8011     72.7582
25800     0.8603      0.7110     0.8011     70.7463
25900     0.8127      0.7489     0.8011     75.1452
26000     0.8065      0.7363     0.8011     71.3608
26100     0.8344      0.7405     0.8011     70.7795
26200     0.8673      0.7215     0.8011     72.0855
26300     0.8253      0.7468     0.8011     71.6746
26400     0.8206      0.7489     0.8011     73.2577
26500     0.7596      0.7468     0.8011     73.1873
26600     0.8325      0.7236     0.8011     71.3453
26700     0.8962      0.7025     0.8011     72.3593
26800     0.6951      0.7975     0.8011     72.0538
26900     0.7989      0.7658     0.8011     70.9331
27000     0.8942      0.7004     0.8011     71.4919
27100     0.7413      0.7511     0.8011     70.8235
27200     1.0278      0.6772     0.8011     71.0096
27300     0.9557      0.7046     0.8011     72.3065
27400     0.8574      0.7342     0.8011     71.7788
27500     0.7855      0.7553     0.8011     71.7481
27600     0.7891      0.7553     0.8011     71.9748
27700     0.8333      0.7131     0.8011     71.2385
27800     0.8636      0.7152     0.8011     71.7799
27900     0.9066      0.7025     0.8011     70.6568
28000     0.9365      0.6920     0.8011     71.4221
28100     0.9204      0.7046     0.8011     73.1541
28200     0.8999      0.7025     0.8011     71.0639
28300     0.9243      0.7194     0.8011     71.8605
28400     0.8701      0.7278     0.8011     72.0948
28500     0.8330      0.7089     0.8011     70.9290
28600     0.8103      0.7363     0.8011     72.0968
28700     0.8384      0.7426     0.8011     70.9721
28800     0.9383      0.6962     0.8011     72.0569
28900     0.8319      0.7658     0.8011     73.4570
29000     0.7877      0.7426     0.8011     70.8044
29100     0.6971      0.8186     0.8011     73.2202
29200     0.8636      0.7300     0.8011     70.5250
29300     0.8520      0.7152     0.8011     71.1319
29400     0.8616      0.7194     0.8011     71.1268
29500     0.8472      0.7236     0.8011     71.9205
29600     0.8576      0.7215     0.8011     70.5871
29700     0.9775      0.6983     0.8011     71.6947
29800     0.8940      0.7194     0.8011     73.7638
29900     0.7412      0.7532     0.8011     71.9329
29999     0.8068      0.7152     0.8011     69.3321
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Start testing:
Test Accuracy: 0.7598
