Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 279, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.0, finetuning_epochs=0, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.0005,0.0001,0.00002', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,10000', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
c0d5124f-3d09-43dd-a679-664377081a74
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.5257      0.1160     0.1004     13.0928
00100     2.5256      0.1013     0.1004     55.7876
00200     2.5256      0.1076     0.1004     55.6553
00300     2.5256      0.0949     0.1004     55.8564
00400     2.5256      0.1055     0.1006     55.7120
00500     2.5256      0.1055     0.1006     57.3621
00600     2.5255      0.1160     0.1006     56.2258
00700     2.5255      0.1266     0.1008     55.1037
00800     2.5255      0.1139     0.1008     55.4607
00900     2.5255      0.0865     0.1008     55.6509
01000     2.5254      0.0886     0.1008     55.6851
01100     2.5254      0.0823     0.1008     57.0650
01200     2.5254      0.0865     0.1008     55.6323
01300     2.5254      0.1160     0.1008     55.7222
01400     2.5254      0.0992     0.1008     56.9438
01500     2.5253      0.0992     0.1008     57.0544
01600     2.5253      0.1034     0.1008     56.6112
01700     2.5253      0.0886     0.1008     56.5386
01800     2.5253      0.1160     0.1008     56.0754
01900     2.5252      0.0633     0.1008     56.6786
02000     2.5252      0.0970     0.1008     54.9511
02100     2.5252      0.1013     0.1008     56.1004
02200     2.5252      0.0949     0.1008     56.8290
02300     2.5252      0.0949     0.1008     55.2525
02400     2.5251      0.1266     0.1008     56.1113
02500     2.5251      0.0675     0.1008     57.2839
02600     2.5251      0.1076     0.1008     56.8147
02700     2.5251      0.0949     0.1008     56.7275
02800     2.5251      0.1013     0.1008     56.2500
02900     2.5250      0.0949     0.1008     55.1117
03000     2.5250      0.1118     0.1008     55.4052
03100     2.5250      0.0949     0.1008     56.4304
03200     2.5250      0.0970     0.1008     56.0592
03300     2.5249      0.0886     0.1008     56.4281
03400     2.5249      0.0928     0.1008     57.4571
03500     2.5249      0.0928     0.1008     55.7235
03600     2.5249      0.1350     0.1008     55.0963
03700     2.5249      0.1034     0.1008     57.4150
03800     2.5248      0.1287     0.1008     57.2442
03900     2.5248      0.0844     0.1008     55.4845
04000     2.5248      0.0781     0.1008     56.7372
04100     2.5248      0.1034     0.1008     56.4522
04200     2.5248      0.1118     0.1008     56.6019
04300     2.5247      0.0823     0.1008     56.6443
04400     2.5247      0.0886     0.1008     55.2039
04500     2.5247      0.1076     0.1008     55.2181
04600     2.5247      0.1076     0.1008     58.2155
04700     2.5247      0.0992     0.1008     56.4428
04800     2.5246      0.1160     0.1008     57.1253
04900     2.5246      0.0970     0.1008     56.1733
05000     2.5246      0.0907     0.1008     56.6461
05100     2.5246      0.0928     0.1008     58.1993
05200     2.5246      0.0886     0.1008     55.3691
05300     2.5245      0.0949     0.1008     57.8006
05400     2.5245      0.0738     0.1008     55.7878
05500     2.5245      0.0844     0.1008     56.5191
05600     2.5245      0.0907     0.1008     55.5566
05700     2.5245      0.1181     0.1008     56.2535
05800     2.5244      0.0865     0.1008     55.3083
05900     2.5244      0.1055     0.1008     55.2321
06000     2.5244      0.0992     0.1008     55.9369
06100     2.5244      0.1287     0.1008     55.8581
06200     2.5244      0.0970     0.1008     56.3962
06300     2.5243      0.0865     0.1008     54.8923
06400     2.5243      0.0907     0.1008     57.3188
06500     2.5243      0.1245     0.1008     57.1374
06600     2.5243      0.0844     0.1008     55.3502
06700     2.5243      0.0992     0.1008     55.9938
06800     2.5242      0.1118     0.1008     55.2483
06900     2.5242      0.0907     0.1008     55.7716
07000     2.5242      0.0823     0.1008     56.5901
07100     2.5242      0.0970     0.1008     56.2709
07200     2.5242      0.0949     0.1008     58.5066
07300     2.5242      0.0928     0.1008     55.7325
07400     2.5241      0.0949     0.1008     55.4610
07500     2.5241      0.1097     0.1008     55.6496
07600     2.5241      0.0759     0.1008     56.7467
07700     2.5241      0.0802     0.1008     55.4908
07800     2.5241      0.1076     0.1008     56.2737
07900     2.5240      0.0844     0.1008     55.8349
08000     2.5240      0.1034     0.1008     57.4741
08100     2.5240      0.0992     0.1008     55.7020
08200     2.5240      0.1055     0.1008     55.6295
08300     2.5240      0.1203     0.1008     56.9052
08400     2.5239      0.0992     0.1008     55.7853
08500     2.5239      0.0928     0.1008     55.5951
08600     2.5239      0.0907     0.1008     56.6821
08700     2.5239      0.1055     0.1008     56.6623
08800     2.5239      0.1034     0.1008     57.2137
08900     2.5239      0.0970     0.1008     56.6498
09000     2.5238      0.1076     0.1008     54.9928
09100     2.5238      0.1076     0.1008     56.8132
09200     2.5238      0.1118     0.1008     56.9255
09300     2.5238      0.0992     0.1008     56.1900
09400     2.5238      0.1055     0.1008     56.5893
09500     2.5237      0.1055     0.1008     56.0041
09600     2.5237      0.1076     0.1008     57.1373
09700     2.5237      0.1181     0.1008     55.2642
09800     2.5237      0.1203     0.1008     55.7304
09900     2.5237      0.0802     0.1008     56.0025
10000     2.5236      0.0992     0.1008     55.7313
10100     2.5236      0.0949     0.1008     56.9067
10200     2.5236      0.1034     0.1008     58.0031
10300     2.5236      0.0992     0.1008     55.8043
10400     2.5236      0.0992     0.1008     56.4994
10500     2.5236      0.1329     0.1008     55.7018
10600     2.5236      0.1266     0.1008     56.7650
10700     2.5236      0.0886     0.1008     56.6293
10800     2.5236      0.1245     0.1008     56.0150
10900     2.5236      0.1160     0.1008     54.8049
11000     2.5236      0.1013     0.1008     56.1126
11100     2.5236      0.1118     0.1008     56.2505
11200     2.5236      0.1013     0.1008     57.3548
11300     2.5236      0.1013     0.1008     55.7146
11400     2.5236      0.1013     0.1008     55.4670
11500     2.5236      0.0865     0.1008     57.8743
11600     2.5236      0.0654     0.1008     55.8597
11700     2.5236      0.1097     0.1008     55.3284
11800     2.5236      0.1160     0.1008     56.7377
11900     2.5236      0.0865     0.1008     55.4467
12000     2.5236      0.1076     0.1008     57.8665
12100     2.5236      0.0992     0.1008     57.9081
12200     2.5236      0.0907     0.1008     56.2188
12300     2.5236      0.0949     0.1008     56.7376
12400     2.5236      0.0886     0.1008     56.5785
12500     2.5236      0.1160     0.1008     57.7526
12600     2.5236      0.0928     0.1008     55.9811
12700     2.5235      0.1055     0.1008     55.5980
12800     2.5235      0.0928     0.1008     57.6913
12900     2.5235      0.0970     0.1008     56.2825
13000     2.5235      0.1055     0.1008     56.8024
13100     2.5235      0.1181     0.1008     58.9051
13200     2.5235      0.0675     0.1008     56.1711
13300     2.5235      0.1139     0.1008     56.7749
13400     2.5235      0.1076     0.1008     57.8228
13500     2.5235      0.0928     0.1008     56.5017
13600     2.5235      0.0907     0.1008     58.2033
13700     2.5235      0.1055     0.1008     56.9580
13800     2.5235      0.0865     0.1008     57.4913
13900     2.5235      0.0717     0.1008     56.4034
14000     2.5235      0.0907     0.1008     56.8289
14100     2.5235      0.1076     0.1008     56.6004
14200     2.5235      0.1055     0.1008     57.6964
14300     2.5235      0.1287     0.1008     56.8025
14400     2.5235      0.1097     0.1008     56.0230
14500     2.5235      0.1224     0.1008     55.7502
14600     2.5235      0.0759     0.1008     56.2993
14700     2.5235      0.1139     0.1008     58.4933
14800     2.5235      0.1055     0.1008     56.4635
14900     2.5235      0.1139     0.1008     55.7904
15000     2.5235      0.0992     0.1008     55.8750
15100     2.5235      0.1266     0.1008     55.9091
15200     2.5235      0.0886     0.1008     57.6724
15300     2.5235      0.1034     0.1008     56.0589
15400     2.5235      0.0823     0.1008     57.0536
15500     2.5234      0.1181     0.1008     56.4177
15600     2.5234      0.1013     0.1008     56.2141
15700     2.5234      0.0928     0.1008     56.4968
15800     2.5234      0.0886     0.1008     56.7720
15900     2.5234      0.1055     0.1008     57.1848
16000     2.5234      0.0949     0.1008     56.8027
16100     2.5234      0.0844     0.1008     56.7504
16200     2.5234      0.1203     0.1008     55.6483
16300     2.5234      0.1055     0.1008     57.3516
16400     2.5234      0.1160     0.1008     56.6220
16500     2.5234      0.1097     0.1008     56.9897
16600     2.5234      0.0992     0.1008     56.6679
16700     2.5234      0.1034     0.1008     56.7889
16800     2.5234      0.0907     0.1008     57.3293
16900     2.5234      0.0970     0.1008     57.7963
17000     2.5234      0.0907     0.1008     55.5741
17100     2.5234      0.1034     0.1008     55.7637
17200     2.5234      0.1055     0.1008     57.0783
17300     2.5234      0.1055     0.1008     55.8553
17400     2.5234      0.1160     0.1008     56.5458
17500     2.5234      0.1203     0.1008     56.0305
17600     2.5234      0.1097     0.1008     55.9571
17700     2.5234      0.1055     0.1008     55.1331
17800     2.5234      0.0970     0.1008     56.6616
17900     2.5234      0.0992     0.1008     56.6270
18000     2.5234      0.1034     0.1008     56.1878
18100     2.5234      0.1013     0.1008     55.4084
18200     2.5234      0.0949     0.1008     55.7537
18300     2.5233      0.0865     0.1008     55.8307
18400     2.5233      0.1097     0.1009     56.5649
18500     2.5233      0.1266     0.1009     57.4350
18600     2.5233      0.1203     0.1009     56.4776
18700     2.5233      0.0949     0.1009     56.2354
18800     2.5233      0.0928     0.1009     55.7368
18900     2.5233      0.0928     0.1009     58.4812
19000     2.5233      0.1013     0.1009     56.8689
19100     2.5233      0.1392     0.1009     56.6359
19200     2.5233      0.0928     0.1009     56.2714
19300     2.5233      0.1034     0.1009     55.7031
19400     2.5233      0.0970     0.1009     57.4195
19500     2.5233      0.0865     0.1009     57.6582
19600     2.5233      0.0907     0.1009     55.9875
19700     2.5233      0.0949     0.1009     56.2376
19800     2.5233      0.0886     0.1009     56.4257
19900     2.5233      0.0844     0.1009     55.4515
20000     2.5233      0.1097     0.1009     57.0683
20100     2.5233      0.0970     0.1009     58.2975
20200     2.5233      0.1118     0.1009     56.4008
20300     2.5233      0.0781     0.1009     56.0019
20400     2.5233      0.0992     0.1009     56.7888
20500     2.5233      0.1097     0.1009     58.6980
20600     2.5233      0.1118     0.1009     56.4947
20700     2.5233      0.1055     0.1009     57.1351
20800     2.5233      0.1245     0.1009     58.0055
20900     2.5233      0.1013     0.1009     56.3852
21000     2.5233      0.0992     0.1009     57.1091
21100     2.5233      0.1139     0.1009     56.1884
21200     2.5233      0.1013     0.1009     57.2088
21300     2.5233      0.0949     0.1009     56.2207
21400     2.5233      0.0907     0.1009     56.1871
21500     2.5233      0.0949     0.1009     56.9315
21600     2.5233      0.1013     0.1009     57.6309
21700     2.5233      0.1139     0.1009     55.6115
21800     2.5233      0.1181     0.1009     58.7587
21900     2.5233      0.1076     0.1009     58.4354
22000     2.5233      0.0949     0.1009     55.8974
22100     2.5233      0.1139     0.1009     55.4552
22200     2.5233      0.1034     0.1009     56.7665
22300     2.5233      0.0865     0.1009     57.7093
22400     2.5233      0.0949     0.1009     57.8233
22500     2.5233      0.1034     0.1009     57.0787
22600     2.5233      0.1097     0.1011     56.0909
22700     2.5233      0.0907     0.1011     57.9700
22800     2.5233      0.1097     0.1011     55.7244
22900     2.5233      0.1139     0.1011     58.0033
23000     2.5233      0.1160     0.1011     58.6875
23100     2.5233      0.0886     0.1011     57.2072
23200     2.5233      0.0992     0.1011     57.3197
23300     2.5233      0.1076     0.1011     56.7897
23400     2.5233      0.1097     0.1011     56.0973
23500     2.5233      0.1013     0.1011     58.0876
23600     2.5233      0.0992     0.1011     56.7333
23700     2.5233      0.1055     0.1011     57.1383
23800     2.5233      0.0886     0.1011     56.3199
23900     2.5233      0.1266     0.1011     56.3570
24000     2.5233      0.0717     0.1011     57.8090
24100     2.5233      0.0992     0.1011     57.0225
24200     2.5233      0.0928     0.1011     57.6406
24300     2.5233      0.0907     0.1011     57.3757
24400     2.5233      0.0781     0.1011     57.5418
24500     2.5232      0.0928     0.1011     56.0396
24600     2.5232      0.0802     0.1011     57.8792
24700     2.5232      0.1118     0.1011     57.4861
24800     2.5232      0.1329     0.1011     56.7164
24900     2.5232      0.0992     0.1011     56.5670
25000     2.5232      0.0949     0.1011     55.8998
25100     2.5232      0.1245     0.1011     57.3969
25200     2.5232      0.0928     0.1011     57.5745
25300     2.5232      0.1139     0.1011     57.3964
25400     2.5232      0.0970     0.1011     57.9770
25500     2.5232      0.1034     0.1011     56.1904
25600     2.5232      0.0992     0.1011     56.5733
25700     2.5232      0.0907     0.1011     57.8780
25800     2.5232      0.0907     0.1011     58.5573
25900     2.5232      0.1160     0.1011     57.4059
26000     2.5232      0.1034     0.1011     57.7538
26100     2.5232      0.1034     0.1011     57.7134
26200     2.5232      0.1055     0.1011     57.8940
26300     2.5232      0.0970     0.1011     57.1964
26400     2.5232      0.1097     0.1011     58.0146
26500     2.5232      0.0844     0.1011     57.2749
26600     2.5232      0.0759     0.1011     58.3117
26700     2.5232      0.1055     0.1011     58.9495
26800     2.5232      0.1224     0.1011     57.4433
26900     2.5232      0.0928     0.1011     57.7564
27000     2.5232      0.1266     0.1011     59.0207
27100     2.5232      0.1097     0.1011     57.8768
27200     2.5232      0.1287     0.1011     57.3388
27300     2.5232      0.1245     0.1011     56.8906
27400     2.5232      0.1013     0.1011     55.9329
27500     2.5232      0.0717     0.1011     56.4764
27600     2.5232      0.0992     0.1011     56.9579
27700     2.5232      0.1055     0.1011     57.9874
27800     2.5232      0.1181     0.1011     58.2825
27900     2.5232      0.1435     0.1011     56.7860
28000     2.5232      0.0738     0.1011     58.6030
28100     2.5232      0.0970     0.1011     59.4647
28200     2.5232      0.1055     0.1011     57.7036
28300     2.5232      0.0844     0.1011     58.7217
28400     2.5232      0.0928     0.1011     57.1758
28500     2.5232      0.1034     0.1011     56.8306
28600     2.5232      0.1076     0.1011     56.5324
28700     2.5232      0.0781     0.1011     56.0509
28800     2.5232      0.1097     0.1011     57.8766
28900     2.5232      0.1055     0.1011     59.6982
29000     2.5232      0.0907     0.1011     56.9500
29100     2.5232      0.1139     0.1011     58.6813
29200     2.5232      0.1160     0.1011     57.2449
29300     2.5232      0.1055     0.1011     54.2767
29400     2.5232      0.0886     0.1011     54.9371
29500     2.5232      0.1224     0.1011     56.0320
29600     2.5232      0.0949     0.1011     57.5344
29700     2.5232      0.0992     0.1011     55.8526
29800     2.5232      0.1034     0.1011     56.7618
29900     2.5232      0.1118     0.1011     58.3413
29999     2.5232      0.1139     0.1011     55.7004
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Start testing:
Test Accuracy: 0.1002
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
d9536006-6568-4bcb-ae07-732b8341b004
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.5257      0.1055     0.1005     10.8837
00100     2.5256      0.0907     0.1005     67.8535
00200     2.5255      0.1076     0.1005     65.9886
00300     2.5254      0.0970     0.1005     67.7944
00400     2.5253      0.0886     0.1005     69.4804
00500     2.5252      0.0949     0.1005     66.8898
00600     2.5251      0.0844     0.1005     67.6896
00700     2.5251      0.1139     0.1005     68.1325
00800     2.5250      0.1097     0.1005     68.5002
00900     2.5249      0.1160     0.1011     68.1465
01000     2.5248      0.0949     0.1011     66.7121
01100     2.5247      0.0886     0.1011     69.8713
01200     2.5247      0.0992     0.1011     68.2731
01300     2.5246      0.1097     0.1011     68.0880
01400     2.5245      0.1371     0.1011     67.1461
01500     2.5244      0.0844     0.1011     67.5185
01600     2.5243      0.0907     0.1011     67.3331
01700     2.5243      0.0992     0.1011     68.6487
01800     2.5242      0.0907     0.1011     67.2082
01900     2.5241      0.0907     0.1011     67.2769
02000     2.5240      0.0886     0.1011     68.0450
02100     2.5240      0.0992     0.1011     68.9036
02200     2.5239      0.0992     0.1011     68.9989
02300     2.5238      0.0928     0.1011     71.3899
02400     2.5237      0.0865     0.1011     68.5406
02500     2.5237      0.0992     0.1011     68.3588
02600     2.5236      0.1097     0.1011     67.8238
02700     2.5235      0.0992     0.1011     67.4392
02800     2.5234      0.1013     0.1011     68.2842
02900     2.5234      0.0886     0.1011     68.4257
03000     2.5233      0.0949     0.1011     68.5993
03100     2.5232      0.1013     0.1011     68.6023
03200     2.5232      0.1097     0.1011     68.9954
03300     2.5231      0.1097     0.1011     68.9828
03400     2.5230      0.1013     0.1011     66.0316
03500     2.5229      0.0738     0.1011     68.9069
03600     2.5229      0.0907     0.1011     68.1734
03700     2.5228      0.0886     0.1011     68.0404
03800     2.5227      0.0886     0.1011     67.2589
03900     2.5227      0.1245     0.1011     69.7038
04000     2.5226      0.0970     0.1011     69.7164
04100     2.5225      0.1118     0.1011     68.5124
04200     2.5225      0.1076     0.1011     68.0709
04300     2.5224      0.0949     0.1011     67.4066
04400     2.5223      0.0907     0.1011     68.2209
04500     2.5222      0.1034     0.1011     70.8397
04600     2.5222      0.0844     0.1011     67.7084
04700     2.5221      0.0949     0.1011     69.8218
04800     2.5220      0.0886     0.1011     68.8002
04900     2.5220      0.1329     0.1011     67.9923
05000     2.5219      0.0633     0.1011     70.3556
05100     2.5218      0.0865     0.1011     67.9478
05200     2.5218      0.0949     0.1011     68.9522
05300     2.5217      0.0970     0.1011     68.3784
05400     2.5216      0.0886     0.1011     68.9646
05500     2.5216      0.0907     0.1011     66.6734
05600     2.5215      0.1097     0.1011     67.6151
05700     2.5214      0.0886     0.1011     67.5491
05800     2.5214      0.0823     0.1011     67.5785
05900     2.5213      0.0949     0.1011     68.2771
06000     2.5212      0.0949     0.1011     67.0423
06100     2.5212      0.0844     0.1011     68.1613
06200     2.5211      0.1034     0.1011     67.4029
06300     2.5210      0.0970     0.1011     67.1591
06400     2.5210      0.1160     0.1011     67.9001
06500     2.5209      0.0696     0.1011     67.9348
06600     2.5208      0.1013     0.1011     69.5548
06700     2.5208      0.1181     0.1011     67.4953
06800     2.5207      0.1034     0.1011     66.7285
06900     2.5206      0.0970     0.1011     66.5905
07000     2.5206      0.1034     0.1011     67.7530
07100     2.5205      0.1055     0.1011     68.8636
07200     2.5204      0.1055     0.1011     68.1103
07300     2.5204      0.0992     0.1011     67.3435
07400     2.5203      0.1245     0.1011     69.5766
07500     2.5202      0.0992     0.1011     68.0903
07600     2.5202      0.1224     0.1011     68.6545
07700     2.5201      0.0992     0.1011     67.4606
07800     2.5200      0.0949     0.1011     68.3638
07900     2.5200      0.0992     0.1011     68.6680
08000     2.5199      0.0759     0.1011     67.9128
08100     2.5199      0.1118     0.1011     68.6415
08200     2.5198      0.0949     0.1011     66.7555
08300     2.5197      0.0865     0.1011     69.1263
08400     2.5197      0.1097     0.1011     67.0914
08500     2.5196      0.0992     0.1011     67.5398
08600     2.5195      0.1076     0.1011     67.0220
08700     2.5195      0.1055     0.1011     67.0264
08800     2.5194      0.0949     0.1011     68.3710
08900     2.5193      0.0949     0.1011     66.7844
09000     2.5193      0.1203     0.1011     68.2595
09100     2.5192      0.1097     0.1011     67.9018
09200     2.5192      0.0970     0.1011     68.0303
09300     2.5191      0.0844     0.1011     69.2273
09400     2.5190      0.0907     0.1011     67.3501
09500     2.5190      0.0865     0.1011     67.5240
09600     2.5189      0.1160     0.1011     67.9016
09700     2.5188      0.0907     0.1011     66.7143
09800     2.5188      0.0970     0.1011     67.3620
09900     2.5187      0.1034     0.1011     68.2439
10000     2.5186      0.0970     0.1011     67.6109
10100     2.5186      0.0907     0.1011     67.7008
10200     2.5186      0.0949     0.1011     67.5426
10300     2.5186      0.1118     0.1011     67.4485
10400     2.5186      0.0717     0.1011     67.0307
10500     2.5186      0.0823     0.1011     67.2219
10600     2.5186      0.0886     0.1011     66.8214
10700     2.5185      0.1097     0.1011     67.5111
10800     2.5185      0.1139     0.1011     68.1456
10900     2.5185      0.1055     0.1011     68.2917
11000     2.5185      0.0802     0.1011     67.6075
11100     2.5185      0.0886     0.1011     68.2231
11200     2.5185      0.0970     0.1011     66.8438
11300     2.5184      0.0844     0.1011     68.9904
11400     2.5184      0.0992     0.1011     68.6455
11500     2.5184      0.0949     0.1011     67.7536
11600     2.5184      0.0865     0.1011     70.2946
11700     2.5184      0.1055     0.1011     67.0464
11800     2.5184      0.0907     0.1011     67.4165
11900     2.5183      0.0970     0.1011     67.3858
12000     2.5183      0.1139     0.1011     67.3600
12100     2.5183      0.1477     0.1011     67.1066
12200     2.5183      0.0928     0.1011     66.4102
12300     2.5183      0.1308     0.1011     67.6176
12400     2.5183      0.0907     0.1011     69.2132
12500     2.5183      0.0886     0.1011     68.1694
12600     2.5182      0.0844     0.1011     68.5452
12700     2.5182      0.0886     0.1011     68.3540
12800     2.5182      0.1076     0.1011     67.7086
12900     2.5182      0.1181     0.1011     68.7209
13000     2.5182      0.0928     0.1011     67.2844
13100     2.5182      0.1076     0.1011     68.8066
13200     2.5181      0.0970     0.1011     67.4398
13300     2.5181      0.0970     0.1011     67.0278
13400     2.5181      0.0928     0.1011     68.1891
13500     2.5181      0.1097     0.1011     66.9500
13600     2.5181      0.0970     0.1011     67.5949
13700     2.5181      0.1034     0.1011     67.2505
13800     2.5180      0.1034     0.1011     69.3118
13900     2.5180      0.0992     0.1011     66.5835
14000     2.5180      0.0886     0.1011     69.1895
14100     2.5180      0.0949     0.1011     67.8713
14200     2.5180      0.1034     0.1011     68.3177
14300     2.5180      0.0865     0.1011     67.5814
14400     2.5180      0.1076     0.1011     68.4484
14500     2.5179      0.1076     0.1011     67.5978
14600     2.5179      0.1013     0.1011     69.2865
14700     2.5179      0.1118     0.1011     68.3677
14800     2.5179      0.0865     0.1011     67.2754
14900     2.5179      0.1371     0.1011     68.1539
15000     2.5179      0.1034     0.1011     68.4842
15100     2.5178      0.1055     0.1011     68.0806
15200     2.5178      0.1308     0.1011     68.7311
15300     2.5178      0.0992     0.1011     68.3336
15400     2.5178      0.1076     0.1011     71.1590
15500     2.5178      0.1055     0.1011     68.3120
15600     2.5178      0.1013     0.1011     66.6944
15700     2.5178      0.1118     0.1011     68.2731
15800     2.5177      0.1203     0.1011     68.6344
15900     2.5177      0.0844     0.1011     69.1604
16000     2.5177      0.1055     0.1011     67.2211
16100     2.5177      0.0738     0.1011     67.4372
16200     2.5177      0.1203     0.1011     67.7310
16300     2.5177      0.1266     0.1011     67.7356
16400     2.5176      0.0970     0.1011     69.2836
16500     2.5176      0.0844     0.1011     66.9541
16600     2.5176      0.1203     0.1011     67.7281
16700     2.5176      0.0928     0.1011     67.0322
16800     2.5176      0.0781     0.1011     67.7988
16900     2.5176      0.0907     0.1011     66.9069
17000     2.5175      0.0992     0.1011     67.5402
17100     2.5175      0.1076     0.1011     68.1404
17200     2.5175      0.1181     0.1011     67.3602
17300     2.5175      0.1118     0.1011     69.4984
17400     2.5175      0.0970     0.1011     68.8856
17500     2.5175      0.1498     0.1011     67.6805
17600     2.5175      0.0844     0.1011     68.9274
17700     2.5174      0.1160     0.1011     66.7339
17800     2.5174      0.0865     0.1011     67.0582
17900     2.5174      0.1118     0.1011     68.4101
18000     2.5174      0.1076     0.1011     67.5522
18100     2.5174      0.0802     0.1011     67.9048
18200     2.5174      0.0823     0.1011     67.9150
18300     2.5173      0.1013     0.1011     68.7292
18400     2.5173      0.1076     0.1011     68.6390
18500     2.5173      0.1013     0.1011     69.4360
18600     2.5173      0.1181     0.1011     69.3006
18700     2.5173      0.1266     0.1011     67.1357
18800     2.5173      0.0886     0.1011     68.1042
18900     2.5172      0.1034     0.1011     67.5262
19000     2.5172      0.1139     0.1011     67.3345
19100     2.5172      0.0907     0.1011     67.7894
19200     2.5172      0.0970     0.1011     67.8831
19300     2.5172      0.1224     0.1011     66.6640
19400     2.5172      0.0970     0.1011     67.2588
19500     2.5172      0.0823     0.1011     68.0975
19600     2.5171      0.1034     0.1011     67.5684
19700     2.5171      0.1034     0.1011     67.7444
19800     2.5171      0.1329     0.1011     68.6281
19900     2.5171      0.0949     0.1011     67.2254
20000     2.5171      0.1118     0.1011     66.9966
20100     2.5171      0.1118     0.1011     67.2784
20199     2.5171      0.0717     0.1011     66.9651
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.5171      0.0949     0.0997     9.6492
00100     2.5171      0.0949     0.0997     67.2641
00200     2.5170      0.0759     0.0997     67.2062
00300     2.5173      0.0717     0.0997     67.9372
00400     2.5172      0.0844     0.0997     68.8836
00500     2.5171      0.0802     0.0997     66.9093
00600     2.5171      0.0591     0.0997     66.9151
00700     2.5170      0.1118     0.0997     68.4894
00800     2.5171      0.1118     0.0997     67.4966
00900     2.5171      0.0759     0.0997     68.0505
01000     2.5171      0.0759     0.0997     67.2094
01100     2.5169      0.0844     0.0997     67.2713
01200     2.5170      0.0865     0.0997     67.3099
01300     2.5169      0.1055     0.0997     67.3398
01400     2.5170      0.0823     0.0997     67.5691
01500     2.5170      0.0823     0.0997     69.8375
01600     2.5171      0.0717     0.0997     68.9754
01700     2.5170      0.0865     0.0997     67.5705
01800     2.5170      0.0886     0.0997     68.6486
01900     2.5171      0.0802     0.0997     67.3143
02000     2.5169      0.0802     0.0997     67.6295
02100     2.5171      0.0844     0.0997     67.2343
02200     2.5171      0.0612     0.0997     67.8520
02300     2.5171      0.0781     0.0997     67.5205
02400     2.5170      0.0717     0.0997     67.8690
02500     2.5171      0.1076     0.0997     70.1808
02600     2.5170      0.0823     0.0997     66.8377
02700     2.5169      0.0907     0.0997     67.2796
02800     2.5170      0.0907     0.0997     67.9634
02900     2.5170      0.0970     0.0997     66.3929
03000     2.5170      0.1055     0.0997     67.4441
03100     2.5171      0.0633     0.0997     68.7833
03200     2.5169      0.0844     0.0997     67.6210
03300     2.5168      0.1034     0.0997     68.8076
03400     2.5170      0.0844     0.0997     67.2823
03500     2.5171      0.0654     0.0997     66.8071
03600     2.5169      0.0696     0.0997     66.2612
03700     2.5169      0.0928     0.0997     66.8929
03800     2.5170      0.0781     0.0997     67.1743
03900     2.5169      0.0696     0.0997     66.7398
04000     2.5170      0.0738     0.0997     65.8212
04100     2.5168      0.0865     0.0997     67.0642
04200     2.5169      0.0717     0.0997     65.1637
04300     2.5170      0.0949     0.0997     65.9779
04400     2.5170      0.0781     0.0997     66.4505
04500     2.5170      0.1013     0.0997     66.7533
04600     2.5169      0.0907     0.0997     66.7694
04700     2.5170      0.0696     0.0997     67.3803
04800     2.5170      0.0844     0.0997     67.6551
04900     2.5170      0.0802     0.0997     67.0642
05000     2.5169      0.1076     0.1008     67.2211
05100     2.5169      0.1076     0.1008     66.8304
05200     2.5168      0.1139     0.1008     67.1430
05300     2.5168      0.1013     0.1008     66.7146
05400     2.5169      0.0823     0.1008     68.0509
05500     2.5169      0.0907     0.1008     68.3270
05600     2.5169      0.1055     0.1008     67.2377
05700     2.5170      0.0675     0.1008     66.8523
05800     2.5170      0.0970     0.1008     66.5673
05900     2.5169      0.1118     0.1008     67.4904
06000     2.5169      0.0949     0.1008     68.3700
06100     2.5169      0.1097     0.1008     69.1164
06200     2.5170      0.0802     0.1008     68.8371
06300     2.5169      0.0717     0.1008     67.6176
06400     2.5169      0.0675     0.1008     66.8381
06500     2.5169      0.1055     0.1008     69.2066
06600     2.5169      0.0654     0.1008     69.1572
06700     2.5169      0.0928     0.1008     67.3703
06800     2.5169      0.0949     0.1008     69.0799
06900     2.5169      0.0844     0.1008     67.0712
07000     2.5169      0.0823     0.1008     67.1043
07100     2.5168      0.1013     0.1008     66.9088
07200     2.5170      0.0928     0.1008     67.2263
07300     2.5171      0.0717     0.1008     66.1890
07400     2.5169      0.0654     0.1008     66.9266
07500     2.5167      0.0992     0.1008     66.6868
07600     2.5170      0.0844     0.1008     67.2316
07700     2.5168      0.1013     0.1008     68.0343
07800     2.5173      0.0612     0.1008     67.0523
07900     2.5168      0.0949     0.1008     66.0223
08000     2.5170      0.0696     0.1008     66.2982
08100     2.5167      0.1371     0.1008     67.5675
08200     2.5168      0.0949     0.1008     68.2401
08300     2.5169      0.0865     0.1008     67.7966
08400     2.5170      0.0928     0.1008     69.0844
08500     2.5169      0.0844     0.1008     70.8903
08600     2.5168      0.0970     0.1008     67.9946
08700     2.5169      0.0886     0.1008     66.7849
08800     2.5168      0.0717     0.1008     67.6070
08900     2.5168      0.0907     0.1008     67.2704
09000     2.5169      0.0823     0.1008     66.4873
09100     2.5168      0.0970     0.1008     67.0503
09200     2.5168      0.0844     0.1008     65.8750
09300     2.5169      0.0907     0.1008     66.4525
09400     2.5170      0.0907     0.1008     67.1075
09500     2.5169      0.0738     0.1008     66.7661
09600     2.5168      0.0992     0.1008     68.7027
09700     2.5168      0.0844     0.1008     67.6999
09800     2.5169      0.0823     0.1008     65.6160
09900     2.5168      0.0949     0.1008     68.8323
Start testing:
Test Accuracy: 0.0935
