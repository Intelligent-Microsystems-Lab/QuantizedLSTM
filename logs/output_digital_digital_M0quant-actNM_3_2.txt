Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 279, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.0, finetuning_epochs=0, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.0005,0.0001,0.00002', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,10000', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
c0d5124f-3d09-43dd-a679-664377081a74
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.5257      0.1160     0.1004     13.0928
00100     2.5256      0.1013     0.1004     55.7876
00200     2.5256      0.1076     0.1004     55.6553
00300     2.5256      0.0949     0.1004     55.8564
00400     2.5256      0.1055     0.1006     55.7120
00500     2.5256      0.1055     0.1006     57.3621
00600     2.5255      0.1160     0.1006     56.2258
00700     2.5255      0.1266     0.1008     55.1037
00800     2.5255      0.1139     0.1008     55.4607
00900     2.5255      0.0865     0.1008     55.6509
01000     2.5254      0.0886     0.1008     55.6851
01100     2.5254      0.0823     0.1008     57.0650
01200     2.5254      0.0865     0.1008     55.6323
01300     2.5254      0.1160     0.1008     55.7222
01400     2.5254      0.0992     0.1008     56.9438
01500     2.5253      0.0992     0.1008     57.0544
01600     2.5253      0.1034     0.1008     56.6112
01700     2.5253      0.0886     0.1008     56.5386
01800     2.5253      0.1160     0.1008     56.0754
01900     2.5252      0.0633     0.1008     56.6786
02000     2.5252      0.0970     0.1008     54.9511
02100     2.5252      0.1013     0.1008     56.1004
02200     2.5252      0.0949     0.1008     56.8290
02300     2.5252      0.0949     0.1008     55.2525
02400     2.5251      0.1266     0.1008     56.1113
02500     2.5251      0.0675     0.1008     57.2839
02600     2.5251      0.1076     0.1008     56.8147
02700     2.5251      0.0949     0.1008     56.7275
02800     2.5251      0.1013     0.1008     56.2500
02900     2.5250      0.0949     0.1008     55.1117
03000     2.5250      0.1118     0.1008     55.4052
03100     2.5250      0.0949     0.1008     56.4304
03200     2.5250      0.0970     0.1008     56.0592
03300     2.5249      0.0886     0.1008     56.4281
03400     2.5249      0.0928     0.1008     57.4571
03500     2.5249      0.0928     0.1008     55.7235
03600     2.5249      0.1350     0.1008     55.0963
03700     2.5249      0.1034     0.1008     57.4150
03800     2.5248      0.1287     0.1008     57.2442
03900     2.5248      0.0844     0.1008     55.4845
04000     2.5248      0.0781     0.1008     56.7372
04100     2.5248      0.1034     0.1008     56.4522
04200     2.5248      0.1118     0.1008     56.6019
04300     2.5247      0.0823     0.1008     56.6443
04400     2.5247      0.0886     0.1008     55.2039
04500     2.5247      0.1076     0.1008     55.2181
04600     2.5247      0.1076     0.1008     58.2155
04700     2.5247      0.0992     0.1008     56.4428
04800     2.5246      0.1160     0.1008     57.1253
04900     2.5246      0.0970     0.1008     56.1733
05000     2.5246      0.0907     0.1008     56.6461
05100     2.5246      0.0928     0.1008     58.1993
05200     2.5246      0.0886     0.1008     55.3691
05300     2.5245      0.0949     0.1008     57.8006
05400     2.5245      0.0738     0.1008     55.7878
05500     2.5245      0.0844     0.1008     56.5191
05600     2.5245      0.0907     0.1008     55.5566
05700     2.5245      0.1181     0.1008     56.2535
05800     2.5244      0.0865     0.1008     55.3083
05900     2.5244      0.1055     0.1008     55.2321
06000     2.5244      0.0992     0.1008     55.9369
06100     2.5244      0.1287     0.1008     55.8581
06200     2.5244      0.0970     0.1008     56.3962
06300     2.5243      0.0865     0.1008     54.8923
06400     2.5243      0.0907     0.1008     57.3188
06500     2.5243      0.1245     0.1008     57.1374
06600     2.5243      0.0844     0.1008     55.3502
06700     2.5243      0.0992     0.1008     55.9938
06800     2.5242      0.1118     0.1008     55.2483
06900     2.5242      0.0907     0.1008     55.7716
07000     2.5242      0.0823     0.1008     56.5901
07100     2.5242      0.0970     0.1008     56.2709
07200     2.5242      0.0949     0.1008     58.5066
07300     2.5242      0.0928     0.1008     55.7325
07400     2.5241      0.0949     0.1008     55.4610
07500     2.5241      0.1097     0.1008     55.6496
07600     2.5241      0.0759     0.1008     56.7467
07700     2.5241      0.0802     0.1008     55.4908
07800     2.5241      0.1076     0.1008     56.2737
07900     2.5240      0.0844     0.1008     55.8349
08000     2.5240      0.1034     0.1008     57.4741
08100     2.5240      0.0992     0.1008     55.7020
08200     2.5240      0.1055     0.1008     55.6295
08300     2.5240      0.1203     0.1008     56.9052
08400     2.5239      0.0992     0.1008     55.7853
08500     2.5239      0.0928     0.1008     55.5951
08600     2.5239      0.0907     0.1008     56.6821
08700     2.5239      0.1055     0.1008     56.6623
08800     2.5239      0.1034     0.1008     57.2137
08900     2.5239      0.0970     0.1008     56.6498
09000     2.5238      0.1076     0.1008     54.9928
09100     2.5238      0.1076     0.1008     56.8132
09200     2.5238      0.1118     0.1008     56.9255
09300     2.5238      0.0992     0.1008     56.1900
09400     2.5238      0.1055     0.1008     56.5893
09500     2.5237      0.1055     0.1008     56.0041
09600     2.5237      0.1076     0.1008     57.1373
09700     2.5237      0.1181     0.1008     55.2642
09800     2.5237      0.1203     0.1008     55.7304
09900     2.5237      0.0802     0.1008     56.0025
10000     2.5236      0.0992     0.1008     55.7313
10100     2.5236      0.0949     0.1008     56.9067
10200     2.5236      0.1034     0.1008     58.0031
10300     2.5236      0.0992     0.1008     55.8043
10400     2.5236      0.0992     0.1008     56.4994
10500     2.5236      0.1329     0.1008     55.7018
10600     2.5236      0.1266     0.1008     56.7650
10700     2.5236      0.0886     0.1008     56.6293
10800     2.5236      0.1245     0.1008     56.0150
10900     2.5236      0.1160     0.1008     54.8049
11000     2.5236      0.1013     0.1008     56.1126
11100     2.5236      0.1118     0.1008     56.2505
11200     2.5236      0.1013     0.1008     57.3548
11300     2.5236      0.1013     0.1008     55.7146
11400     2.5236      0.1013     0.1008     55.4670
11500     2.5236      0.0865     0.1008     57.8743
11600     2.5236      0.0654     0.1008     55.8597
11700     2.5236      0.1097     0.1008     55.3284
11800     2.5236      0.1160     0.1008     56.7377
11900     2.5236      0.0865     0.1008     55.4467
12000     2.5236      0.1076     0.1008     57.8665
12100     2.5236      0.0992     0.1008     57.9081
12200     2.5236      0.0907     0.1008     56.2188
12300     2.5236      0.0949     0.1008     56.7376
12400     2.5236      0.0886     0.1008     56.5785
12500     2.5236      0.1160     0.1008     57.7526
12600     2.5236      0.0928     0.1008     55.9811
12700     2.5235      0.1055     0.1008     55.5980
12800     2.5235      0.0928     0.1008     57.6913
12900     2.5235      0.0970     0.1008     56.2825
13000     2.5235      0.1055     0.1008     56.8024
13100     2.5235      0.1181     0.1008     58.9051
13200     2.5235      0.0675     0.1008     56.1711
13300     2.5235      0.1139     0.1008     56.7749
13400     2.5235      0.1076     0.1008     57.8228
13500     2.5235      0.0928     0.1008     56.5017
13600     2.5235      0.0907     0.1008     58.2033
13700     2.5235      0.1055     0.1008     56.9580
13800     2.5235      0.0865     0.1008     57.4913
13900     2.5235      0.0717     0.1008     56.4034
14000     2.5235      0.0907     0.1008     56.8289
14100     2.5235      0.1076     0.1008     56.6004
14200     2.5235      0.1055     0.1008     57.6964
14300     2.5235      0.1287     0.1008     56.8025
14400     2.5235      0.1097     0.1008     56.0230
14500     2.5235      0.1224     0.1008     55.7502
14600     2.5235      0.0759     0.1008     56.2993
14700     2.5235      0.1139     0.1008     58.4933
14800     2.5235      0.1055     0.1008     56.4635
14900     2.5235      0.1139     0.1008     55.7904
15000     2.5235      0.0992     0.1008     55.8750
15100     2.5235      0.1266     0.1008     55.9091
15200     2.5235      0.0886     0.1008     57.6724
15300     2.5235      0.1034     0.1008     56.0589
15400     2.5235      0.0823     0.1008     57.0536
15500     2.5234      0.1181     0.1008     56.4177
15600     2.5234      0.1013     0.1008     56.2141
15700     2.5234      0.0928     0.1008     56.4968
15800     2.5234      0.0886     0.1008     56.7720
15900     2.5234      0.1055     0.1008     57.1848
16000     2.5234      0.0949     0.1008     56.8027
16100     2.5234      0.0844     0.1008     56.7504
16200     2.5234      0.1203     0.1008     55.6483
16300     2.5234      0.1055     0.1008     57.3516
16400     2.5234      0.1160     0.1008     56.6220
16500     2.5234      0.1097     0.1008     56.9897
16600     2.5234      0.0992     0.1008     56.6679
16700     2.5234      0.1034     0.1008     56.7889
16800     2.5234      0.0907     0.1008     57.3293
16900     2.5234      0.0970     0.1008     57.7963
17000     2.5234      0.0907     0.1008     55.5741
17100     2.5234      0.1034     0.1008     55.7637
17200     2.5234      0.1055     0.1008     57.0783
17300     2.5234      0.1055     0.1008     55.8553
17400     2.5234      0.1160     0.1008     56.5458
17500     2.5234      0.1203     0.1008     56.0305
17600     2.5234      0.1097     0.1008     55.9571
17700     2.5234      0.1055     0.1008     55.1331
17800     2.5234      0.0970     0.1008     56.6616
17900     2.5234      0.0992     0.1008     56.6270
18000     2.5234      0.1034     0.1008     56.1878
18100     2.5234      0.1013     0.1008     55.4084
18200     2.5234      0.0949     0.1008     55.7537
18300     2.5233      0.0865     0.1008     55.8307
18400     2.5233      0.1097     0.1009     56.5649
18500     2.5233      0.1266     0.1009     57.4350
18600     2.5233      0.1203     0.1009     56.4776
18700     2.5233      0.0949     0.1009     56.2354
18800     2.5233      0.0928     0.1009     55.7368
18900     2.5233      0.0928     0.1009     58.4812
19000     2.5233      0.1013     0.1009     56.8689
19100     2.5233      0.1392     0.1009     56.6359
19200     2.5233      0.0928     0.1009     56.2714
19300     2.5233      0.1034     0.1009     55.7031
19400     2.5233      0.0970     0.1009     57.4195
19500     2.5233      0.0865     0.1009     57.6582
19600     2.5233      0.0907     0.1009     55.9875
19700     2.5233      0.0949     0.1009     56.2376
19800     2.5233      0.0886     0.1009     56.4257
19900     2.5233      0.0844     0.1009     55.4515
20000     2.5233      0.1097     0.1009     57.0683
20100     2.5233      0.0970     0.1009     58.2975
20200     2.5233      0.1118     0.1009     56.4008
20300     2.5233      0.0781     0.1009     56.0019
20400     2.5233      0.0992     0.1009     56.7888
20500     2.5233      0.1097     0.1009     58.6980
20600     2.5233      0.1118     0.1009     56.4947
20700     2.5233      0.1055     0.1009     57.1351
20800     2.5233      0.1245     0.1009     58.0055
20900     2.5233      0.1013     0.1009     56.3852
21000     2.5233      0.0992     0.1009     57.1091
21100     2.5233      0.1139     0.1009     56.1884
21200     2.5233      0.1013     0.1009     57.2088
21300     2.5233      0.0949     0.1009     56.2207
21400     2.5233      0.0907     0.1009     56.1871
21500     2.5233      0.0949     0.1009     56.9315
21600     2.5233      0.1013     0.1009     57.6309
21700     2.5233      0.1139     0.1009     55.6115
21800     2.5233      0.1181     0.1009     58.7587
21900     2.5233      0.1076     0.1009     58.4354
22000     2.5233      0.0949     0.1009     55.8974
22100     2.5233      0.1139     0.1009     55.4552
22200     2.5233      0.1034     0.1009     56.7665
22300     2.5233      0.0865     0.1009     57.7093
22400     2.5233      0.0949     0.1009     57.8233
22500     2.5233      0.1034     0.1009     57.0787
22600     2.5233      0.1097     0.1011     56.0909
22700     2.5233      0.0907     0.1011     57.9700
22800     2.5233      0.1097     0.1011     55.7244
22900     2.5233      0.1139     0.1011     58.0033
23000     2.5233      0.1160     0.1011     58.6875
23100     2.5233      0.0886     0.1011     57.2072
23200     2.5233      0.0992     0.1011     57.3197
23300     2.5233      0.1076     0.1011     56.7897
23400     2.5233      0.1097     0.1011     56.0973
23500     2.5233      0.1013     0.1011     58.0876
23600     2.5233      0.0992     0.1011     56.7333
23700     2.5233      0.1055     0.1011     57.1383
23800     2.5233      0.0886     0.1011     56.3199
23900     2.5233      0.1266     0.1011     56.3570
24000     2.5233      0.0717     0.1011     57.8090
24100     2.5233      0.0992     0.1011     57.0225
24200     2.5233      0.0928     0.1011     57.6406
24300     2.5233      0.0907     0.1011     57.3757
24400     2.5233      0.0781     0.1011     57.5418
24500     2.5232      0.0928     0.1011     56.0396
24600     2.5232      0.0802     0.1011     57.8792
24700     2.5232      0.1118     0.1011     57.4861
24800     2.5232      0.1329     0.1011     56.7164
24900     2.5232      0.0992     0.1011     56.5670
25000     2.5232      0.0949     0.1011     55.8998
25100     2.5232      0.1245     0.1011     57.3969
25200     2.5232      0.0928     0.1011     57.5745
25300     2.5232      0.1139     0.1011     57.3964
25400     2.5232      0.0970     0.1011     57.9770
25500     2.5232      0.1034     0.1011     56.1904
25600     2.5232      0.0992     0.1011     56.5733
25700     2.5232      0.0907     0.1011     57.8780
25800     2.5232      0.0907     0.1011     58.5573
25900     2.5232      0.1160     0.1011     57.4059
26000     2.5232      0.1034     0.1011     57.7538
26100     2.5232      0.1034     0.1011     57.7134
26200     2.5232      0.1055     0.1011     57.8940
26300     2.5232      0.0970     0.1011     57.1964
26400     2.5232      0.1097     0.1011     58.0146
26500     2.5232      0.0844     0.1011     57.2749
26600     2.5232      0.0759     0.1011     58.3117
26700     2.5232      0.1055     0.1011     58.9495
26800     2.5232      0.1224     0.1011     57.4433
26900     2.5232      0.0928     0.1011     57.7564
27000     2.5232      0.1266     0.1011     59.0207
27100     2.5232      0.1097     0.1011     57.8768
27200     2.5232      0.1287     0.1011     57.3388
27300     2.5232      0.1245     0.1011     56.8906
27400     2.5232      0.1013     0.1011     55.9329
27500     2.5232      0.0717     0.1011     56.4764
27600     2.5232      0.0992     0.1011     56.9579
27700     2.5232      0.1055     0.1011     57.9874
27800     2.5232      0.1181     0.1011     58.2825
27900     2.5232      0.1435     0.1011     56.7860
28000     2.5232      0.0738     0.1011     58.6030
28100     2.5232      0.0970     0.1011     59.4647
28200     2.5232      0.1055     0.1011     57.7036
28300     2.5232      0.0844     0.1011     58.7217
28400     2.5232      0.0928     0.1011     57.1758
28500     2.5232      0.1034     0.1011     56.8306
28600     2.5232      0.1076     0.1011     56.5324
28700     2.5232      0.0781     0.1011     56.0509
28800     2.5232      0.1097     0.1011     57.8766
28900     2.5232      0.1055     0.1011     59.6982
29000     2.5232      0.0907     0.1011     56.9500
29100     2.5232      0.1139     0.1011     58.6813
29200     2.5232      0.1160     0.1011     57.2449
29300     2.5232      0.1055     0.1011     54.2767
29400     2.5232      0.0886     0.1011     54.9371
29500     2.5232      0.1224     0.1011     56.0320
29600     2.5232      0.0949     0.1011     57.5344
29700     2.5232      0.0992     0.1011     55.8526
29800     2.5232      0.1034     0.1011     56.7618
29900     2.5232      0.1118     0.1011     58.3413
29999     2.5232      0.1139     0.1011     55.7004
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Start testing:
Test Accuracy: 0.1002
