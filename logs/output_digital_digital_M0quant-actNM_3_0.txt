Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 279, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.0, finetuning_epochs=0, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.0005,0.0001,0.00002', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,10000', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
b148832d-e854-4db5-a0f6-85d33781e687
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.5257      0.0970     0.0998     12.0621
00100     2.5256      0.1076     0.1003     57.0835
00200     2.5256      0.0717     0.1003     58.3946
00300     2.5256      0.1076     0.1003     58.4579
00400     2.5256      0.0886     0.1003     56.8505
00500     2.5256      0.0738     0.1003     57.4368
00600     2.5255      0.0928     0.1003     59.6014
00700     2.5255      0.1097     0.1003     57.7401
00800     2.5255      0.0886     0.1003     57.6449
00900     2.5255      0.1371     0.1003     56.5995
01000     2.5254      0.0992     0.1003     57.0693
01100     2.5254      0.1013     0.1003     57.8128
01200     2.5254      0.0928     0.1003     57.3454
01300     2.5254      0.1160     0.1003     58.0794
01400     2.5254      0.1203     0.1003     57.7012
01500     2.5253      0.0865     0.1008     57.1321
01600     2.5253      0.1118     0.1008     57.7431
01700     2.5253      0.0907     0.1008     57.2389
01800     2.5253      0.1055     0.1008     56.8806
01900     2.5252      0.1160     0.1008     58.2877
02000     2.5252      0.1055     0.1008     58.9038
02100     2.5252      0.1118     0.1008     58.5352
02200     2.5252      0.0949     0.1008     58.1531
02300     2.5252      0.0992     0.1008     58.5430
02400     2.5251      0.0949     0.1008     59.4991
02500     2.5251      0.1160     0.1008     57.7113
02600     2.5251      0.0865     0.1008     57.8908
02700     2.5251      0.1203     0.1008     60.2783
02800     2.5251      0.0970     0.1008     58.9005
02900     2.5250      0.0844     0.1008     58.1915
03000     2.5250      0.0865     0.1008     57.7078
03100     2.5250      0.1034     0.1008     57.5471
03200     2.5250      0.1097     0.1008     58.6722
03300     2.5249      0.0886     0.1008     56.8155
03400     2.5249      0.1118     0.1008     57.7068
03500     2.5249      0.1076     0.1008     58.0831
03600     2.5249      0.1139     0.1008     58.0170
03700     2.5249      0.0759     0.1008     58.0303
03800     2.5248      0.0823     0.1008     59.0229
03900     2.5248      0.0992     0.1008     57.4304
04000     2.5248      0.1013     0.1008     57.5739
04100     2.5248      0.0781     0.1008     57.8342
04200     2.5248      0.0992     0.1008     57.2945
04300     2.5247      0.1160     0.1008     58.1183
04400     2.5247      0.1097     0.1008     58.9020
04500     2.5247      0.0992     0.1008     59.3352
04600     2.5247      0.0886     0.1008     57.6458
04700     2.5247      0.1076     0.1008     59.2300
04800     2.5246      0.0844     0.1008     58.6826
04900     2.5246      0.1076     0.1008     58.3799
05000     2.5246      0.0865     0.1008     57.0122
05100     2.5246      0.0928     0.1008     59.0887
05200     2.5246      0.0865     0.1008     57.7713
05300     2.5245      0.0738     0.1008     57.3888
05400     2.5245      0.0865     0.1008     57.6185
05500     2.5245      0.0970     0.1008     56.9520
05600     2.5245      0.1266     0.1008     60.0604
05700     2.5245      0.1076     0.1008     58.2427
05800     2.5244      0.0928     0.1008     58.8760
05900     2.5244      0.0759     0.1008     58.8859
06000     2.5244      0.1013     0.1008     57.5599
06100     2.5244      0.1076     0.1008     58.1549
06200     2.5244      0.1203     0.1008     58.9735
06300     2.5243      0.1097     0.1008     57.6371
06400     2.5243      0.1055     0.1008     58.6269
06500     2.5243      0.0992     0.1008     57.8834
06600     2.5243      0.1139     0.1008     58.3761
06700     2.5243      0.0612     0.1008     58.0660
06800     2.5242      0.1034     0.1008     58.2084
06900     2.5242      0.0823     0.1008     56.8982
07000     2.5242      0.0970     0.1008     58.3188
07100     2.5242      0.1034     0.1008     58.8069
07200     2.5242      0.0823     0.1008     57.8628
07300     2.5242      0.0970     0.1008     56.9518
07400     2.5241      0.0886     0.1008     57.8361
07500     2.5241      0.0970     0.1008     58.8355
07600     2.5241      0.1139     0.1008     59.0266
07700     2.5241      0.1055     0.1008     57.7256
07800     2.5241      0.1055     0.1008     58.2721
07900     2.5240      0.0992     0.1008     58.5586
08000     2.5240      0.0928     0.1008     58.7988
08100     2.5240      0.1013     0.1008     57.2146
08200     2.5240      0.1266     0.1008     58.5786
08300     2.5240      0.0886     0.1008     57.6613
08400     2.5239      0.1013     0.1008     58.1487
08500     2.5239      0.0865     0.1008     57.3948
08600     2.5239      0.0992     0.1008     58.1514
08700     2.5239      0.0992     0.1008     57.7365
08800     2.5239      0.1245     0.1008     59.1711
08900     2.5239      0.0970     0.1008     58.4861
09000     2.5238      0.0886     0.1008     58.0042
09100     2.5238      0.0907     0.1008     57.7027
09200     2.5238      0.1371     0.1008     58.2530
09300     2.5238      0.0949     0.1008     57.5264
09400     2.5238      0.1076     0.1009     60.2721
09500     2.5237      0.0928     0.1009     57.4432
09600     2.5237      0.0781     0.1009     59.5031
09700     2.5237      0.1181     0.1009     58.1508
09800     2.5237      0.0992     0.1009     57.5997
09900     2.5237      0.1097     0.1009     58.4949
10000     2.5236      0.1076     0.1009     56.6684
10100     2.5236      0.1118     0.1009     57.8440
10200     2.5236      0.0907     0.1009     59.6012
10300     2.5236      0.1266     0.1009     57.2524
10400     2.5236      0.0928     0.1009     57.4962
10500     2.5236      0.0928     0.1009     57.3676
10600     2.5236      0.0865     0.1009     58.6742
10700     2.5236      0.0907     0.1009     58.6307
10800     2.5236      0.0992     0.1009     57.6317
10900     2.5236      0.1224     0.1009     56.5800
11000     2.5236      0.0886     0.1009     59.3960
11100     2.5236      0.1160     0.1009     59.6818
11200     2.5236      0.0802     0.1009     58.5661
11300     2.5236      0.0717     0.1009     57.7226
11400     2.5236      0.0992     0.1009     58.4712
11500     2.5236      0.0717     0.1009     57.8380
11600     2.5236      0.0949     0.1009     57.3366
11700     2.5236      0.0886     0.1009     56.7053
11800     2.5236      0.0865     0.1009     58.0913
11900     2.5236      0.0907     0.1009     58.0079
12000     2.5236      0.0928     0.1009     58.7287
12100     2.5236      0.0949     0.1009     57.7202
12200     2.5236      0.1118     0.1009     57.3864
12300     2.5236      0.1013     0.1009     57.4862
12400     2.5236      0.1160     0.1009     58.1341
12500     2.5236      0.0907     0.1009     57.4291
12600     2.5236      0.0781     0.1009     58.1559
12700     2.5235      0.1076     0.1009     57.3444
12800     2.5235      0.1055     0.1009     57.8119
12900     2.5235      0.0970     0.1009     57.8269
13000     2.5235      0.1203     0.1009     58.4573
13100     2.5235      0.0970     0.1009     57.8022
13200     2.5235      0.0823     0.1009     58.6125
13300     2.5235      0.0886     0.1009     57.9658
13400     2.5235      0.1139     0.1009     57.7160
13500     2.5235      0.0907     0.1009     57.3108
13600     2.5235      0.0949     0.1009     60.0398
13700     2.5235      0.0992     0.1009     57.5819
13800     2.5235      0.1034     0.1009     59.5233
13900     2.5235      0.0865     0.1009     57.4790
14000     2.5235      0.0886     0.1009     57.8321
14100     2.5235      0.1076     0.1009     57.1609
14200     2.5235      0.1055     0.1009     58.4095
14300     2.5235      0.1055     0.1009     57.1613
14400     2.5235      0.0738     0.1009     58.6432
14500     2.5235      0.0844     0.1009     57.4786
14600     2.5235      0.0675     0.1009     57.3103
14700     2.5235      0.1224     0.1009     59.4383
14800     2.5235      0.0823     0.1009     59.2682
14900     2.5235      0.1139     0.1009     58.0280
15000     2.5235      0.1266     0.1009     59.3842
15100     2.5235      0.0992     0.1009     57.1713
15200     2.5235      0.1160     0.1009     58.4913
15300     2.5235      0.1076     0.1009     58.1713
15400     2.5235      0.0928     0.1009     57.0672
15500     2.5234      0.0781     0.1009     58.9832
15600     2.5234      0.0928     0.1009     58.8889
15700     2.5234      0.1034     0.1009     58.3048
15800     2.5234      0.0970     0.1009     58.9688
15900     2.5234      0.1034     0.1009     57.8511
16000     2.5234      0.0949     0.1009     60.5784
16100     2.5234      0.0865     0.1009     58.1297
16200     2.5234      0.0970     0.1009     57.0255
16300     2.5234      0.1076     0.1009     57.3680
16400     2.5234      0.0886     0.1009     57.3684
16500     2.5234      0.0992     0.1009     56.7924
16600     2.5234      0.1160     0.1009     58.0567
16700     2.5234      0.0949     0.1009     58.7356
16800     2.5234      0.1266     0.1009     58.0323
16900     2.5234      0.0907     0.1009     57.9938
17000     2.5234      0.0844     0.1009     57.4216
17100     2.5234      0.1055     0.1009     57.6460
17200     2.5234      0.0970     0.1009     58.5532
17300     2.5234      0.0886     0.1009     58.0246
17400     2.5234      0.0992     0.1009     57.7433
17500     2.5234      0.1055     0.1009     57.3199
17600     2.5234      0.1013     0.1009     60.2514
17700     2.5234      0.1224     0.1009     57.8273
17800     2.5234      0.1034     0.1009     58.8905
17900     2.5234      0.0928     0.1009     58.3531
18000     2.5234      0.1034     0.1009     58.5280
18100     2.5234      0.1034     0.1011     57.6561
18200     2.5234      0.1118     0.1011     57.5337
18300     2.5233      0.0886     0.1011     56.6413
18400     2.5233      0.1013     0.1011     58.8123
18500     2.5233      0.1118     0.1011     57.8156
18600     2.5233      0.0865     0.1011     57.8550
18700     2.5233      0.1181     0.1011     57.7146
18800     2.5233      0.1076     0.1011     58.2555
18900     2.5233      0.0781     0.1011     57.3161
19000     2.5233      0.0886     0.1011     59.0731
19100     2.5233      0.0717     0.1011     57.3064
19200     2.5233      0.0949     0.1011     58.4692
19300     2.5233      0.1308     0.1011     57.1575
19400     2.5233      0.0907     0.1011     56.9514
19500     2.5233      0.0928     0.1011     59.5134
19600     2.5233      0.1266     0.1011     59.3333
19700     2.5233      0.1203     0.1011     58.1404
19800     2.5233      0.0928     0.1011     58.6472
19900     2.5233      0.1097     0.1011     58.1330
20000     2.5233      0.0865     0.1011     58.3135
20100     2.5233      0.0992     0.1011     58.8361
20200     2.5233      0.1160     0.1011     57.9113
20300     2.5233      0.0844     0.1011     57.9386
20400     2.5233      0.1034     0.1011     57.1386
20500     2.5233      0.1013     0.1011     57.9021
20600     2.5233      0.0992     0.1011     58.2705
20700     2.5233      0.0949     0.1011     58.7660
20800     2.5233      0.1076     0.1011     60.0330
20900     2.5233      0.1224     0.1011     58.5337
21000     2.5233      0.1160     0.1011     58.3810
21100     2.5233      0.0970     0.1011     58.3491
21200     2.5233      0.0992     0.1011     57.2106
21300     2.5233      0.0781     0.1011     57.4553
21400     2.5233      0.1203     0.1011     57.8587
21500     2.5233      0.0823     0.1011     57.8776
21600     2.5233      0.1034     0.1011     58.4797
21700     2.5233      0.0865     0.1011     57.3441
21800     2.5233      0.0823     0.1011     57.7482
21900     2.5233      0.1076     0.1011     58.6869
22000     2.5233      0.0865     0.1011     57.6369
22100     2.5233      0.0949     0.1011     57.9536
22200     2.5233      0.0907     0.1011     57.3661
22300     2.5233      0.0949     0.1011     57.2738
22400     2.5233      0.0823     0.1011     58.3368
22500     2.5233      0.1034     0.1011     58.3167
22600     2.5233      0.1034     0.1011     58.4555
22700     2.5233      0.0844     0.1011     58.5775
22800     2.5233      0.0928     0.1011     57.4295
22900     2.5233      0.1224     0.1011     57.4721
23000     2.5233      0.0886     0.1011     59.1582
23100     2.5233      0.1055     0.1011     57.6804
23200     2.5233      0.1013     0.1011     57.8518
23300     2.5233      0.0738     0.1011     57.1565
23400     2.5233      0.0992     0.1011     57.6179
23500     2.5233      0.0823     0.1011     57.8536
23600     2.5233      0.1013     0.1011     58.7473
23700     2.5233      0.0949     0.1011     57.8116
23800     2.5233      0.1350     0.1011     57.3031
23900     2.5233      0.1350     0.1011     57.5506
24000     2.5233      0.0844     0.1011     57.7990
24100     2.5233      0.1013     0.1011     57.3074
24200     2.5233      0.0907     0.1011     58.1427
24300     2.5233      0.0949     0.1011     58.1211
24400     2.5233      0.0992     0.1011     59.0476
24500     2.5232      0.0738     0.1011     57.2527
24600     2.5232      0.1034     0.1011     59.1947
24700     2.5232      0.1034     0.1011     57.7124
24800     2.5232      0.1203     0.1011     58.9713
24900     2.5232      0.1055     0.1011     57.6343
25000     2.5232      0.0949     0.1011     58.7018
25100     2.5232      0.1308     0.1011     58.0091
25200     2.5232      0.1139     0.1011     57.3427
25300     2.5232      0.0949     0.1011     57.7297
25400     2.5232      0.1013     0.1011     58.5829
25500     2.5232      0.0675     0.1011     58.4052
25600     2.5232      0.1076     0.1011     58.4790
25700     2.5232      0.1224     0.1011     57.8912
25800     2.5232      0.0970     0.1011     58.8085
25900     2.5232      0.0949     0.1011     60.6067
26000     2.5232      0.0992     0.1011     57.9619
26100     2.5232      0.1097     0.1011     59.5849
26200     2.5232      0.1076     0.1011     60.1896
26300     2.5232      0.0823     0.1011     58.0321
26400     2.5232      0.1160     0.1011     58.4608
26500     2.5232      0.1245     0.1011     57.1395
26600     2.5232      0.1118     0.1011     57.3270
26700     2.5232      0.0738     0.1011     57.9519
26800     2.5232      0.0781     0.1011     57.7734
26900     2.5232      0.0907     0.1011     57.5891
27000     2.5232      0.0907     0.1011     58.4635
27100     2.5232      0.1034     0.1011     58.6525
27200     2.5232      0.1013     0.1011     57.9902
27300     2.5232      0.1034     0.1011     58.2175
27400     2.5232      0.1034     0.1011     58.6722
27500     2.5232      0.0781     0.1011     58.4062
27600     2.5232      0.0759     0.1011     58.7275
27700     2.5232      0.0886     0.1011     58.0244
27800     2.5232      0.0823     0.1011     58.6905
27900     2.5232      0.0781     0.1011     57.8247
28000     2.5232      0.0928     0.1011     58.3269
28100     2.5232      0.0802     0.1011     57.7792
28200     2.5232      0.0992     0.1011     58.2554
28300     2.5232      0.0907     0.1011     58.3690
28400     2.5232      0.0781     0.1011     59.0664
28500     2.5232      0.1266     0.1011     57.7208
28600     2.5232      0.0928     0.1011     59.0530
28700     2.5232      0.1076     0.1011     60.6645
28800     2.5232      0.0949     0.1011     59.9561
28900     2.5232      0.0949     0.1011     58.4954
29000     2.5232      0.0865     0.1011     58.1227
29100     2.5232      0.0928     0.1011     58.5565
29200     2.5232      0.1076     0.1011     55.7395
29300     2.5232      0.0907     0.1011     56.7354
29400     2.5232      0.0992     0.1011     57.8558
29500     2.5232      0.0949     0.1011     57.6986
29600     2.5232      0.0886     0.1012     58.4001
29700     2.5232      0.1034     0.1012     56.9850
29800     2.5232      0.0970     0.1012     58.7473
29900     2.5232      0.0696     0.1012     58.9773
29999     2.5232      0.1076     0.1012     56.9219
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Start testing:
Test Accuracy: 0.1002
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
491165e2-a994-4cb4-80b1-cf247964578d
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.5257      0.0823     0.0995     13.3181
00100     2.5256      0.1013     0.1004     70.2116
00200     2.5255      0.1034     0.1008     70.3867
00300     2.5254      0.0612     0.1008     70.4458
00400     2.5253      0.0949     0.1008     69.7464
00500     2.5252      0.0907     0.1008     69.6921
00600     2.5251      0.0865     0.1008     69.8952
00700     2.5251      0.1034     0.1008     69.5277
00800     2.5250      0.0992     0.1008     70.3770
00900     2.5249      0.0844     0.1008     69.6047
01000     2.5248      0.0865     0.1008     70.1806
01100     2.5247      0.1097     0.1008     69.9468
01200     2.5247      0.0970     0.1008     69.8424
01300     2.5246      0.0907     0.1008     69.9314
01400     2.5245      0.0759     0.1008     70.5019
01500     2.5244      0.0970     0.1008     70.4429
01600     2.5243      0.1034     0.1008     70.9507
01700     2.5243      0.1055     0.1008     70.5479
01800     2.5242      0.1139     0.1008     70.3890
01900     2.5241      0.0928     0.1008     71.5374
02000     2.5240      0.0949     0.1008     72.5241
02100     2.5240      0.1181     0.1008     71.9644
02200     2.5239      0.1139     0.1008     72.5673
02300     2.5238      0.1203     0.1008     72.4816
02400     2.5237      0.0865     0.1008     73.3066
02500     2.5237      0.0928     0.1008     73.0306
02600     2.5236      0.0949     0.1008     72.8092
02700     2.5235      0.1097     0.1008     72.9069
02800     2.5234      0.0992     0.1008     73.0880
02900     2.5234      0.0970     0.1008     72.8842
03000     2.5233      0.0949     0.1008     73.5558
03100     2.5232      0.1097     0.1008     72.8718
03200     2.5232      0.0781     0.1008     72.0538
03300     2.5231      0.1181     0.1008     72.4067
03400     2.5230      0.0781     0.1008     73.1785
03500     2.5229      0.1013     0.1008     73.4815
03600     2.5229      0.1181     0.1008     72.9087
03700     2.5228      0.1055     0.1008     73.1095
03800     2.5227      0.0865     0.1008     73.2729
03900     2.5227      0.0928     0.1008     72.5733
04000     2.5226      0.1245     0.1008     74.3719
04100     2.5225      0.1034     0.1008     74.1422
04200     2.5225      0.0907     0.1008     74.4705
04300     2.5224      0.1055     0.1008     73.6919
04400     2.5223      0.1224     0.1008     74.0416
04500     2.5222      0.0802     0.1008     75.0513
04600     2.5222      0.0949     0.1008     74.1286
04700     2.5221      0.0802     0.1008     73.4344
04800     2.5220      0.1118     0.1008     74.9529
04900     2.5220      0.0949     0.1008     73.8506
05000     2.5219      0.1076     0.1008     73.5732
05100     2.5218      0.0844     0.1008     75.1879
05200     2.5218      0.0823     0.1008     74.9845
05300     2.5217      0.1034     0.1008     74.1846
05400     2.5216      0.0907     0.1008     76.2800
05500     2.5216      0.1118     0.1008     74.9528
05600     2.5215      0.1097     0.1008     77.6356
05700     2.5214      0.1245     0.1008     74.8495
05800     2.5214      0.0949     0.1008     75.1162
05900     2.5213      0.1055     0.1008     75.8661
06000     2.5212      0.0886     0.1009     75.6339
06100     2.5212      0.0970     0.1009     77.2505
06200     2.5211      0.1013     0.1009     77.6279
06300     2.5210      0.1013     0.1009     76.2909
06400     2.5210      0.1203     0.1009     76.7988
06500     2.5209      0.1160     0.1009     76.4667
06600     2.5208      0.1139     0.1009     76.2813
06700     2.5208      0.1055     0.1009     77.3095
06800     2.5207      0.0886     0.1009     78.0439
06900     2.5206      0.0844     0.1009     77.3918
07000     2.5206      0.0970     0.1009     77.5528
07100     2.5205      0.0907     0.1009     78.6607
07200     2.5204      0.0823     0.1009     80.7964
07300     2.5204      0.1329     0.1009     77.5646
07400     2.5203      0.1013     0.1009     76.5163
07500     2.5202      0.1055     0.1009     76.3164
07600     2.5202      0.0612     0.1009     76.9592
07700     2.5201      0.1076     0.1009     77.3401
07800     2.5200      0.1013     0.1009     77.5236
07900     2.5200      0.1181     0.1009     77.1160
08000     2.5199      0.1181     0.1009     76.8151
08100     2.5199      0.1181     0.1009     76.4624
08200     2.5198      0.0865     0.1009     76.3282
08300     2.5197      0.0928     0.1009     76.8376
08400     2.5197      0.0865     0.1009     77.4188
08500     2.5196      0.1076     0.1009     76.2873
08600     2.5195      0.0970     0.1009     77.1630
08700     2.5195      0.1097     0.1009     76.5393
08800     2.5194      0.1139     0.1009     77.0049
08900     2.5193      0.1055     0.1009     76.4576
09000     2.5193      0.0928     0.1009     76.6716
09100     2.5192      0.1203     0.1009     76.4546
09200     2.5192      0.1224     0.1009     76.8325
09300     2.5191      0.0992     0.1009     75.3053
09400     2.5190      0.0865     0.1009     77.4929
09500     2.5190      0.1076     0.1009     78.0562
09600     2.5189      0.0759     0.1009     77.7723
09700     2.5188      0.0992     0.1009     75.3073
09800     2.5188      0.1055     0.1009     77.0217
09900     2.5187      0.1097     0.1009     76.0382
10000     2.5186      0.1350     0.1009     76.8022
10100     2.5186      0.1118     0.1009     76.6444
10200     2.5186      0.1034     0.1009     75.6105
10300     2.5186      0.0970     0.1009     77.0189
10400     2.5186      0.0823     0.1009     77.3016
10500     2.5186      0.0802     0.1009     76.2423
10600     2.5186      0.0970     0.1009     76.5565
10700     2.5185      0.0886     0.1009     76.2497
10800     2.5185      0.1245     0.1009     76.3701
10900     2.5185      0.1350     0.1009     77.6897
11000     2.5185      0.0738     0.1009     77.6526
11100     2.5185      0.0823     0.1009     76.3110
11200     2.5185      0.0865     0.1009     78.3375
11300     2.5184      0.1055     0.1009     76.3622
11400     2.5184      0.0992     0.1009     77.2178
11500     2.5184      0.0865     0.1009     76.3658
11600     2.5184      0.1097     0.1009     76.6819
11700     2.5184      0.0992     0.1009     76.4841
11800     2.5184      0.1118     0.1009     77.4113
11900     2.5183      0.1139     0.1009     78.2788
12000     2.5183      0.0781     0.1009     77.9199
12100     2.5183      0.1097     0.1009     77.7898
12200     2.5183      0.1203     0.1009     77.4903
12300     2.5183      0.0886     0.1009     76.5379
12400     2.5183      0.1076     0.1009     76.0177
12500     2.5183      0.0970     0.1009     76.5905
12600     2.5182      0.0781     0.1009     78.5398
12700     2.5182      0.1013     0.1009     76.8144
12800     2.5182      0.0717     0.1009     76.0936
12900     2.5182      0.0886     0.1009     76.5572
13000     2.5182      0.1181     0.1009     78.0373
13100     2.5182      0.1097     0.1009     78.8639
13200     2.5181      0.0970     0.1009     77.1873
13300     2.5181      0.0928     0.1009     77.6798
13400     2.5181      0.0844     0.1009     77.4458
13500     2.5181      0.1034     0.1009     78.6339
13600     2.5181      0.1266     0.1009     77.2124
13700     2.5181      0.0928     0.1009     76.6071
13800     2.5180      0.0949     0.1009     78.1216
13900     2.5180      0.1160     0.1009     79.1670
14000     2.5180      0.0886     0.1009     78.9113
14100     2.5180      0.1519     0.1009     78.3029
14200     2.5180      0.0865     0.1009     77.1824
14300     2.5180      0.0865     0.1009     79.6983
14400     2.5180      0.1013     0.1009     76.4823
14500     2.5179      0.0865     0.1009     77.6035
14600     2.5179      0.0928     0.1009     78.3305
14700     2.5179      0.0970     0.1009     77.7327
14800     2.5179      0.0738     0.1009     76.8744
14900     2.5179      0.0928     0.1009     78.1909
15000     2.5179      0.1097     0.1058     76.9222
15100     2.5178      0.0886     0.1058     77.6785
15200     2.5178      0.1076     0.1058     77.7181
15300     2.5178      0.0949     0.1058     75.5583
15400     2.5178      0.1118     0.1058     76.1288
15500     2.5178      0.1139     0.1058     78.6148
15600     2.5178      0.1118     0.1058     78.4949
15700     2.5178      0.1097     0.1058     76.3725
15800     2.5177      0.0823     0.1058     77.9537
15900     2.5177      0.1224     0.1058     76.1940
16000     2.5177      0.0970     0.1058     76.7155
16100     2.5177      0.0865     0.1058     77.6925
16200     2.5177      0.1013     0.1058     76.4699
16300     2.5177      0.1245     0.1058     78.2287
16400     2.5176      0.1034     0.1058     78.6948
16500     2.5176      0.1181     0.1058     77.1695
16600     2.5176      0.1097     0.1058     77.9391
16700     2.5176      0.1097     0.1058     77.0299
16800     2.5176      0.0844     0.1058     78.2207
16900     2.5176      0.0844     0.1058     77.8930
17000     2.5175      0.0949     0.1058     77.7945
17100     2.5175      0.0886     0.1058     76.5810
17200     2.5175      0.0844     0.1058     78.1561
17300     2.5175      0.0992     0.1058     76.3663
17400     2.5175      0.1118     0.1058     76.9638
17500     2.5175      0.0907     0.1058     77.9250
17600     2.5175      0.0802     0.1058     78.5054
17700     2.5174      0.1055     0.1058     77.4510
17800     2.5174      0.1181     0.1058     78.2349
17900     2.5174      0.1160     0.1058     77.5043
18000     2.5174      0.0823     0.1058     76.1433
18100     2.5174      0.0865     0.1058     77.9181
18200     2.5174      0.1055     0.1058     77.4918
18300     2.5173      0.0992     0.1058     76.8561
18400     2.5173      0.0928     0.1058     77.1188
18500     2.5173      0.1139     0.1058     76.7624
18600     2.5173      0.0970     0.1058     78.9258
18700     2.5173      0.0928     0.1058     77.8818
18800     2.5173      0.1097     0.1058     76.9889
18900     2.5172      0.0886     0.1058     76.2822
19000     2.5172      0.0759     0.1058     77.4452
19100     2.5172      0.0928     0.1058     77.3299
19200     2.5172      0.0970     0.1058     78.9393
19300     2.5172      0.0886     0.1058     77.3976
19400     2.5172      0.1013     0.1058     76.8166
19500     2.5172      0.0928     0.1058     77.1181
19600     2.5171      0.0992     0.1058     77.8955
19700     2.5171      0.0970     0.1058     79.3179
19800     2.5171      0.1076     0.1058     79.1053
19900     2.5171      0.0949     0.1058     78.5936
20000     2.5171      0.0928     0.1058     77.0298
20100     2.5171      0.1224     0.1058     78.0084
20199     2.5171      0.0907     0.1058     75.1515
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.5171      0.0549     0.0986     11.1509
00100     2.5168      0.0907     0.0986     76.5341
00200     2.5172      0.0907     0.0986     77.2375
00300     2.5171      0.0823     0.0986     76.7922
00400     2.5170      0.1013     0.0986     76.9141
00500     2.5170      0.1034     0.0986     76.4334
00600     2.5168      0.1034     0.0986     77.0081
00700     2.5171      0.0717     0.0986     75.8126
00800     2.5171      0.0781     0.0986     77.0104
00900     2.5170      0.0823     0.0986     76.5591
01000     2.5171      0.0591     0.0986     76.0370
01100     2.5172      0.0675     0.0986     76.5309
01200     2.5170      0.0907     0.0986     78.3510
01300     2.5172      0.0823     0.0999     76.7729
01400     2.5170      0.0992     0.0999     76.0215
01500     2.5169      0.1118     0.0999     77.3450
01600     2.5170      0.1139     0.0999     76.4569
01700     2.5172      0.0844     0.0999     76.8678
01800     2.5171      0.1139     0.0999     77.2640
01900     2.5171      0.0759     0.1021     77.0928
02000     2.5170      0.0865     0.1021     76.2701
02100     2.5170      0.0907     0.1021     77.9497
02200     2.5171      0.0738     0.1021     77.2987
02300     2.5170      0.0823     0.1021     77.9102
02400     2.5170      0.0802     0.1021     77.1547
02500     2.5170      0.0844     0.1021     78.7331
02600     2.5169      0.0970     0.1021     76.8295
02700     2.5170      0.0907     0.1021     77.8691
02800     2.5171      0.0781     0.1021     77.5614
02900     2.5168      0.0886     0.1021     77.5215
03000     2.5171      0.0738     0.1021     77.9890
03100     2.5169      0.0970     0.1021     77.2233
03200     2.5168      0.0886     0.1021     77.7769
03300     2.5169      0.0759     0.1021     76.1294
03400     2.5169      0.0717     0.1021     76.0052
03500     2.5170      0.0886     0.1021     76.9542
03600     2.5171      0.0612     0.1021     76.7410
03700     2.5169      0.0738     0.1021     76.6534
03800     2.5170      0.0612     0.1021     77.1282
03900     2.5170      0.0802     0.1021     77.1864
04000     2.5170      0.0738     0.1021     79.0372
04100     2.5169      0.0759     0.1021     77.1378
04200     2.5171      0.0717     0.1021     78.1710
04300     2.5169      0.0738     0.1021     76.9070
04400     2.5170      0.0612     0.1021     77.4888
04500     2.5170      0.0696     0.1021     78.1381
04600     2.5170      0.1097     0.1021     78.5372
04700     2.5169      0.0949     0.1021     77.3222
04800     2.5170      0.0949     0.1021     76.8583
04900     2.5169      0.0759     0.1021     76.6658
05000     2.5170      0.0633     0.1021     77.7673
05100     2.5169      0.0717     0.1021     75.2718
05200     2.5170      0.1076     0.1021     77.5360
05300     2.5171      0.1055     0.1021     75.6598
05400     2.5170      0.0549     0.1021     76.1529
05500     2.5169      0.1013     0.1021     76.6938
05600     2.5169      0.0759     0.1021     77.0763
05700     2.5169      0.0802     0.1021     78.2862
05800     2.5170      0.0992     0.1021     77.2393
05900     2.5169      0.0970     0.1021     76.3419
06000     2.5170      0.0696     0.1021     76.6045
06100     2.5170      0.0759     0.1021     76.0861
06200     2.5169      0.0759     0.1021     77.3133
06300     2.5168      0.1013     0.1049     76.5354
06400     2.5169      0.1013     0.1049     76.6246
06500     2.5168      0.1181     0.1049     76.3028
06600     2.5168      0.1076     0.1049     77.2068
06700     2.5169      0.1118     0.1049     76.3789
06800     2.5169      0.0907     0.1049     76.2723
06900     2.5170      0.1203     0.1049     78.6585
07000     2.5169      0.0928     0.1049     77.7023
07100     2.5169      0.1076     0.1049     76.8713
07200     2.5168      0.1245     0.1049     76.5514
07300     2.5167      0.1034     0.1049     76.1645
07400     2.5170      0.0696     0.1049     77.9929
07500     2.5168      0.1076     0.1049     78.6519
07600     2.5168      0.1097     0.1049     78.7648
07700     2.5167      0.1139     0.1049     77.2719
07800     2.5170      0.0717     0.1049     76.7431
07900     2.5170      0.1076     0.1049     76.8628
08000     2.5169      0.0970     0.1049     77.3859
08100     2.5170      0.0422     0.1049     77.5043
08200     2.5169      0.0844     0.1049     77.3616
08300     2.5170      0.0654     0.1049     77.1240
08400     2.5169      0.0759     0.1049     77.9068
08500     2.5168      0.0549     0.1049     79.4970
08600     2.5166      0.0886     0.1049     78.7940
08700     2.5168      0.0717     0.1049     76.6133
08800     2.5170      0.0928     0.1049     77.7472
08900     2.5170      0.0886     0.1049     77.7781
09000     2.5166      0.0907     0.1049     76.5574
09100     2.5166      0.1139     0.1049     78.2979
09200     2.5166      0.0654     0.1049     76.3533
09300     2.5166      0.1076     0.1049     76.7296
09400     2.5168      0.0992     0.1049     76.0634
09500     2.5168      0.0949     0.1049     76.9888
09600     2.5170      0.0844     0.1049     75.6930
09700     2.5167      0.0970     0.1049     75.9478
09800     2.5169      0.0759     0.1049     77.0917
09900     2.5167      0.0696     0.1049     78.3367
Start testing:
Test Accuracy: 0.1023
