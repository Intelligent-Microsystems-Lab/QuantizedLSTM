Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 279, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.0, finetuning_epochs=0, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.0005,0.0001,0.00002', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,10000', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
b148832d-e854-4db5-a0f6-85d33781e687
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.5257      0.0970     0.0998     12.0621
00100     2.5256      0.1076     0.1003     57.0835
00200     2.5256      0.0717     0.1003     58.3946
00300     2.5256      0.1076     0.1003     58.4579
00400     2.5256      0.0886     0.1003     56.8505
00500     2.5256      0.0738     0.1003     57.4368
00600     2.5255      0.0928     0.1003     59.6014
00700     2.5255      0.1097     0.1003     57.7401
00800     2.5255      0.0886     0.1003     57.6449
00900     2.5255      0.1371     0.1003     56.5995
01000     2.5254      0.0992     0.1003     57.0693
01100     2.5254      0.1013     0.1003     57.8128
01200     2.5254      0.0928     0.1003     57.3454
01300     2.5254      0.1160     0.1003     58.0794
01400     2.5254      0.1203     0.1003     57.7012
01500     2.5253      0.0865     0.1008     57.1321
01600     2.5253      0.1118     0.1008     57.7431
01700     2.5253      0.0907     0.1008     57.2389
01800     2.5253      0.1055     0.1008     56.8806
01900     2.5252      0.1160     0.1008     58.2877
02000     2.5252      0.1055     0.1008     58.9038
02100     2.5252      0.1118     0.1008     58.5352
02200     2.5252      0.0949     0.1008     58.1531
02300     2.5252      0.0992     0.1008     58.5430
02400     2.5251      0.0949     0.1008     59.4991
02500     2.5251      0.1160     0.1008     57.7113
02600     2.5251      0.0865     0.1008     57.8908
02700     2.5251      0.1203     0.1008     60.2783
02800     2.5251      0.0970     0.1008     58.9005
02900     2.5250      0.0844     0.1008     58.1915
03000     2.5250      0.0865     0.1008     57.7078
03100     2.5250      0.1034     0.1008     57.5471
03200     2.5250      0.1097     0.1008     58.6722
03300     2.5249      0.0886     0.1008     56.8155
03400     2.5249      0.1118     0.1008     57.7068
03500     2.5249      0.1076     0.1008     58.0831
03600     2.5249      0.1139     0.1008     58.0170
03700     2.5249      0.0759     0.1008     58.0303
03800     2.5248      0.0823     0.1008     59.0229
03900     2.5248      0.0992     0.1008     57.4304
04000     2.5248      0.1013     0.1008     57.5739
04100     2.5248      0.0781     0.1008     57.8342
04200     2.5248      0.0992     0.1008     57.2945
04300     2.5247      0.1160     0.1008     58.1183
04400     2.5247      0.1097     0.1008     58.9020
04500     2.5247      0.0992     0.1008     59.3352
04600     2.5247      0.0886     0.1008     57.6458
04700     2.5247      0.1076     0.1008     59.2300
04800     2.5246      0.0844     0.1008     58.6826
04900     2.5246      0.1076     0.1008     58.3799
05000     2.5246      0.0865     0.1008     57.0122
05100     2.5246      0.0928     0.1008     59.0887
05200     2.5246      0.0865     0.1008     57.7713
05300     2.5245      0.0738     0.1008     57.3888
05400     2.5245      0.0865     0.1008     57.6185
05500     2.5245      0.0970     0.1008     56.9520
05600     2.5245      0.1266     0.1008     60.0604
05700     2.5245      0.1076     0.1008     58.2427
05800     2.5244      0.0928     0.1008     58.8760
05900     2.5244      0.0759     0.1008     58.8859
06000     2.5244      0.1013     0.1008     57.5599
06100     2.5244      0.1076     0.1008     58.1549
06200     2.5244      0.1203     0.1008     58.9735
06300     2.5243      0.1097     0.1008     57.6371
06400     2.5243      0.1055     0.1008     58.6269
06500     2.5243      0.0992     0.1008     57.8834
06600     2.5243      0.1139     0.1008     58.3761
06700     2.5243      0.0612     0.1008     58.0660
06800     2.5242      0.1034     0.1008     58.2084
06900     2.5242      0.0823     0.1008     56.8982
07000     2.5242      0.0970     0.1008     58.3188
07100     2.5242      0.1034     0.1008     58.8069
07200     2.5242      0.0823     0.1008     57.8628
07300     2.5242      0.0970     0.1008     56.9518
07400     2.5241      0.0886     0.1008     57.8361
07500     2.5241      0.0970     0.1008     58.8355
07600     2.5241      0.1139     0.1008     59.0266
07700     2.5241      0.1055     0.1008     57.7256
07800     2.5241      0.1055     0.1008     58.2721
07900     2.5240      0.0992     0.1008     58.5586
08000     2.5240      0.0928     0.1008     58.7988
08100     2.5240      0.1013     0.1008     57.2146
08200     2.5240      0.1266     0.1008     58.5786
08300     2.5240      0.0886     0.1008     57.6613
08400     2.5239      0.1013     0.1008     58.1487
08500     2.5239      0.0865     0.1008     57.3948
08600     2.5239      0.0992     0.1008     58.1514
08700     2.5239      0.0992     0.1008     57.7365
08800     2.5239      0.1245     0.1008     59.1711
08900     2.5239      0.0970     0.1008     58.4861
09000     2.5238      0.0886     0.1008     58.0042
09100     2.5238      0.0907     0.1008     57.7027
09200     2.5238      0.1371     0.1008     58.2530
09300     2.5238      0.0949     0.1008     57.5264
09400     2.5238      0.1076     0.1009     60.2721
09500     2.5237      0.0928     0.1009     57.4432
09600     2.5237      0.0781     0.1009     59.5031
09700     2.5237      0.1181     0.1009     58.1508
09800     2.5237      0.0992     0.1009     57.5997
09900     2.5237      0.1097     0.1009     58.4949
10000     2.5236      0.1076     0.1009     56.6684
10100     2.5236      0.1118     0.1009     57.8440
10200     2.5236      0.0907     0.1009     59.6012
10300     2.5236      0.1266     0.1009     57.2524
10400     2.5236      0.0928     0.1009     57.4962
10500     2.5236      0.0928     0.1009     57.3676
10600     2.5236      0.0865     0.1009     58.6742
10700     2.5236      0.0907     0.1009     58.6307
10800     2.5236      0.0992     0.1009     57.6317
10900     2.5236      0.1224     0.1009     56.5800
11000     2.5236      0.0886     0.1009     59.3960
11100     2.5236      0.1160     0.1009     59.6818
11200     2.5236      0.0802     0.1009     58.5661
11300     2.5236      0.0717     0.1009     57.7226
11400     2.5236      0.0992     0.1009     58.4712
11500     2.5236      0.0717     0.1009     57.8380
11600     2.5236      0.0949     0.1009     57.3366
11700     2.5236      0.0886     0.1009     56.7053
11800     2.5236      0.0865     0.1009     58.0913
11900     2.5236      0.0907     0.1009     58.0079
12000     2.5236      0.0928     0.1009     58.7287
12100     2.5236      0.0949     0.1009     57.7202
12200     2.5236      0.1118     0.1009     57.3864
12300     2.5236      0.1013     0.1009     57.4862
12400     2.5236      0.1160     0.1009     58.1341
12500     2.5236      0.0907     0.1009     57.4291
12600     2.5236      0.0781     0.1009     58.1559
12700     2.5235      0.1076     0.1009     57.3444
12800     2.5235      0.1055     0.1009     57.8119
12900     2.5235      0.0970     0.1009     57.8269
13000     2.5235      0.1203     0.1009     58.4573
13100     2.5235      0.0970     0.1009     57.8022
13200     2.5235      0.0823     0.1009     58.6125
13300     2.5235      0.0886     0.1009     57.9658
13400     2.5235      0.1139     0.1009     57.7160
13500     2.5235      0.0907     0.1009     57.3108
13600     2.5235      0.0949     0.1009     60.0398
13700     2.5235      0.0992     0.1009     57.5819
13800     2.5235      0.1034     0.1009     59.5233
13900     2.5235      0.0865     0.1009     57.4790
14000     2.5235      0.0886     0.1009     57.8321
14100     2.5235      0.1076     0.1009     57.1609
14200     2.5235      0.1055     0.1009     58.4095
14300     2.5235      0.1055     0.1009     57.1613
14400     2.5235      0.0738     0.1009     58.6432
14500     2.5235      0.0844     0.1009     57.4786
14600     2.5235      0.0675     0.1009     57.3103
14700     2.5235      0.1224     0.1009     59.4383
14800     2.5235      0.0823     0.1009     59.2682
14900     2.5235      0.1139     0.1009     58.0280
15000     2.5235      0.1266     0.1009     59.3842
15100     2.5235      0.0992     0.1009     57.1713
15200     2.5235      0.1160     0.1009     58.4913
15300     2.5235      0.1076     0.1009     58.1713
15400     2.5235      0.0928     0.1009     57.0672
15500     2.5234      0.0781     0.1009     58.9832
15600     2.5234      0.0928     0.1009     58.8889
15700     2.5234      0.1034     0.1009     58.3048
15800     2.5234      0.0970     0.1009     58.9688
15900     2.5234      0.1034     0.1009     57.8511
16000     2.5234      0.0949     0.1009     60.5784
16100     2.5234      0.0865     0.1009     58.1297
16200     2.5234      0.0970     0.1009     57.0255
16300     2.5234      0.1076     0.1009     57.3680
16400     2.5234      0.0886     0.1009     57.3684
16500     2.5234      0.0992     0.1009     56.7924
16600     2.5234      0.1160     0.1009     58.0567
16700     2.5234      0.0949     0.1009     58.7356
16800     2.5234      0.1266     0.1009     58.0323
16900     2.5234      0.0907     0.1009     57.9938
17000     2.5234      0.0844     0.1009     57.4216
17100     2.5234      0.1055     0.1009     57.6460
17200     2.5234      0.0970     0.1009     58.5532
17300     2.5234      0.0886     0.1009     58.0246
17400     2.5234      0.0992     0.1009     57.7433
17500     2.5234      0.1055     0.1009     57.3199
17600     2.5234      0.1013     0.1009     60.2514
17700     2.5234      0.1224     0.1009     57.8273
17800     2.5234      0.1034     0.1009     58.8905
17900     2.5234      0.0928     0.1009     58.3531
18000     2.5234      0.1034     0.1009     58.5280
18100     2.5234      0.1034     0.1011     57.6561
18200     2.5234      0.1118     0.1011     57.5337
18300     2.5233      0.0886     0.1011     56.6413
18400     2.5233      0.1013     0.1011     58.8123
18500     2.5233      0.1118     0.1011     57.8156
18600     2.5233      0.0865     0.1011     57.8550
18700     2.5233      0.1181     0.1011     57.7146
18800     2.5233      0.1076     0.1011     58.2555
18900     2.5233      0.0781     0.1011     57.3161
19000     2.5233      0.0886     0.1011     59.0731
19100     2.5233      0.0717     0.1011     57.3064
19200     2.5233      0.0949     0.1011     58.4692
19300     2.5233      0.1308     0.1011     57.1575
19400     2.5233      0.0907     0.1011     56.9514
19500     2.5233      0.0928     0.1011     59.5134
19600     2.5233      0.1266     0.1011     59.3333
19700     2.5233      0.1203     0.1011     58.1404
19800     2.5233      0.0928     0.1011     58.6472
19900     2.5233      0.1097     0.1011     58.1330
20000     2.5233      0.0865     0.1011     58.3135
20100     2.5233      0.0992     0.1011     58.8361
20200     2.5233      0.1160     0.1011     57.9113
20300     2.5233      0.0844     0.1011     57.9386
20400     2.5233      0.1034     0.1011     57.1386
20500     2.5233      0.1013     0.1011     57.9021
20600     2.5233      0.0992     0.1011     58.2705
20700     2.5233      0.0949     0.1011     58.7660
20800     2.5233      0.1076     0.1011     60.0330
20900     2.5233      0.1224     0.1011     58.5337
21000     2.5233      0.1160     0.1011     58.3810
21100     2.5233      0.0970     0.1011     58.3491
21200     2.5233      0.0992     0.1011     57.2106
21300     2.5233      0.0781     0.1011     57.4553
21400     2.5233      0.1203     0.1011     57.8587
21500     2.5233      0.0823     0.1011     57.8776
21600     2.5233      0.1034     0.1011     58.4797
21700     2.5233      0.0865     0.1011     57.3441
21800     2.5233      0.0823     0.1011     57.7482
21900     2.5233      0.1076     0.1011     58.6869
22000     2.5233      0.0865     0.1011     57.6369
22100     2.5233      0.0949     0.1011     57.9536
22200     2.5233      0.0907     0.1011     57.3661
22300     2.5233      0.0949     0.1011     57.2738
22400     2.5233      0.0823     0.1011     58.3368
22500     2.5233      0.1034     0.1011     58.3167
22600     2.5233      0.1034     0.1011     58.4555
22700     2.5233      0.0844     0.1011     58.5775
22800     2.5233      0.0928     0.1011     57.4295
22900     2.5233      0.1224     0.1011     57.4721
23000     2.5233      0.0886     0.1011     59.1582
23100     2.5233      0.1055     0.1011     57.6804
23200     2.5233      0.1013     0.1011     57.8518
23300     2.5233      0.0738     0.1011     57.1565
23400     2.5233      0.0992     0.1011     57.6179
23500     2.5233      0.0823     0.1011     57.8536
23600     2.5233      0.1013     0.1011     58.7473
23700     2.5233      0.0949     0.1011     57.8116
23800     2.5233      0.1350     0.1011     57.3031
23900     2.5233      0.1350     0.1011     57.5506
24000     2.5233      0.0844     0.1011     57.7990
24100     2.5233      0.1013     0.1011     57.3074
24200     2.5233      0.0907     0.1011     58.1427
24300     2.5233      0.0949     0.1011     58.1211
24400     2.5233      0.0992     0.1011     59.0476
24500     2.5232      0.0738     0.1011     57.2527
24600     2.5232      0.1034     0.1011     59.1947
24700     2.5232      0.1034     0.1011     57.7124
24800     2.5232      0.1203     0.1011     58.9713
24900     2.5232      0.1055     0.1011     57.6343
25000     2.5232      0.0949     0.1011     58.7018
25100     2.5232      0.1308     0.1011     58.0091
25200     2.5232      0.1139     0.1011     57.3427
25300     2.5232      0.0949     0.1011     57.7297
25400     2.5232      0.1013     0.1011     58.5829
25500     2.5232      0.0675     0.1011     58.4052
25600     2.5232      0.1076     0.1011     58.4790
25700     2.5232      0.1224     0.1011     57.8912
25800     2.5232      0.0970     0.1011     58.8085
25900     2.5232      0.0949     0.1011     60.6067
26000     2.5232      0.0992     0.1011     57.9619
26100     2.5232      0.1097     0.1011     59.5849
26200     2.5232      0.1076     0.1011     60.1896
26300     2.5232      0.0823     0.1011     58.0321
26400     2.5232      0.1160     0.1011     58.4608
26500     2.5232      0.1245     0.1011     57.1395
26600     2.5232      0.1118     0.1011     57.3270
26700     2.5232      0.0738     0.1011     57.9519
26800     2.5232      0.0781     0.1011     57.7734
26900     2.5232      0.0907     0.1011     57.5891
27000     2.5232      0.0907     0.1011     58.4635
27100     2.5232      0.1034     0.1011     58.6525
27200     2.5232      0.1013     0.1011     57.9902
27300     2.5232      0.1034     0.1011     58.2175
27400     2.5232      0.1034     0.1011     58.6722
27500     2.5232      0.0781     0.1011     58.4062
27600     2.5232      0.0759     0.1011     58.7275
27700     2.5232      0.0886     0.1011     58.0244
27800     2.5232      0.0823     0.1011     58.6905
27900     2.5232      0.0781     0.1011     57.8247
28000     2.5232      0.0928     0.1011     58.3269
28100     2.5232      0.0802     0.1011     57.7792
28200     2.5232      0.0992     0.1011     58.2554
28300     2.5232      0.0907     0.1011     58.3690
28400     2.5232      0.0781     0.1011     59.0664
28500     2.5232      0.1266     0.1011     57.7208
28600     2.5232      0.0928     0.1011     59.0530
28700     2.5232      0.1076     0.1011     60.6645
28800     2.5232      0.0949     0.1011     59.9561
28900     2.5232      0.0949     0.1011     58.4954
29000     2.5232      0.0865     0.1011     58.1227
29100     2.5232      0.0928     0.1011     58.5565
29200     2.5232      0.1076     0.1011     55.7395
29300     2.5232      0.0907     0.1011     56.7354
29400     2.5232      0.0992     0.1011     57.8558
29500     2.5232      0.0949     0.1011     57.6986
29600     2.5232      0.0886     0.1012     58.4001
29700     2.5232      0.1034     0.1012     56.9850
29800     2.5232      0.0970     0.1012     58.7473
29900     2.5232      0.0696     0.1012     58.9773
29999     2.5232      0.1076     0.1012     56.9219
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Start testing:
Test Accuracy: 0.1002
