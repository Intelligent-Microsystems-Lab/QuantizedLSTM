Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=5, quant_actNM=5, quant_inp=5, quant_w=5, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
0fb5b5f4-fa87-4c23-a387-c5d211227fa2
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, 1.)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 82, in forward
    if len(x_range) > 1:
TypeError: object of type 'float' has no len()
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=5, quant_actNM=5, quant_inp=5, quant_w=5, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
dda42298-0626-4c78-a3e0-9c806bb70245
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, torch.tensor([1.]))
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 94, in forward
    x_scaled = x/x_range
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=5, quant_actNM=5, quant_inp=5, quant_w=5, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
014af1b3-309e-48dd-a09c-ae74c42b13a1
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, torch.tensor([1.]).device(input.device))
TypeError: 'torch.device' object is not callable
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=5, quant_actNM=5, quant_inp=5, quant_w=5, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
8d16b852-a939-423e-b543-911639b1165b
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.8573      0.0654     0.0846     9.7828
00100     2.5158      0.0970     0.0935     70.1551
00200     2.4901      0.1857     0.1843     71.0003
00300     2.3938      0.1751     0.2425     71.4924
00400     2.2978      0.1793     0.2425     72.0358
00500     2.1205      0.2637     0.2848     71.3931
00600     1.9860      0.3629     0.3866     70.7421
00700     1.8294      0.4409     0.4434     70.9794
00800     1.6636      0.4916     0.5288     71.1628
00900     1.6099      0.5084     0.5484     72.1534
01000     1.4741      0.5443     0.5534     71.5123
01100     1.4708      0.5527     0.5711     70.9687
01200     1.3838      0.5527     0.5833     70.7282
01300     1.3606      0.5802     0.6080     70.9191
01400     1.4537      0.5464     0.6080     70.6219
01500     1.4284      0.5633     0.6080     70.5113
01600     1.3724      0.5759     0.6243     72.1654
01700     1.3362      0.6160     0.6298     70.6127
01800     1.3474      0.5696     0.6298     70.7298
01900     1.2625      0.5886     0.6332     71.2695
02000     1.2851      0.6034     0.6536     70.5685
02100     1.1977      0.6371     0.6536     70.0577
02200     1.1772      0.6118     0.6599     70.5625
02300     1.2554      0.5907     0.6599     69.9715
02400     1.1686      0.6245     0.6599     71.0063
02500     1.3020      0.5823     0.6628     71.2500
02600     1.2195      0.6076     0.6628     71.0985
02700     1.2305      0.6435     0.6628     70.9756
02800     1.1990      0.6350     0.6760     70.6403
02900     1.2891      0.6224     0.6760     70.9884
03000     1.1541      0.6624     0.6760     71.4764
03100     1.1424      0.6456     0.6760     73.1855
03200     1.2123      0.6181     0.6760     72.7159
03300     1.1657      0.6392     0.6780     71.5422
03400     1.2325      0.5992     0.6780     71.3364
03500     1.2872      0.5823     0.6780     72.2318
03600     1.1597      0.6392     0.6780     71.2118
03700     1.1865      0.6371     0.6904     70.9794
03800     1.1726      0.6561     0.6904     71.2994
03900     1.1826      0.6266     0.6904     71.7860
04000     1.1395      0.6456     0.6904     71.9034
04100     1.0950      0.6456     0.6904     72.0785
04200     1.2390      0.6308     0.6905     71.1632
04300     1.2210      0.6329     0.7027     71.9134
04400     1.2040      0.6350     0.7099     71.3275
04500     1.0838      0.6667     0.7099     71.3248
04600     1.1201      0.6392     0.7113     70.6820
04700     1.0267      0.6709     0.7115     70.5692
04800     1.0564      0.6941     0.7115     71.9520
04900     1.0841      0.6646     0.7115     71.0034
05000     1.0376      0.6835     0.7115     71.0696
05100     1.0816      0.6603     0.7115     71.5319
05200     1.1014      0.6688     0.7115     72.4887
05300     1.1665      0.6266     0.7115     71.8220
05400     1.1414      0.6287     0.7115     71.3461
05500     1.0856      0.6582     0.7115     70.9394
05600     1.0882      0.6709     0.7119     72.1164
05700     1.1433      0.6414     0.7140     70.4826
05800     1.1330      0.6498     0.7178     71.1581
05900     1.1260      0.6477     0.7178     71.0417
06000     1.1474      0.6287     0.7178     70.6378
06100     1.2169      0.6203     0.7178     71.2323
06200     1.0970      0.6519     0.7178     70.9854
06300     1.0898      0.6688     0.7178     71.2051
06400     1.0721      0.6709     0.7178     71.8010
06500     1.1278      0.6477     0.7178     71.4346
06600     1.1220      0.6646     0.7178     72.6961
06700     1.0577      0.6793     0.7178     71.8212
06800     1.1000      0.6435     0.7221     71.3805
06900     1.0312      0.7046     0.7221     71.5859
07000     1.1633      0.6582     0.7221     72.2767
07100     1.0656      0.6414     0.7221     71.6628
07200     1.1088      0.6582     0.7221     71.6042
07300     1.2205      0.6245     0.7221     71.2957
07400     1.0882      0.6941     0.7221     71.9271
07500     1.0346      0.6793     0.7221     71.6771
07600     1.1171      0.6582     0.7221     71.4154
07700     0.9911      0.7194     0.7221     71.5412
07800     1.1253      0.6287     0.7221     72.3418
07900     1.1810      0.6371     0.7221     71.5509
08000     1.1122      0.6477     0.7221     71.7524
08100     1.1596      0.6540     0.7221     71.9825
08200     1.0273      0.6793     0.7221     72.0366
08300     1.0315      0.6878     0.7221     71.8142
08400     1.1128      0.6561     0.7221     70.9124
08500     1.0565      0.6814     0.7221     71.8894
08600     1.0589      0.6857     0.7221     72.2764
08700     1.0108      0.6814     0.7221     72.9279
08800     1.0797      0.6814     0.7221     72.6863
08900     1.1102      0.6287     0.7221     71.8916
09000     1.1214      0.6667     0.7221     71.7332
09100     1.0413      0.6709     0.7221     72.1280
09200     1.1461      0.6477     0.7221     71.8314
09300     1.0373      0.6688     0.7221     71.7984
09400     1.0510      0.6751     0.7221     71.9176
09500     1.2142      0.6329     0.7221     72.3344
09600     1.0521      0.6709     0.7280     71.4133
09700     1.0980      0.6603     0.7280     71.5042
09800     1.0889      0.6435     0.7280     71.4262
09900     1.0651      0.6730     0.7280     72.8512
10000     1.0269      0.6899     0.7280     72.4060
10100     1.0792      0.6857     0.7287     71.7629
10200     1.0437      0.6793     0.7287     73.1415
10300     1.0590      0.6477     0.7287     72.2191
10400     1.0288      0.6814     0.7287     71.4818
10500     1.0016      0.6793     0.7287     72.2847
10600     1.0334      0.6899     0.7287     72.0645
10700     1.0571      0.6624     0.7317     71.9609
10800     1.1008      0.6582     0.7347     72.6670
10900     1.0814      0.6793     0.7347     72.2877
11000     0.9790      0.6878     0.7347     72.3417
11100     1.0211      0.7046     0.7347     72.3149
11200     0.9394      0.6983     0.7347     72.4810
11300     1.1019      0.6519     0.7347     71.9326
11400     1.1205      0.6519     0.7347     71.5701
11500     1.0575      0.6688     0.7347     73.1775
11600     1.0739      0.6350     0.7347     73.3281
11700     1.0922      0.6392     0.7347     71.4968
11800     0.9931      0.7004     0.7347     72.8093
11900     1.0586      0.6730     0.7347     72.1726
12000     1.0138      0.6835     0.7381     72.1035
12100     0.9538      0.7046     0.7381     71.9590
12200     1.0899      0.6603     0.7381     71.4809
12300     1.0065      0.6920     0.7381     71.9937
12400     1.1177      0.6540     0.7381     72.6215
12500     1.0843      0.6371     0.7381     71.8127
12600     1.1435      0.6519     0.7381     72.0726
12700     1.0924      0.6582     0.7381     72.5776
12800     1.0612      0.6814     0.7381     73.1808
12900     1.0395      0.6920     0.7381     72.6322
13000     1.0307      0.6857     0.7381     72.5732
13100     1.0652      0.6814     0.7381     72.7482
13200     1.0632      0.6667     0.7381     72.2674
13300     1.0775      0.6688     0.7381     72.2869
13400     1.0842      0.6857     0.7381     73.6213
13500     1.1485      0.6456     0.7381     73.0516
13600     1.0834      0.6498     0.7381     72.8840
13700     1.0714      0.6646     0.7381     73.0279
13800     1.0189      0.6646     0.7381     72.6372
13900     0.9681      0.6962     0.7381     74.0910
14000     1.0727      0.6709     0.7381     73.0814
14100     1.0191      0.6793     0.7381     72.2656
14200     0.9678      0.7068     0.7381     72.8241
14300     1.0580      0.6709     0.7381     72.9164
14400     1.0760      0.6582     0.7381     73.0366
14500     1.0462      0.6793     0.7381     72.2887
14600     1.0607      0.6667     0.7381     71.5402
14700     0.9463      0.7173     0.7412     73.1413
14800     1.1038      0.6519     0.7412     73.2083
14900     1.1577      0.6435     0.7412     73.2447
15000     1.0058      0.7004     0.7412     73.0218
15100     1.1059      0.6456     0.7415     73.5545
15200     1.0019      0.6899     0.7415     73.6156
15300     0.9772      0.7173     0.7415     72.8762
15400     1.0604      0.6772     0.7415     74.1937
15500     1.1405      0.6498     0.7415     73.5460
15600     1.0449      0.6624     0.7415     73.0760
15700     0.9664      0.7131     0.7415     73.6830
15800     1.1877      0.6245     0.7415     72.7914
15900     1.0051      0.6709     0.7415     73.7195
16000     0.9916      0.6899     0.7415     72.7724
16100     1.0854      0.6603     0.7435     73.3121
16200     1.0242      0.6983     0.7435     72.3725
16300     1.0483      0.6793     0.7435     73.5786
16400     0.9180      0.7215     0.7435     72.6752
16500     1.0060      0.6878     0.7435     73.5391
16600     1.0892      0.6582     0.7435     72.7176
16700     0.9990      0.6920     0.7435     72.9740
16800     1.0228      0.6646     0.7435     73.5816
16900     1.0751      0.6793     0.7435     73.1130
17000     1.1397      0.6350     0.7451     73.4875
17100     0.9596      0.7025     0.7451     72.9820
17200     1.0557      0.6603     0.7451     73.5866
17300     1.0847      0.6498     0.7451     72.8136
17400     1.1431      0.6350     0.7451     73.4243
17500     1.2473      0.6203     0.7451     72.5138
17600     1.0996      0.6308     0.7451     73.2248
17700     1.0396      0.6899     0.7451     72.6681
17800     0.9932      0.6814     0.7451     72.6479
17900     1.0353      0.6730     0.7451     73.3316
18000     0.9993      0.6899     0.7451     73.3521
18100     1.0797      0.6646     0.7451     73.7378
18200     1.1417      0.6287     0.7451     72.3809
18300     1.0425      0.6962     0.7451     72.8443
18400     1.0982      0.6498     0.7451     72.8066
18500     0.9943      0.6857     0.7451     72.0876
18600     1.1159      0.6435     0.7451     72.9551
18700     1.0507      0.6624     0.7451     73.0854
18800     1.0606      0.6899     0.7451     72.4959
18900     1.0049      0.6899     0.7451     72.7107
19000     1.0553      0.6878     0.7451     73.2588
19100     0.9482      0.7131     0.7451     72.7878
19200     1.0613      0.6371     0.7451     73.3110
19300     1.1098      0.6561     0.7451     73.7118
19400     1.1279      0.6287     0.7451     72.5973
19500     1.0415      0.6920     0.7451     72.7075
19600     1.0352      0.6730     0.7451     73.1544
19700     1.0361      0.6582     0.7451     73.7963
19800     1.1324      0.6371     0.7451     73.2150
19900     1.0992      0.6582     0.7451     72.7198
20000     1.0071      0.6920     0.7451     74.7415
20100     1.1197      0.6498     0.7451     74.6386
20199     1.0242      0.6709     0.7451     72.7847
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     1.0413      0.6835     0.7299     10.6659
00100     0.8750      0.7236     0.7489     70.6595
00200     1.0537      0.6793     0.7489     71.5347
00300     0.9707      0.7046     0.7492     71.9461
00400     0.9484      0.6920     0.7492     70.7946
00500     0.9499      0.6920     0.7492     72.5124
00600     0.9643      0.7068     0.7492     70.7999
00700     0.9560      0.7004     0.7492     70.9631
00800     0.8846      0.7405     0.7492     70.9426
00900     1.0135      0.6772     0.7492     71.3349
01000     1.0111      0.6793     0.7492     71.5021
01100     1.0263      0.6920     0.7492     70.6474
01200     1.0338      0.6857     0.7492     70.0856
01300     0.9622      0.6878     0.7492     70.8381
01400     1.0427      0.6772     0.7492     70.9802
01500     0.9763      0.6941     0.7492     70.9302
01600     1.0144      0.6793     0.7492     70.6167
01700     0.9588      0.7152     0.7492     71.1504
01800     0.8654      0.7363     0.7492     71.0566
01900     0.8928      0.7215     0.7492     71.3743
02000     1.0449      0.6561     0.7492     71.1146
02100     0.9496      0.7046     0.7492     70.5019
02200     0.9124      0.7278     0.7492     70.8628
02300     0.9170      0.7173     0.7492     70.9636
02400     0.8895      0.7321     0.7492     70.1972
02500     0.9207      0.7236     0.7492     70.8796
02600     1.0702      0.6751     0.7492     71.0028
02700     0.9761      0.6920     0.7492     70.8919
02800     0.9786      0.7089     0.7492     71.1950
02900     0.9233      0.7046     0.7492     70.3250
03000     0.9491      0.7257     0.7492     71.5592
03100     1.0284      0.6899     0.7492     71.6606
03200     0.9638      0.7046     0.7518     72.1545
03300     0.8587      0.7384     0.7518     71.6695
03400     0.9040      0.7278     0.7518     71.9086
03500     1.1272      0.6456     0.7518     72.8479
03600     0.8977      0.7131     0.7518     72.4377
03700     1.0047      0.6899     0.7518     72.6814
03800     0.9502      0.7173     0.7518     72.5696
03900     0.9198      0.7068     0.7518     72.4268
04000     0.9717      0.7173     0.7518     72.9451
04100     1.0211      0.6772     0.7518     73.2395
04200     0.9605      0.7004     0.7518     72.3787
04300     0.9176      0.7152     0.7518     72.9685
04400     0.9551      0.7194     0.7518     71.4491
04500     0.9518      0.7004     0.7518     71.4464
04600     0.9129      0.7110     0.7518     72.1584
04700     1.0066      0.6878     0.7518     72.3198
04800     0.9020      0.7173     0.7518     72.3269
04900     0.9717      0.6899     0.7518     73.3459
05000     0.9433      0.7363     0.7518     72.7950
05100     0.9511      0.7110     0.7518     73.8910
05200     1.0165      0.7004     0.7518     72.8619
05300     0.9867      0.6941     0.7518     72.5917
05400     1.0479      0.6624     0.7518     73.5931
05500     0.9463      0.7173     0.7518     75.8004
05600     0.8879      0.7342     0.7518     70.4086
05700     0.9908      0.6878     0.7518     73.1266
05800     0.9818      0.6814     0.7518     72.6456
05900     1.0166      0.6878     0.7518     72.3451
06000     0.9060      0.7173     0.7518     71.5775
06100     1.0775      0.6667     0.7518     72.1037
06200     0.9444      0.7300     0.7562     72.6989
06300     1.0288      0.6899     0.7562     72.9298
06400     0.9583      0.7131     0.7562     74.9631
06500     0.9595      0.7110     0.7562     75.9626
06600     0.8903      0.7257     0.7562     74.2148
06700     1.0166      0.6793     0.7562     73.6607
06800     0.9837      0.6962     0.7562     72.6727
06900     0.8361      0.7722     0.7562     70.8450
07000     0.9062      0.7215     0.7562     72.6920
07100     0.9601      0.6983     0.7562     72.6769
07200     0.9729      0.7131     0.7562     71.6327
07300     1.0244      0.6793     0.7562     72.8158
07400     0.9401      0.7046     0.7562     72.5209
07500     0.7809      0.7743     0.7562     72.2254
07600     0.9700      0.6941     0.7562     71.4580
07700     0.9936      0.6920     0.7562     72.5315
07800     0.9235      0.7257     0.7562     71.5290
07900     0.9544      0.6983     0.7562     71.5110
08000     0.9067      0.7152     0.7562     70.7153
08100     0.9046      0.7068     0.7562     72.2470
08200     0.9391      0.7278     0.7562     71.0054
08300     0.8685      0.7489     0.7562     72.7189
08400     0.9352      0.7046     0.7562     73.5655
08500     0.9226      0.7384     0.7562     74.4904
08600     0.9737      0.6962     0.7562     74.3304
08700     0.9034      0.7489     0.7562     72.4965
08800     1.0099      0.6920     0.7562     72.7472
08900     0.9276      0.7342     0.7562     73.2084
09000     0.8893      0.7426     0.7562     72.9681
09100     0.9515      0.7068     0.7562     72.7544
09200     0.9667      0.6920     0.7562     71.1404
09300     0.9479      0.6899     0.7562     72.0800
09400     0.9805      0.6941     0.7562     72.2205
09500     0.9603      0.6962     0.7562     71.1656
09600     0.9645      0.7110     0.7562     71.3821
09700     1.0737      0.6624     0.7562     73.2327
09800     0.9521      0.7257     0.7562     72.8108
09900     0.9553      0.7004     0.7562     72.4963
Start testing:
Test Accuracy: 0.7235
