Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=7, quant_actNM=7, quant_inp=7, quant_w=7, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
8ddda952-8dcc-4e3c-8e5f-f5747e4f71cb
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, 1.)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 82, in forward
    if len(x_range) > 1:
TypeError: object of type 'float' has no len()
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=7, quant_actNM=7, quant_inp=7, quant_w=7, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
a2a556ff-f0ca-4d9e-88a6-d24279d85fdf
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, torch.tensor([1.]))
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 94, in forward
    x_scaled = x/x_range
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=7, quant_actNM=7, quant_inp=7, quant_w=7, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
b16486f3-4089-45e9-b219-98c5ca2b4ffe
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.9917      0.0865     0.0747     10.6717
00100     2.1947      0.2722     0.2989     72.8097
00200     1.8630      0.3903     0.4445     72.5682
00300     1.5532      0.5042     0.5287     74.4584
00400     1.3046      0.5823     0.5769     73.3850
00500     1.2707      0.6139     0.6198     74.7344
00600     1.1407      0.6245     0.6552     71.3208
00700     1.1130      0.6603     0.6899     71.9037
00800     1.0169      0.6709     0.7050     71.3673
00900     0.9748      0.7173     0.7159     71.9060
01000     0.9565      0.7025     0.7317     73.2324
01100     0.8906      0.7384     0.7451     71.1657
01200     0.9114      0.7046     0.7530     71.3826
01300     0.8900      0.7215     0.7530     72.7541
01400     0.8327      0.7595     0.7644     72.7266
01500     0.8024      0.7574     0.7657     70.9661
01600     0.8151      0.7553     0.7684     72.5211
01700     0.8654      0.7342     0.7684     73.4834
01800     0.8321      0.7300     0.7695     72.9311
01900     0.7392      0.7869     0.7781     73.1225
02000     0.7409      0.7574     0.7815     71.6291
02100     0.8391      0.7489     0.7820     73.6769
02200     0.8137      0.7595     0.7820     73.3440
02300     0.6718      0.8186     0.7820     72.1230
02400     0.7576      0.7553     0.7820     73.6500
02500     0.7848      0.7658     0.7842     72.3698
02600     0.7468      0.7806     0.7842     72.0941
02700     0.7012      0.7954     0.7857     72.4784
02800     0.7234      0.7996     0.7890     73.6232
02900     0.8027      0.7637     0.7890     71.7660
03000     0.6808      0.7911     0.7890     71.9006
03100     0.7602      0.7637     0.7963     71.9281
03200     0.7192      0.7890     0.7963     72.0330
03300     0.6572      0.8017     0.7972     73.7654
03400     0.7559      0.7848     0.7991     74.0208
03500     0.8164      0.7553     0.7991     72.9503
03600     0.6544      0.8017     0.7991     71.3861
03700     0.6885      0.7806     0.8087     73.8462
03800     0.6110      0.8228     0.8087     72.0310
03900     0.7244      0.8017     0.8087     72.4219
04000     0.7961      0.7511     0.8087     72.4854
04100     0.6839      0.8059     0.8087     72.9233
04200     0.6665      0.7996     0.8087     72.5907
04300     0.6749      0.8017     0.8087     72.4776
04400     0.7389      0.7764     0.8087     72.3970
04500     0.7287      0.7848     0.8087     72.0751
04600     0.7554      0.7658     0.8087     73.2519
04700     0.6627      0.7869     0.8087     73.6902
04800     0.7236      0.7722     0.8087     74.8420
04900     0.7098      0.7932     0.8087     75.0900
05000     0.6807      0.8038     0.8087     75.7694
05100     0.6593      0.7932     0.8087     76.6450
05200     0.7703      0.7700     0.8087     74.8558
05300     0.6499      0.8249     0.8087     72.9287
05400     0.6532      0.8017     0.8087     73.3031
05500     0.6538      0.7996     0.8087     74.1990
05600     0.7656      0.7785     0.8087     73.0234
05700     0.6857      0.7827     0.8093     72.1001
05800     0.7239      0.7869     0.8093     74.1208
05900     0.6046      0.8186     0.8093     74.8937
06000     0.6763      0.8143     0.8160     73.3045
06100     0.6925      0.7806     0.8160     73.5047
06200     0.6505      0.8143     0.8160     73.0436
06300     0.6037      0.8291     0.8160     71.6510
06400     0.7259      0.7700     0.8160     74.5851
06500     0.6749      0.7996     0.8160     72.3874
06600     0.7707      0.7827     0.8160     73.7049
06700     0.7181      0.8017     0.8160     74.2595
06800     0.6816      0.7954     0.8160     74.2752
06900     0.6540      0.7996     0.8160     74.6594
07000     0.6784      0.8165     0.8160     72.8176
07100     0.7217      0.7869     0.8160     70.6044
07200     0.7430      0.7679     0.8160     73.9691
07300     0.6085      0.8101     0.8160     72.3085
07400     0.6944      0.7911     0.8160     72.4730
07500     0.6484      0.8165     0.8160     70.2889
07600     0.6544      0.8038     0.8210     73.8654
07700     0.6236      0.7996     0.8210     74.9735
07800     0.6439      0.8122     0.8210     71.7809
07900     0.6829      0.8059     0.8210     72.9166
08000     0.5995      0.8186     0.8229     72.0195
08100     0.7574      0.7743     0.8229     73.4723
08200     0.6203      0.8122     0.8237     73.4289
08300     0.7022      0.7806     0.8237     74.8045
08400     0.7108      0.7869     0.8237     72.7466
08500     0.6255      0.8017     0.8237     73.1067
08600     0.7117      0.7932     0.8237     73.4625
08700     0.6578      0.7996     0.8237     72.7179
08800     0.7316      0.7806     0.8237     72.2135
08900     0.5954      0.8080     0.8237     71.3376
09000     0.6928      0.7911     0.8237     73.6207
09100     0.6112      0.8333     0.8237     74.4088
09200     0.7337      0.7827     0.8237     73.3255
09300     0.6935      0.7658     0.8237     73.1768
09400     0.6806      0.7932     0.8237     73.7420
09500     0.6756      0.7785     0.8237     71.1358
09600     0.6570      0.8186     0.8237     71.8782
09700     0.6520      0.8122     0.8237     74.6289
09800     0.5921      0.8291     0.8237     71.9701
09900     0.6894      0.8038     0.8237     72.5246
10000     0.6838      0.7911     0.8237     72.4203
10100     0.6026      0.8207     0.8237     74.0557
10200     0.6132      0.8376     0.8237     72.9427
10300     0.5924      0.8312     0.8237     72.5066
10400     0.6640      0.7848     0.8237     72.8582
10500     0.6141      0.8312     0.8237     73.3546
10600     0.6416      0.8017     0.8237     73.3199
10700     0.5725      0.8186     0.8237     72.7283
10800     0.6896      0.7827     0.8237     72.8695
10900     0.6065      0.8333     0.8237     73.0627
11000     0.6063      0.8186     0.8237     72.1923
11100     0.5108      0.8333     0.8237     72.2130
11200     0.5721      0.8291     0.8237     72.4020
11300     0.5417      0.8270     0.8247     73.3811
11400     0.5770      0.8291     0.8247     72.6969
11500     0.6769      0.7932     0.8247     72.4323
11600     0.7058      0.7975     0.8247     71.2172
11700     0.7469      0.7764     0.8247     72.8553
11800     0.6316      0.8165     0.8247     72.9310
11900     0.7270      0.7658     0.8248     71.9724
12000     0.6149      0.8186     0.8248     75.5518
12100     0.6690      0.8101     0.8248     73.5997
12200     0.6598      0.8038     0.8248     73.5236
12300     0.6200      0.8143     0.8248     72.5499
12400     0.6371      0.8228     0.8248     73.0764
12500     0.6728      0.7996     0.8248     72.4400
12600     0.6541      0.8186     0.8248     71.6997
12700     0.6776      0.7785     0.8248     71.7049
12800     0.6528      0.8080     0.8248     73.0536
12900     0.5807      0.8418     0.8248     71.9247
13000     0.5751      0.8312     0.8248     73.7429
13100     0.6185      0.8165     0.8248     73.7772
13200     0.6220      0.8059     0.8248     73.1458
13300     0.5874      0.8101     0.8248     72.4694
13400     0.6353      0.8017     0.8248     74.6824
13500     0.6293      0.8080     0.8248     72.4255
13600     0.5836      0.8354     0.8248     75.7579
13700     0.6617      0.8059     0.8248     72.7462
13800     0.6447      0.8038     0.8248     72.9571
13900     0.6285      0.8101     0.8248     73.2954
14000     0.5471      0.8460     0.8248     72.6580
14100     0.6664      0.7975     0.8248     72.6696
14200     0.5501      0.8502     0.8248     73.7761
14300     0.6004      0.8143     0.8248     72.6012
14400     0.5388      0.8439     0.8248     72.3569
14500     0.6353      0.8270     0.8248     74.1330
14600     0.7096      0.7785     0.8248     74.6630
14700     0.5801      0.8291     0.8248     71.6554
14800     0.5923      0.8228     0.8248     71.9534
14900     0.5712      0.8354     0.8248     73.6499
15000     0.6040      0.8249     0.8315     75.2930
15100     0.6151      0.8291     0.8315     72.7073
15200     0.6340      0.8122     0.8315     73.1767
15300     0.6253      0.8122     0.8315     72.5242
15400     0.6898      0.7996     0.8315     72.8979
15500     0.5783      0.8376     0.8315     72.6564
15600     0.5499      0.8502     0.8315     72.2353
15700     0.6740      0.8059     0.8315     72.7228
15800     0.5692      0.8291     0.8315     72.7194
15900     0.6726      0.7932     0.8315     76.1548
16000     0.5689      0.8207     0.8315     72.8875
16100     0.6454      0.8017     0.8315     72.9771
16200     0.6267      0.8143     0.8315     74.0743
16300     0.6023      0.8143     0.8315     73.1545
16400     0.6689      0.8228     0.8315     71.5966
16500     0.5296      0.8418     0.8315     72.8734
16600     0.5766      0.8333     0.8315     71.3173
16700     0.5059      0.8523     0.8315     71.8777
16800     0.5843      0.8249     0.8315     71.6620
16900     0.6385      0.8101     0.8315     72.8862
17000     0.6035      0.8017     0.8315     73.2021
17100     0.6668      0.7975     0.8315     71.9665
17200     0.6052      0.8228     0.8315     71.8133
17300     0.6107      0.8165     0.8315     73.4281
17400     0.7033      0.7848     0.8315     71.2796
17500     0.5113      0.8586     0.8315     72.1093
17600     0.6419      0.7890     0.8315     72.6446
17700     0.6077      0.8439     0.8315     72.8956
17800     0.5870      0.8017     0.8315     71.9368
17900     0.6361      0.7932     0.8315     74.8245
18000     0.6013      0.8291     0.8315     71.7357
18100     0.6515      0.7975     0.8315     71.9829
18200     0.7130      0.7932     0.8315     71.7864
18300     0.6347      0.8059     0.8315     72.2513
18400     0.5608      0.8354     0.8315     73.0744
18500     0.6811      0.7743     0.8315     72.7664
18600     0.6453      0.8080     0.8315     73.5224
18700     0.6402      0.8186     0.8315     73.8969
18800     0.5943      0.8270     0.8315     72.8613
18900     0.6943      0.7869     0.8315     72.4336
19000     0.5966      0.8207     0.8315     71.9395
19100     0.6310      0.8059     0.8315     72.4236
19200     0.6567      0.8059     0.8315     74.2309
19300     0.5749      0.8059     0.8315     73.0823
19400     0.6569      0.8165     0.8315     71.1987
19500     0.6066      0.8165     0.8315     72.7312
19600     0.6286      0.8122     0.8315     71.4233
19700     0.6075      0.8059     0.8315     74.1734
19800     0.6242      0.7869     0.8315     73.7067
19900     0.5700      0.8249     0.8315     72.9591
20000     0.5993      0.8080     0.8315     75.6046
20100     0.6705      0.7932     0.8315     73.5472
20199     0.5514      0.8481     0.8315     70.7977
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     0.4526      0.8692     0.8203     9.9976
00100     0.4607      0.8629     0.8310     72.6195
00200     0.4823      0.8397     0.8310     73.5672
00300     0.4454      0.8586     0.8310     73.9040
00400     0.5311      0.8460     0.8310     72.5991
00500     0.5239      0.8439     0.8310     72.8054
00600     0.5469      0.8481     0.8310     72.7147
00700     0.5683      0.8481     0.8310     73.0953
00800     0.5138      0.8354     0.8310     71.4483
00900     0.6310      0.8080     0.8310     72.1510
01000     0.5829      0.8165     0.8310     72.3991
01100     0.5462      0.8481     0.8310     71.2458
01200     0.5684      0.8312     0.8310     72.2080
01300     0.5846      0.8312     0.8310     73.1911
01400     0.5305      0.8270     0.8310     72.4718
01500     0.4789      0.8608     0.8310     72.7096
01600     0.5060      0.8502     0.8310     73.0786
01700     0.4960      0.8544     0.8316     72.1191
01800     0.4303      0.8797     0.8316     72.7135
01900     0.4910      0.8418     0.8316     73.4987
02000     0.5097      0.8650     0.8316     73.0937
02100     0.5502      0.8354     0.8316     72.5398
02200     0.5234      0.8460     0.8316     72.8494
02300     0.5437      0.8270     0.8316     72.7806
02400     0.5055      0.8544     0.8316     73.4905
02500     0.5150      0.8460     0.8316     71.5899
02600     0.4959      0.8418     0.8316     70.5692
02700     0.5159      0.8354     0.8316     71.2724
02800     0.5383      0.8460     0.8316     72.9790
02900     0.4807      0.8671     0.8316     73.0125
03000     0.5039      0.8397     0.8316     73.1679
03100     0.5878      0.8165     0.8316     71.7857
03200     0.5054      0.8544     0.8316     72.1682
03300     0.4814      0.8650     0.8316     73.8400
03400     0.5379      0.8544     0.8316     72.6105
03500     0.5039      0.8586     0.8316     73.1090
03600     0.4637      0.8523     0.8318     74.1787
03700     0.5401      0.8439     0.8318     71.0452
03800     0.5240      0.8523     0.8318     72.0977
03900     0.5768      0.8291     0.8318     73.4023
04000     0.5843      0.8333     0.8318     71.9733
04100     0.4998      0.8481     0.8318     74.2358
04200     0.5984      0.8354     0.8318     74.9611
04300     0.5571      0.8397     0.8318     74.5040
04400     0.5436      0.8418     0.8318     72.8700
04500     0.5209      0.8418     0.8318     74.5423
04600     0.5417      0.8376     0.8318     74.0193
04700     0.5474      0.8544     0.8318     73.9041
04800     0.4813      0.8629     0.8318     74.1014
04900     0.5691      0.8333     0.8318     74.0378
05000     0.6235      0.8270     0.8318     74.1896
05100     0.5919      0.8228     0.8318     73.1354
05200     0.4566      0.8565     0.8318     72.5799
05300     0.5129      0.8418     0.8318     74.8578
05400     0.5751      0.8354     0.8318     75.3296
05500     0.5901      0.8143     0.8318     74.8195
05600     0.5109      0.8502     0.8318     74.6510
05700     0.4583      0.8671     0.8318     73.6738
05800     0.5650      0.8122     0.8318     72.7418
05900     0.5426      0.8207     0.8326     74.0830
06000     0.6077      0.8059     0.8326     72.8536
06100     0.4715      0.8502     0.8326     72.8230
06200     0.4812      0.8544     0.8326     72.2927
06300     0.5101      0.8502     0.8326     73.1639
06400     0.5469      0.8354     0.8326     72.3492
06500     0.5624      0.8333     0.8326     73.1491
06600     0.5893      0.8333     0.8326     72.9522
06700     0.4887      0.8481     0.8326     75.0823
06800     0.4804      0.8629     0.8326     73.4530
06900     0.5182      0.8608     0.8326     73.3434
07000     0.5033      0.8608     0.8326     74.1148
07100     0.5274      0.8523     0.8326     74.2596
07200     0.5569      0.8186     0.8326     73.1762
07300     0.5213      0.8460     0.8326     73.3691
07400     0.4892      0.8502     0.8326     75.1027
07500     0.5804      0.8312     0.8326     74.4069
07600     0.5092      0.8481     0.8326     74.8045
07700     0.5422      0.8354     0.8326     73.1278
07800     0.4955      0.8523     0.8326     71.8827
07900     0.4702      0.8713     0.8326     71.8636
08000     0.5659      0.8249     0.8326     72.9526
08100     0.3845      0.8966     0.8326     73.2982
08200     0.5052      0.8481     0.8326     72.8347
08300     0.5223      0.8439     0.8326     73.8929
08400     0.5132      0.8586     0.8326     72.0608
08500     0.5833      0.8143     0.8326     71.5070
08600     0.6761      0.8122     0.8332     73.6996
08700     0.5777      0.8354     0.8332     72.3239
08800     0.5663      0.8418     0.8332     73.9732
08900     0.5959      0.8312     0.8332     70.5670
09000     0.5315      0.8565     0.8332     72.1216
09100     0.5342      0.8481     0.8332     72.2877
09200     0.5353      0.8544     0.8341     72.2332
09300     0.4807      0.8629     0.8341     73.0247
09400     0.5733      0.8228     0.8341     71.3566
09500     0.5239      0.8481     0.8341     74.0030
09600     0.4964      0.8629     0.8341     72.5561
09700     0.5292      0.8523     0.8341     73.7341
09800     0.5873      0.8143     0.8341     72.9871
09900     0.4438      0.8629     0.8341     74.7748
Start testing:
Test Accuracy: 0.8221
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=7, quant_actNM=7, quant_inp=7, quant_w=7, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
499d80fb-376e-4973-90f0-a230a100fefc
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.9917      0.0865     0.0747     10.4458
00100     2.1947      0.2722     0.2989     71.8971
00200     1.8630      0.3903     0.4445     70.9748
00300     1.5532      0.5042     0.5287     73.2617
00400     1.3046      0.5823     0.5769     70.7538
00500     1.2707      0.6139     0.6198     70.5013
00600     1.1407      0.6245     0.6552     70.8531
00700     1.1130      0.6603     0.6899     70.5721
00800     1.0169      0.6709     0.7050     70.6852
00900     0.9748      0.7173     0.7159     70.3722
01000     0.9565      0.7025     0.7317     70.0011
01100     0.8906      0.7384     0.7451     70.5092
01200     0.9114      0.7046     0.7530     70.0810
01300     0.8900      0.7215     0.7530     70.2369
01400     0.8327      0.7595     0.7644     70.6453
01500     0.8024      0.7574     0.7657     70.0713
01600     0.8151      0.7553     0.7684     70.3473
01700     0.8654      0.7342     0.7684     69.5294
01800     0.8321      0.7300     0.7695     70.9238
01900     0.7392      0.7869     0.7781     70.2484
02000     0.7409      0.7574     0.7815     70.3547
02100     0.8391      0.7489     0.7820     69.9991
02200     0.8137      0.7595     0.7820     70.6730
02300     0.6718      0.8186     0.7820     70.7194
02400     0.7576      0.7553     0.7820     70.4426
02500     0.7848      0.7658     0.7842     70.4504
02600     0.7468      0.7806     0.7842     70.2664
02700     0.7012      0.7954     0.7857     70.9624
02800     0.7234      0.7996     0.7890     70.2863
02900     0.8027      0.7637     0.7890     69.8444
03000     0.6808      0.7911     0.7890     70.4331
03100     0.7602      0.7637     0.7963     70.6610
03200     0.7192      0.7890     0.7963     70.7401
03300     0.6572      0.8017     0.7972     69.9219
03400     0.7559      0.7848     0.7991     70.1653
03500     0.8164      0.7553     0.7991     72.6944
03600     0.6544      0.8017     0.7991     70.2107
03700     0.6885      0.7806     0.8087     70.3598
03800     0.6110      0.8228     0.8087     70.7297
03900     0.7244      0.8017     0.8087     70.8572
04000     0.7961      0.7511     0.8087     70.4367
04100     0.6839      0.8059     0.8087     69.8096
04200     0.6665      0.7996     0.8087     70.2504
04300     0.6749      0.8017     0.8087     70.8348
04400     0.7389      0.7764     0.8087     70.6239
04500     0.7287      0.7848     0.8087     70.1965
04600     0.7554      0.7658     0.8087     70.5407
04700     0.6627      0.7869     0.8087     70.2327
04800     0.7236      0.7722     0.8087     70.6021
04900     0.7098      0.7932     0.8087     70.2527
05000     0.6807      0.8038     0.8087     70.5927
05100     0.6593      0.7932     0.8087     69.9668
05200     0.7703      0.7700     0.8087     70.7743
05300     0.6499      0.8249     0.8087     70.6120
05400     0.6532      0.8017     0.8087     70.7644
05500     0.6538      0.7996     0.8087     70.6935
05600     0.7656      0.7785     0.8087     71.1948
05700     0.6857      0.7827     0.8093     70.6142
05800     0.7239      0.7869     0.8093     70.6151
05900     0.6046      0.8186     0.8093     71.1787
06000     0.6763      0.8143     0.8160     70.1711
06100     0.6925      0.7806     0.8160     70.6794
06200     0.6505      0.8143     0.8160     69.7956
06300     0.6037      0.8291     0.8160     70.2104
06400     0.7259      0.7700     0.8160     70.5670
06500     0.6749      0.7996     0.8160     70.2658
06600     0.7707      0.7827     0.8160     70.4370
06700     0.7181      0.8017     0.8160     70.7662
06800     0.6816      0.7954     0.8160     70.5547
06900     0.6540      0.7996     0.8160     70.4809
07000     0.6784      0.8165     0.8160     70.5304
07100     0.7217      0.7869     0.8160     70.7308
07200     0.7430      0.7679     0.8160     70.2636
07300     0.6085      0.8101     0.8160     70.6958
07400     0.6944      0.7911     0.8160     70.3882
07500     0.6484      0.8165     0.8160     70.7674
07600     0.6544      0.8038     0.8210     70.2816
07700     0.6236      0.7996     0.8210     69.8075
07800     0.6439      0.8122     0.8210     70.2295
07900     0.6829      0.8059     0.8210     69.2120
08000     0.5995      0.8186     0.8229     70.5324
08100     0.7574      0.7743     0.8229     70.1340
08200     0.6203      0.8122     0.8237     70.6500
08300     0.7022      0.7806     0.8237     69.5772
08400     0.7108      0.7869     0.8237     71.5557
08500     0.6255      0.8017     0.8237     70.6814
08600     0.7117      0.7932     0.8237     70.2186
08700     0.6578      0.7996     0.8237     70.4166
08800     0.7316      0.7806     0.8237     70.7554
08900     0.5954      0.8080     0.8237     70.4891
09000     0.6928      0.7911     0.8237     69.9513
09100     0.6112      0.8333     0.8237     70.6855
09200     0.7337      0.7827     0.8237     70.0258
09300     0.6935      0.7658     0.8237     70.1258
09400     0.6806      0.7932     0.8237     70.8019
09500     0.6756      0.7785     0.8237     70.6943
09600     0.6570      0.8186     0.8237     70.6772
09700     0.6520      0.8122     0.8237     70.5329
09800     0.5921      0.8291     0.8237     70.2943
09900     0.6894      0.8038     0.8237     70.1772
10000     0.6838      0.7911     0.8237     70.9781
10100     0.6026      0.8207     0.8237     70.6945
10200     0.6132      0.8376     0.8237     70.9501
10300     0.5924      0.8312     0.8237     70.5354
10400     0.6640      0.7848     0.8237     71.0508
10500     0.6141      0.8312     0.8237     70.2090
10600     0.6416      0.8017     0.8237     70.2851
10700     0.5725      0.8186     0.8237     71.0990
10800     0.6896      0.7827     0.8237     70.2102
10900     0.6065      0.8333     0.8237     70.4192
11000     0.6063      0.8186     0.8237     70.3274
11100     0.5108      0.8333     0.8237     69.9183
11200     0.5721      0.8291     0.8237     69.7568
11300     0.5417      0.8270     0.8247     70.3575
11400     0.5770      0.8291     0.8247     70.6486
11500     0.6769      0.7932     0.8247     71.3413
11600     0.7058      0.7975     0.8247     70.8669
11700     0.7469      0.7764     0.8247     70.5178
11800     0.6316      0.8165     0.8247     70.5904
11900     0.7270      0.7658     0.8248     70.7243
12000     0.6149      0.8186     0.8248     70.7821
12100     0.6690      0.8101     0.8248     70.8666
12200     0.6598      0.8038     0.8248     70.7831
12300     0.6200      0.8143     0.8248     70.8562
12400     0.6371      0.8228     0.8248     70.5610
12500     0.6728      0.7996     0.8248     71.3361
12600     0.6541      0.8186     0.8248     71.1719
12700     0.6776      0.7785     0.8248     71.2471
12800     0.6528      0.8080     0.8248     71.2150
12900     0.5807      0.8418     0.8248     70.6659
13000     0.5751      0.8312     0.8248     70.2989
