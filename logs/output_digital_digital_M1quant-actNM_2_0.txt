Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 362, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.0, finetuning_epochs=0, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.0005,0.0001,0.00002', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,10000', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
2ee38fa6-e8ad-4cb6-b5f5-77f4a01efb01
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.5282      0.0844     0.0801     11.8678
00100     2.5208      0.0696     0.0803     76.3316
00200     2.5150      0.0781     0.0805     76.9573
00300     2.5107      0.0802     0.0813     77.1214
00400     2.5077      0.0802     0.0813     76.4431
00500     2.5056      0.0865     0.0813     76.6793
00600     2.5042      0.0802     0.0813     76.4131
00700     2.5032      0.0781     0.0813     76.1671
00800     2.5026      0.0823     0.0813     76.2449
00900     2.5021      0.0970     0.0813     76.3016
01000     2.5017      0.0844     0.0813     77.4287
01100     2.5014      0.0738     0.0813     77.2400
01200     2.5012      0.0675     0.0813     76.7033
01300     2.5010      0.0802     0.0813     76.0039
01400     2.5009      0.0781     0.0813     75.9946
01500     2.5008      0.0907     0.0813     76.1414
01600     2.5007      0.0802     0.0813     76.6016
01700     2.5006      0.0675     0.0813     76.1511
01800     2.5005      0.0802     0.0813     76.8708
01900     2.5005      0.0654     0.0813     77.1101
02000     2.5004      0.0738     0.0813     76.6204
02100     2.5003      0.0928     0.0813     76.2638
02200     2.5003      0.0992     0.0813     77.4514
02300     2.5003      0.0738     0.0813     75.8962
02400     2.5002      0.0949     0.0813     76.3869
02500     2.5002      0.1034     0.0813     76.1706
02600     2.5001      0.0675     0.0813     76.1409
02700     2.5001      0.0886     0.0813     76.0967
02800     2.5001      0.0949     0.0813     76.2798
02900     2.5001      0.0907     0.0813     76.3287
03000     2.5000      0.0907     0.0813     77.0759
03100     2.5000      0.0738     0.0813     76.3704
03200     2.5000      0.0886     0.0813     76.9730
03300     2.5000      0.0992     0.0813     76.2358
03400     2.4999      0.0759     0.0813     77.1989
03500     2.4999      0.0802     0.0813     76.4003
03600     2.4999      0.0886     0.0813     76.1886
03700     2.4999      0.0717     0.0813     75.9161
03800     2.4998      0.0675     0.0813     75.3872
03900     2.4998      0.0949     0.0813     76.1271
04000     2.4998      0.0802     0.0813     76.5992
04100     2.4998      0.0886     0.0813     76.8643
04200     2.4998      0.0696     0.0813     76.4687
04300     2.4997      0.0907     0.0813     76.9636
04400     2.4997      0.0907     0.0813     76.8342
04500     2.4997      0.0886     0.0813     76.1895
04600     2.4997      0.0738     0.0813     76.8318
04700     2.4997      0.0928     0.0813     77.6559
04800     2.4996      0.0865     0.0813     76.6153
04900     2.4996      0.0738     0.0813     76.7034
05000     2.4996      0.0717     0.0813     77.2115
05100     2.4996      0.0865     0.0813     76.9128
05200     2.4996      0.0738     0.0813     76.0955
05300     2.4995      0.0717     0.0813     76.4097
05400     2.4995      0.0907     0.0813     76.7159
05500     2.4995      0.0738     0.0813     76.0567
05600     2.4995      0.0949     0.0813     77.5348
05700     2.4995      0.0907     0.0813     76.0419
05800     2.4994      0.0717     0.0813     76.0175
05900     2.4994      0.0992     0.0813     76.7574
06000     2.4994      0.0591     0.0813     75.8335
06100     2.4994      0.0992     0.0813     76.3488
06200     2.4994      0.0844     0.0813     76.6504
06300     2.4993      0.0696     0.0813     75.9604
06400     2.4993      0.0928     0.0813     76.8784
06500     2.4993      0.0886     0.0813     76.7697
06600     2.4993      0.0781     0.0813     76.6167
06700     2.4993      0.0759     0.0813     76.7524
06800     2.4992      0.1034     0.0813     76.3611
06900     2.4992      0.0949     0.0813     77.1577
07000     2.4992      0.0802     0.0813     77.1419
07100     2.4992      0.0928     0.0813     76.0541
07200     2.4992      0.0696     0.0813     77.5703
07300     2.4992      0.0970     0.0813     76.9022
07400     2.4991      0.0591     0.0813     76.1214
07500     2.4991      0.0781     0.0813     76.3650
07600     2.4991      0.0759     0.0813     76.3371
07700     2.4991      0.0633     0.0813     75.8512
07800     2.4991      0.0844     0.0813     76.3241
07900     2.4990      0.0675     0.0813     76.2331
08000     2.4990      0.0823     0.0813     76.7485
08100     2.4990      0.1013     0.0813     76.6511
08200     2.4990      0.0633     0.0813     76.6236
08300     2.4990      0.0738     0.0813     76.7623
08400     2.4989      0.0633     0.0813     76.5675
08500     2.4989      0.0738     0.0813     76.3837
08600     2.4989      0.0844     0.0813     77.2163
08700     2.4989      0.0928     0.0813     75.8458
08800     2.4989      0.0633     0.0813     76.5041
08900     2.4989      0.0612     0.0813     77.0077
09000     2.4988      0.0738     0.0813     76.6128
09100     2.4988      0.0633     0.0813     77.3409
09200     2.4988      0.0570     0.0813     76.2042
09300     2.4988      0.0844     0.0813     76.4331
09400     2.4988      0.0823     0.0813     77.6769
09500     2.4987      0.0738     0.0813     77.2658
09600     2.4987      0.0696     0.0813     76.9708
09700     2.4987      0.0781     0.0813     76.7020
09800     2.4987      0.0591     0.0813     76.9509
09900     2.4987      0.0759     0.0813     76.5790
10000     2.4986      0.0759     0.0813     77.2051
10100     2.4986      0.0781     0.0813     76.3358
10200     2.4986      0.0865     0.0813     77.1099
10300     2.4986      0.0696     0.0813     76.5176
10400     2.4986      0.0928     0.0813     76.9923
10500     2.4986      0.0970     0.0813     76.8216
10600     2.4986      0.0844     0.0813     76.4620
10700     2.4986      0.0949     0.0813     76.9412
10800     2.4986      0.0675     0.0813     76.1523
10900     2.4986      0.0696     0.0813     75.4978
11000     2.4986      0.0992     0.0813     76.2864
11100     2.4986      0.0781     0.0813     76.0441
11200     2.4986      0.0738     0.0813     76.6349
11300     2.4986      0.0738     0.0813     76.4751
11400     2.4986      0.0654     0.0813     76.4927
11500     2.4986      0.0633     0.0813     77.2739
11600     2.4986      0.0886     0.0813     76.9882
11700     2.4986      0.0844     0.0813     76.4543
11800     2.4986      0.0823     0.0813     77.0546
11900     2.4986      0.0781     0.0813     77.3188
12000     2.4986      0.0759     0.0813     77.3504
12100     2.4986      0.0633     0.0813     77.0516
12200     2.4986      0.0802     0.0813     77.0307
12300     2.4986      0.0907     0.0813     77.6812
12400     2.4986      0.0654     0.0813     76.7293
12500     2.4986      0.0549     0.0813     76.8016
12600     2.4986      0.0886     0.0813     76.9139
12700     2.4985      0.0738     0.0813     77.0736
12800     2.4985      0.0717     0.0813     77.1287
12900     2.4985      0.0844     0.0813     77.1818
13000     2.4985      0.0907     0.0813     76.9612
13100     2.4985      0.0802     0.0813     77.4894
13200     2.4985      0.0907     0.0813     77.8435
13300     2.4985      0.0949     0.0813     77.1086
13400     2.4985      0.0823     0.0813     77.4774
13500     2.4985      0.0696     0.0813     76.8295
13600     2.4985      0.0802     0.0813     77.5340
13700     2.4985      0.0865     0.0813     76.6527
13800     2.4985      0.0886     0.0813     76.4192
13900     2.4985      0.0738     0.0813     77.6977
14000     2.4985      0.0949     0.0813     78.5234
14100     2.4985      0.0865     0.0813     77.0649
14200     2.4985      0.0865     0.0813     77.3921
14300     2.4985      0.0696     0.0813     77.7350
14400     2.4985      0.1034     0.0813     77.8000
14500     2.4985      0.0717     0.0813     77.2921
14600     2.4985      0.0738     0.0813     76.7497
14700     2.4985      0.0886     0.0813     77.5731
14800     2.4985      0.0696     0.0813     76.7672
14900     2.4985      0.0823     0.0813     77.0665
15000     2.4985      0.0844     0.0813     77.0849
15100     2.4985      0.1055     0.0813     77.0908
15200     2.4985      0.0717     0.0813     78.0353
15300     2.4985      0.0696     0.0813     76.7988
15400     2.4985      0.0970     0.0813     76.5532
15500     2.4984      0.0759     0.0813     77.6675
15600     2.4984      0.0928     0.0813     76.4192
15700     2.4984      0.0696     0.0813     77.4089
15800     2.4984      0.0696     0.0813     78.0259
15900     2.4984      0.0759     0.0813     77.5014
16000     2.4984      0.1160     0.0813     77.5803
16100     2.4984      0.0844     0.0813     76.9104
16200     2.4984      0.0907     0.0813     76.4376
16300     2.4984      0.0823     0.0813     77.2363
16400     2.4984      0.0886     0.0813     76.5780
16500     2.4984      0.0654     0.0813     76.7817
16600     2.4984      0.1034     0.0813     77.2878
16700     2.4984      0.0970     0.0813     76.8253
16800     2.4984      0.0696     0.0813     76.9752
16900     2.4984      0.0886     0.0813     76.9447
17000     2.4984      0.0886     0.0813     76.6860
17100     2.4984      0.0886     0.0813     77.3520
17200     2.4984      0.0549     0.0813     76.9410
17300     2.4984      0.0506     0.0813     77.3230
17400     2.4984      0.0823     0.0813     76.8084
17500     2.4984      0.0738     0.0813     76.6518
17600     2.4984      0.0802     0.0813     77.1762
17700     2.4984      0.0928     0.0813     77.1883
17800     2.4984      0.0949     0.0813     76.6055
17900     2.4984      0.0654     0.0813     77.2901
18000     2.4984      0.0928     0.0813     76.3498
18100     2.4984      0.0759     0.0813     76.9084
18200     2.4984      0.0612     0.0813     76.9292
18300     2.4983      0.0802     0.0813     76.4350
18400     2.4983      0.0823     0.0813     77.6474
18500     2.4983      0.0865     0.0813     77.1775
18600     2.4983      0.0992     0.0813     76.7563
18700     2.4983      0.0717     0.0813     77.5172
18800     2.4983      0.0738     0.0813     76.7982
18900     2.4983      0.0464     0.0813     77.3914
19000     2.4983      0.0886     0.0813     77.4345
19100     2.4983      0.0527     0.0813     77.2891
19200     2.4983      0.0886     0.0813     77.5920
19300     2.4983      0.0928     0.0813     77.0618
19400     2.4983      0.0781     0.0813     76.9429
19500     2.4983      0.0865     0.0813     77.8598
19600     2.4983      0.0802     0.0813     77.2936
19700     2.4983      0.0865     0.0813     76.5668
19800     2.4983      0.0844     0.0813     77.3361
19900     2.4983      0.0654     0.0813     77.4424
20000     2.4983      0.0717     0.0813     77.1856
20100     2.4983      0.0886     0.0813     76.9861
20200     2.4983      0.0717     0.0813     76.4029
20300     2.4983      0.0802     0.0817     77.6998
20400     2.4983      0.0675     0.0817     77.1008
20500     2.4983      0.0907     0.0817     76.3798
20600     2.4983      0.0781     0.0817     77.1240
20700     2.4983      0.0612     0.0817     76.7491
20800     2.4983      0.0717     0.0817     77.3641
20900     2.4983      0.0717     0.0817     77.1111
21000     2.4983      0.0865     0.0817     77.4863
21100     2.4983      0.0675     0.0817     76.9825
21200     2.4983      0.0781     0.0817     76.6120
21300     2.4983      0.0633     0.0817     76.3199
21400     2.4983      0.0802     0.0817     77.2428
21500     2.4983      0.0759     0.0817     77.0338
21600     2.4983      0.0738     0.0817     78.1995
21700     2.4983      0.0970     0.0817     77.2925
21800     2.4983      0.0823     0.0817     77.3669
21900     2.4983      0.1076     0.0817     77.4978
22000     2.4983      0.0591     0.0817     77.1122
22100     2.4983      0.0759     0.0817     77.1786
22200     2.4983      0.0865     0.0817     77.3864
22300     2.4983      0.0738     0.0817     76.9296
22400     2.4983      0.0633     0.0817     77.8118
22500     2.4983      0.0781     0.0817     77.4722
22600     2.4983      0.0759     0.0817     77.1330
22700     2.4983      0.0781     0.0817     78.1276
22800     2.4983      0.0612     0.0817     77.5404
22900     2.4983      0.0802     0.0817     76.8574
23000     2.4983      0.1034     0.0817     77.3656
23100     2.4983      0.0570     0.0817     76.9677
23200     2.4983      0.0907     0.0817     77.8749
23300     2.4983      0.0717     0.0817     76.9661
23400     2.4983      0.0823     0.0817     76.7705
23500     2.4983      0.0781     0.0817     77.2529
23600     2.4983      0.0696     0.0817     76.9373
23700     2.4983      0.0675     0.0817     76.4809
23800     2.4983      0.0696     0.0817     77.5283
23900     2.4983      0.0823     0.0817     77.4071
24000     2.4983      0.0696     0.0817     77.6316
24100     2.4983      0.0907     0.0817     76.5481
24200     2.4983      0.0717     0.0817     77.0629
24300     2.4983      0.0570     0.0817     77.5667
24400     2.4983      0.0781     0.0817     77.3223
24500     2.4982      0.0781     0.0817     76.7789
24600     2.4982      0.0781     0.0817     76.5942
24700     2.4982      0.0844     0.0817     76.6784
24800     2.4982      0.0696     0.0817     77.5542
24900     2.4982      0.0738     0.0817     76.9851
25000     2.4982      0.0696     0.0817     77.1058
25100     2.4982      0.0717     0.0817     78.2220
25200     2.4982      0.0802     0.0817     77.6834
25300     2.4982      0.0675     0.0817     78.0461
25400     2.4982      0.0759     0.0817     77.4027
25500     2.4982      0.0802     0.0817     77.4494
25600     2.4982      0.0928     0.0817     77.8816
25700     2.4982      0.0696     0.0817     77.1538
25800     2.4982      0.0633     0.0817     77.1413
25900     2.4982      0.0591     0.0817     77.1180
26000     2.4982      0.1055     0.0817     76.4161
26100     2.4982      0.0844     0.0817     77.5277
26200     2.4982      0.0549     0.0817     77.6525
26300     2.4982      0.0738     0.0817     77.5027
26400     2.4982      0.1055     0.0817     77.7035
26500     2.4982      0.0738     0.0817     77.6323
26600     2.4982      0.0675     0.0817     77.8955
26700     2.4982      0.0823     0.0817     78.1637
26800     2.4982      0.0570     0.0817     77.4560
26900     2.4982      0.0865     0.0817     77.0704
27000     2.4982      0.0696     0.0817     77.5239
27100     2.4982      0.0949     0.0817     77.7827
27200     2.4982      0.0823     0.0817     77.7404
27300     2.4982      0.0823     0.0817     77.5875
27400     2.4982      0.0759     0.0817     76.9096
27500     2.4982      0.0802     0.0817     77.9505
27600     2.4982      0.0633     0.0817     77.3791
27700     2.4982      0.0759     0.0817     77.3980
27800     2.4982      0.0612     0.0817     78.2008
27900     2.4982      0.0696     0.0817     76.9919
28000     2.4982      0.0591     0.0817     77.8152
28100     2.4982      0.0949     0.0817     76.7237
28200     2.4982      0.1013     0.0817     76.5384
28300     2.4982      0.0802     0.0817     76.2106
28400     2.4982      0.0570     0.0817     76.1689
28500     2.4982      0.0865     0.0817     76.9471
28600     2.4982      0.0717     0.0817     76.7804
28700     2.4982      0.0675     0.0817     76.1332
28800     2.4982      0.0907     0.0817     77.0905
28900     2.4982      0.0928     0.0817     76.6319
29000     2.4982      0.0717     0.0817     76.6906
29100     2.4982      0.0823     0.0817     77.1040
29200     2.4982      0.0928     0.0817     77.3688
29300     2.4982      0.0633     0.0817     77.6637
29400     2.4982      0.0527     0.0817     77.5047
29500     2.4982      0.0802     0.0817     77.8206
29600     2.4982      0.0717     0.0817     77.8145
29700     2.4982      0.0738     0.0817     78.7066
29800     2.4982      0.0675     0.0817     77.5473
29900     2.4982      0.0759     0.0817     76.0164
29999     2.4982      0.0949     0.0817     76.0448
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Start testing:
Test Accuracy: 0.0789
