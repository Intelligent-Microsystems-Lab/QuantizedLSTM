Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 362, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.0, finetuning_epochs=0, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.0005,0.0001,0.00002', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,10000', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
2ee38fa6-e8ad-4cb6-b5f5-77f4a01efb01
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.5282      0.0844     0.0801     11.8678
00100     2.5208      0.0696     0.0803     76.3316
00200     2.5150      0.0781     0.0805     76.9573
00300     2.5107      0.0802     0.0813     77.1214
00400     2.5077      0.0802     0.0813     76.4431
00500     2.5056      0.0865     0.0813     76.6793
00600     2.5042      0.0802     0.0813     76.4131
00700     2.5032      0.0781     0.0813     76.1671
00800     2.5026      0.0823     0.0813     76.2449
00900     2.5021      0.0970     0.0813     76.3016
01000     2.5017      0.0844     0.0813     77.4287
01100     2.5014      0.0738     0.0813     77.2400
01200     2.5012      0.0675     0.0813     76.7033
01300     2.5010      0.0802     0.0813     76.0039
01400     2.5009      0.0781     0.0813     75.9946
01500     2.5008      0.0907     0.0813     76.1414
01600     2.5007      0.0802     0.0813     76.6016
01700     2.5006      0.0675     0.0813     76.1511
01800     2.5005      0.0802     0.0813     76.8708
01900     2.5005      0.0654     0.0813     77.1101
02000     2.5004      0.0738     0.0813     76.6204
02100     2.5003      0.0928     0.0813     76.2638
02200     2.5003      0.0992     0.0813     77.4514
02300     2.5003      0.0738     0.0813     75.8962
02400     2.5002      0.0949     0.0813     76.3869
02500     2.5002      0.1034     0.0813     76.1706
02600     2.5001      0.0675     0.0813     76.1409
02700     2.5001      0.0886     0.0813     76.0967
02800     2.5001      0.0949     0.0813     76.2798
02900     2.5001      0.0907     0.0813     76.3287
03000     2.5000      0.0907     0.0813     77.0759
03100     2.5000      0.0738     0.0813     76.3704
03200     2.5000      0.0886     0.0813     76.9730
03300     2.5000      0.0992     0.0813     76.2358
03400     2.4999      0.0759     0.0813     77.1989
03500     2.4999      0.0802     0.0813     76.4003
03600     2.4999      0.0886     0.0813     76.1886
03700     2.4999      0.0717     0.0813     75.9161
03800     2.4998      0.0675     0.0813     75.3872
03900     2.4998      0.0949     0.0813     76.1271
04000     2.4998      0.0802     0.0813     76.5992
04100     2.4998      0.0886     0.0813     76.8643
04200     2.4998      0.0696     0.0813     76.4687
04300     2.4997      0.0907     0.0813     76.9636
04400     2.4997      0.0907     0.0813     76.8342
04500     2.4997      0.0886     0.0813     76.1895
04600     2.4997      0.0738     0.0813     76.8318
04700     2.4997      0.0928     0.0813     77.6559
04800     2.4996      0.0865     0.0813     76.6153
04900     2.4996      0.0738     0.0813     76.7034
05000     2.4996      0.0717     0.0813     77.2115
05100     2.4996      0.0865     0.0813     76.9128
05200     2.4996      0.0738     0.0813     76.0955
05300     2.4995      0.0717     0.0813     76.4097
05400     2.4995      0.0907     0.0813     76.7159
05500     2.4995      0.0738     0.0813     76.0567
05600     2.4995      0.0949     0.0813     77.5348
05700     2.4995      0.0907     0.0813     76.0419
05800     2.4994      0.0717     0.0813     76.0175
05900     2.4994      0.0992     0.0813     76.7574
06000     2.4994      0.0591     0.0813     75.8335
06100     2.4994      0.0992     0.0813     76.3488
06200     2.4994      0.0844     0.0813     76.6504
06300     2.4993      0.0696     0.0813     75.9604
06400     2.4993      0.0928     0.0813     76.8784
06500     2.4993      0.0886     0.0813     76.7697
06600     2.4993      0.0781     0.0813     76.6167
06700     2.4993      0.0759     0.0813     76.7524
06800     2.4992      0.1034     0.0813     76.3611
06900     2.4992      0.0949     0.0813     77.1577
07000     2.4992      0.0802     0.0813     77.1419
07100     2.4992      0.0928     0.0813     76.0541
07200     2.4992      0.0696     0.0813     77.5703
07300     2.4992      0.0970     0.0813     76.9022
07400     2.4991      0.0591     0.0813     76.1214
07500     2.4991      0.0781     0.0813     76.3650
07600     2.4991      0.0759     0.0813     76.3371
07700     2.4991      0.0633     0.0813     75.8512
07800     2.4991      0.0844     0.0813     76.3241
07900     2.4990      0.0675     0.0813     76.2331
08000     2.4990      0.0823     0.0813     76.7485
08100     2.4990      0.1013     0.0813     76.6511
08200     2.4990      0.0633     0.0813     76.6236
08300     2.4990      0.0738     0.0813     76.7623
08400     2.4989      0.0633     0.0813     76.5675
08500     2.4989      0.0738     0.0813     76.3837
08600     2.4989      0.0844     0.0813     77.2163
08700     2.4989      0.0928     0.0813     75.8458
08800     2.4989      0.0633     0.0813     76.5041
08900     2.4989      0.0612     0.0813     77.0077
09000     2.4988      0.0738     0.0813     76.6128
09100     2.4988      0.0633     0.0813     77.3409
09200     2.4988      0.0570     0.0813     76.2042
09300     2.4988      0.0844     0.0813     76.4331
09400     2.4988      0.0823     0.0813     77.6769
09500     2.4987      0.0738     0.0813     77.2658
09600     2.4987      0.0696     0.0813     76.9708
09700     2.4987      0.0781     0.0813     76.7020
09800     2.4987      0.0591     0.0813     76.9509
09900     2.4987      0.0759     0.0813     76.5790
10000     2.4986      0.0759     0.0813     77.2051
10100     2.4986      0.0781     0.0813     76.3358
10200     2.4986      0.0865     0.0813     77.1099
10300     2.4986      0.0696     0.0813     76.5176
10400     2.4986      0.0928     0.0813     76.9923
10500     2.4986      0.0970     0.0813     76.8216
10600     2.4986      0.0844     0.0813     76.4620
10700     2.4986      0.0949     0.0813     76.9412
10800     2.4986      0.0675     0.0813     76.1523
10900     2.4986      0.0696     0.0813     75.4978
11000     2.4986      0.0992     0.0813     76.2864
11100     2.4986      0.0781     0.0813     76.0441
11200     2.4986      0.0738     0.0813     76.6349
11300     2.4986      0.0738     0.0813     76.4751
11400     2.4986      0.0654     0.0813     76.4927
11500     2.4986      0.0633     0.0813     77.2739
11600     2.4986      0.0886     0.0813     76.9882
11700     2.4986      0.0844     0.0813     76.4543
11800     2.4986      0.0823     0.0813     77.0546
11900     2.4986      0.0781     0.0813     77.3188
12000     2.4986      0.0759     0.0813     77.3504
12100     2.4986      0.0633     0.0813     77.0516
12200     2.4986      0.0802     0.0813     77.0307
12300     2.4986      0.0907     0.0813     77.6812
12400     2.4986      0.0654     0.0813     76.7293
12500     2.4986      0.0549     0.0813     76.8016
12600     2.4986      0.0886     0.0813     76.9139
12700     2.4985      0.0738     0.0813     77.0736
12800     2.4985      0.0717     0.0813     77.1287
12900     2.4985      0.0844     0.0813     77.1818
13000     2.4985      0.0907     0.0813     76.9612
13100     2.4985      0.0802     0.0813     77.4894
13200     2.4985      0.0907     0.0813     77.8435
13300     2.4985      0.0949     0.0813     77.1086
13400     2.4985      0.0823     0.0813     77.4774
13500     2.4985      0.0696     0.0813     76.8295
13600     2.4985      0.0802     0.0813     77.5340
13700     2.4985      0.0865     0.0813     76.6527
13800     2.4985      0.0886     0.0813     76.4192
13900     2.4985      0.0738     0.0813     77.6977
14000     2.4985      0.0949     0.0813     78.5234
14100     2.4985      0.0865     0.0813     77.0649
14200     2.4985      0.0865     0.0813     77.3921
14300     2.4985      0.0696     0.0813     77.7350
14400     2.4985      0.1034     0.0813     77.8000
14500     2.4985      0.0717     0.0813     77.2921
14600     2.4985      0.0738     0.0813     76.7497
14700     2.4985      0.0886     0.0813     77.5731
14800     2.4985      0.0696     0.0813     76.7672
14900     2.4985      0.0823     0.0813     77.0665
15000     2.4985      0.0844     0.0813     77.0849
15100     2.4985      0.1055     0.0813     77.0908
15200     2.4985      0.0717     0.0813     78.0353
15300     2.4985      0.0696     0.0813     76.7988
15400     2.4985      0.0970     0.0813     76.5532
15500     2.4984      0.0759     0.0813     77.6675
15600     2.4984      0.0928     0.0813     76.4192
15700     2.4984      0.0696     0.0813     77.4089
15800     2.4984      0.0696     0.0813     78.0259
15900     2.4984      0.0759     0.0813     77.5014
16000     2.4984      0.1160     0.0813     77.5803
16100     2.4984      0.0844     0.0813     76.9104
16200     2.4984      0.0907     0.0813     76.4376
16300     2.4984      0.0823     0.0813     77.2363
16400     2.4984      0.0886     0.0813     76.5780
16500     2.4984      0.0654     0.0813     76.7817
16600     2.4984      0.1034     0.0813     77.2878
16700     2.4984      0.0970     0.0813     76.8253
16800     2.4984      0.0696     0.0813     76.9752
16900     2.4984      0.0886     0.0813     76.9447
17000     2.4984      0.0886     0.0813     76.6860
17100     2.4984      0.0886     0.0813     77.3520
17200     2.4984      0.0549     0.0813     76.9410
17300     2.4984      0.0506     0.0813     77.3230
17400     2.4984      0.0823     0.0813     76.8084
17500     2.4984      0.0738     0.0813     76.6518
17600     2.4984      0.0802     0.0813     77.1762
17700     2.4984      0.0928     0.0813     77.1883
17800     2.4984      0.0949     0.0813     76.6055
17900     2.4984      0.0654     0.0813     77.2901
18000     2.4984      0.0928     0.0813     76.3498
18100     2.4984      0.0759     0.0813     76.9084
18200     2.4984      0.0612     0.0813     76.9292
18300     2.4983      0.0802     0.0813     76.4350
18400     2.4983      0.0823     0.0813     77.6474
18500     2.4983      0.0865     0.0813     77.1775
18600     2.4983      0.0992     0.0813     76.7563
18700     2.4983      0.0717     0.0813     77.5172
18800     2.4983      0.0738     0.0813     76.7982
18900     2.4983      0.0464     0.0813     77.3914
19000     2.4983      0.0886     0.0813     77.4345
19100     2.4983      0.0527     0.0813     77.2891
19200     2.4983      0.0886     0.0813     77.5920
19300     2.4983      0.0928     0.0813     77.0618
19400     2.4983      0.0781     0.0813     76.9429
19500     2.4983      0.0865     0.0813     77.8598
19600     2.4983      0.0802     0.0813     77.2936
19700     2.4983      0.0865     0.0813     76.5668
19800     2.4983      0.0844     0.0813     77.3361
19900     2.4983      0.0654     0.0813     77.4424
20000     2.4983      0.0717     0.0813     77.1856
20100     2.4983      0.0886     0.0813     76.9861
20200     2.4983      0.0717     0.0813     76.4029
20300     2.4983      0.0802     0.0817     77.6998
20400     2.4983      0.0675     0.0817     77.1008
20500     2.4983      0.0907     0.0817     76.3798
20600     2.4983      0.0781     0.0817     77.1240
20700     2.4983      0.0612     0.0817     76.7491
20800     2.4983      0.0717     0.0817     77.3641
20900     2.4983      0.0717     0.0817     77.1111
21000     2.4983      0.0865     0.0817     77.4863
21100     2.4983      0.0675     0.0817     76.9825
21200     2.4983      0.0781     0.0817     76.6120
21300     2.4983      0.0633     0.0817     76.3199
21400     2.4983      0.0802     0.0817     77.2428
21500     2.4983      0.0759     0.0817     77.0338
21600     2.4983      0.0738     0.0817     78.1995
21700     2.4983      0.0970     0.0817     77.2925
21800     2.4983      0.0823     0.0817     77.3669
21900     2.4983      0.1076     0.0817     77.4978
22000     2.4983      0.0591     0.0817     77.1122
22100     2.4983      0.0759     0.0817     77.1786
22200     2.4983      0.0865     0.0817     77.3864
22300     2.4983      0.0738     0.0817     76.9296
22400     2.4983      0.0633     0.0817     77.8118
22500     2.4983      0.0781     0.0817     77.4722
22600     2.4983      0.0759     0.0817     77.1330
22700     2.4983      0.0781     0.0817     78.1276
22800     2.4983      0.0612     0.0817     77.5404
22900     2.4983      0.0802     0.0817     76.8574
23000     2.4983      0.1034     0.0817     77.3656
23100     2.4983      0.0570     0.0817     76.9677
23200     2.4983      0.0907     0.0817     77.8749
23300     2.4983      0.0717     0.0817     76.9661
23400     2.4983      0.0823     0.0817     76.7705
23500     2.4983      0.0781     0.0817     77.2529
23600     2.4983      0.0696     0.0817     76.9373
23700     2.4983      0.0675     0.0817     76.4809
23800     2.4983      0.0696     0.0817     77.5283
23900     2.4983      0.0823     0.0817     77.4071
24000     2.4983      0.0696     0.0817     77.6316
24100     2.4983      0.0907     0.0817     76.5481
24200     2.4983      0.0717     0.0817     77.0629
24300     2.4983      0.0570     0.0817     77.5667
24400     2.4983      0.0781     0.0817     77.3223
24500     2.4982      0.0781     0.0817     76.7789
24600     2.4982      0.0781     0.0817     76.5942
24700     2.4982      0.0844     0.0817     76.6784
24800     2.4982      0.0696     0.0817     77.5542
24900     2.4982      0.0738     0.0817     76.9851
25000     2.4982      0.0696     0.0817     77.1058
25100     2.4982      0.0717     0.0817     78.2220
25200     2.4982      0.0802     0.0817     77.6834
25300     2.4982      0.0675     0.0817     78.0461
25400     2.4982      0.0759     0.0817     77.4027
25500     2.4982      0.0802     0.0817     77.4494
25600     2.4982      0.0928     0.0817     77.8816
25700     2.4982      0.0696     0.0817     77.1538
25800     2.4982      0.0633     0.0817     77.1413
25900     2.4982      0.0591     0.0817     77.1180
26000     2.4982      0.1055     0.0817     76.4161
26100     2.4982      0.0844     0.0817     77.5277
26200     2.4982      0.0549     0.0817     77.6525
26300     2.4982      0.0738     0.0817     77.5027
26400     2.4982      0.1055     0.0817     77.7035
26500     2.4982      0.0738     0.0817     77.6323
26600     2.4982      0.0675     0.0817     77.8955
26700     2.4982      0.0823     0.0817     78.1637
26800     2.4982      0.0570     0.0817     77.4560
26900     2.4982      0.0865     0.0817     77.0704
27000     2.4982      0.0696     0.0817     77.5239
27100     2.4982      0.0949     0.0817     77.7827
27200     2.4982      0.0823     0.0817     77.7404
27300     2.4982      0.0823     0.0817     77.5875
27400     2.4982      0.0759     0.0817     76.9096
27500     2.4982      0.0802     0.0817     77.9505
27600     2.4982      0.0633     0.0817     77.3791
27700     2.4982      0.0759     0.0817     77.3980
27800     2.4982      0.0612     0.0817     78.2008
27900     2.4982      0.0696     0.0817     76.9919
28000     2.4982      0.0591     0.0817     77.8152
28100     2.4982      0.0949     0.0817     76.7237
28200     2.4982      0.1013     0.0817     76.5384
28300     2.4982      0.0802     0.0817     76.2106
28400     2.4982      0.0570     0.0817     76.1689
28500     2.4982      0.0865     0.0817     76.9471
28600     2.4982      0.0717     0.0817     76.7804
28700     2.4982      0.0675     0.0817     76.1332
28800     2.4982      0.0907     0.0817     77.0905
28900     2.4982      0.0928     0.0817     76.6319
29000     2.4982      0.0717     0.0817     76.6906
29100     2.4982      0.0823     0.0817     77.1040
29200     2.4982      0.0928     0.0817     77.3688
29300     2.4982      0.0633     0.0817     77.6637
29400     2.4982      0.0527     0.0817     77.5047
29500     2.4982      0.0802     0.0817     77.8206
29600     2.4982      0.0717     0.0817     77.8145
29700     2.4982      0.0738     0.0817     78.7066
29800     2.4982      0.0675     0.0817     77.5473
29900     2.4982      0.0759     0.0817     76.0164
29999     2.4982      0.0949     0.0817     76.0448
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Start testing:
Test Accuracy: 0.0789
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
206fa635-057d-4521-bd65-460df6a65300
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.5282      0.0717     0.0807     12.7940
00100     2.5074      0.0570     0.0810     58.7632
00200     2.5025      0.0865     0.0810     58.3537
00300     2.5012      0.0823     0.0810     58.6898
00400     2.5007      0.0591     0.0810     58.1095
00500     2.5004      0.0907     0.0810     57.9227
00600     2.5002      0.0886     0.0810     58.6872
00700     2.5001      0.0970     0.0810     58.5013
00800     2.5000      0.0738     0.0810     59.1466
00900     2.4999      0.0717     0.0810     58.6026
01000     2.4998      0.0928     0.0810     58.5727
01100     2.4997      0.0759     0.0810     58.5413
01200     2.4997      0.0738     0.0810     58.6471
01300     2.4996      0.0907     0.0810     58.1068
01400     2.4995      0.0654     0.0810     58.6064
01500     2.4994      0.0781     0.0810     58.6816
01600     2.4993      0.0696     0.0810     58.7108
01700     2.4993      0.0949     0.0810     58.4249
01800     2.4992      0.0844     0.0810     58.4982
01900     2.4991      0.0738     0.0810     58.4960
02000     2.4990      0.0675     0.0810     58.2138
02100     2.4990      0.0970     0.0810     58.4660
02200     2.4989      0.0844     0.0810     59.0449
02300     2.4988      0.0844     0.0810     58.2249
02400     2.4987      0.0823     0.0810     58.8825
02500     2.4987      0.1097     0.0810     58.2112
02600     2.4986      0.0738     0.0810     58.1654
02700     2.4985      0.0970     0.0810     58.8439
02800     2.4984      0.0928     0.0810     58.1752
02900     2.4984      0.0865     0.0810     58.9556
03000     2.4983      0.0654     0.0810     58.8229
03100     2.4982      0.0759     0.0810     58.5693
03200     2.4982      0.0949     0.0810     58.7312
03300     2.4981      0.1076     0.0810     58.5640
03400     2.4980      0.0570     0.0810     58.1194
03500     2.4979      0.0633     0.0810     59.0723
03600     2.4979      0.0886     0.0810     58.2169
03700     2.4978      0.0823     0.0810     58.5896
03800     2.4977      0.0759     0.0810     58.9445
03900     2.4977      0.0759     0.0810     58.1685
04000     2.4976      0.0527     0.0810     58.7010
04100     2.4975      0.0654     0.0810     58.4432
04200     2.4975      0.0865     0.0810     58.5374
04300     2.4974      0.0802     0.0810     59.0600
04400     2.4973      0.0844     0.0810     58.7274
04500     2.4972      0.0802     0.0810     58.3657
04600     2.4972      0.0802     0.0810     59.2170
04700     2.4971      0.0527     0.0814     58.1886
04800     2.4970      0.0633     0.0814     58.6808
04900     2.4970      0.0992     0.0814     58.8030
05000     2.4969      0.1055     0.0814     58.7796
05100     2.4968      0.0781     0.0814     59.0529
05200     2.4968      0.0802     0.0814     58.6816
05300     2.4967      0.0823     0.0814     58.4924
05400     2.4966      0.0527     0.0814     58.5453
05500     2.4966      0.0738     0.0814     58.5320
05600     2.4965      0.0970     0.0814     58.9998
05700     2.4964      0.0612     0.0814     58.1634
05800     2.4964      0.0823     0.0814     58.5461
05900     2.4963      0.0675     0.0814     58.6915
06000     2.4962      0.0928     0.0814     58.8313
06100     2.4962      0.0759     0.0814     58.9353
06200     2.4961      0.0992     0.0814     58.9198
06300     2.4960      0.1076     0.0814     58.4353
06400     2.4960      0.0802     0.0814     58.5835
06500     2.4959      0.0654     0.0814     58.2958
06600     2.4958      0.0844     0.0814     59.0918
06700     2.4958      0.1076     0.0814     58.6997
06800     2.4957      0.0738     0.0814     58.5997
06900     2.4956      0.0886     0.0814     58.2096
07000     2.4956      0.0570     0.0814     59.0246
07100     2.4955      0.0992     0.0814     58.6519
07200     2.4954      0.0907     0.0814     59.0384
07300     2.4954      0.0886     0.0814     58.6640
07400     2.4953      0.0949     0.0814     58.1946
07500     2.4952      0.0759     0.0814     58.6396
07600     2.4952      0.0738     0.0814     58.3691
07700     2.4951      0.0886     0.0814     58.6029
07800     2.4950      0.0865     0.0814     58.8990
07900     2.4950      0.0781     0.0814     58.8907
08000     2.4949      0.0886     0.0814     59.1694
08100     2.4949      0.0823     0.0814     58.8323
08200     2.4948      0.0675     0.0814     58.6280
08300     2.4947      0.0865     0.0814     59.1330
08400     2.4947      0.0802     0.0814     59.4754
08500     2.4946      0.0717     0.0814     59.4482
08600     2.4945      0.0781     0.0814     58.9445
08700     2.4945      0.0717     0.0814     58.6400
08800     2.4944      0.0781     0.0814     59.2713
08900     2.4943      0.0802     0.0814     60.3672
09000     2.4943      0.1034     0.0814     61.0018
09100     2.4942      0.0738     0.0814     59.7436
09200     2.4942      0.0865     0.0814     59.1646
09300     2.4941      0.0823     0.0814     59.3004
09400     2.4940      0.0759     0.0814     59.4025
09500     2.4940      0.1160     0.0814     58.8840
09600     2.4939      0.0759     0.0814     60.5472
09700     2.4938      0.1076     0.0814     58.9854
09800     2.4938      0.0907     0.0814     59.0382
09900     2.4937      0.0654     0.0814     59.6553
10000     2.4936      0.0717     0.0814     59.4013
10100     2.4936      0.0949     0.0814     59.1314
10200     2.4936      0.0781     0.0814     59.0724
10300     2.4936      0.0823     0.0814     59.4988
10400     2.4936      0.0970     0.0814     60.0527
10500     2.4936      0.0675     0.0814     58.7529
10600     2.4936      0.0633     0.0814     59.1153
10700     2.4935      0.0823     0.0814     59.0891
10800     2.4935      0.0675     0.0814     59.2906
10900     2.4935      0.0717     0.0814     58.7712
11000     2.4935      0.0907     0.0814     59.2224
11100     2.4935      0.0802     0.0814     59.7121
11200     2.4935      0.0759     0.0814     59.7993
11300     2.4934      0.0612     0.0814     58.9824
11400     2.4934      0.0928     0.0814     58.9904
11500     2.4934      0.0654     0.0814     59.5797
11600     2.4934      0.1013     0.0814     59.3570
11700     2.4934      0.0844     0.0814     59.3157
11800     2.4934      0.0886     0.0814     59.4625
11900     2.4933      0.0823     0.0814     60.1505
12000     2.4933      0.0928     0.0814     60.0416
12100     2.4933      0.0802     0.0814     58.8577
12200     2.4933      0.0738     0.0814     58.7254
12300     2.4933      0.0844     0.0814     59.1640
12400     2.4933      0.0759     0.0814     59.4087
12500     2.4933      0.0823     0.0814     60.0493
12600     2.4932      0.0612     0.0814     60.1578
12700     2.4932      0.0865     0.0814     59.2200
12800     2.4932      0.0886     0.0814     59.6546
12900     2.4932      0.0949     0.0814     59.1692
13000     2.4932      0.0844     0.0814     59.2696
13100     2.4932      0.0992     0.0814     59.9215
13200     2.4931      0.0907     0.0814     59.2748
13300     2.4931      0.0738     0.0814     59.1074
13400     2.4931      0.0865     0.0814     59.9484
13500     2.4931      0.0612     0.0814     59.4831
13600     2.4931      0.1013     0.0814     59.4802
13700     2.4931      0.0781     0.0814     59.2134
13800     2.4930      0.0844     0.0814     59.6764
13900     2.4930      0.1013     0.0814     59.2455
14000     2.4930      0.0970     0.0814     58.8241
14100     2.4930      0.0949     0.0814     59.1862
14200     2.4930      0.0823     0.0814     59.6924
14300     2.4930      0.0612     0.0814     58.7631
14400     2.4930      0.0696     0.0814     59.6330
14500     2.4929      0.0970     0.0814     58.9455
14600     2.4929      0.0992     0.0814     59.3120
14700     2.4929      0.0823     0.0814     59.0873
14800     2.4929      0.0907     0.0814     59.4120
14900     2.4929      0.0865     0.0814     59.3877
15000     2.4929      0.0949     0.0814     59.6845
15100     2.4928      0.1097     0.0814     59.0097
15200     2.4928      0.0886     0.0814     59.4301
15300     2.4928      0.0759     0.0814     74.4974
15400     2.4928      0.0907     0.0814     59.6513
15500     2.4928      0.1097     0.0814     59.4449
15600     2.4928      0.0844     0.0814     58.8856
15700     2.4928      0.0865     0.0814     59.2084
15800     2.4927      0.0886     0.0814     59.2469
15900     2.4927      0.0506     0.0814     59.6282
16000     2.4927      0.0802     0.0814     59.7637
16100     2.4927      0.0823     0.0814     59.1656
16200     2.4927      0.0949     0.0814     59.5479
16300     2.4927      0.0781     0.0814     60.4240
16400     2.4926      0.0781     0.0814     59.8529
16500     2.4926      0.0970     0.0814     60.1286
16600     2.4926      0.1055     0.0814     60.0729
16700     2.4926      0.0717     0.0814     59.2600
16800     2.4926      0.0570     0.0814     59.6114
16900     2.4926      0.0907     0.0814     59.3163
17000     2.4925      0.0907     0.0814     59.4880
17100     2.4925      0.0696     0.0814     59.8411
17200     2.4925      0.0781     0.0814     59.6515
17300     2.4925      0.0865     0.0814     58.9393
17400     2.4925      0.0654     0.0814     59.7722
17500     2.4925      0.0823     0.0814     58.8276
17600     2.4925      0.0802     0.0814     59.6814
17700     2.4924      0.0907     0.0814     59.9535
17800     2.4924      0.0865     0.0814     59.4786
17900     2.4924      0.0886     0.0814     60.0547
18000     2.4924      0.0717     0.0814     59.2116
18100     2.4924      0.0633     0.0814     59.7263
18200     2.4924      0.0823     0.0814     60.0224
18300     2.4923      0.0654     0.0814     59.6474
18400     2.4923      0.1013     0.0814     59.8556
18500     2.4923      0.0886     0.0814     59.5548
18600     2.4923      0.0738     0.0814     59.1380
18700     2.4923      0.0759     0.0814     58.9147
18800     2.4923      0.0675     0.0814     59.4355
18900     2.4922      0.0949     0.0814     58.8971
19000     2.4922      0.0781     0.0814     59.1077
19100     2.4922      0.0865     0.0814     58.6055
19200     2.4922      0.0759     0.0814     58.8000
19300     2.4922      0.0759     0.0814     58.6033
19400     2.4922      0.0865     0.0814     58.6723
19500     2.4922      0.0970     0.0814     59.3095
19600     2.4921      0.0907     0.0814     58.8128
19700     2.4921      0.0949     0.0814     58.7410
19800     2.4921      0.0865     0.0814     59.0827
19900     2.4921      0.0717     0.0814     58.8250
20000     2.4921      0.0802     0.0814     58.4770
20100     2.4921      0.0802     0.0814     58.0996
20199     2.4921      0.0485     0.0814     58.1219
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.4921      0.0949     0.0805     9.6909
00100     2.4921      0.0907     0.0805     57.7147
00200     2.4921      0.0675     0.0805     57.6730
00300     2.4921      0.0781     0.0807     58.0355
00400     2.4921      0.0591     0.0807     57.7467
00500     2.4921      0.0865     0.0807     57.2228
00600     2.4921      0.0633     0.0807     57.7652
00700     2.4921      0.0992     0.0807     57.7076
00800     2.4921      0.0802     0.0807     57.1369
00900     2.4920      0.1076     0.0809     57.7835
01000     2.4920      0.0802     0.0809     57.3948
01100     2.4920      0.0738     0.0809     57.9265
01200     2.4920      0.0802     0.0809     57.2021
01300     2.4920      0.0781     0.0809     57.8465
01400     2.4920      0.1034     0.0809     57.5062
01500     2.4920      0.0907     0.0809     57.5354
01600     2.4920      0.0738     0.0814     58.1206
01700     2.4920      0.0928     0.0814     58.1380
01800     2.4920      0.0928     0.0814     57.9210
01900     2.4920      0.0696     0.0814     57.9876
02000     2.4920      0.0759     0.0814     57.2541
02100     2.4920      0.0696     0.0814     57.5512
02200     2.4920      0.0696     0.0814     57.6765
02300     2.4920      0.0844     0.0814     56.8681
02400     2.4920      0.0696     0.0814     57.2260
02500     2.4920      0.0886     0.0814     58.2347
02600     2.4920      0.0886     0.0814     57.8829
02700     2.4920      0.0886     0.0814     58.5088
02800     2.4920      0.0781     0.0814     57.5511
02900     2.4920      0.0823     0.0814     57.4451
03000     2.4920      0.0696     0.0814     58.0911
03100     2.4920      0.0759     0.0814     57.7555
03200     2.4920      0.0823     0.0814     57.5208
03300     2.4920      0.0717     0.0814     57.8772
03400     2.4920      0.0738     0.0814     57.8005
03500     2.4920      0.0886     0.0814     58.3419
03600     2.4920      0.0696     0.0814     57.7844
03700     2.4920      0.0717     0.0814     57.4993
03800     2.4920      0.0886     0.0814     58.6588
03900     2.4920      0.0591     0.0814     57.4569
04000     2.4920      0.0549     0.0814     57.7657
04100     2.4920      0.0759     0.0814     58.5174
04200     2.4920      0.0675     0.0814     57.1289
04300     2.4920      0.0675     0.0814     58.2063
04400     2.4920      0.0781     0.0814     57.3883
04500     2.4920      0.0844     0.0814     57.4058
04600     2.4920      0.0591     0.0814     58.1599
04700     2.4920      0.0802     0.0814     57.4747
04800     2.4920      0.0844     0.0814     57.7855
04900     2.4920      0.0886     0.0814     58.2275
05000     2.4920      0.0928     0.0814     57.5804
05100     2.4919      0.0675     0.0814     58.1753
05200     2.4919      0.0992     0.0814     58.1774
05300     2.4919      0.0928     0.0814     58.5863
05400     2.4919      0.0865     0.0814     58.1054
05500     2.4919      0.0759     0.0814     57.6221
05600     2.4919      0.0970     0.0814     57.7034
05700     2.4919      0.1013     0.0814     57.9643
05800     2.4919      0.0865     0.0814     58.0981
05900     2.4919      0.0759     0.0814     59.5622
06000     2.4919      0.0738     0.0814     58.1539
06100     2.4919      0.0949     0.0814     57.6125
06200     2.4919      0.0928     0.0814     58.1802
06300     2.4919      0.0865     0.0814     57.4702
06400     2.4919      0.0865     0.0814     57.4107
06500     2.4919      0.1076     0.0814     57.9081
06600     2.4919      0.0907     0.0814     57.6822
06700     2.4919      0.0738     0.0814     58.4309
06800     2.4919      0.0907     0.0814     57.7420
06900     2.4919      0.0907     0.0814     57.4581
07000     2.4919      0.0759     0.0814     58.8290
07100     2.4919      0.0527     0.0814     58.3725
07200     2.4919      0.0949     0.0814     57.5369
07300     2.4919      0.0907     0.0814     57.8203
07400     2.4919      0.0612     0.0814     57.7767
07500     2.4919      0.0865     0.0814     59.2219
07600     2.4919      0.0738     0.0814     57.8889
07700     2.4919      0.0738     0.0814     58.0996
07800     2.4919      0.0781     0.0814     58.7282
07900     2.4919      0.0802     0.0814     57.9350
08000     2.4919      0.0675     0.0814     58.3676
08100     2.4919      0.0865     0.0814     58.2565
08200     2.4919      0.0612     0.0814     58.3317
08300     2.4919      0.0865     0.0814     57.9912
08400     2.4919      0.0738     0.0814     57.9952
08500     2.4919      0.0992     0.0814     57.5597
08600     2.4919      0.0844     0.0814     58.4603
08700     2.4919      0.1076     0.0814     57.1020
08800     2.4919      0.0506     0.0814     57.8395
08900     2.4919      0.0781     0.0814     57.8220
09000     2.4919      0.0696     0.0814     57.6109
09100     2.4919      0.0886     0.0814     58.2101
09200     2.4919      0.0717     0.0814     58.0503
09300     2.4918      0.0654     0.0814     57.7605
09400     2.4918      0.0696     0.0814     58.1813
09500     2.4918      0.0865     0.0814     56.5982
09600     2.4918      0.0696     0.0814     56.3967
09700     2.4918      0.0717     0.0814     57.8922
09800     2.4918      0.0907     0.0814     57.7494
09900     2.4918      0.0738     0.0814     59.0949
Start testing:
Test Accuracy: 0.0789
