Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 362, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 362, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.0, finetuning_epochs=0, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.0005,0.0001,0.00002', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,10000', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
7ea981cf-cdb1-4beb-9e55-af215ddab508
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.5285      0.0781     0.0808     13.1931
00100     2.5211      0.0844     0.0811     75.4794
00200     2.5152      0.0907     0.0811     75.0794
00300     2.5108      0.0696     0.0811     76.1706
00400     2.5077      0.0738     0.0811     74.5570
00500     2.5057      0.0907     0.0811     74.6757
00600     2.5042      0.0886     0.0811     74.6821
00700     2.5033      0.0886     0.0811     75.6970
00800     2.5026      0.0738     0.0811     75.6961
00900     2.5021      0.0675     0.0811     75.0034
01000     2.5017      0.0759     0.0811     75.2551
01100     2.5014      0.0696     0.0811     75.7591
01200     2.5012      0.0928     0.0811     75.8593
01300     2.5010      0.0759     0.0811     75.2928
01400     2.5009      0.0823     0.0811     75.9833
01500     2.5008      0.1013     0.0811     75.4593
01600     2.5007      0.0633     0.0811     75.5979
01700     2.5006      0.0970     0.0811     75.1230
01800     2.5005      0.0928     0.0811     75.7013
01900     2.5005      0.0612     0.0811     75.0748
02000     2.5004      0.0759     0.0811     75.0760
02100     2.5003      0.0949     0.0811     75.2243
02200     2.5003      0.0865     0.0811     75.3254
02300     2.5003      0.0759     0.0811     75.5499
02400     2.5002      0.0738     0.0811     75.8810
02500     2.5002      0.0802     0.0811     76.0680
02600     2.5001      0.0759     0.0811     76.1796
02700     2.5001      0.0949     0.0811     75.8228
02800     2.5001      0.0738     0.0811     75.2531
02900     2.5001      0.0823     0.0811     75.5236
03000     2.5000      0.0823     0.0811     75.3962
03100     2.5000      0.1076     0.0811     74.9999
03200     2.5000      0.0759     0.0811     75.6231
03300     2.5000      0.0696     0.0811     75.4173
03400     2.4999      0.1013     0.0811     75.6545
03500     2.4999      0.0738     0.0811     76.0593
03600     2.4999      0.0844     0.0811     75.3225
03700     2.4999      0.0717     0.0811     74.6773
03800     2.4998      0.0696     0.0811     75.9103
03900     2.4998      0.0844     0.0811     75.3023
04000     2.4998      0.0570     0.0811     75.3560
04100     2.4998      0.0759     0.0814     75.0225
04200     2.4998      0.0949     0.0814     75.5378
04300     2.4997      0.0759     0.0814     75.5740
04400     2.4997      0.0654     0.0814     75.4430
04500     2.4997      0.0949     0.0814     74.8419
04600     2.4997      0.0654     0.0814     75.2156
04700     2.4997      0.1034     0.0814     75.1675
04800     2.4996      0.0675     0.0814     75.2719
04900     2.4996      0.0865     0.0814     74.9368
05000     2.4996      0.0654     0.0814     75.6741
05100     2.4996      0.0654     0.0814     76.5231
05200     2.4996      0.0886     0.0814     74.9777
05300     2.4995      0.0654     0.0814     74.9949
05400     2.4995      0.0696     0.0814     75.2863
05500     2.4995      0.1013     0.0814     74.5710
05600     2.4995      0.0717     0.0814     75.0757
05700     2.4995      0.0696     0.0814     75.0114
05800     2.4994      0.0781     0.0814     74.9032
05900     2.4994      0.0612     0.0814     75.4870
06000     2.4994      0.0675     0.0814     74.8916
06100     2.4994      0.0717     0.0814     75.9252
06200     2.4994      0.0928     0.0814     75.7938
06300     2.4993      0.0759     0.0814     74.7794
06400     2.4993      0.0823     0.0814     74.9931
06500     2.4993      0.0865     0.0814     75.0095
06600     2.4993      0.0802     0.0814     74.9186
06700     2.4993      0.0949     0.0814     75.9630
06800     2.4992      0.0781     0.0814     74.9380
06900     2.4992      0.0633     0.0814     75.5155
07000     2.4992      0.0802     0.0814     75.4404
07100     2.4992      0.1076     0.0814     75.1071
07200     2.4992      0.1118     0.0814     77.0279
07300     2.4992      0.0949     0.0814     75.4676
07400     2.4991      0.0949     0.0814     75.5585
07500     2.4991      0.0823     0.0814     76.0888
07600     2.4991      0.0781     0.0814     75.3364
07700     2.4991      0.0992     0.0814     75.3297
07800     2.4991      0.0717     0.0814     75.0519
07900     2.4990      0.0717     0.0816     75.7647
08000     2.4990      0.0928     0.0816     75.9864
08100     2.4990      0.0759     0.0816     75.7384
08200     2.4990      0.0675     0.0816     75.6871
08300     2.4990      0.1055     0.0816     76.8057
08400     2.4989      0.0802     0.0816     76.3559
08500     2.4989      0.0696     0.0816     75.8951
08600     2.4989      0.0738     0.0816     76.6411
08700     2.4989      0.0738     0.0816     74.9857
08800     2.4989      0.0570     0.0816     75.6706
08900     2.4989      0.0781     0.0816     75.4978
09000     2.4988      0.0865     0.0816     76.4575
09100     2.4988      0.0865     0.0816     75.9650
09200     2.4988      0.0759     0.0816     75.0042
09300     2.4988      0.0696     0.0816     74.8945
09400     2.4988      0.0992     0.0816     76.1008
09500     2.4987      0.0802     0.0816     75.5796
09600     2.4987      0.0717     0.0816     76.2740
09700     2.4987      0.0696     0.0816     75.2618
09800     2.4987      0.0907     0.0816     75.5938
09900     2.4987      0.0591     0.0816     75.3229
10000     2.4986      0.0759     0.0816     76.1939
10100     2.4986      0.0823     0.0816     75.6335
10200     2.4986      0.0696     0.0816     75.9585
10300     2.4986      0.0907     0.0816     74.8765
10400     2.4986      0.0802     0.0816     75.2362
10500     2.4986      0.0802     0.0816     75.3528
10600     2.4986      0.0865     0.0816     75.0520
10700     2.4986      0.0591     0.0816     76.2208
10800     2.4986      0.1118     0.0816     75.9654
10900     2.4986      0.1076     0.0816     75.5498
11000     2.4986      0.0696     0.0816     76.2437
11100     2.4986      0.0633     0.0816     75.5999
11200     2.4986      0.0696     0.0816     76.4822
11300     2.4986      0.0970     0.0816     74.9021
11400     2.4986      0.0443     0.0816     75.1514
11500     2.4986      0.0907     0.0816     75.6531
11600     2.4986      0.0738     0.0816     75.0083
11700     2.4986      0.0886     0.0816     75.5525
11800     2.4986      0.0886     0.0816     76.1643
11900     2.4986      0.1139     0.0816     75.4495
12000     2.4986      0.0633     0.0816     76.4310
12100     2.4986      0.0570     0.0816     75.6951
12200     2.4986      0.0802     0.0816     75.2624
12300     2.4986      0.0675     0.0816     75.4916
12400     2.4986      0.0654     0.0816     76.3789
12500     2.4986      0.1013     0.0816     75.4968
12600     2.4986      0.0802     0.0816     75.7733
12700     2.4985      0.0865     0.0816     75.8091
12800     2.4985      0.0823     0.0816     75.9610
12900     2.4985      0.1034     0.0816     75.1466
13000     2.4985      0.0823     0.0816     75.2359
13100     2.4985      0.0823     0.0816     75.9845
13200     2.4985      0.1013     0.0816     75.4004
13300     2.4985      0.0802     0.0816     75.9103
13400     2.4985      0.0717     0.0816     76.3911
13500     2.4985      0.0781     0.0816     75.3201
13600     2.4985      0.0865     0.0816     76.4040
13700     2.4985      0.0633     0.0816     75.5047
13800     2.4985      0.0738     0.0816     75.5459
13900     2.4985      0.0717     0.0816     76.0962
14000     2.4985      0.0759     0.0816     75.3211
14100     2.4985      0.0759     0.0816     75.2477
14200     2.4985      0.0907     0.0816     75.4068
14300     2.4985      0.0738     0.0816     75.2374
14400     2.4985      0.1034     0.0816     76.1253
14500     2.4985      0.0865     0.0816     75.0995
14600     2.4985      0.0738     0.0816     74.7568
14700     2.4985      0.0907     0.0816     76.1027
14800     2.4985      0.0738     0.0816     75.2657
14900     2.4985      0.0549     0.0816     74.4465
15000     2.4985      0.0844     0.0816     76.6756
15100     2.4985      0.0886     0.0816     75.2476
15200     2.4985      0.0823     0.0816     76.4089
15300     2.4985      0.0907     0.0816     75.7600
15400     2.4985      0.0696     0.0816     75.2693
15500     2.4984      0.0928     0.0816     76.1097
15600     2.4984      0.0633     0.0816     75.4561
15700     2.4984      0.1055     0.0816     75.5345
15800     2.4984      0.0738     0.0816     75.5261
15900     2.4984      0.0633     0.0816     75.1748
16000     2.4984      0.0928     0.0816     75.9546
16100     2.4984      0.0738     0.0816     75.6911
16200     2.4984      0.0928     0.0816     75.5469
16300     2.4984      0.0781     0.0816     75.4264
16400     2.4984      0.0823     0.0816     75.0795
16500     2.4984      0.0823     0.0816     75.6268
16600     2.4984      0.0823     0.0816     76.5276
16700     2.4984      0.0738     0.0816     75.2836
16800     2.4984      0.0970     0.0816     75.8295
16900     2.4984      0.0823     0.0816     75.5140
17000     2.4984      0.0717     0.0816     75.8932
17100     2.4984      0.0675     0.0816     76.2809
17200     2.4984      0.0675     0.0816     75.5411
17300     2.4984      0.0654     0.0816     75.8670
17400     2.4984      0.0717     0.0816     75.5987
17500     2.4984      0.0738     0.0816     75.5404
17600     2.4984      0.0612     0.0816     75.6721
17700     2.4984      0.0675     0.0816     75.3763
17800     2.4984      0.0654     0.0816     75.2996
17900     2.4984      0.0886     0.0816     76.3304
18000     2.4984      0.1097     0.0816     75.7597
18100     2.4984      0.0886     0.0816     75.6429
18200     2.4984      0.0717     0.0816     76.3091
18300     2.4983      0.0696     0.0816     75.8823
18400     2.4983      0.0738     0.0816     76.0703
18500     2.4983      0.0633     0.0816     74.4657
18600     2.4983      0.0907     0.0816     76.0902
18700     2.4983      0.0781     0.0816     75.9304
18800     2.4983      0.1181     0.0816     75.7357
18900     2.4983      0.0759     0.0816     75.0727
19000     2.4983      0.0696     0.0816     75.7927
19100     2.4983      0.0570     0.0816     75.0637
19200     2.4983      0.0907     0.0816     75.8173
19300     2.4983      0.0485     0.0816     75.0401
19400     2.4983      0.0781     0.0816     75.5653
19500     2.4983      0.0570     0.0816     75.9339
19600     2.4983      0.0949     0.0816     76.1414
19700     2.4983      0.0570     0.0816     76.3795
19800     2.4983      0.0633     0.0816     76.4431
19900     2.4983      0.0675     0.0816     75.0836
20000     2.4983      0.0633     0.0816     75.9004
20100     2.4983      0.0907     0.0816     76.3302
20200     2.4983      0.0717     0.0816     75.5335
20300     2.4983      0.0865     0.0816     75.3942
20400     2.4983      0.0992     0.0816     75.7003
20500     2.4983      0.0781     0.0816     75.6624
20600     2.4983      0.0759     0.0816     76.3099
20700     2.4983      0.0591     0.0816     74.7675
20800     2.4983      0.0865     0.0816     76.1102
20900     2.4983      0.0675     0.0816     75.7247
21000     2.4983      0.0759     0.0816     75.5467
21100     2.4983      0.0865     0.0816     77.4542
21200     2.4983      0.0738     0.0816     75.7717
21300     2.4983      0.0992     0.0816     75.9495
21400     2.4983      0.0802     0.0816     76.4393
21500     2.4983      0.0802     0.0816     75.3458
21600     2.4983      0.0907     0.0816     76.0348
21700     2.4983      0.0759     0.0816     75.7034
21800     2.4983      0.0781     0.0816     75.4260
21900     2.4983      0.0844     0.0816     76.5213
22000     2.4983      0.0591     0.0816     75.1937
22100     2.4983      0.0907     0.0816     75.6690
22200     2.4983      0.0527     0.0816     76.7545
22300     2.4983      0.0759     0.0816     76.4932
22400     2.4983      0.0612     0.0816     77.3501
22500     2.4983      0.0781     0.0816     74.9402
22600     2.4983      0.0949     0.0816     75.6809
22700     2.4983      0.0781     0.0816     75.8787
22800     2.4983      0.0992     0.0816     75.7338
22900     2.4983      0.0612     0.0816     75.8126
23000     2.4983      0.0612     0.0816     76.7494
23100     2.4983      0.0675     0.0816     75.4512
23200     2.4983      0.0591     0.0816     75.9603
23300     2.4983      0.0759     0.0816     75.7540
23400     2.4983      0.1160     0.0816     75.7515
23500     2.4983      0.0717     0.0816     76.3613
23600     2.4983      0.0802     0.0816     76.0617
23700     2.4983      0.0781     0.0816     75.4499
23800     2.4983      0.0675     0.0816     75.6270
23900     2.4983      0.0759     0.0816     75.8602
24000     2.4983      0.0886     0.0816     76.6935
24100     2.4983      0.0759     0.0816     75.6810
24200     2.4983      0.0781     0.0816     75.9511
24300     2.4983      0.0802     0.0816     76.4226
24400     2.4983      0.0612     0.0816     75.9613
24500     2.4982      0.0675     0.0816     76.6639
24600     2.4982      0.0823     0.0816     76.2608
24700     2.4982      0.0802     0.0816     76.3060
24800     2.4982      0.0886     0.0816     76.7426
24900     2.4982      0.0759     0.0816     76.3077
25000     2.4982      0.0907     0.0816     74.9402
25100     2.4982      0.1076     0.0816     75.9851
25200     2.4982      0.0633     0.0816     75.7592
25300     2.4982      0.1013     0.0816     75.2895
25400     2.4982      0.0886     0.0816     76.2188
25500     2.4982      0.0844     0.0816     75.8822
25600     2.4982      0.1013     0.0816     75.8175
25700     2.4982      0.0654     0.0816     75.8795
25800     2.4982      0.0928     0.0816     76.3465
25900     2.4982      0.0633     0.0816     76.9010
26000     2.4982      0.1055     0.0816     75.8370
26100     2.4982      0.0781     0.0816     75.2383
26200     2.4982      0.0949     0.0816     76.0025
26300     2.4982      0.1034     0.0816     75.5817
26400     2.4982      0.0844     0.0816     76.4408
26500     2.4982      0.0928     0.0816     72.1057
26600     2.4982      0.0612     0.0816     69.9789
26700     2.4982      0.0527     0.0816     70.3833
26800     2.4982      0.0570     0.0816     71.1213
26900     2.4982      0.0781     0.0816     71.1000
27000     2.4982      0.0781     0.0816     70.7796
27100     2.4982      0.0823     0.0816     70.6262
27200     2.4982      0.0591     0.0816     71.7017
27300     2.4982      0.0970     0.0816     72.5169
27400     2.4982      0.0675     0.0816     68.8299
27500     2.4982      0.0844     0.0816     69.1379
27600     2.4982      0.0886     0.0816     69.5050
27700     2.4982      0.0570     0.0816     70.2793
27800     2.4982      0.0675     0.0816     69.3350
27900     2.4982      0.0844     0.0816     69.5273
28000     2.4982      0.0970     0.0816     68.5303
28100     2.4982      0.0717     0.0816     69.0412
28200     2.4982      0.0738     0.0816     69.7924
28300     2.4982      0.0802     0.0816     69.8440
28400     2.4982      0.0591     0.0816     68.9757
28500     2.4982      0.0949     0.0816     69.8242
28600     2.4982      0.0675     0.0816     69.2555
28700     2.4982      0.0633     0.0816     68.3829
28800     2.4982      0.0654     0.0816     69.0037
28900     2.4982      0.0591     0.0816     69.0366
29000     2.4982      0.0844     0.0816     69.9065
29100     2.4982      0.0549     0.0816     69.2196
29200     2.4982      0.0781     0.0816     68.4251
29300     2.4982      0.0802     0.0816     68.1173
29400     2.4982      0.0781     0.0816     69.7854
29500     2.4982      0.0654     0.0816     69.2811
29600     2.4982      0.0485     0.0816     68.8208
29700     2.4982      0.0696     0.0816     68.9271
29800     2.4982      0.0675     0.0816     69.7012
29900     2.4982      0.0823     0.0816     70.0660
29999     2.4982      0.0907     0.0816     68.1973
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Start testing:
Test Accuracy: 0.0789
