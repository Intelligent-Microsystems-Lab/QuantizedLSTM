Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 362, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 362, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.0, finetuning_epochs=0, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.0005,0.0001,0.00002', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,10000', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
7ea981cf-cdb1-4beb-9e55-af215ddab508
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.5285      0.0781     0.0808     13.1931
00100     2.5211      0.0844     0.0811     75.4794
00200     2.5152      0.0907     0.0811     75.0794
00300     2.5108      0.0696     0.0811     76.1706
00400     2.5077      0.0738     0.0811     74.5570
00500     2.5057      0.0907     0.0811     74.6757
00600     2.5042      0.0886     0.0811     74.6821
00700     2.5033      0.0886     0.0811     75.6970
00800     2.5026      0.0738     0.0811     75.6961
00900     2.5021      0.0675     0.0811     75.0034
01000     2.5017      0.0759     0.0811     75.2551
01100     2.5014      0.0696     0.0811     75.7591
01200     2.5012      0.0928     0.0811     75.8593
01300     2.5010      0.0759     0.0811     75.2928
01400     2.5009      0.0823     0.0811     75.9833
01500     2.5008      0.1013     0.0811     75.4593
01600     2.5007      0.0633     0.0811     75.5979
01700     2.5006      0.0970     0.0811     75.1230
01800     2.5005      0.0928     0.0811     75.7013
01900     2.5005      0.0612     0.0811     75.0748
02000     2.5004      0.0759     0.0811     75.0760
02100     2.5003      0.0949     0.0811     75.2243
02200     2.5003      0.0865     0.0811     75.3254
02300     2.5003      0.0759     0.0811     75.5499
02400     2.5002      0.0738     0.0811     75.8810
02500     2.5002      0.0802     0.0811     76.0680
02600     2.5001      0.0759     0.0811     76.1796
02700     2.5001      0.0949     0.0811     75.8228
02800     2.5001      0.0738     0.0811     75.2531
02900     2.5001      0.0823     0.0811     75.5236
03000     2.5000      0.0823     0.0811     75.3962
03100     2.5000      0.1076     0.0811     74.9999
03200     2.5000      0.0759     0.0811     75.6231
03300     2.5000      0.0696     0.0811     75.4173
03400     2.4999      0.1013     0.0811     75.6545
03500     2.4999      0.0738     0.0811     76.0593
03600     2.4999      0.0844     0.0811     75.3225
03700     2.4999      0.0717     0.0811     74.6773
03800     2.4998      0.0696     0.0811     75.9103
03900     2.4998      0.0844     0.0811     75.3023
04000     2.4998      0.0570     0.0811     75.3560
04100     2.4998      0.0759     0.0814     75.0225
04200     2.4998      0.0949     0.0814     75.5378
04300     2.4997      0.0759     0.0814     75.5740
04400     2.4997      0.0654     0.0814     75.4430
04500     2.4997      0.0949     0.0814     74.8419
04600     2.4997      0.0654     0.0814     75.2156
04700     2.4997      0.1034     0.0814     75.1675
04800     2.4996      0.0675     0.0814     75.2719
04900     2.4996      0.0865     0.0814     74.9368
05000     2.4996      0.0654     0.0814     75.6741
05100     2.4996      0.0654     0.0814     76.5231
05200     2.4996      0.0886     0.0814     74.9777
05300     2.4995      0.0654     0.0814     74.9949
05400     2.4995      0.0696     0.0814     75.2863
05500     2.4995      0.1013     0.0814     74.5710
05600     2.4995      0.0717     0.0814     75.0757
05700     2.4995      0.0696     0.0814     75.0114
05800     2.4994      0.0781     0.0814     74.9032
05900     2.4994      0.0612     0.0814     75.4870
06000     2.4994      0.0675     0.0814     74.8916
06100     2.4994      0.0717     0.0814     75.9252
06200     2.4994      0.0928     0.0814     75.7938
06300     2.4993      0.0759     0.0814     74.7794
06400     2.4993      0.0823     0.0814     74.9931
06500     2.4993      0.0865     0.0814     75.0095
06600     2.4993      0.0802     0.0814     74.9186
06700     2.4993      0.0949     0.0814     75.9630
06800     2.4992      0.0781     0.0814     74.9380
06900     2.4992      0.0633     0.0814     75.5155
07000     2.4992      0.0802     0.0814     75.4404
07100     2.4992      0.1076     0.0814     75.1071
07200     2.4992      0.1118     0.0814     77.0279
07300     2.4992      0.0949     0.0814     75.4676
07400     2.4991      0.0949     0.0814     75.5585
07500     2.4991      0.0823     0.0814     76.0888
07600     2.4991      0.0781     0.0814     75.3364
07700     2.4991      0.0992     0.0814     75.3297
07800     2.4991      0.0717     0.0814     75.0519
07900     2.4990      0.0717     0.0816     75.7647
08000     2.4990      0.0928     0.0816     75.9864
08100     2.4990      0.0759     0.0816     75.7384
08200     2.4990      0.0675     0.0816     75.6871
08300     2.4990      0.1055     0.0816     76.8057
08400     2.4989      0.0802     0.0816     76.3559
08500     2.4989      0.0696     0.0816     75.8951
08600     2.4989      0.0738     0.0816     76.6411
08700     2.4989      0.0738     0.0816     74.9857
08800     2.4989      0.0570     0.0816     75.6706
08900     2.4989      0.0781     0.0816     75.4978
09000     2.4988      0.0865     0.0816     76.4575
09100     2.4988      0.0865     0.0816     75.9650
09200     2.4988      0.0759     0.0816     75.0042
09300     2.4988      0.0696     0.0816     74.8945
09400     2.4988      0.0992     0.0816     76.1008
09500     2.4987      0.0802     0.0816     75.5796
09600     2.4987      0.0717     0.0816     76.2740
09700     2.4987      0.0696     0.0816     75.2618
09800     2.4987      0.0907     0.0816     75.5938
09900     2.4987      0.0591     0.0816     75.3229
10000     2.4986      0.0759     0.0816     76.1939
10100     2.4986      0.0823     0.0816     75.6335
10200     2.4986      0.0696     0.0816     75.9585
10300     2.4986      0.0907     0.0816     74.8765
10400     2.4986      0.0802     0.0816     75.2362
10500     2.4986      0.0802     0.0816     75.3528
10600     2.4986      0.0865     0.0816     75.0520
10700     2.4986      0.0591     0.0816     76.2208
10800     2.4986      0.1118     0.0816     75.9654
10900     2.4986      0.1076     0.0816     75.5498
11000     2.4986      0.0696     0.0816     76.2437
11100     2.4986      0.0633     0.0816     75.5999
11200     2.4986      0.0696     0.0816     76.4822
11300     2.4986      0.0970     0.0816     74.9021
11400     2.4986      0.0443     0.0816     75.1514
11500     2.4986      0.0907     0.0816     75.6531
11600     2.4986      0.0738     0.0816     75.0083
11700     2.4986      0.0886     0.0816     75.5525
11800     2.4986      0.0886     0.0816     76.1643
11900     2.4986      0.1139     0.0816     75.4495
12000     2.4986      0.0633     0.0816     76.4310
12100     2.4986      0.0570     0.0816     75.6951
12200     2.4986      0.0802     0.0816     75.2624
12300     2.4986      0.0675     0.0816     75.4916
12400     2.4986      0.0654     0.0816     76.3789
12500     2.4986      0.1013     0.0816     75.4968
12600     2.4986      0.0802     0.0816     75.7733
12700     2.4985      0.0865     0.0816     75.8091
12800     2.4985      0.0823     0.0816     75.9610
12900     2.4985      0.1034     0.0816     75.1466
13000     2.4985      0.0823     0.0816     75.2359
13100     2.4985      0.0823     0.0816     75.9845
13200     2.4985      0.1013     0.0816     75.4004
13300     2.4985      0.0802     0.0816     75.9103
13400     2.4985      0.0717     0.0816     76.3911
13500     2.4985      0.0781     0.0816     75.3201
13600     2.4985      0.0865     0.0816     76.4040
13700     2.4985      0.0633     0.0816     75.5047
13800     2.4985      0.0738     0.0816     75.5459
13900     2.4985      0.0717     0.0816     76.0962
14000     2.4985      0.0759     0.0816     75.3211
14100     2.4985      0.0759     0.0816     75.2477
14200     2.4985      0.0907     0.0816     75.4068
14300     2.4985      0.0738     0.0816     75.2374
14400     2.4985      0.1034     0.0816     76.1253
14500     2.4985      0.0865     0.0816     75.0995
14600     2.4985      0.0738     0.0816     74.7568
14700     2.4985      0.0907     0.0816     76.1027
14800     2.4985      0.0738     0.0816     75.2657
14900     2.4985      0.0549     0.0816     74.4465
15000     2.4985      0.0844     0.0816     76.6756
15100     2.4985      0.0886     0.0816     75.2476
15200     2.4985      0.0823     0.0816     76.4089
15300     2.4985      0.0907     0.0816     75.7600
15400     2.4985      0.0696     0.0816     75.2693
15500     2.4984      0.0928     0.0816     76.1097
15600     2.4984      0.0633     0.0816     75.4561
15700     2.4984      0.1055     0.0816     75.5345
15800     2.4984      0.0738     0.0816     75.5261
15900     2.4984      0.0633     0.0816     75.1748
16000     2.4984      0.0928     0.0816     75.9546
16100     2.4984      0.0738     0.0816     75.6911
16200     2.4984      0.0928     0.0816     75.5469
16300     2.4984      0.0781     0.0816     75.4264
16400     2.4984      0.0823     0.0816     75.0795
16500     2.4984      0.0823     0.0816     75.6268
16600     2.4984      0.0823     0.0816     76.5276
16700     2.4984      0.0738     0.0816     75.2836
16800     2.4984      0.0970     0.0816     75.8295
16900     2.4984      0.0823     0.0816     75.5140
17000     2.4984      0.0717     0.0816     75.8932
17100     2.4984      0.0675     0.0816     76.2809
17200     2.4984      0.0675     0.0816     75.5411
17300     2.4984      0.0654     0.0816     75.8670
17400     2.4984      0.0717     0.0816     75.5987
17500     2.4984      0.0738     0.0816     75.5404
17600     2.4984      0.0612     0.0816     75.6721
17700     2.4984      0.0675     0.0816     75.3763
17800     2.4984      0.0654     0.0816     75.2996
17900     2.4984      0.0886     0.0816     76.3304
18000     2.4984      0.1097     0.0816     75.7597
18100     2.4984      0.0886     0.0816     75.6429
18200     2.4984      0.0717     0.0816     76.3091
18300     2.4983      0.0696     0.0816     75.8823
18400     2.4983      0.0738     0.0816     76.0703
18500     2.4983      0.0633     0.0816     74.4657
18600     2.4983      0.0907     0.0816     76.0902
18700     2.4983      0.0781     0.0816     75.9304
18800     2.4983      0.1181     0.0816     75.7357
18900     2.4983      0.0759     0.0816     75.0727
19000     2.4983      0.0696     0.0816     75.7927
19100     2.4983      0.0570     0.0816     75.0637
19200     2.4983      0.0907     0.0816     75.8173
19300     2.4983      0.0485     0.0816     75.0401
19400     2.4983      0.0781     0.0816     75.5653
19500     2.4983      0.0570     0.0816     75.9339
19600     2.4983      0.0949     0.0816     76.1414
19700     2.4983      0.0570     0.0816     76.3795
19800     2.4983      0.0633     0.0816     76.4431
19900     2.4983      0.0675     0.0816     75.0836
20000     2.4983      0.0633     0.0816     75.9004
20100     2.4983      0.0907     0.0816     76.3302
20200     2.4983      0.0717     0.0816     75.5335
20300     2.4983      0.0865     0.0816     75.3942
20400     2.4983      0.0992     0.0816     75.7003
20500     2.4983      0.0781     0.0816     75.6624
20600     2.4983      0.0759     0.0816     76.3099
20700     2.4983      0.0591     0.0816     74.7675
20800     2.4983      0.0865     0.0816     76.1102
20900     2.4983      0.0675     0.0816     75.7247
21000     2.4983      0.0759     0.0816     75.5467
21100     2.4983      0.0865     0.0816     77.4542
21200     2.4983      0.0738     0.0816     75.7717
21300     2.4983      0.0992     0.0816     75.9495
21400     2.4983      0.0802     0.0816     76.4393
21500     2.4983      0.0802     0.0816     75.3458
21600     2.4983      0.0907     0.0816     76.0348
21700     2.4983      0.0759     0.0816     75.7034
21800     2.4983      0.0781     0.0816     75.4260
21900     2.4983      0.0844     0.0816     76.5213
22000     2.4983      0.0591     0.0816     75.1937
22100     2.4983      0.0907     0.0816     75.6690
22200     2.4983      0.0527     0.0816     76.7545
22300     2.4983      0.0759     0.0816     76.4932
22400     2.4983      0.0612     0.0816     77.3501
22500     2.4983      0.0781     0.0816     74.9402
22600     2.4983      0.0949     0.0816     75.6809
22700     2.4983      0.0781     0.0816     75.8787
22800     2.4983      0.0992     0.0816     75.7338
22900     2.4983      0.0612     0.0816     75.8126
23000     2.4983      0.0612     0.0816     76.7494
23100     2.4983      0.0675     0.0816     75.4512
23200     2.4983      0.0591     0.0816     75.9603
23300     2.4983      0.0759     0.0816     75.7540
23400     2.4983      0.1160     0.0816     75.7515
23500     2.4983      0.0717     0.0816     76.3613
23600     2.4983      0.0802     0.0816     76.0617
23700     2.4983      0.0781     0.0816     75.4499
23800     2.4983      0.0675     0.0816     75.6270
23900     2.4983      0.0759     0.0816     75.8602
24000     2.4983      0.0886     0.0816     76.6935
24100     2.4983      0.0759     0.0816     75.6810
24200     2.4983      0.0781     0.0816     75.9511
24300     2.4983      0.0802     0.0816     76.4226
24400     2.4983      0.0612     0.0816     75.9613
24500     2.4982      0.0675     0.0816     76.6639
24600     2.4982      0.0823     0.0816     76.2608
24700     2.4982      0.0802     0.0816     76.3060
24800     2.4982      0.0886     0.0816     76.7426
24900     2.4982      0.0759     0.0816     76.3077
25000     2.4982      0.0907     0.0816     74.9402
25100     2.4982      0.1076     0.0816     75.9851
25200     2.4982      0.0633     0.0816     75.7592
25300     2.4982      0.1013     0.0816     75.2895
25400     2.4982      0.0886     0.0816     76.2188
25500     2.4982      0.0844     0.0816     75.8822
25600     2.4982      0.1013     0.0816     75.8175
25700     2.4982      0.0654     0.0816     75.8795
25800     2.4982      0.0928     0.0816     76.3465
25900     2.4982      0.0633     0.0816     76.9010
26000     2.4982      0.1055     0.0816     75.8370
26100     2.4982      0.0781     0.0816     75.2383
26200     2.4982      0.0949     0.0816     76.0025
26300     2.4982      0.1034     0.0816     75.5817
26400     2.4982      0.0844     0.0816     76.4408
26500     2.4982      0.0928     0.0816     72.1057
26600     2.4982      0.0612     0.0816     69.9789
26700     2.4982      0.0527     0.0816     70.3833
26800     2.4982      0.0570     0.0816     71.1213
26900     2.4982      0.0781     0.0816     71.1000
27000     2.4982      0.0781     0.0816     70.7796
27100     2.4982      0.0823     0.0816     70.6262
27200     2.4982      0.0591     0.0816     71.7017
27300     2.4982      0.0970     0.0816     72.5169
27400     2.4982      0.0675     0.0816     68.8299
27500     2.4982      0.0844     0.0816     69.1379
27600     2.4982      0.0886     0.0816     69.5050
27700     2.4982      0.0570     0.0816     70.2793
27800     2.4982      0.0675     0.0816     69.3350
27900     2.4982      0.0844     0.0816     69.5273
28000     2.4982      0.0970     0.0816     68.5303
28100     2.4982      0.0717     0.0816     69.0412
28200     2.4982      0.0738     0.0816     69.7924
28300     2.4982      0.0802     0.0816     69.8440
28400     2.4982      0.0591     0.0816     68.9757
28500     2.4982      0.0949     0.0816     69.8242
28600     2.4982      0.0675     0.0816     69.2555
28700     2.4982      0.0633     0.0816     68.3829
28800     2.4982      0.0654     0.0816     69.0037
28900     2.4982      0.0591     0.0816     69.0366
29000     2.4982      0.0844     0.0816     69.9065
29100     2.4982      0.0549     0.0816     69.2196
29200     2.4982      0.0781     0.0816     68.4251
29300     2.4982      0.0802     0.0816     68.1173
29400     2.4982      0.0781     0.0816     69.7854
29500     2.4982      0.0654     0.0816     69.2811
29600     2.4982      0.0485     0.0816     68.8208
29700     2.4982      0.0696     0.0816     68.9271
29800     2.4982      0.0675     0.0816     69.7012
29900     2.4982      0.0823     0.0816     70.0660
29999     2.4982      0.0907     0.0816     68.1973
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Start testing:
Test Accuracy: 0.0789
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
bea5ed9a-b813-4554-9737-c64b4abfc57f
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.5285      0.0802     0.0805     13.4336
00100     2.5074      0.0675     0.0807     60.0412
00200     2.5025      0.0717     0.0808     59.3672
00300     2.5012      0.0781     0.0808     59.7841
00400     2.5007      0.0717     0.0808     59.0137
00500     2.5004      0.0949     0.0808     59.0255
00600     2.5002      0.0654     0.0812     59.7464
00700     2.5001      0.0928     0.0812     59.6502
00800     2.5000      0.1055     0.0812     59.4437
00900     2.4999      0.0949     0.0812     59.0457
01000     2.4998      0.0928     0.0812     59.2062
01100     2.4997      0.0823     0.0812     59.6100
01200     2.4997      0.0907     0.0812     59.0749
01300     2.4996      0.0675     0.0812     59.2234
01400     2.4995      0.0506     0.0812     59.7057
01500     2.4994      0.0970     0.0812     59.7479
01600     2.4993      0.0738     0.0812     59.9134
01700     2.4993      0.0928     0.0812     59.1245
01800     2.4992      0.0802     0.0812     59.3850
01900     2.4991      0.0949     0.0812     59.4355
02000     2.4990      0.0633     0.0812     58.7682
02100     2.4990      0.0759     0.0812     59.4448
02200     2.4989      0.0802     0.0812     60.0812
02300     2.4988      0.0696     0.0812     60.0029
02400     2.4987      0.0907     0.0812     59.9469
02500     2.4987      0.0907     0.0812     59.1470
02600     2.4986      0.1034     0.0812     58.7428
02700     2.4985      0.0865     0.0812     59.6522
02800     2.4984      0.0717     0.0812     59.1289
02900     2.4984      0.0844     0.0812     59.1969
03000     2.4983      0.0907     0.0812     59.0933
03100     2.4982      0.1266     0.0812     58.6281
03200     2.4982      0.0802     0.0812     59.6611
03300     2.4981      0.0781     0.0812     58.8156
03400     2.4980      0.0802     0.0812     59.4687
03500     2.4979      0.0759     0.0812     60.0269
03600     2.4979      0.0675     0.0812     58.9113
03700     2.4978      0.0738     0.0812     58.8645
03800     2.4977      0.1013     0.0812     59.2353
03900     2.4977      0.0886     0.0812     58.6974
04000     2.4976      0.0781     0.0812     60.1412
04100     2.4975      0.0823     0.0812     59.1102
04200     2.4975      0.0781     0.0812     59.2740
04300     2.4974      0.0717     0.0812     59.4921
04400     2.4973      0.0696     0.0812     59.2738
04500     2.4972      0.0970     0.0812     58.6340
04600     2.4972      0.0907     0.0812     59.5126
04700     2.4971      0.0717     0.0812     59.4541
04800     2.4970      0.0759     0.0812     60.0426
04900     2.4970      0.0633     0.0812     59.2522
05000     2.4969      0.0759     0.0812     59.0845
05100     2.4968      0.0717     0.0812     59.5347
05200     2.4968      0.0675     0.0812     59.4010
05300     2.4967      0.0759     0.0812     59.2146
05400     2.4966      0.0802     0.0812     60.1983
05500     2.4966      0.0802     0.0812     59.1026
05600     2.4965      0.0886     0.0812     59.3480
05700     2.4964      0.0844     0.0812     58.6868
05800     2.4964      0.0485     0.0812     58.8442
05900     2.4963      0.0928     0.0812     59.3067
06000     2.4962      0.1034     0.0812     58.9169
06100     2.4962      0.0949     0.0812     59.3765
06200     2.4961      0.0781     0.0812     59.6773
06300     2.4960      0.0696     0.0812     59.2852
06400     2.4960      0.0802     0.0812     59.8635
06500     2.4959      0.0844     0.0812     59.3094
06600     2.4958      0.0717     0.0812     59.1852
06700     2.4958      0.1013     0.0812     59.6617
06800     2.4957      0.0717     0.0812     59.5101
06900     2.4956      0.0781     0.0812     59.7225
07000     2.4956      0.0654     0.0812     59.7292
07100     2.4955      0.0823     0.0812     59.1534
07200     2.4954      0.0886     0.0812     59.5857
07300     2.4954      0.0696     0.0812     58.9421
07400     2.4953      0.0717     0.0812     59.5318
07500     2.4952      0.0865     0.0812     59.8327
07600     2.4952      0.0928     0.0812     59.2423
07700     2.4951      0.0781     0.0813     59.8682
07800     2.4950      0.0886     0.0813     60.1090
07900     2.4950      0.0696     0.0813     59.3775
08000     2.4949      0.0612     0.0813     59.2809
08100     2.4949      0.1013     0.0813     58.8145
08200     2.4948      0.0675     0.0813     59.0205
08300     2.4947      0.0506     0.0813     61.0944
08400     2.4947      0.0823     0.0813     59.1149
08500     2.4946      0.0759     0.0813     59.2374
08600     2.4945      0.0633     0.0813     59.1682
08700     2.4945      0.0865     0.0813     59.2901
08800     2.4944      0.0823     0.0813     59.5198
08900     2.4943      0.0781     0.0813     59.0985
09000     2.4943      0.0675     0.0813     59.1631
09100     2.4942      0.0802     0.0813     59.8095
09200     2.4942      0.1013     0.0813     58.9640
09300     2.4941      0.0907     0.0813     59.1417
09400     2.4940      0.0738     0.0813     60.1799
09500     2.4940      0.0949     0.0813     59.4033
09600     2.4939      0.0865     0.0813     60.4250
09700     2.4938      0.0654     0.0813     59.3405
09800     2.4938      0.0654     0.0813     59.3590
09900     2.4937      0.0759     0.0813     59.8058
10000     2.4936      0.0823     0.0813     59.1230
10100     2.4936      0.0802     0.0813     59.4768
10200     2.4936      0.0696     0.0813     59.8048
10300     2.4936      0.0865     0.0813     59.8967
10400     2.4936      0.0675     0.0813     60.4939
10500     2.4936      0.0992     0.0813     58.5944
10600     2.4936      0.0717     0.0813     58.7641
10700     2.4935      0.0886     0.0813     60.1891
10800     2.4935      0.0823     0.0813     58.8464
10900     2.4935      0.0612     0.0813     59.3049
11000     2.4935      0.0928     0.0813     59.5628
11100     2.4935      0.0591     0.0813     59.3623
11200     2.4935      0.0675     0.0813     59.8280
11300     2.4934      0.0844     0.0813     59.5109
11400     2.4934      0.0844     0.0813     59.0109
11500     2.4934      0.1076     0.0813     59.4230
11600     2.4934      0.0717     0.0814     59.2891
11700     2.4934      0.0633     0.0814     59.1691
11800     2.4934      0.0928     0.0814     59.7669
11900     2.4933      0.0844     0.0814     59.4289
12000     2.4933      0.1160     0.0814     60.1966
12100     2.4933      0.0886     0.0814     59.4452
12200     2.4933      0.0886     0.0814     60.0402
12300     2.4933      0.0591     0.0814     59.9571
12400     2.4933      0.0738     0.0814     59.4144
12500     2.4933      0.0844     0.0814     58.8790
12600     2.4932      0.0802     0.0814     59.3998
12700     2.4932      0.0781     0.0814     59.1067
12800     2.4932      0.0781     0.0814     59.8320
12900     2.4932      0.0886     0.0814     59.2559
13000     2.4932      0.0928     0.0814     59.0414
13100     2.4932      0.0949     0.0814     59.6566
13200     2.4931      0.0675     0.0814     59.3041
13300     2.4931      0.0802     0.0814     59.0242
13400     2.4931      0.0802     0.0814     60.0469
13500     2.4931      0.0549     0.0814     59.1061
13600     2.4931      0.0738     0.0814     60.1129
13700     2.4931      0.0823     0.0814     59.6221
13800     2.4930      0.0844     0.0814     59.1404
13900     2.4930      0.0844     0.0814     59.7631
14000     2.4930      0.0886     0.0814     59.3563
14100     2.4930      0.0696     0.0814     59.3263
14200     2.4930      0.0823     0.0814     59.8332
14300     2.4930      0.0781     0.0814     59.3595
14400     2.4930      0.0844     0.0814     59.9214
14500     2.4929      0.0781     0.0814     59.7091
14600     2.4929      0.0781     0.0814     58.7648
14700     2.4929      0.0886     0.0814     60.6388
14800     2.4929      0.0738     0.0814     59.3493
14900     2.4929      0.0865     0.0814     59.1251
15000     2.4929      0.0823     0.0814     59.3078
15100     2.4928      0.0738     0.0814     59.2546
15200     2.4928      0.0844     0.0814     59.7363
15300     2.4928      0.0759     0.0814     59.2246
15400     2.4928      0.0717     0.0814     59.6723
15500     2.4928      0.0759     0.0814     59.2710
15600     2.4928      0.0886     0.0814     59.3130
15700     2.4928      0.0675     0.0814     59.3736
15800     2.4927      0.0844     0.0814     60.5178
15900     2.4927      0.1076     0.0814     59.7561
16000     2.4927      0.0612     0.0814     60.4165
16100     2.4927      0.0928     0.0814     59.6313
16200     2.4927      0.0802     0.0814     59.6019
16300     2.4927      0.0802     0.0814     59.1059
16400     2.4926      0.0738     0.0814     59.8432
16500     2.4926      0.0844     0.0814     61.1863
16600     2.4926      0.1076     0.0814     59.9692
16700     2.4926      0.0886     0.0814     59.5023
16800     2.4926      0.0612     0.0814     59.7024
16900     2.4926      0.0612     0.0814     59.5978
17000     2.4925      0.0612     0.0814     59.3738
17100     2.4925      0.0886     0.0814     59.5128
17200     2.4925      0.0717     0.0814     59.4099
17300     2.4925      0.0928     0.0814     59.1130
17400     2.4925      0.0844     0.0814     59.4946
17500     2.4925      0.0802     0.0817     59.5724
17600     2.4925      0.0886     0.0817     59.7092
17700     2.4924      0.0759     0.0817     59.8628
17800     2.4924      0.0844     0.0817     59.8783
17900     2.4924      0.0802     0.0817     59.7800
18000     2.4924      0.0633     0.0817     59.5555
18100     2.4924      0.0717     0.0817     59.2618
18200     2.4924      0.0823     0.0817     60.3332
18300     2.4923      0.0970     0.0817     59.7531
18400     2.4923      0.0738     0.0817     59.6604
18500     2.4923      0.1055     0.0817     59.7393
18600     2.4923      0.0781     0.0817     59.5541
18700     2.4923      0.0865     0.0817     59.7369
18800     2.4923      0.0823     0.0817     59.4765
18900     2.4922      0.0823     0.0817     58.2038
19000     2.4922      0.0970     0.0817     59.7055
19100     2.4922      0.0886     0.0817     59.2246
19200     2.4922      0.0907     0.0817     59.5152
19300     2.4922      0.0992     0.0817     59.6808
19400     2.4922      0.0865     0.0817     59.4904
19500     2.4922      0.0485     0.0817     59.6585
19600     2.4921      0.0823     0.0817     59.3437
19700     2.4921      0.0570     0.0817     58.8931
19800     2.4921      0.1055     0.0817     60.1307
19900     2.4921      0.0717     0.0817     59.5270
20000     2.4921      0.0675     0.0817     59.7324
20100     2.4921      0.0823     0.0817     59.3841
20199     2.4921      0.0844     0.0817     58.8533
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.4921      0.1076     0.0807     9.3573
00100     2.4921      0.0696     0.0807     59.0454
00200     2.4921      0.0823     0.0809     58.4455
00300     2.4921      0.0802     0.0809     59.2254
00400     2.4921      0.0865     0.0809     58.8105
00500     2.4921      0.0781     0.0809     58.1222
00600     2.4921      0.0865     0.0809     59.0857
00700     2.4921      0.0844     0.0809     58.3612
00800     2.4921      0.0612     0.0809     59.1364
00900     2.4920      0.0992     0.0810     59.0301
01000     2.4920      0.0654     0.0810     58.7978
01100     2.4920      0.0907     0.0810     58.8289
01200     2.4920      0.0907     0.0810     58.5842
01300     2.4920      0.0633     0.0813     58.8233
01400     2.4920      0.0696     0.0813     58.7204
01500     2.4920      0.0802     0.0813     58.7788
01600     2.4920      0.0696     0.0813     58.4900
01700     2.4920      0.0802     0.0813     58.9823
01800     2.4920      0.0823     0.0813     58.2171
01900     2.4920      0.0717     0.0813     58.9044
02000     2.4920      0.0907     0.0813     58.6786
02100     2.4920      0.0717     0.0813     58.6323
02200     2.4920      0.0759     0.0813     59.1206
02300     2.4920      0.0802     0.0813     58.6411
02400     2.4920      0.0759     0.0813     58.8896
02500     2.4920      0.0823     0.0813     58.8111
02600     2.4920      0.0675     0.0813     58.5126
02700     2.4920      0.0633     0.0813     58.7884
02800     2.4920      0.0802     0.0813     58.9357
02900     2.4920      0.0759     0.0813     58.2338
03000     2.4920      0.0970     0.0813     59.4977
03100     2.4920      0.0717     0.0813     58.9647
03200     2.4920      0.0886     0.0813     58.3705
03300     2.4920      0.0844     0.0813     59.4640
03400     2.4920      0.1224     0.0813     58.4621
03500     2.4920      0.0633     0.0813     58.7760
03600     2.4920      0.0759     0.0813     58.4052
03700     2.4920      0.0781     0.0813     58.1397
03800     2.4920      0.0865     0.0813     58.6289
03900     2.4920      0.0549     0.0813     58.4798
04000     2.4920      0.0823     0.0813     58.6528
04100     2.4920      0.0675     0.0813     58.9865
04200     2.4920      0.0759     0.0813     58.6321
04300     2.4920      0.0654     0.0813     58.7964
04400     2.4920      0.0844     0.0813     58.6101
04500     2.4920      0.0696     0.0813     58.5084
04600     2.4920      0.0738     0.0813     59.4946
04700     2.4920      0.0717     0.0813     58.8926
04800     2.4920      0.0865     0.0813     58.4287
04900     2.4920      0.0949     0.0813     59.0922
05000     2.4920      0.1013     0.0813     58.9353
05100     2.4919      0.0823     0.0813     60.1317
05200     2.4919      0.0549     0.0813     58.8340
05300     2.4919      0.0844     0.0813     58.5708
05400     2.4919      0.0865     0.0813     58.6704
05500     2.4919      0.0802     0.0813     58.9462
05600     2.4919      0.0970     0.0813     58.4410
05700     2.4919      0.0802     0.0813     59.2738
05800     2.4919      0.0844     0.0813     58.7874
05900     2.4919      0.0675     0.0813     58.8892
06000     2.4919      0.0886     0.0813     58.6450
06100     2.4919      0.0886     0.0813     58.9155
06200     2.4919      0.0907     0.0813     59.4073
06300     2.4919      0.0907     0.0813     58.2134
06400     2.4919      0.0970     0.0813     58.5699
06500     2.4919      0.0886     0.0813     59.2488
06600     2.4919      0.0717     0.0813     58.7628
06700     2.4919      0.0781     0.0813     59.1521
06800     2.4919      0.0738     0.0813     58.6507
06900     2.4919      0.0675     0.0813     58.6161
07000     2.4919      0.0823     0.0813     59.8902
07100     2.4919      0.0696     0.0813     58.8218
07200     2.4919      0.0591     0.0813     58.8434
07300     2.4919      0.0781     0.0813     59.5495
07400     2.4919      0.0802     0.0813     59.0885
07500     2.4919      0.0970     0.0813     59.2441
07600     2.4919      0.0612     0.0813     58.7777
07700     2.4919      0.0485     0.0813     58.9110
07800     2.4919      0.0970     0.0813     59.2433
07900     2.4919      0.0781     0.0813     58.3657
08000     2.4919      0.0612     0.0813     57.5719
08100     2.4919      0.1034     0.0813     59.2759
08200     2.4919      0.0844     0.0813     58.7046
08300     2.4919      0.0802     0.0813     59.3938
08400     2.4919      0.0865     0.0813     58.7777
08500     2.4919      0.0865     0.0813     59.0940
08600     2.4919      0.0738     0.0813     59.4196
08700     2.4919      0.0802     0.0813     58.6998
08800     2.4919      0.0823     0.0813     58.7154
08900     2.4919      0.0570     0.0813     59.8941
09000     2.4919      0.0928     0.0813     59.2745
09100     2.4919      0.0802     0.0813     59.4810
09200     2.4919      0.0591     0.0813     59.3520
09300     2.4918      0.0759     0.0813     60.6259
09400     2.4918      0.0485     0.0813     63.1405
09500     2.4918      0.0844     0.0813     59.7983
09600     2.4918      0.0527     0.0813     59.5342
09700     2.4918      0.0949     0.0813     60.2375
09800     2.4918      0.0717     0.0813     59.4297
09900     2.4918      0.0759     0.0813     59.5681
Start testing:
Test Accuracy: 0.0789
