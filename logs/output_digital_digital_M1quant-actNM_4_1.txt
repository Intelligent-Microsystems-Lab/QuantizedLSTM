Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 49, in __init__
    torch.nn.init.uniform_(self.cell.weight_ih, a = -limit2, b = limit2)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 49, in __init__
    torch.nn.init.uniform_(self.cell.weight_ih, a = -limit2, b = limit2)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 49, in __init__
    torch.nn.init.uniform_(self.cell.weight_ih, a = -limit2, b = limit2)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=235899598, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=235899598, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 362, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.0, finetuning_epochs=0, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.0005,0.0001,0.00002', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=235899598, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,10000', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
eda1603b-93f1-4a4c-9a23-82a01fd88615
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.5642      0.0633     0.0805     11.4660
00100     2.5434      0.0612     0.0811     70.6894
00200     2.5993      0.1350     0.1840     70.5969
00300     2.9394      0.1540     0.2079     71.7214
00400     3.7557      0.2194     0.2333     70.5594
00500     3.7062      0.2616     0.2722     71.1439
00600     3.7080      0.2869     0.2742     73.2723
00700     4.2374      0.2447     0.2806     71.6464
00800     3.8359      0.2996     0.3133     71.4535
00900     3.7161      0.2975     0.3133     71.4841
01000     4.0293      0.3017     0.3167     71.6197
01100     3.7792      0.3291     0.3409     71.3913
01200     3.9577      0.2532     0.3464     70.9542
01300     3.7447      0.3312     0.3464     70.6847
01400     3.7653      0.3249     0.3464     71.1049
01500     3.8506      0.3312     0.3554     70.7839
01600     3.6052      0.3312     0.3554     71.2689
01700     3.7570      0.3228     0.3576     71.1016
01800     3.3627      0.3460     0.3664     70.3732
01900     3.4964      0.3270     0.3664     71.5255
02000     3.5691      0.3608     0.3788     70.4760
02100     3.5147      0.3333     0.3788     71.2017
02200     3.4112      0.3608     0.3788     71.3717
02300     3.1988      0.3755     0.3788     71.0221
02400     3.4756      0.3333     0.3788     70.9159
02500     3.2834      0.3755     0.3879     71.4973
02600     3.2643      0.3502     0.3879     70.6749
02700     3.2841      0.3755     0.3879     72.4350
02800     3.0539      0.3713     0.3920     70.8099
02900     3.1120      0.3861     0.3997     70.5463
03000     2.9724      0.3755     0.4224     70.7102
03100     3.0389      0.3608     0.4234     71.4190
03200     2.9461      0.4177     0.4332     71.7697
03300     3.0338      0.4219     0.4354     71.3474
03400     2.9466      0.3629     0.4354     71.5845
03500     2.4743      0.4177     0.4402     71.2346
03600     2.6475      0.4198     0.4876     71.6375
03700     2.5066      0.4620     0.4876     71.1374
03800     2.4338      0.4831     0.4885     71.1763
03900     2.5600      0.4852     0.5281     71.4296
04000     2.4462      0.4726     0.5281     71.6090
04100     2.1378      0.5274     0.5335     71.3703
04200     2.4404      0.4768     0.5485     71.2521
04300     2.5352      0.5042     0.5493     71.5045
04400     2.3339      0.4705     0.5493     71.1113
04500     2.2076      0.5000     0.5565     71.5660
04600     2.3563      0.5464     0.5565     72.0244
04700     2.4638      0.4726     0.5565     71.3379
04800     2.4476      0.4937     0.5565     72.4766
04900     2.4867      0.5148     0.5565     71.2699
05000     2.6107      0.4473     0.5565     71.9661
05100     2.4951      0.4810     0.5565     72.4709
05200     2.4370      0.4895     0.5565     71.4192
05300     2.2578      0.5042     0.5565     70.9738
05400     2.1298      0.4937     0.5660     72.0666
05500     2.1114      0.5105     0.5660     71.5439
05600     2.3289      0.4789     0.5660     71.8607
05700     2.1963      0.5338     0.5660     71.9820
05800     2.1253      0.5485     0.5660     71.5831
05900     2.1973      0.5211     0.5730     72.3684
06000     2.1285      0.5316     0.5730     71.0727
06100     2.0447      0.5295     0.5730     72.1309
06200     2.1583      0.5000     0.5730     71.6833
06300     2.1755      0.5127     0.5765     72.0140
06400     1.8542      0.5654     0.5887     72.4243
06500     2.0817      0.5232     0.5887     72.0056
06600     2.3187      0.5063     0.5887     71.5671
06700     1.8729      0.5781     0.5887     72.7841
06800     2.1703      0.5485     0.6065     72.0337
06900     2.1625      0.5886     0.6065     72.0309
07000     1.9333      0.5549     0.6065     72.2371
07100     2.1270      0.5485     0.6098     72.1516
07200     1.8472      0.5464     0.6098     72.6822
07300     2.0617      0.5464     0.6098     72.1327
07400     1.9260      0.5232     0.6098     72.0291
07500     2.2969      0.4937     0.6098     72.6944
07600     1.9891      0.5527     0.6098     71.6750
07700     2.0139      0.5464     0.6116     71.7574
07800     1.9739      0.5591     0.6116     73.2699
07900     1.9025      0.5485     0.6116     71.8822
08000     1.8985      0.5443     0.6116     71.9785
08100     2.0003      0.5443     0.6116     72.1357
08200     2.0629      0.5675     0.6150     73.5789
08300     2.0611      0.5401     0.6150     72.0683
08400     2.0529      0.4768     0.6150     71.7536
08500     1.9086      0.5591     0.6150     72.5725
08600     1.7690      0.5633     0.6150     72.5535
08700     2.1660      0.5359     0.6150     72.0159
08800     2.0222      0.5359     0.6150     72.6122
08900     1.9533      0.5359     0.6150     72.6917
09000     1.7460      0.5781     0.6269     72.9603
09100     1.9336      0.5295     0.6269     72.4858
09200     1.6453      0.5802     0.6384     72.2492
09300     1.8302      0.5211     0.6384     71.9843
09400     2.0862      0.5338     0.6384     72.6739
09500     1.8344      0.5570     0.6384     72.6011
09600     1.9343      0.5549     0.6409     72.7520
09700     1.8722      0.5633     0.6409     72.1656
09800     1.5911      0.5865     0.6409     71.6535
09900     1.7773      0.5485     0.6409     72.7835
10000     1.6893      0.5949     0.6506     71.9609
10100     1.5838      0.6013     0.6701     72.2184
10200     1.6664      0.5802     0.6701     72.1290
10300     1.7692      0.5928     0.6701     72.0603
10400     1.7821      0.6224     0.6701     72.4842
10500     1.6716      0.5717     0.6701     72.2920
10600     1.9292      0.5759     0.6701     72.6656
10700     1.9099      0.5844     0.6701     72.6165
10800     1.8596      0.5738     0.6701     72.6001
10900     1.6993      0.6118     0.6701     71.9170
11000     1.6293      0.5654     0.6701     72.3392
11100     1.7202      0.6013     0.6701     72.0217
11200     1.9204      0.5654     0.6701     72.3948
11300     2.0040      0.5886     0.6701     72.2022
11400     1.8338      0.5928     0.6701     71.9473
11500     1.6763      0.5781     0.6701     73.0710
11600     1.7699      0.5970     0.6701     71.8960
11700     1.7534      0.5928     0.6701     72.8649
11800     1.8230      0.6013     0.6701     73.0449
11900     1.8686      0.6034     0.6701     72.1828
12000     1.6720      0.5844     0.6701     72.9203
12100     1.6697      0.5781     0.6701     72.0444
12200     1.9054      0.5696     0.6701     71.7237
12300     1.7404      0.5949     0.6701     72.1543
12400     1.7987      0.5886     0.6701     72.2072
12500     1.4153      0.6371     0.6701     72.1275
12600     1.4362      0.6097     0.6705     72.3307
12700     1.6635      0.6034     0.6705     72.3484
12800     1.8480      0.5675     0.6705     72.2699
12900     1.7105      0.5886     0.6705     72.3695
13000     1.7077      0.6118     0.6705     71.4761
13100     1.9532      0.5802     0.6705     72.8874
13200     1.9758      0.5844     0.6705     72.9488
13300     1.6095      0.6203     0.6705     72.3850
13400     1.5413      0.6076     0.6705     72.3092
13500     1.9055      0.5612     0.6705     71.8788
13600     1.6146      0.6118     0.6705     72.4322
13700     1.7841      0.6118     0.6705     72.3487
13800     1.7614      0.5844     0.6705     72.9573
13900     1.6702      0.5970     0.6705     72.7568
14000     1.6564      0.6203     0.6705     72.5790
14100     1.6938      0.5359     0.6705     72.0691
14200     1.6648      0.5781     0.6705     72.6383
14300     1.7571      0.5781     0.6705     72.5814
14400     1.8147      0.5759     0.6705     72.6647
14500     1.6766      0.5823     0.6705     73.0064
14600     1.6738      0.6034     0.6705     72.5636
14700     1.7881      0.5717     0.6705     72.6480
14800     1.6135      0.6139     0.6705     71.7335
14900     1.4776      0.6013     0.6705     71.9623
15000     1.5536      0.5738     0.6705     72.3971
15100     1.4886      0.6329     0.6705     71.9834
15200     1.6623      0.5928     0.6705     72.6485
15300     1.8818      0.5443     0.6705     72.7175
15400     1.5744      0.6076     0.6705     72.3068
15500     1.6424      0.6076     0.6705     72.9786
15600     1.6478      0.5802     0.6705     72.3302
15700     1.8597      0.5549     0.6705     72.2871
15800     1.5676      0.6266     0.6705     72.8030
15900     1.5778      0.6203     0.6705     71.3497
16000     1.5170      0.6118     0.6705     72.9177
16100     1.8215      0.5928     0.6705     70.7992
16200     1.6729      0.5591     0.6705     72.2204
16300     1.5705      0.6203     0.6705     73.1165
16400     1.5979      0.5949     0.6705     72.1752
16500     1.4877      0.6203     0.6705     72.6848
16600     1.7841      0.5717     0.6705     72.7315
16700     1.5911      0.6287     0.6705     72.3446
16800     1.6571      0.5970     0.6705     72.7494
16900     1.6355      0.5696     0.6705     72.7226
17000     1.8178      0.5717     0.6705     73.0725
17100     1.6102      0.5802     0.6705     71.9983
17200     1.6838      0.6097     0.6705     72.6492
17300     1.7460      0.6013     0.6705     71.8891
17400     1.7775      0.5759     0.6705     72.5243
17500     1.4937      0.6350     0.6705     72.2328
17600     1.6706      0.5612     0.6705     71.5338
17700     1.5678      0.6013     0.6705     72.8012
17800     1.7286      0.5781     0.6705     72.5247
17900     1.6194      0.5781     0.6705     72.5850
18000     1.5149      0.6118     0.6705     72.0712
18100     1.6897      0.6224     0.6705     72.2074
18200     1.7411      0.5759     0.6705     72.7990
18300     1.7645      0.5970     0.6705     72.0033
18400     1.8324      0.5380     0.6705     72.0832
18500     1.6936      0.5717     0.6705     71.7311
18600     1.8716      0.5738     0.6705     72.5538
18700     1.5450      0.5949     0.6705     72.5600
18800     1.4849      0.6477     0.6808     72.1505
18900     1.4013      0.6392     0.6808     71.8964
19000     1.5680      0.6287     0.6808     72.5973
19100     2.0121      0.5949     0.6808     72.3511
19200     1.5577      0.6013     0.6808     73.0374
19300     1.7255      0.6308     0.6808     71.5814
19400     1.6301      0.6139     0.6808     72.0956
19500     1.9408      0.5949     0.6808     72.7411
19600     1.5278      0.6329     0.6939     72.6909
19700     1.7344      0.6371     0.6939     72.2971
19800     1.6048      0.6582     0.6939     71.7239
19900     1.5978      0.5738     0.6939     72.2511
20000     1.4161      0.6688     0.6939     72.4645
20100     1.6592      0.5865     0.6939     72.6917
20200     1.5230      0.6350     0.6939     72.7296
20300     1.2329      0.6519     0.6939     72.7110
20400     1.6579      0.5886     0.6939     72.1248
20500     1.6479      0.6097     0.6939     71.7061
20600     1.6306      0.6055     0.6939     71.9590
20700     1.5037      0.6203     0.6939     71.5551
20800     1.7138      0.5781     0.6939     72.4120
20900     1.6242      0.6139     0.6939     71.7784
21000     1.6881      0.6034     0.6939     71.5789
21100     1.6780      0.6160     0.6939     72.9007
21200     1.5416      0.5949     0.6939     71.8149
21300     1.5933      0.6329     0.6939     71.9215
21400     1.6037      0.6245     0.6939     71.3002
21500     1.5902      0.6266     0.6939     72.4424
21600     1.6456      0.5759     0.6939     72.5580
21700     1.6910      0.6097     0.6939     71.8681
21800     1.8867      0.5886     0.6939     72.1402
21900     1.6130      0.6034     0.6939     72.8904
22000     1.8451      0.6203     0.6939     72.0707
22100     1.3698      0.6477     0.6939     71.8797
22200     1.7845      0.5675     0.6939     73.2626
22300     1.6666      0.6160     0.6939     71.7972
22400     1.6039      0.6287     0.6939     72.1187
22500     1.4247      0.6139     0.6939     71.6584
22600     1.8910      0.5802     0.6939     71.9354
22700     1.7247      0.6224     0.6939     72.8584
22800     1.4981      0.6224     0.6939     71.7600
22900     1.7503      0.5759     0.6939     72.4578
23000     1.5557      0.6076     0.6939     73.1565
23100     1.8369      0.5886     0.6939     72.0382
23200     1.3699      0.6582     0.6939     72.4386
23300     1.5708      0.6160     0.6939     71.8252
23400     1.9129      0.5802     0.6939     71.3851
23500     1.3967      0.6519     0.6939     72.9428
23600     1.6944      0.6414     0.6939     72.5164
23700     1.6765      0.6076     0.6939     72.6239
23800     1.4813      0.6414     0.6939     72.3918
23900     1.6492      0.6139     0.6939     72.9842
24000     1.5916      0.6224     0.6939     73.2917
24100     1.8267      0.5717     0.6939     71.9982
24200     1.5775      0.6435     0.6939     71.4130
24300     1.6755      0.6435     0.6939     73.0402
24400     1.6836      0.6160     0.6939     73.1436
24500     1.6040      0.6287     0.6939     72.5006
24600     1.5351      0.6350     0.6939     72.0535
24700     1.6212      0.6224     0.6939     72.4629
24800     1.7154      0.5928     0.6939     72.3267
24900     1.4913      0.6329     0.6939     72.7664
25000     1.5141      0.6245     0.6939     72.6659
25100     1.6692      0.6371     0.6939     73.2260
25200     1.4042      0.6139     0.6939     72.5685
25300     1.7148      0.6118     0.6939     73.1114
25400     1.7069      0.5928     0.6939     72.8997
25500     1.6488      0.6139     0.6939     71.6153
25600     1.5496      0.6160     0.6939     73.0825
25700     1.5835      0.5781     0.6939     72.8181
25800     1.7780      0.5949     0.6939     72.6736
25900     1.7696      0.5823     0.6939     72.9096
26000     1.6112      0.5949     0.6939     72.7111
26100     1.5132      0.6371     0.6939     71.7483
26200     1.7713      0.5907     0.6939     72.5069
26300     1.6854      0.6118     0.6939     72.1066
26400     1.8622      0.6055     0.6939     72.1200
26500     1.8975      0.5907     0.6943     72.2291
26600     1.6600      0.6308     0.6943     71.7595
26700     1.8833      0.5949     0.6943     72.1191
26800     1.7426      0.5970     0.6943     71.7847
26900     1.5468      0.6287     0.6943     72.1713
27000     1.5068      0.6160     0.6943     72.3334
27100     1.5775      0.6245     0.6943     72.2496
27200     1.6391      0.6245     0.6943     72.2777
27300     1.4625      0.6160     0.6943     73.2740
27400     1.5786      0.6013     0.6943     71.9279
27500     1.5929      0.5970     0.6943     72.3026
27600     1.7189      0.5949     0.6943     71.8124
27700     1.9594      0.5485     0.6943     72.3189
27800     1.5038      0.6139     0.6943     72.3304
27900     1.5931      0.5970     0.6943     73.1362
28000     1.7384      0.5844     0.6943     72.5986
28100     1.6276      0.5886     0.6943     73.0020
28200     1.5267      0.6350     0.6943     72.4871
28300     1.6064      0.6329     0.6943     72.5382
28400     1.6310      0.5844     0.6943     71.7788
28500     1.5279      0.6329     0.6943     71.9381
28600     1.5556      0.6245     0.6943     73.0629
28700     1.5340      0.6646     0.6943     72.0700
28800     1.7267      0.6371     0.6943     73.3422
28900     1.9457      0.6181     0.6943     72.3944
29000     1.7664      0.6118     0.6943     72.5292
29100     1.7052      0.6139     0.6943     72.4333
29200     1.6578      0.5992     0.6943     72.5357
29300     1.5752      0.6456     0.6943     72.3201
29400     1.6961      0.5949     0.6943     72.6263
29500     1.7462      0.6203     0.6943     71.7383
29600     1.9095      0.5970     0.6943     72.8080
29700     1.7084      0.6266     0.6943     71.6634
29800     1.5963      0.6245     0.6943     72.7121
29900     1.6802      0.5886     0.6943     72.8918
29999     1.7783      0.5717     0.6943     71.4469
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Start testing:
Test Accuracy: 0.6879
