Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 49, in __init__
    torch.nn.init.uniform_(self.cell.weight_ih, a = -limit2, b = limit2)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 49, in __init__
    torch.nn.init.uniform_(self.cell.weight_ih, a = -limit2, b = limit2)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 49, in __init__
    torch.nn.init.uniform_(self.cell.weight_ih, a = -limit2, b = limit2)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=235899598, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=235899598, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 362, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.0, finetuning_epochs=0, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.0005,0.0001,0.00002', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=235899598, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,10000', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
eda1603b-93f1-4a4c-9a23-82a01fd88615
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.5642      0.0633     0.0805     11.4660
00100     2.5434      0.0612     0.0811     70.6894
00200     2.5993      0.1350     0.1840     70.5969
00300     2.9394      0.1540     0.2079     71.7214
00400     3.7557      0.2194     0.2333     70.5594
00500     3.7062      0.2616     0.2722     71.1439
00600     3.7080      0.2869     0.2742     73.2723
00700     4.2374      0.2447     0.2806     71.6464
00800     3.8359      0.2996     0.3133     71.4535
00900     3.7161      0.2975     0.3133     71.4841
01000     4.0293      0.3017     0.3167     71.6197
01100     3.7792      0.3291     0.3409     71.3913
01200     3.9577      0.2532     0.3464     70.9542
01300     3.7447      0.3312     0.3464     70.6847
01400     3.7653      0.3249     0.3464     71.1049
01500     3.8506      0.3312     0.3554     70.7839
01600     3.6052      0.3312     0.3554     71.2689
01700     3.7570      0.3228     0.3576     71.1016
01800     3.3627      0.3460     0.3664     70.3732
01900     3.4964      0.3270     0.3664     71.5255
02000     3.5691      0.3608     0.3788     70.4760
02100     3.5147      0.3333     0.3788     71.2017
02200     3.4112      0.3608     0.3788     71.3717
02300     3.1988      0.3755     0.3788     71.0221
02400     3.4756      0.3333     0.3788     70.9159
02500     3.2834      0.3755     0.3879     71.4973
02600     3.2643      0.3502     0.3879     70.6749
02700     3.2841      0.3755     0.3879     72.4350
02800     3.0539      0.3713     0.3920     70.8099
02900     3.1120      0.3861     0.3997     70.5463
03000     2.9724      0.3755     0.4224     70.7102
03100     3.0389      0.3608     0.4234     71.4190
03200     2.9461      0.4177     0.4332     71.7697
03300     3.0338      0.4219     0.4354     71.3474
03400     2.9466      0.3629     0.4354     71.5845
03500     2.4743      0.4177     0.4402     71.2346
03600     2.6475      0.4198     0.4876     71.6375
03700     2.5066      0.4620     0.4876     71.1374
03800     2.4338      0.4831     0.4885     71.1763
03900     2.5600      0.4852     0.5281     71.4296
04000     2.4462      0.4726     0.5281     71.6090
04100     2.1378      0.5274     0.5335     71.3703
04200     2.4404      0.4768     0.5485     71.2521
04300     2.5352      0.5042     0.5493     71.5045
04400     2.3339      0.4705     0.5493     71.1113
04500     2.2076      0.5000     0.5565     71.5660
04600     2.3563      0.5464     0.5565     72.0244
04700     2.4638      0.4726     0.5565     71.3379
04800     2.4476      0.4937     0.5565     72.4766
04900     2.4867      0.5148     0.5565     71.2699
05000     2.6107      0.4473     0.5565     71.9661
05100     2.4951      0.4810     0.5565     72.4709
05200     2.4370      0.4895     0.5565     71.4192
05300     2.2578      0.5042     0.5565     70.9738
05400     2.1298      0.4937     0.5660     72.0666
05500     2.1114      0.5105     0.5660     71.5439
05600     2.3289      0.4789     0.5660     71.8607
05700     2.1963      0.5338     0.5660     71.9820
05800     2.1253      0.5485     0.5660     71.5831
05900     2.1973      0.5211     0.5730     72.3684
06000     2.1285      0.5316     0.5730     71.0727
06100     2.0447      0.5295     0.5730     72.1309
06200     2.1583      0.5000     0.5730     71.6833
06300     2.1755      0.5127     0.5765     72.0140
06400     1.8542      0.5654     0.5887     72.4243
06500     2.0817      0.5232     0.5887     72.0056
06600     2.3187      0.5063     0.5887     71.5671
06700     1.8729      0.5781     0.5887     72.7841
06800     2.1703      0.5485     0.6065     72.0337
06900     2.1625      0.5886     0.6065     72.0309
07000     1.9333      0.5549     0.6065     72.2371
07100     2.1270      0.5485     0.6098     72.1516
07200     1.8472      0.5464     0.6098     72.6822
07300     2.0617      0.5464     0.6098     72.1327
07400     1.9260      0.5232     0.6098     72.0291
07500     2.2969      0.4937     0.6098     72.6944
07600     1.9891      0.5527     0.6098     71.6750
07700     2.0139      0.5464     0.6116     71.7574
07800     1.9739      0.5591     0.6116     73.2699
07900     1.9025      0.5485     0.6116     71.8822
08000     1.8985      0.5443     0.6116     71.9785
08100     2.0003      0.5443     0.6116     72.1357
08200     2.0629      0.5675     0.6150     73.5789
08300     2.0611      0.5401     0.6150     72.0683
08400     2.0529      0.4768     0.6150     71.7536
08500     1.9086      0.5591     0.6150     72.5725
08600     1.7690      0.5633     0.6150     72.5535
08700     2.1660      0.5359     0.6150     72.0159
08800     2.0222      0.5359     0.6150     72.6122
08900     1.9533      0.5359     0.6150     72.6917
09000     1.7460      0.5781     0.6269     72.9603
09100     1.9336      0.5295     0.6269     72.4858
09200     1.6453      0.5802     0.6384     72.2492
09300     1.8302      0.5211     0.6384     71.9843
09400     2.0862      0.5338     0.6384     72.6739
09500     1.8344      0.5570     0.6384     72.6011
09600     1.9343      0.5549     0.6409     72.7520
09700     1.8722      0.5633     0.6409     72.1656
09800     1.5911      0.5865     0.6409     71.6535
09900     1.7773      0.5485     0.6409     72.7835
10000     1.6893      0.5949     0.6506     71.9609
10100     1.5838      0.6013     0.6701     72.2184
10200     1.6664      0.5802     0.6701     72.1290
10300     1.7692      0.5928     0.6701     72.0603
10400     1.7821      0.6224     0.6701     72.4842
10500     1.6716      0.5717     0.6701     72.2920
10600     1.9292      0.5759     0.6701     72.6656
10700     1.9099      0.5844     0.6701     72.6165
10800     1.8596      0.5738     0.6701     72.6001
10900     1.6993      0.6118     0.6701     71.9170
11000     1.6293      0.5654     0.6701     72.3392
11100     1.7202      0.6013     0.6701     72.0217
11200     1.9204      0.5654     0.6701     72.3948
11300     2.0040      0.5886     0.6701     72.2022
11400     1.8338      0.5928     0.6701     71.9473
11500     1.6763      0.5781     0.6701     73.0710
11600     1.7699      0.5970     0.6701     71.8960
11700     1.7534      0.5928     0.6701     72.8649
11800     1.8230      0.6013     0.6701     73.0449
11900     1.8686      0.6034     0.6701     72.1828
12000     1.6720      0.5844     0.6701     72.9203
12100     1.6697      0.5781     0.6701     72.0444
12200     1.9054      0.5696     0.6701     71.7237
12300     1.7404      0.5949     0.6701     72.1543
12400     1.7987      0.5886     0.6701     72.2072
12500     1.4153      0.6371     0.6701     72.1275
12600     1.4362      0.6097     0.6705     72.3307
12700     1.6635      0.6034     0.6705     72.3484
12800     1.8480      0.5675     0.6705     72.2699
12900     1.7105      0.5886     0.6705     72.3695
13000     1.7077      0.6118     0.6705     71.4761
13100     1.9532      0.5802     0.6705     72.8874
13200     1.9758      0.5844     0.6705     72.9488
13300     1.6095      0.6203     0.6705     72.3850
13400     1.5413      0.6076     0.6705     72.3092
13500     1.9055      0.5612     0.6705     71.8788
13600     1.6146      0.6118     0.6705     72.4322
13700     1.7841      0.6118     0.6705     72.3487
13800     1.7614      0.5844     0.6705     72.9573
13900     1.6702      0.5970     0.6705     72.7568
14000     1.6564      0.6203     0.6705     72.5790
14100     1.6938      0.5359     0.6705     72.0691
14200     1.6648      0.5781     0.6705     72.6383
14300     1.7571      0.5781     0.6705     72.5814
14400     1.8147      0.5759     0.6705     72.6647
14500     1.6766      0.5823     0.6705     73.0064
14600     1.6738      0.6034     0.6705     72.5636
14700     1.7881      0.5717     0.6705     72.6480
14800     1.6135      0.6139     0.6705     71.7335
14900     1.4776      0.6013     0.6705     71.9623
15000     1.5536      0.5738     0.6705     72.3971
15100     1.4886      0.6329     0.6705     71.9834
15200     1.6623      0.5928     0.6705     72.6485
15300     1.8818      0.5443     0.6705     72.7175
15400     1.5744      0.6076     0.6705     72.3068
15500     1.6424      0.6076     0.6705     72.9786
15600     1.6478      0.5802     0.6705     72.3302
15700     1.8597      0.5549     0.6705     72.2871
15800     1.5676      0.6266     0.6705     72.8030
15900     1.5778      0.6203     0.6705     71.3497
16000     1.5170      0.6118     0.6705     72.9177
16100     1.8215      0.5928     0.6705     70.7992
16200     1.6729      0.5591     0.6705     72.2204
16300     1.5705      0.6203     0.6705     73.1165
16400     1.5979      0.5949     0.6705     72.1752
16500     1.4877      0.6203     0.6705     72.6848
16600     1.7841      0.5717     0.6705     72.7315
16700     1.5911      0.6287     0.6705     72.3446
16800     1.6571      0.5970     0.6705     72.7494
16900     1.6355      0.5696     0.6705     72.7226
17000     1.8178      0.5717     0.6705     73.0725
17100     1.6102      0.5802     0.6705     71.9983
17200     1.6838      0.6097     0.6705     72.6492
17300     1.7460      0.6013     0.6705     71.8891
17400     1.7775      0.5759     0.6705     72.5243
17500     1.4937      0.6350     0.6705     72.2328
17600     1.6706      0.5612     0.6705     71.5338
17700     1.5678      0.6013     0.6705     72.8012
17800     1.7286      0.5781     0.6705     72.5247
17900     1.6194      0.5781     0.6705     72.5850
18000     1.5149      0.6118     0.6705     72.0712
18100     1.6897      0.6224     0.6705     72.2074
18200     1.7411      0.5759     0.6705     72.7990
18300     1.7645      0.5970     0.6705     72.0033
18400     1.8324      0.5380     0.6705     72.0832
18500     1.6936      0.5717     0.6705     71.7311
18600     1.8716      0.5738     0.6705     72.5538
18700     1.5450      0.5949     0.6705     72.5600
18800     1.4849      0.6477     0.6808     72.1505
18900     1.4013      0.6392     0.6808     71.8964
19000     1.5680      0.6287     0.6808     72.5973
19100     2.0121      0.5949     0.6808     72.3511
19200     1.5577      0.6013     0.6808     73.0374
19300     1.7255      0.6308     0.6808     71.5814
19400     1.6301      0.6139     0.6808     72.0956
19500     1.9408      0.5949     0.6808     72.7411
19600     1.5278      0.6329     0.6939     72.6909
19700     1.7344      0.6371     0.6939     72.2971
19800     1.6048      0.6582     0.6939     71.7239
19900     1.5978      0.5738     0.6939     72.2511
20000     1.4161      0.6688     0.6939     72.4645
20100     1.6592      0.5865     0.6939     72.6917
20200     1.5230      0.6350     0.6939     72.7296
20300     1.2329      0.6519     0.6939     72.7110
20400     1.6579      0.5886     0.6939     72.1248
20500     1.6479      0.6097     0.6939     71.7061
20600     1.6306      0.6055     0.6939     71.9590
20700     1.5037      0.6203     0.6939     71.5551
20800     1.7138      0.5781     0.6939     72.4120
20900     1.6242      0.6139     0.6939     71.7784
21000     1.6881      0.6034     0.6939     71.5789
21100     1.6780      0.6160     0.6939     72.9007
21200     1.5416      0.5949     0.6939     71.8149
21300     1.5933      0.6329     0.6939     71.9215
21400     1.6037      0.6245     0.6939     71.3002
21500     1.5902      0.6266     0.6939     72.4424
21600     1.6456      0.5759     0.6939     72.5580
21700     1.6910      0.6097     0.6939     71.8681
21800     1.8867      0.5886     0.6939     72.1402
21900     1.6130      0.6034     0.6939     72.8904
22000     1.8451      0.6203     0.6939     72.0707
22100     1.3698      0.6477     0.6939     71.8797
22200     1.7845      0.5675     0.6939     73.2626
22300     1.6666      0.6160     0.6939     71.7972
22400     1.6039      0.6287     0.6939     72.1187
22500     1.4247      0.6139     0.6939     71.6584
22600     1.8910      0.5802     0.6939     71.9354
22700     1.7247      0.6224     0.6939     72.8584
22800     1.4981      0.6224     0.6939     71.7600
22900     1.7503      0.5759     0.6939     72.4578
23000     1.5557      0.6076     0.6939     73.1565
23100     1.8369      0.5886     0.6939     72.0382
23200     1.3699      0.6582     0.6939     72.4386
23300     1.5708      0.6160     0.6939     71.8252
23400     1.9129      0.5802     0.6939     71.3851
23500     1.3967      0.6519     0.6939     72.9428
23600     1.6944      0.6414     0.6939     72.5164
23700     1.6765      0.6076     0.6939     72.6239
23800     1.4813      0.6414     0.6939     72.3918
23900     1.6492      0.6139     0.6939     72.9842
24000     1.5916      0.6224     0.6939     73.2917
24100     1.8267      0.5717     0.6939     71.9982
24200     1.5775      0.6435     0.6939     71.4130
24300     1.6755      0.6435     0.6939     73.0402
24400     1.6836      0.6160     0.6939     73.1436
24500     1.6040      0.6287     0.6939     72.5006
24600     1.5351      0.6350     0.6939     72.0535
24700     1.6212      0.6224     0.6939     72.4629
24800     1.7154      0.5928     0.6939     72.3267
24900     1.4913      0.6329     0.6939     72.7664
25000     1.5141      0.6245     0.6939     72.6659
25100     1.6692      0.6371     0.6939     73.2260
25200     1.4042      0.6139     0.6939     72.5685
25300     1.7148      0.6118     0.6939     73.1114
25400     1.7069      0.5928     0.6939     72.8997
25500     1.6488      0.6139     0.6939     71.6153
25600     1.5496      0.6160     0.6939     73.0825
25700     1.5835      0.5781     0.6939     72.8181
25800     1.7780      0.5949     0.6939     72.6736
25900     1.7696      0.5823     0.6939     72.9096
26000     1.6112      0.5949     0.6939     72.7111
26100     1.5132      0.6371     0.6939     71.7483
26200     1.7713      0.5907     0.6939     72.5069
26300     1.6854      0.6118     0.6939     72.1066
26400     1.8622      0.6055     0.6939     72.1200
26500     1.8975      0.5907     0.6943     72.2291
26600     1.6600      0.6308     0.6943     71.7595
26700     1.8833      0.5949     0.6943     72.1191
26800     1.7426      0.5970     0.6943     71.7847
26900     1.5468      0.6287     0.6943     72.1713
27000     1.5068      0.6160     0.6943     72.3334
27100     1.5775      0.6245     0.6943     72.2496
27200     1.6391      0.6245     0.6943     72.2777
27300     1.4625      0.6160     0.6943     73.2740
27400     1.5786      0.6013     0.6943     71.9279
27500     1.5929      0.5970     0.6943     72.3026
27600     1.7189      0.5949     0.6943     71.8124
27700     1.9594      0.5485     0.6943     72.3189
27800     1.5038      0.6139     0.6943     72.3304
27900     1.5931      0.5970     0.6943     73.1362
28000     1.7384      0.5844     0.6943     72.5986
28100     1.6276      0.5886     0.6943     73.0020
28200     1.5267      0.6350     0.6943     72.4871
28300     1.6064      0.6329     0.6943     72.5382
28400     1.6310      0.5844     0.6943     71.7788
28500     1.5279      0.6329     0.6943     71.9381
28600     1.5556      0.6245     0.6943     73.0629
28700     1.5340      0.6646     0.6943     72.0700
28800     1.7267      0.6371     0.6943     73.3422
28900     1.9457      0.6181     0.6943     72.3944
29000     1.7664      0.6118     0.6943     72.5292
29100     1.7052      0.6139     0.6943     72.4333
29200     1.6578      0.5992     0.6943     72.5357
29300     1.5752      0.6456     0.6943     72.3201
29400     1.6961      0.5949     0.6943     72.6263
29500     1.7462      0.6203     0.6943     71.7383
29600     1.9095      0.5970     0.6943     72.8080
29700     1.7084      0.6266     0.6943     71.6634
29800     1.5963      0.6245     0.6943     72.7121
29900     1.6802      0.5886     0.6943     72.8918
29999     1.7783      0.5717     0.6943     71.4469
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Start testing:
Test Accuracy: 0.6879
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=235899598, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
ee99f928-46b6-40b9-958d-12f1e500003a
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.6457      0.0633     0.0802     15.5975
00100     2.5963      0.1160     0.0807     76.6568
00200     4.0631      0.2215     0.0815     76.5385
00300     4.4124      0.2363     0.0815     75.9607
00400     4.1100      0.2764     0.0821     77.6572
00500     3.9041      0.2616     0.0821     74.7252
00600     3.5338      0.3333     0.0821     77.1516
00700     3.5916      0.3207     0.0865     76.2108
00800     3.2320      0.4008     0.0865     75.3648
00900     2.6367      0.3945     0.0950     75.9923
01000     2.5965      0.4451     0.0950     76.7731
01100     2.6830      0.3903     0.1029     77.5745
01200     2.5357      0.4831     0.1152     75.0160
01300     2.3221      0.4641     0.1194     76.9670
01400     2.1936      0.4705     0.1194     76.9279
01500     2.0051      0.5105     0.1504     76.1906
01600     2.2600      0.5169     0.1710     78.2145
01700     1.6851      0.6013     0.1710     76.5074
01800     2.0256      0.5359     0.1710     76.8638
01900     1.9448      0.5401     0.1710     75.7713
02000     1.8848      0.5274     0.1710     76.1381
02100     1.7483      0.5401     0.1710     74.4808
02200     1.9906      0.5422     0.1779     76.0854
02300     1.9260      0.5506     0.1779     74.3471
02400     2.0918      0.5148     0.1932     76.7413
02500     1.8669      0.5316     0.1932     74.6019
02600     1.8080      0.5485     0.1932     76.7646
02700     1.8222      0.5169     0.1932     75.5659
02800     1.9213      0.5759     0.1932     76.4273
02900     1.9201      0.5802     0.2029     76.7612
03000     1.8253      0.5506     0.2029     75.9638
03100     1.6008      0.5781     0.2029     75.3793
03200     1.9107      0.5907     0.2029     76.1163
03300     1.6845      0.5570     0.2095     74.8565
03400     1.7542      0.5886     0.2095     76.8824
03500     1.6123      0.6329     0.2095     75.2808
03600     1.9175      0.5464     0.2095     73.8796
03700     1.7484      0.5781     0.2095     75.4141
03800     1.5810      0.5886     0.2095     76.2558
03900     1.7824      0.6498     0.2181     76.8221
04000     1.9259      0.5865     0.2181     74.9983
04100     1.6817      0.5992     0.2181     75.4744
04200     1.8952      0.5738     0.2181     77.7496
04300     2.0164      0.5886     0.2181     75.9767
04400     1.7877      0.6055     0.2181     76.3453
04500     1.4975      0.6498     0.2181     76.4799
04600     1.4747      0.6076     0.2181     76.8553
04700     1.7364      0.5738     0.2181     76.0921
04800     1.4186      0.6371     0.2181     76.3749
04900     1.7771      0.6055     0.2181     76.8268
05000     1.7356      0.6287     0.2183     76.2634
05100     1.7075      0.6266     0.2344     77.0481
05200     1.6273      0.6203     0.2344     76.6066
05300     1.5522      0.6266     0.2344     77.4414
05400     1.7307      0.5696     0.2344     76.9520
05500     1.6194      0.6224     0.2344     76.6245
05600     1.6919      0.5907     0.2344     75.4680
05700     1.5702      0.6371     0.2344     74.3714
05800     1.6529      0.6266     0.2344     76.2704
05900     1.7989      0.5844     0.2518     77.4435
06000     1.6970      0.5696     0.2549     77.2437
06100     1.9293      0.6034     0.2549     77.9984
06200     1.5814      0.6203     0.2549     75.2766
06300     1.4514      0.6266     0.2569     75.4439
06400     1.6495      0.5738     0.2569     75.8678
06500     1.4259      0.6498     0.2569     74.4938
06600     1.7614      0.6055     0.2569     74.7075
06700     1.5016      0.6624     0.2569     76.4454
06800     1.6519      0.6392     0.2662     75.8708
06900     1.3559      0.6519     0.2725     75.7048
07000     1.6451      0.6203     0.2729     74.9552
07100     1.6912      0.5928     0.2788     76.4966
07200     1.8663      0.5696     0.2863     76.6826
07300     1.9119      0.6371     0.2863     76.4414
07400     1.3352      0.6519     0.2863     76.7708
07500     1.4582      0.6498     0.2957     76.4919
07600     1.4384      0.6730     0.2957     77.1104
07700     1.6187      0.6477     0.2957     76.3232
07800     1.4965      0.6540     0.2957     77.6466
07900     1.6306      0.6076     0.2957     75.4406
08000     1.5993      0.6245     0.2957     77.2889
08100     1.1968      0.7046     0.2957     75.1736
08200     1.6537      0.6266     0.2957     75.6196
08300     1.6613      0.6519     0.2957     76.1498
08400     1.5612      0.6582     0.3066     75.6458
08500     1.4605      0.6709     0.3066     76.9123
08600     1.4357      0.6793     0.3066     75.1652
08700     1.7551      0.6224     0.3066     76.8247
08800     1.6121      0.6561     0.3066     75.2576
08900     1.4936      0.6582     0.3066     76.2510
09000     1.4235      0.6435     0.3066     77.0604
09100     1.7402      0.6160     0.3226     74.8696
09200     1.5211      0.6329     0.3226     75.6849
09300     1.4173      0.6540     0.3226     75.5346
09400     1.5635      0.6561     0.3226     75.3572
09500     1.7013      0.6371     0.3226     76.8735
09600     1.4717      0.6667     0.3226     75.6528
09700     1.5532      0.6414     0.3291     76.8856
09800     1.4782      0.6519     0.3291     75.4641
09900     1.5023      0.6603     0.3291     76.4431
10000     1.4529      0.6646     0.3291     77.6878
10100     1.2919      0.6709     0.3291     75.8067
10200     1.4398      0.6519     0.3291     76.0963
10300     1.4103      0.6878     0.3291     76.9196
10400     1.3362      0.6814     0.3291     75.3569
10500     1.7424      0.6034     0.3291     76.8268
10600     1.4351      0.6540     0.3291     75.5422
10700     1.5579      0.6688     0.3291     76.1997
10800     1.3819      0.6350     0.3291     77.9131
10900     1.5445      0.6097     0.3291     77.6591
11000     1.3462      0.6477     0.3291     76.6314
11100     1.4327      0.6793     0.3291     76.5794
11200     1.3920      0.6772     0.3291     78.7834
11300     1.2720      0.7046     0.3296     76.1851
11400     1.6509      0.6603     0.3296     76.2227
11500     1.3232      0.6540     0.3296     75.3978
11600     1.2828      0.7131     0.3296     76.2579
11700     1.4763      0.6414     0.3296     75.7489
11800     1.4593      0.6793     0.3296     75.6619
11900     1.6186      0.6203     0.3296     76.4548
12000     1.5690      0.6646     0.3296     76.1505
12100     1.5007      0.6624     0.3296     75.4880
12200     1.3273      0.6688     0.3296     76.6255
12300     1.4890      0.6793     0.3296     76.3304
12400     1.6394      0.6582     0.3296     76.6910
12500     1.3511      0.6962     0.3296     76.3081
12600     1.5200      0.6667     0.3296     75.4602
12700     1.4736      0.6730     0.3296     77.8227
12800     1.3575      0.6667     0.3296     76.2219
12900     1.3615      0.6835     0.3296     75.2534
13000     1.5306      0.6371     0.3296     77.7235
13100     1.3430      0.6646     0.3296     76.9317
13200     1.3960      0.6688     0.3296     74.8605
13300     1.5042      0.6772     0.3296     75.5911
13400     1.5499      0.6857     0.3296     74.8745
13500     1.2677      0.6983     0.3296     75.7645
13600     1.4288      0.6793     0.3296     76.9580
13700     1.5822      0.6435     0.3296     75.3573
13800     1.2692      0.7046     0.3296     77.8885
13900     1.3347      0.6793     0.3296     76.6441
14000     1.4004      0.6835     0.3296     75.7783
14100     1.2831      0.7046     0.3296     74.8948
14200     1.2562      0.6582     0.3296     76.4091
14300     1.6233      0.6667     0.3296     76.5793
14400     1.4874      0.6414     0.3296     76.5946
14500     1.2820      0.6941     0.3296     75.3537
14600     1.4643      0.6519     0.3296     76.8281
14700     1.7130      0.6160     0.3296     77.3896
14800     1.4518      0.6181     0.3296     77.8673
14900     1.5017      0.6329     0.3296     75.6966
15000     1.2515      0.7025     0.3366     77.0327
15100     1.2120      0.6793     0.3366     75.3809
15200     1.4269      0.7046     0.3366     75.8841
15300     1.3053      0.6709     0.3366     77.1726
15400     1.4715      0.6456     0.3366     76.4155
15500     1.4995      0.6751     0.3366     75.6175
15600     1.2289      0.7046     0.3366     76.1267
15700     1.2156      0.6835     0.3366     75.7651
15800     1.3805      0.7215     0.3366     77.2463
15900     1.4281      0.6920     0.3366     75.6803
16000     1.5782      0.6350     0.3366     75.7861
16100     1.4101      0.6667     0.3366     76.9528
16200     1.4277      0.6392     0.3366     75.0433
16300     1.4473      0.6435     0.3366     77.7148
16400     1.5094      0.6730     0.3366     76.7167
16500     1.4121      0.6878     0.3366     76.2127
16600     1.3876      0.6561     0.3366     76.3829
16700     1.3898      0.6624     0.3366     77.8400
16800     1.4303      0.6498     0.3366     78.2843
16900     1.5248      0.6414     0.3366     76.6096
17000     1.3654      0.7089     0.3366     77.0921
17100     1.5927      0.6603     0.3366     75.4420
17200     1.6158      0.6350     0.3366     76.9676
17300     1.3339      0.6772     0.3366     75.6028
17400     1.2918      0.6751     0.3366     77.3435
17500     1.2953      0.6793     0.3366     74.9988
17600     1.5371      0.6561     0.3366     76.2075
17700     1.5862      0.6624     0.3366     76.2072
17800     1.4664      0.6350     0.3366     77.1690
17900     1.1662      0.6962     0.3366     77.5442
18000     1.3305      0.6751     0.3366     75.1354
18100     1.5097      0.6667     0.3366     76.2588
18200     1.5893      0.6519     0.3366     75.6186
18300     1.4899      0.6857     0.3366     76.1444
18400     1.5390      0.6772     0.3366     76.3233
18500     1.4796      0.6456     0.3366     75.5318
18600     1.6379      0.6667     0.3366     76.7049
18700     1.1184      0.6920     0.3366     74.4668
18800     1.3326      0.6561     0.3366     74.9406
18900     1.5340      0.6540     0.3366     76.0504
19000     1.4390      0.6392     0.3366     76.3210
19100     1.2601      0.6878     0.3366     76.8739
19200     1.3204      0.6920     0.3366     77.1195
19300     1.5018      0.6751     0.3366     77.1348
19400     1.2727      0.7004     0.3366     77.9830
19500     1.3497      0.6962     0.3366     76.7947
19600     1.3985      0.6646     0.3366     75.9737
19700     1.5667      0.6772     0.3366     76.0796
19800     1.4318      0.6371     0.3366     76.9633
19900     1.6141      0.6435     0.3366     76.7976
20000     1.3954      0.7004     0.3366     77.5953
20100     1.2990      0.6793     0.3366     76.5547
20199     1.2722      0.6814     0.3366     75.8151
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     4.9559      0.2890     0.2871     9.9006
00100     4.4951      0.2975     0.3434     76.3088
00200     4.0321      0.3186     0.3434     75.2507
00300     3.4198      0.3376     0.3799     75.8140
00400     3.5193      0.3861     0.3991     74.9231
00500     3.5914      0.3755     0.4489     74.3206
00600     3.0771      0.3713     0.4765     74.6826
00700     2.9152      0.4536     0.4765     75.6906
00800     2.8220      0.4430     0.5150     74.9919
00900     3.1498      0.3945     0.5150     75.8932
01000     2.6399      0.4325     0.5150     73.9798
01100     2.8956      0.4726     0.5351     75.5519
01200     2.5411      0.4473     0.5351     74.4950
01300     2.5304      0.4789     0.5351     74.3207
01400     2.7083      0.4873     0.5396     74.6054
01500     2.6466      0.4916     0.5396     74.4678
01600     2.7045      0.4599     0.5488     73.3133
01700     2.6071      0.4494     0.5535     75.3839
01800     2.5770      0.4726     0.5590     75.6338
01900     2.9840      0.4219     0.5590     73.9840
02000     2.7125      0.4578     0.5590     74.1470
02100     2.6599      0.4958     0.5590     75.9492
02200     2.4242      0.5000     0.5590     74.7494
02300     2.8852      0.4958     0.5702     74.2273
02400     2.7315      0.5084     0.5725     76.7059
02500     2.6422      0.5169     0.5725     73.5936
02600     2.5987      0.4325     0.5725     74.4395
02700     2.4194      0.5105     0.5725     76.1691
02800     2.3258      0.5127     0.5753     75.8575
02900     2.6143      0.5316     0.5753     74.7895
03000     2.5026      0.5000     0.5753     74.4716
03100     2.4284      0.5000     0.5753     76.1134
03200     2.5844      0.5211     0.5753     76.6847
03300     2.5315      0.4684     0.5939     75.1571
03400     2.1858      0.5401     0.5939     78.1816
03500     2.5296      0.4536     0.5939     75.7866
03600     2.0520      0.5633     0.6143     75.2867
03700     2.6086      0.5190     0.6143     75.9410
03800     2.6229      0.4789     0.6160     77.1916
03900     2.2505      0.5295     0.6160     75.8194
04000     2.5588      0.5422     0.6160     77.0253
04100     2.2020      0.5907     0.6160     76.5123
04200     2.0937      0.5527     0.6160     75.1601
04300     2.6033      0.4852     0.6197     76.3064
04400     2.1155      0.5844     0.6197     74.9393
04500     2.1827      0.5190     0.6197     75.4515
04600     2.1727      0.5823     0.6301     75.1756
04700     2.1858      0.5380     0.6301     74.8756
04800     2.3871      0.5633     0.6301     74.2653
04900     2.0161      0.5148     0.6301     75.8348
05000     2.0868      0.5063     0.6301     75.2811
05100     1.9752      0.5633     0.6301     75.9021
05200     2.1617      0.5506     0.6301     76.2447
05300     2.0871      0.5591     0.6321     75.0978
05400     2.0505      0.5970     0.6321     76.2928
05500     2.3514      0.5527     0.6321     74.6486
05600     1.9677      0.5654     0.6321     76.0433
05700     1.9074      0.6266     0.6321     73.9020
05800     2.4790      0.5380     0.6321     75.7922
05900     2.3086      0.5738     0.6377     76.1292
06000     2.0035      0.5759     0.6377     74.0671
06100     2.1179      0.5042     0.6377     76.6844
06200     2.0800      0.5844     0.6396     77.8551
06300     2.3023      0.5675     0.6396     74.6698
06400     2.5245      0.5338     0.6396     76.4717
06500     2.2852      0.5380     0.6396     75.9084
06600     2.0648      0.5591     0.6396     74.8941
06700     2.3256      0.5717     0.6396     75.7003
06800     2.2974      0.5190     0.6396     75.1254
06900     2.1712      0.5274     0.6496     77.6323
07000     2.0946      0.5992     0.6496     74.7392
07100     2.2982      0.5527     0.6496     75.5351
07200     2.1350      0.5970     0.6496     77.2352
07300     2.0761      0.5781     0.6496     75.2609
07400     2.3814      0.5274     0.6496     75.1233
07500     2.5762      0.5042     0.6496     76.4683
07600     1.8259      0.5696     0.6496     74.8204
07700     2.2696      0.5612     0.6496     74.9882
07800     2.0080      0.5823     0.6496     75.8092
07900     1.9662      0.6097     0.6496     75.0315
08000     2.0502      0.6013     0.6496     77.0033
08100     2.0288      0.6076     0.6496     74.8365
08200     2.2315      0.5865     0.6560     76.6949
08300     1.5819      0.6329     0.6560     77.3019
08400     2.4857      0.5464     0.6560     76.2679
08500     2.0257      0.5802     0.6560     73.7176
08600     2.2570      0.5211     0.6560     75.8106
08700     1.8380      0.5928     0.6560     76.0164
08800     2.3396      0.5654     0.6560     75.3727
08900     2.0123      0.5612     0.6560     75.7876
09000     2.2907      0.5422     0.6560     77.7064
09100     2.0186      0.5570     0.6560     76.0562
09200     1.8573      0.6076     0.6560     75.2766
09300     2.4403      0.5675     0.6560     75.6010
09400     1.9408      0.5781     0.6560     74.5080
09500     2.1393      0.5654     0.6593     76.6076
09600     2.2353      0.5612     0.6593     74.6659
09700     2.1329      0.5633     0.6593     76.4340
09800     2.0931      0.4937     0.6593     76.8369
09900     1.8000      0.6329     0.6700     75.8318
Start testing:
Test Accuracy: 0.7284
