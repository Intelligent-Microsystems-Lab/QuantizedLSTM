Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=494, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=6, quant_actNM=6, quant_inp=6, quant_w=6, random_seed=235899598, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
9005834c-a101-4480-bfd4-7bbb551227ee
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 290, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 211, in forward
    part2 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(hx, self.a11), self.ib, self.a11), self.weight_hh * w_mask, self.bias_hh, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 141, in forward
    noise_w = torch.randn(weight.shape, device = input.device) * max_w * nl
RuntimeError: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 11.78 GiB total capacity; 9.52 GiB already allocated; 17.38 MiB free; 10.60 GiB reserved in total by PyTorch)
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=100, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=494, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=6, quant_actNM=6, quant_inp=6, quant_w=6, random_seed=235899598, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
8c2f1733-5a1e-491c-84ae-d1937d6189de
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 167, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 290, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 211, in forward
    part2 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(hx, self.a11), self.ib, self.a11), self.weight_hh * w_mask, self.bias_hh, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 146, in forward
    wq = quant_pass(weight, wb, torch.tensor([1.]).to(input.device))
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 98, in forward
    x01q =  torch.round(x01 * step_d ) / step_d
RuntimeError: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 10.76 GiB total capacity; 8.64 GiB already allocated; 11.12 MiB free; 9.79 GiB reserved in total by PyTorch)
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=100, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=494, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=6, quant_actNM=6, quant_inp=6, quant_w=6, random_seed=235899598, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
0831a10c-cf62-4258-a86a-80d2b9ec8bed
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.8294      0.0800     0.0840     18.1487
00100     1.3284      0.5200     0.0859     62.5694
00200     1.0153      0.6900     0.1019     62.1789
00300     0.6174      0.8100     0.1019     61.7702
00400     0.7712      0.7700     0.1062     62.7860
00500     0.7257      0.7500     0.1062     64.2852
00600     0.7665      0.7300     0.1138     63.0662
00700     0.5930      0.8200     0.1138     65.0904
00800     0.4879      0.8600     0.1162     63.1096
00900     0.6520      0.8000     0.1162     61.8427
01000     0.6397      0.7900     0.1216     61.8968
01100     0.4638      0.8600     0.1227     62.2627
01200     0.4254      0.8900     0.1227     64.6742
01300     0.6139      0.7800     0.1274     62.5432
01400     0.3830      0.8900     0.1283     62.2639
01500     0.3985      0.9000     0.1283     62.3586
01600     0.4192      0.8800     0.1283     63.9639
01700     0.3134      0.9100     0.1305     66.6589
01800     0.5052      0.8100     0.1429     62.4253
01900     0.3060      0.9100     0.1429     63.6700
02000     0.4077      0.9000     0.1429     62.0219
02100     0.4429      0.8800     0.1429     62.2636
02200     0.4182      0.8800     0.1429     65.0254
02300     0.4924      0.9100     0.1429     65.2839
02400     0.5444      0.8600     0.1429     62.8483
02500     0.3535      0.8900     0.1488     63.4034
02600     0.5888      0.8100     0.1488     65.4912
02700     0.2753      0.9100     0.1590     64.2935
02800     0.4370      0.8800     0.1590     61.9088
02900     0.4083      0.8700     0.1590     61.9272
03000     0.4365      0.8400     0.1590     62.3120
03100     0.3240      0.9100     0.1681     62.7869
03200     0.3048      0.9300     0.1713     66.6507
03300     0.4263      0.8600     0.1713     62.3984
03400     0.3153      0.8900     0.1769     64.5765
03500     0.3338      0.9000     0.1769     63.5140
03600     0.4024      0.8600     0.1805     62.2054
03700     0.3524      0.8800     0.1934     62.3986
03800     0.3276      0.8800     0.1934     62.2063
03900     0.4289      0.8700     0.1934     65.2557
04000     0.2297      0.9500     0.1934     61.6646
04100     0.2405      0.9500     0.2013     63.9566
04200     0.5871      0.8700     0.2020     64.0458
04300     0.2681      0.9500     0.2020     62.6771
04400     0.3587      0.9000     0.2020     62.8790
04500     0.2365      0.9200     0.2020     61.3488
04600     0.2452      0.9500     0.2071     64.2087
04700     0.2320      0.9500     0.2071     63.8197
04800     0.1513      0.9500     0.2111     64.1218
04900     0.3834      0.9000     0.2111     63.2747
05000     0.1625      0.9400     0.2173     66.3702
05100     0.2475      0.9400     0.2173     61.5451
05200     0.4129      0.8900     0.2173     62.8256
05300     0.3236      0.9100     0.2273     62.5784
05400     0.2411      0.9400     0.2273     61.4520
05500     0.3161      0.8900     0.2273     66.3770
05600     0.2726      0.9200     0.2563     63.9930
05700     0.3553      0.9000     0.2563     66.1635
05800     0.2938      0.9200     0.2563     65.5842
05900     0.3129      0.9200     0.2563     62.4573
06000     0.2278      0.9300     0.2563     63.4389
06100     0.2659      0.9000     0.2563     62.6492
06200     0.1796      0.9500     0.2563     63.6522
06300     0.2593      0.9300     0.2571     63.4449
06400     0.2486      0.9300     0.2760     68.4304
06500     0.3683      0.8800     0.2902     67.7974
06600     0.3963      0.9000     0.2969     67.9183
06700     0.3136      0.9000     0.2969     69.1896
06800     0.3438      0.8800     0.2969     67.8811
06900     0.3376      0.9100     0.2969     67.8001
07000     0.2466      0.9300     0.2984     71.9263
07100     0.2603      0.9200     0.3092     68.0034
07200     0.2146      0.9500     0.3092     65.5415
07300     0.2672      0.8900     0.3228     68.5142
07400     0.3529      0.9100     0.3266     69.4822
07500     0.3616      0.9000     0.3266     67.7151
07600     0.2007      0.9300     0.3266     68.2370
07700     0.3152      0.9200     0.3266     70.6933
07800     0.1782      0.9600     0.3266     66.4661
07900     0.2682      0.9400     0.3421     69.8243
08000     0.3292      0.9100     0.3522     67.5734
08100     0.3132      0.8700     0.3522     69.3381
08200     0.3399      0.9100     0.3522     69.4705
08300     0.3930      0.9200     0.3620     69.1990
08400     0.3952      0.8900     0.3620     69.1435
08500     0.2459      0.9600     0.3756     66.6299
08600     0.1974      0.9500     0.3756     67.5825
08700     0.2622      0.9400     0.3848     68.1831
08800     0.1823      0.9600     0.3848     66.4211
08900     0.2908      0.9100     0.3884     68.6598
09000     0.1590      0.9800     0.3936     67.0514
09100     0.1505      0.9700     0.3936     67.6603
09200     0.2062      0.9300     0.3936     68.7074
09300     0.1555      0.9700     0.3936     66.8854
09400     0.1856      0.9600     0.4073     67.9181
09500     0.1927      0.9400     0.4226     67.4751
09600     0.2983      0.9100     0.4226     67.1074
09700     0.2038      0.9400     0.4226     68.8866
09800     0.4441      0.8900     0.4226     67.4829
09900     0.3442      0.9500     0.4226     68.4891
10000     0.2089      0.9300     0.4226     70.1689
10100     0.2428      0.9300     0.4226     69.1537
10200     0.1098      0.9900     0.4226     67.3519
10300     0.2889      0.9300     0.4226     67.1091
10400     0.2248      0.9000     0.4311     69.0987
10500     0.2154      0.9400     0.4313     69.6244
10600     0.2307      0.9400     0.4313     68.9175
10700     0.3375      0.9100     0.4313     68.1270
10800     0.1330      0.9700     0.4313     68.6297
10900     0.2773      0.9300     0.4313     69.0820
11000     0.2083      0.9500     0.4328     67.4602
11100     0.1378      0.9700     0.4328     67.6420
11200     0.1914      0.9500     0.4416     68.0984
11300     0.1660      0.9700     0.4416     68.8818
11400     0.1899      0.9500     0.4416     67.9860
11500     0.1992      0.9600     0.4416     68.2473
11600     0.2121      0.9400     0.4416     66.7723
11700     0.1644      0.9600     0.4416     67.8295
11800     0.2233      0.9600     0.4416     68.7988
11900     0.1344      0.9600     0.4416     70.0459
12000     0.1968      0.9300     0.4416     62.3560
12100     0.1846      0.9400     0.4416     60.2457
12200     0.1038      0.9800     0.4416     61.6576
12300     0.2315      0.9300     0.4451     63.1949
12400     0.1001      0.9800     0.4451     60.9038
12500     0.2363      0.9500     0.4451     60.3735
12600     0.2153      0.9400     0.4476     61.0490
12700     0.2345      0.9500     0.4476     60.3233
12800     0.1914      0.9500     0.4476     60.3018
12900     0.0764      0.9800     0.4476     60.7180
13000     0.1903      0.9400     0.4476     60.3961
13100     0.1841      0.9600     0.4476     60.6309
13200     0.1765      0.9300     0.4476     61.3839
13300     0.1189      0.9600     0.4476     60.8974
13400     0.1222      0.9700     0.4476     60.7280
13500     0.0906      0.9900     0.4476     60.3845
13600     0.1172      0.9700     0.4476     62.3281
13700     0.1786      0.9500     0.4531     62.8606
13800     0.1700      0.9600     0.4531     60.4331
13900     0.1457      0.9600     0.4531     60.8055
14000     0.2234      0.9300     0.4531     60.8049
14100     0.2010      0.9500     0.4531     60.5059
14200     0.1719      0.9700     0.4531     60.2895
14300     0.1636      0.9600     0.4531     60.8926
14400     0.1307      0.9700     0.4531     60.5166
14500     0.1188      0.9700     0.4531     60.3672
14600     0.2038      0.9300     0.4531     60.7651
14700     0.1439      0.9600     0.4531     60.6119
14800     0.1103      0.9700     0.4531     61.5334
14900     0.1914      0.9500     0.4531     60.4887
15000     0.1531      0.9600     0.4531     61.8295
15100     0.1045      0.9700     0.4531     60.3787
15200     0.0780      0.9900     0.4531     60.8177
15300     0.1830      0.9700     0.4531     63.3704
15400     0.1202      0.9700     0.4531     60.7884
15500     0.2112      0.9300     0.4531     60.6837
15600     0.0861      0.9800     0.4531     62.0676
15700     0.2757      0.9200     0.4531     62.0220
15800     0.1392      0.9600     0.4531     62.5774
15900     0.1822      0.9300     0.4531     61.5095
16000     0.1048      0.9800     0.4531     62.0057
16100     0.1350      0.9700     0.4531     62.2059
16200     0.1624      0.9800     0.4531     60.8583
16300     0.1105      0.9800     0.4531     62.5403
16400     0.0910      0.9700     0.4531     61.7582
16500     0.1572      0.9700     0.4531     62.4639
16600     0.1557      0.9700     0.4531     62.2698
16700     0.0768      0.9900     0.4531     61.0557
16800     0.1342      0.9700     0.4531     61.8150
16900     0.2387      0.9300     0.4531     60.5365
17000     0.1768      0.9500     0.4531     62.2385
17100     0.1701      0.9500     0.4531     61.5602
17200     0.1330      0.9500     0.4531     61.5827
17300     0.1759      0.9700     0.4531     61.8765
17400     0.2251      0.9100     0.4531     63.3334
17500     0.1558      0.9600     0.4531     63.7105
17600     0.1590      0.9500     0.4602     68.3313
17700     0.1075      0.9800     0.4602     63.3913
17800     0.0901      0.9900     0.4602     65.0891
17900     0.1532      0.9700     0.4602     61.8198
18000     0.1180      0.9600     0.4602     61.6591
18100     0.1011      0.9800     0.4602     61.3086
18200     0.1099      0.9800     0.4602     61.9813
18300     0.2742      0.9100     0.4602     61.8315
18400     0.1562      0.9700     0.4602     62.0729
18500     0.0824      0.9900     0.4602     61.7925
18600     0.1118      0.9800     0.4602     62.0210
18700     0.0990      0.9800     0.4602     63.6479
18800     0.1627      0.9600     0.4602     62.5824
18900     0.1238      0.9700     0.4602     61.9824
19000     0.1342      0.9600     0.4602     61.4624
19100     0.1115      0.9700     0.4602     62.1089
19200     0.1023      0.9600     0.4602     61.9105
19300     0.1202      0.9800     0.4602     60.8648
19400     0.0803      0.9800     0.4602     60.8377
19500     0.1175      0.9900     0.4602     60.9814
19600     0.1166      0.9400     0.4602     60.1212
19700     0.1216      0.9700     0.4602     61.5261
19800     0.2563      0.9300     0.4602     59.4141
19900     0.0687      0.9900     0.4602     61.4372
20000     0.1379      0.9500     0.4602     61.3437
20100     0.1064      0.9800     0.4602     62.1512
20199     0.1188      0.9600     0.4602     62.1579
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     3.1886      0.4700     0.4335     18.1886
00100     1.2604      0.6800     0.6728     61.7680
00200     1.1220      0.7200     0.7326     61.2877
00300     0.6620      0.8100     0.7657     62.3016
00400     1.2672      0.6700     0.7780     61.2865
00500     0.7156      0.8100     0.7951     61.6745
00600     0.5901      0.8600     0.8074     60.6031
00700     0.8498      0.8500     0.8127     61.2271
00800     0.6589      0.8400     0.8127     60.9066
00900     0.7773      0.8000     0.8263     61.7955
01000     0.3720      0.9000     0.8263     62.7799
01100     0.8891      0.7600     0.8263     62.0151
01200     0.6858      0.8500     0.8263     62.9812
01300     0.5965      0.8600     0.8263     61.5207
01400     0.6420      0.8400     0.8292     64.7370
01500     0.5817      0.8600     0.8335     63.4636
01600     0.5124      0.8700     0.8350     62.9905
01700     0.7474      0.7400     0.8350     61.5165
01800     0.5039      0.8900     0.8415     62.2492
01900     0.4461      0.8700     0.8415     63.4422
02000     0.5003      0.8800     0.8444     63.0669
02100     0.8501      0.7800     0.8444     62.8752
02200     0.3463      0.9000     0.8444     62.8995
02300     0.5342      0.8700     0.8444     61.6182
02400     0.6426      0.8100     0.8444     62.4076
02500     0.4752      0.8500     0.8444     62.4847
02600     0.4764      0.8700     0.8444     63.0161
02700     0.3665      0.8700     0.8535     63.8793
02800     0.6836      0.8400     0.8535     62.0263
02900     0.7305      0.8300     0.8535     63.3673
03000     0.6579      0.8300     0.8535     63.1568
03100     0.7133      0.8000     0.8535     62.2194
03200     0.6589      0.8200     0.8535     64.3031
03300     0.5123      0.8600     0.8535     62.8833
03400     0.3794      0.8900     0.8535     63.3223
03500     0.6460      0.8000     0.8535     62.3218
03600     0.6489      0.8200     0.8535     64.2594
03700     0.4887      0.8600     0.8535     62.1239
03800     0.4512      0.8400     0.8535     62.4451
03900     0.4475      0.8200     0.8535     63.2452
04000     0.5062      0.8400     0.8545     63.6384
04100     0.7323      0.8000     0.8611     63.2250
04200     0.4154      0.8600     0.8611     62.3480
04300     0.3946      0.8800     0.8611     62.4160
04400     0.4778      0.8700     0.8657     63.5514
04500     0.5503      0.8700     0.8657     61.9378
04600     0.7308      0.8200     0.8657     61.8727
04700     0.4304      0.8700     0.8657     61.2817
04800     0.8986      0.7900     0.8657     62.6682
04900     0.5674      0.8400     0.8657     62.6348
05000     0.5527      0.8600     0.8657     61.8150
05100     0.5160      0.8400     0.8673     62.4430
05200     0.4173      0.8700     0.8673     61.0025
05300     0.5761      0.8600     0.8673     62.7763
05400     0.6528      0.8500     0.8673     62.3505
05500     0.3419      0.9000     0.8673     62.0118
05600     0.7161      0.8300     0.8673     61.1314
05700     0.4525      0.8700     0.8673     61.7298
05800     0.4944      0.8400     0.8679     63.8570
05900     0.5164      0.8700     0.8700     62.3495
06000     0.5497      0.8800     0.8700     61.1407
06100     0.3701      0.9000     0.8700     62.5907
06200     0.6035      0.8800     0.8700     61.9551
06300     0.2474      0.9300     0.8700     62.7652
06400     0.3291      0.9100     0.8700     61.6139
06500     0.8423      0.7900     0.8700     62.0076
06600     0.5494      0.8700     0.8750     62.5141
06700     0.3960      0.8900     0.8750     61.5472
06800     0.5028      0.8400     0.8750     62.8130
06900     0.5046      0.8800     0.8750     61.5614
07000     0.4084      0.8800     0.8750     62.3171
07100     0.5605      0.8200     0.8750     62.5532
07200     0.7155      0.8200     0.8750     62.4567
07300     0.4710      0.8800     0.8750     60.3779
07400     0.6140      0.8100     0.8750     60.9137
07500     0.2815      0.9300     0.8750     60.3925
07600     0.3090      0.8700     0.8750     62.0808
07700     0.4609      0.8700     0.8750     62.9600
07800     0.3436      0.9200     0.8750     63.6208
07900     0.5820      0.8300     0.8750     62.6229
08000     0.3983      0.8600     0.8750     61.5729
08100     0.4215      0.8800     0.8750     62.2905
08200     0.4939      0.8600     0.8750     63.2883
08300     0.5916      0.8600     0.8788     63.1070
08400     0.5360      0.8700     0.8788     62.4378
08500     0.4330      0.8600     0.8788     61.6105
08600     0.4587      0.8700     0.8788     61.9841
08700     0.4990      0.8700     0.8788     63.9388
08800     0.4724      0.8500     0.8788     61.9437
08900     0.7441      0.8000     0.8788     61.5460
09000     0.4398      0.8700     0.8788     60.5514
09100     0.2866      0.9200     0.8788     61.5034
09200     0.4160      0.8900     0.8788     62.5097
09300     0.3845      0.8800     0.8788     61.8552
09400     0.6060      0.8500     0.8788     62.1572
09500     0.5220      0.8500     0.8788     59.9851
09600     0.3688      0.9100     0.8798     60.8092
09700     0.4586      0.8600     0.8798     62.2404
09800     0.4160      0.8900     0.8798     62.9294
09900     0.3750      0.8900     0.8798     62.0756
Start testing:
Test Accuracy: 0.9018
