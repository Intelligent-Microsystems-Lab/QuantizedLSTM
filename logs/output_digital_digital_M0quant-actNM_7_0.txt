Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=7, quant_actNM=7, quant_inp=7, quant_w=7, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
fba8a6ba-cfc5-482f-9277-8410582382ed
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, 1.)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 82, in forward
    if len(x_range) > 1:
TypeError: object of type 'float' has no len()
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=7, quant_actNM=7, quant_inp=7, quant_w=7, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
e03e0c9e-0cb5-4679-9694-98ddecc49968
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, torch.tensor([1.]))
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 94, in forward
    x_scaled = x/x_range
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=7, quant_actNM=7, quant_inp=7, quant_w=7, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
cdf799ff-cf3b-4ece-a3f5-12d10c0fca57
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, torch.tensor([1.]).device(input.device))
TypeError: 'torch.device' object is not callable
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=7, quant_actNM=7, quant_inp=7, quant_w=7, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
9fbd27d3-42de-4260-bb6b-110bb5605424
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.8061      0.0759     0.0847     10.4137
00100     2.4111      0.2025     0.2564     72.0770
00200     1.9427      0.3523     0.4220     72.9338
00300     1.5613      0.4873     0.5327     74.0948
00400     1.3796      0.5970     0.5994     72.3699
00500     1.2072      0.6224     0.6631     72.2488
00600     1.1774      0.6350     0.6724     74.9926
00700     1.0578      0.6814     0.6853     72.8369
00800     1.0428      0.6772     0.7142     72.6256
00900     1.0646      0.6646     0.7142     73.6397
01000     0.9605      0.7278     0.7334     72.4302
01100     0.9526      0.7236     0.7334     71.5851
01200     0.9168      0.7068     0.7397     73.0362
01300     0.9029      0.7342     0.7452     75.0005
01400     1.0101      0.6962     0.7483     74.1940
01500     0.9096      0.7152     0.7537     73.4704
01600     0.9079      0.7384     0.7590     74.4087
01700     0.7992      0.7658     0.7606     72.7134
01800     0.8277      0.7532     0.7609     71.6489
01900     0.7906      0.7785     0.7782     73.7839
02000     0.8055      0.7848     0.7782     71.3391
02100     0.7773      0.7806     0.7782     72.2482
02200     0.8223      0.7743     0.7782     73.2605
02300     0.8158      0.7595     0.7782     72.4414
02400     0.7276      0.7743     0.7799     75.2506
02500     0.8613      0.7384     0.7924     72.9237
02600     0.8174      0.7489     0.7924     73.0384
02700     0.7559      0.7785     0.7924     74.1027
02800     0.7921      0.7658     0.7924     71.9373
02900     0.9144      0.7321     0.7943     73.3204
03000     0.7919      0.7743     0.7943     72.5509
03100     0.7832      0.7637     0.7943     73.7665
03200     0.7861      0.7405     0.7943     73.3989
03300     0.7404      0.7658     0.7977     72.1235
03400     0.7982      0.7532     0.8085     74.9185
03500     0.7711      0.7637     0.8085     72.6784
03600     0.7838      0.7637     0.8085     73.1497
03700     0.7778      0.7722     0.8085     72.2701
03800     0.7367      0.7975     0.8085     74.5380
03900     0.7549      0.7827     0.8085     74.2512
04000     0.6679      0.8101     0.8085     73.5389
04100     0.6908      0.7954     0.8085     75.2162
04200     0.7671      0.7743     0.8137     73.4151
04300     0.7351      0.7996     0.8137     73.6858
04400     0.7675      0.7785     0.8137     74.1878
04500     0.6717      0.7890     0.8137     74.1260
04600     0.7317      0.7743     0.8137     73.2175
04700     0.6285      0.8186     0.8137     74.0949
04800     0.7087      0.7806     0.8137     72.6175
04900     0.6389      0.7996     0.8141     73.5223
05000     0.7127      0.7890     0.8141     73.7943
05100     0.7862      0.7658     0.8141     72.9501
05200     0.7351      0.7911     0.8151     73.0994
05300     0.7326      0.7848     0.8152     74.4921
05400     0.7361      0.7827     0.8185     74.0287
05500     0.6704      0.7932     0.8185     74.9034
05600     0.5949      0.8376     0.8185     73.0795
05700     0.6892      0.7975     0.8185     72.7607
05800     0.7079      0.7848     0.8185     73.4506
05900     0.7284      0.7890     0.8185     71.6120
06000     0.7270      0.7785     0.8185     73.8741
06100     0.7727      0.7827     0.8185     72.1969
06200     0.6408      0.8249     0.8185     73.4318
06300     0.6475      0.8165     0.8185     73.4304
06400     0.6502      0.7996     0.8248     74.1771
06500     0.6326      0.8143     0.8248     74.1644
06600     0.6893      0.8101     0.8248     72.7510
06700     0.7193      0.7679     0.8248     73.9112
06800     0.6530      0.8143     0.8248     73.7057
06900     0.6408      0.7996     0.8248     72.4333
07000     0.7115      0.8059     0.8248     72.7480
07100     0.7108      0.7932     0.8248     72.1942
07200     0.6658      0.7932     0.8248     72.2395
07300     0.7516      0.7890     0.8248     71.2799
07400     0.6500      0.7975     0.8248     74.2241
07500     0.6365      0.8122     0.8248     73.8207
07600     0.7047      0.7911     0.8248     74.2248
07700     0.6277      0.8249     0.8248     73.6617
07800     0.7322      0.7911     0.8260     71.8402
07900     0.7471      0.7806     0.8260     73.9485
08000     0.6836      0.8059     0.8260     72.5853
08100     0.7270      0.7743     0.8260     71.3289
08200     0.6413      0.7996     0.8260     73.7197
08300     0.6027      0.8312     0.8260     75.4824
08400     0.7041      0.7806     0.8260     72.3469
08500     0.6118      0.8080     0.8260     73.5916
08600     0.6399      0.8080     0.8260     73.6050
08700     0.6844      0.7932     0.8260     72.1136
08800     0.6956      0.7954     0.8260     73.6993
08900     0.7299      0.7869     0.8260     73.7852
09000     0.6099      0.8122     0.8260     73.0824
09100     0.6473      0.8122     0.8260     74.5194
09200     0.7069      0.7827     0.8260     72.8514
09300     0.5995      0.8165     0.8260     75.9563
09400     0.6937      0.7869     0.8260     76.1963
09500     0.6928      0.8038     0.8260     74.0257
09600     0.5984      0.8143     0.8260     73.2983
09700     0.6470      0.8059     0.8260     73.3966
09800     0.6488      0.7996     0.8260     74.6487
09900     0.6799      0.8080     0.8260     73.9976
10000     0.6324      0.8249     0.8260     74.4363
10100     0.6926      0.8038     0.8260     75.6035
10200     0.6606      0.8038     0.8260     73.5024
10300     0.5567      0.8376     0.8294     72.2507
10400     0.6042      0.8228     0.8294     73.1622
10500     0.6122      0.8207     0.8294     73.8216
10600     0.6696      0.7869     0.8294     74.3249
10700     0.7151      0.7954     0.8294     72.4424
10800     0.6442      0.8122     0.8294     73.6528
10900     0.6168      0.8101     0.8304     72.2683
11000     0.5503      0.8502     0.8304     73.7409
11100     0.6289      0.8080     0.8304     74.8144
11200     0.5461      0.8333     0.8304     75.0800
11300     0.6609      0.8038     0.8304     73.1285
11400     0.6658      0.7996     0.8304     72.6356
11500     0.6391      0.8143     0.8304     74.9806
11600     0.6751      0.7996     0.8304     73.9550
11700     0.6974      0.7954     0.8304     72.4891
11800     0.6265      0.8228     0.8304     72.7525
11900     0.6503      0.8017     0.8364     73.5276
12000     0.5754      0.8354     0.8364     73.6053
12100     0.5767      0.8228     0.8364     73.5899
12200     0.6286      0.8186     0.8364     75.1020
12300     0.6169      0.8059     0.8364     73.5679
12400     0.7153      0.7890     0.8364     73.8262
12500     0.6246      0.8165     0.8364     72.8657
12600     0.6101      0.8397     0.8364     74.6409
12700     0.6912      0.7806     0.8364     72.7691
12800     0.5959      0.8143     0.8364     74.3547
12900     0.6516      0.8059     0.8364     72.7176
13000     0.5795      0.8270     0.8364     72.7199
13100     0.5584      0.8397     0.8364     75.2991
13200     0.5891      0.8270     0.8364     73.1722
13300     0.6431      0.8376     0.8364     72.8639
13400     0.5830      0.8418     0.8364     72.2938
13500     0.6019      0.8122     0.8364     71.2105
13600     0.5984      0.8354     0.8364     74.8637
13700     0.6307      0.8228     0.8364     73.7056
13800     0.6334      0.7869     0.8364     74.6690
13900     0.6288      0.8312     0.8364     73.8010
14000     0.6215      0.8059     0.8364     73.9327
14100     0.6164      0.8291     0.8364     72.0022
14200     0.5848      0.8207     0.8364     72.5128
14300     0.6827      0.8059     0.8364     72.5505
14400     0.6287      0.8059     0.8364     73.3781
14500     0.6491      0.8122     0.8364     72.5201
14600     0.6554      0.7975     0.8364     74.8497
14700     0.5866      0.8207     0.8364     73.6937
14800     0.6358      0.8207     0.8364     73.9504
14900     0.6360      0.7954     0.8364     73.6192
15000     0.5908      0.8270     0.8364     74.1848
15100     0.6593      0.8080     0.8364     73.4546
15200     0.5824      0.8397     0.8364     73.7842
15300     0.5875      0.8270     0.8364     72.4271
15400     0.6626      0.7996     0.8364     71.9225
15500     0.7173      0.7848     0.8364     73.6307
15600     0.6462      0.8122     0.8364     72.9434
15700     0.5740      0.8376     0.8364     74.1227
15800     0.7190      0.7848     0.8364     73.9221
15900     0.5868      0.8228     0.8364     73.3312
16000     0.6104      0.8312     0.8364     75.6417
16100     0.7044      0.7996     0.8364     72.1130
16200     0.6095      0.8080     0.8364     73.1780
16300     0.6433      0.8312     0.8364     73.5457
16400     0.5486      0.8354     0.8364     72.5335
16500     0.6531      0.8059     0.8364     73.5892
16600     0.5729      0.8333     0.8364     71.6336
16700     0.5871      0.8228     0.8364     71.6895
16800     0.5878      0.8165     0.8364     72.4412
16900     0.6579      0.8165     0.8364     72.7378
17000     0.6569      0.7996     0.8364     72.0279
17100     0.6513      0.8080     0.8364     72.8256
17200     0.7127      0.7806     0.8364     74.1672
17300     0.7073      0.7975     0.8364     73.7803
17400     0.5988      0.8270     0.8364     73.7443
17500     0.7103      0.7869     0.8364     72.0195
17600     0.6250      0.8186     0.8364     73.8379
17700     0.6429      0.8228     0.8364     73.7412
17800     0.6406      0.8122     0.8364     74.8487
17900     0.6010      0.8270     0.8364     74.0862
18000     0.6172      0.8249     0.8364     73.2904
18100     0.5566      0.8291     0.8364     72.5272
18200     0.6673      0.8059     0.8364     73.9537
18300     0.5721      0.8354     0.8364     75.3296
18400     0.5976      0.8481     0.8389     73.8706
18500     0.6055      0.8207     0.8389     73.7550
18600     0.7485      0.7827     0.8389     74.1021
18700     0.6183      0.8207     0.8389     74.7095
18800     0.5895      0.8312     0.8389     72.8839
18900     0.6254      0.8122     0.8389     72.9071
19000     0.5508      0.8502     0.8389     73.8955
19100     0.5596      0.8418     0.8389     72.5308
19200     0.5779      0.8207     0.8389     73.6541
19300     0.6657      0.8017     0.8389     72.5648
19400     0.6158      0.8186     0.8389     73.5124
19500     0.5376      0.8439     0.8389     73.4442
19600     0.5929      0.8249     0.8389     71.6455
19700     0.6472      0.8186     0.8389     73.8910
19800     0.6953      0.7700     0.8389     74.1485
19900     0.6345      0.8059     0.8389     75.2000
20000     0.6106      0.8165     0.8389     73.2465
20100     0.5899      0.8376     0.8389     72.1522
20199     0.6465      0.8101     0.8389     70.9634
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     0.5717      0.8333     0.8304     10.2881
00100     0.4334      0.8819     0.8339     71.1902
00200     0.5820      0.8376     0.8403     71.5147
00300     0.6141      0.8143     0.8403     72.8355
00400     0.4995      0.8608     0.8403     74.9776
00500     0.5053      0.8502     0.8403     73.9907
00600     0.5403      0.8354     0.8403     75.5069
00700     0.4753      0.8650     0.8403     74.5298
00800     0.4986      0.8460     0.8403     75.2255
00900     0.6012      0.8333     0.8403     74.3931
01000     0.4728      0.8692     0.8403     71.9982
01100     0.5824      0.8165     0.8403     74.8286
01200     0.6277      0.8080     0.8403     74.4434
01300     0.5164      0.8481     0.8403     75.9266
01400     0.5680      0.8397     0.8403     76.7957
01500     0.5621      0.8397     0.8403     74.7629
01600     0.5671      0.8397     0.8403     73.9799
01700     0.5284      0.8586     0.8403     73.5046
01800     0.4483      0.8755     0.8403     72.6074
01900     0.4563      0.8608     0.8403     74.3562
02000     0.5505      0.8439     0.8403     72.0301
02100     0.5654      0.8270     0.8403     73.6119
02200     0.5298      0.8397     0.8411     72.8845
02300     0.5143      0.8502     0.8411     73.8913
02400     0.4210      0.8776     0.8411     72.4119
02500     0.5242      0.8565     0.8411     74.0719
02600     0.6061      0.8207     0.8411     73.1501
02700     0.4298      0.8840     0.8411     73.0954
02800     0.5805      0.8291     0.8411     73.1735
02900     0.5122      0.8312     0.8411     71.2212
03000     0.5221      0.8418     0.8411     72.8054
03100     0.5703      0.8354     0.8411     72.9710
03200     0.4975      0.8439     0.8411     75.5325
03300     0.4764      0.8650     0.8411     74.3171
03400     0.5055      0.8481     0.8411     73.0902
03500     0.6126      0.8165     0.8411     73.6452
03600     0.5035      0.8544     0.8411     72.8351
03700     0.6707      0.7911     0.8411     73.0758
03800     0.4653      0.8544     0.8411     75.1292
03900     0.4749      0.8713     0.8411     73.0775
04000     0.5704      0.8228     0.8411     74.5364
04100     0.5101      0.8586     0.8411     74.2879
04200     0.5363      0.8333     0.8411     74.3041
04300     0.5217      0.8439     0.8411     75.1796
04400     0.4806      0.8460     0.8411     73.2628
04500     0.5799      0.8312     0.8411     72.2630
04600     0.5401      0.8460     0.8411     74.0377
04700     0.5430      0.8439     0.8411     74.2275
04800     0.4729      0.8797     0.8411     74.1411
04900     0.5257      0.8312     0.8411     72.9389
05000     0.5475      0.8481     0.8411     75.0890
05100     0.5126      0.8460     0.8411     73.8987
05200     0.5561      0.8523     0.8411     73.8278
05300     0.5149      0.8544     0.8411     76.9982
05400     0.5499      0.8207     0.8411     74.6965
05500     0.5051      0.8544     0.8411     73.7657
05600     0.5193      0.8586     0.8411     72.8603
05700     0.5883      0.8228     0.8411     74.3230
05800     0.4515      0.8924     0.8411     74.7310
05900     0.5467      0.8502     0.8411     72.5076
06000     0.5121      0.8586     0.8411     71.5463
06100     0.5446      0.8270     0.8411     72.8809
06200     0.4647      0.8755     0.8411     74.0609
06300     0.5279      0.8376     0.8411     74.1022
06400     0.4786      0.8502     0.8411     73.7801
06500     0.5502      0.8481     0.8411     70.6046
06600     0.5221      0.8439     0.8411     71.2718
06700     0.5527      0.8354     0.8411     73.1806
06800     0.5956      0.8186     0.8411     72.2018
06900     0.4627      0.8713     0.8411     72.0548
07000     0.5312      0.8523     0.8437     74.2473
07100     0.6085      0.8122     0.8437     72.2497
07200     0.4940      0.8481     0.8437     73.6794
07300     0.6448      0.8122     0.8437     73.8734
07400     0.5862      0.8249     0.8437     74.9657
07500     0.4847      0.8608     0.8437     72.4999
07600     0.5302      0.8460     0.8437     73.4715
07700     0.5339      0.8502     0.8437     72.3640
07800     0.4989      0.8481     0.8448     73.9817
07900     0.6011      0.8207     0.8448     72.8855
08000     0.5327      0.8376     0.8448     73.9189
08100     0.4739      0.8418     0.8448     73.7412
08200     0.5110      0.8460     0.8448     73.5355
08300     0.4410      0.8713     0.8448     73.9070
08400     0.5079      0.8418     0.8448     73.9573
08500     0.4980      0.8460     0.8448     72.7916
08600     0.5349      0.8418     0.8448     74.6747
08700     0.5383      0.8481     0.8448     73.1686
08800     0.5645      0.8460     0.8448     75.1557
08900     0.5441      0.8418     0.8448     74.6228
09000     0.5381      0.8439     0.8448     73.7587
09100     0.5157      0.8502     0.8448     73.7793
09200     0.5392      0.8418     0.8448     73.2167
09300     0.6081      0.8186     0.8448     73.7117
09400     0.5989      0.8312     0.8448     74.2939
09500     0.5685      0.8481     0.8448     73.5612
09600     0.5693      0.8354     0.8448     74.9181
09700     0.5941      0.8059     0.8448     74.2868
09800     0.5230      0.8523     0.8448     74.7949
09900     0.5087      0.8565     0.8448     73.9250
Start testing:
Test Accuracy: 0.8142
