Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=7, quant_actNM=7, quant_inp=7, quant_w=7, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
fba8a6ba-cfc5-482f-9277-8410582382ed
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, 1.)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 82, in forward
    if len(x_range) > 1:
TypeError: object of type 'float' has no len()
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=7, quant_actNM=7, quant_inp=7, quant_w=7, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
e03e0c9e-0cb5-4679-9694-98ddecc49968
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, torch.tensor([1.]))
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 94, in forward
    x_scaled = x/x_range
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=7, quant_actNM=7, quant_inp=7, quant_w=7, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
cdf799ff-cf3b-4ece-a3f5-12d10c0fca57
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, torch.tensor([1.]).device(input.device))
TypeError: 'torch.device' object is not callable
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=7, quant_actNM=7, quant_inp=7, quant_w=7, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
9fbd27d3-42de-4260-bb6b-110bb5605424
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.8061      0.0759     0.0847     10.4137
00100     2.4111      0.2025     0.2564     72.0770
00200     1.9427      0.3523     0.4220     72.9338
00300     1.5613      0.4873     0.5327     74.0948
00400     1.3796      0.5970     0.5994     72.3699
00500     1.2072      0.6224     0.6631     72.2488
00600     1.1774      0.6350     0.6724     74.9926
00700     1.0578      0.6814     0.6853     72.8369
00800     1.0428      0.6772     0.7142     72.6256
00900     1.0646      0.6646     0.7142     73.6397
01000     0.9605      0.7278     0.7334     72.4302
01100     0.9526      0.7236     0.7334     71.5851
01200     0.9168      0.7068     0.7397     73.0362
01300     0.9029      0.7342     0.7452     75.0005
01400     1.0101      0.6962     0.7483     74.1940
01500     0.9096      0.7152     0.7537     73.4704
01600     0.9079      0.7384     0.7590     74.4087
01700     0.7992      0.7658     0.7606     72.7134
01800     0.8277      0.7532     0.7609     71.6489
01900     0.7906      0.7785     0.7782     73.7839
02000     0.8055      0.7848     0.7782     71.3391
02100     0.7773      0.7806     0.7782     72.2482
02200     0.8223      0.7743     0.7782     73.2605
02300     0.8158      0.7595     0.7782     72.4414
02400     0.7276      0.7743     0.7799     75.2506
02500     0.8613      0.7384     0.7924     72.9237
02600     0.8174      0.7489     0.7924     73.0384
02700     0.7559      0.7785     0.7924     74.1027
02800     0.7921      0.7658     0.7924     71.9373
02900     0.9144      0.7321     0.7943     73.3204
03000     0.7919      0.7743     0.7943     72.5509
03100     0.7832      0.7637     0.7943     73.7665
03200     0.7861      0.7405     0.7943     73.3989
03300     0.7404      0.7658     0.7977     72.1235
03400     0.7982      0.7532     0.8085     74.9185
03500     0.7711      0.7637     0.8085     72.6784
03600     0.7838      0.7637     0.8085     73.1497
03700     0.7778      0.7722     0.8085     72.2701
03800     0.7367      0.7975     0.8085     74.5380
03900     0.7549      0.7827     0.8085     74.2512
04000     0.6679      0.8101     0.8085     73.5389
04100     0.6908      0.7954     0.8085     75.2162
04200     0.7671      0.7743     0.8137     73.4151
04300     0.7351      0.7996     0.8137     73.6858
04400     0.7675      0.7785     0.8137     74.1878
04500     0.6717      0.7890     0.8137     74.1260
04600     0.7317      0.7743     0.8137     73.2175
04700     0.6285      0.8186     0.8137     74.0949
04800     0.7087      0.7806     0.8137     72.6175
04900     0.6389      0.7996     0.8141     73.5223
05000     0.7127      0.7890     0.8141     73.7943
05100     0.7862      0.7658     0.8141     72.9501
05200     0.7351      0.7911     0.8151     73.0994
05300     0.7326      0.7848     0.8152     74.4921
05400     0.7361      0.7827     0.8185     74.0287
05500     0.6704      0.7932     0.8185     74.9034
05600     0.5949      0.8376     0.8185     73.0795
05700     0.6892      0.7975     0.8185     72.7607
05800     0.7079      0.7848     0.8185     73.4506
05900     0.7284      0.7890     0.8185     71.6120
06000     0.7270      0.7785     0.8185     73.8741
06100     0.7727      0.7827     0.8185     72.1969
06200     0.6408      0.8249     0.8185     73.4318
06300     0.6475      0.8165     0.8185     73.4304
06400     0.6502      0.7996     0.8248     74.1771
06500     0.6326      0.8143     0.8248     74.1644
06600     0.6893      0.8101     0.8248     72.7510
06700     0.7193      0.7679     0.8248     73.9112
06800     0.6530      0.8143     0.8248     73.7057
06900     0.6408      0.7996     0.8248     72.4333
07000     0.7115      0.8059     0.8248     72.7480
07100     0.7108      0.7932     0.8248     72.1942
07200     0.6658      0.7932     0.8248     72.2395
07300     0.7516      0.7890     0.8248     71.2799
07400     0.6500      0.7975     0.8248     74.2241
07500     0.6365      0.8122     0.8248     73.8207
07600     0.7047      0.7911     0.8248     74.2248
07700     0.6277      0.8249     0.8248     73.6617
07800     0.7322      0.7911     0.8260     71.8402
07900     0.7471      0.7806     0.8260     73.9485
08000     0.6836      0.8059     0.8260     72.5853
08100     0.7270      0.7743     0.8260     71.3289
08200     0.6413      0.7996     0.8260     73.7197
08300     0.6027      0.8312     0.8260     75.4824
08400     0.7041      0.7806     0.8260     72.3469
08500     0.6118      0.8080     0.8260     73.5916
08600     0.6399      0.8080     0.8260     73.6050
08700     0.6844      0.7932     0.8260     72.1136
08800     0.6956      0.7954     0.8260     73.6993
08900     0.7299      0.7869     0.8260     73.7852
09000     0.6099      0.8122     0.8260     73.0824
09100     0.6473      0.8122     0.8260     74.5194
09200     0.7069      0.7827     0.8260     72.8514
09300     0.5995      0.8165     0.8260     75.9563
09400     0.6937      0.7869     0.8260     76.1963
09500     0.6928      0.8038     0.8260     74.0257
09600     0.5984      0.8143     0.8260     73.2983
09700     0.6470      0.8059     0.8260     73.3966
09800     0.6488      0.7996     0.8260     74.6487
09900     0.6799      0.8080     0.8260     73.9976
10000     0.6324      0.8249     0.8260     74.4363
10100     0.6926      0.8038     0.8260     75.6035
10200     0.6606      0.8038     0.8260     73.5024
10300     0.5567      0.8376     0.8294     72.2507
10400     0.6042      0.8228     0.8294     73.1622
10500     0.6122      0.8207     0.8294     73.8216
10600     0.6696      0.7869     0.8294     74.3249
10700     0.7151      0.7954     0.8294     72.4424
10800     0.6442      0.8122     0.8294     73.6528
10900     0.6168      0.8101     0.8304     72.2683
11000     0.5503      0.8502     0.8304     73.7409
11100     0.6289      0.8080     0.8304     74.8144
11200     0.5461      0.8333     0.8304     75.0800
11300     0.6609      0.8038     0.8304     73.1285
11400     0.6658      0.7996     0.8304     72.6356
11500     0.6391      0.8143     0.8304     74.9806
11600     0.6751      0.7996     0.8304     73.9550
11700     0.6974      0.7954     0.8304     72.4891
11800     0.6265      0.8228     0.8304     72.7525
11900     0.6503      0.8017     0.8364     73.5276
12000     0.5754      0.8354     0.8364     73.6053
12100     0.5767      0.8228     0.8364     73.5899
12200     0.6286      0.8186     0.8364     75.1020
12300     0.6169      0.8059     0.8364     73.5679
12400     0.7153      0.7890     0.8364     73.8262
12500     0.6246      0.8165     0.8364     72.8657
12600     0.6101      0.8397     0.8364     74.6409
12700     0.6912      0.7806     0.8364     72.7691
12800     0.5959      0.8143     0.8364     74.3547
12900     0.6516      0.8059     0.8364     72.7176
13000     0.5795      0.8270     0.8364     72.7199
13100     0.5584      0.8397     0.8364     75.2991
13200     0.5891      0.8270     0.8364     73.1722
13300     0.6431      0.8376     0.8364     72.8639
13400     0.5830      0.8418     0.8364     72.2938
13500     0.6019      0.8122     0.8364     71.2105
13600     0.5984      0.8354     0.8364     74.8637
13700     0.6307      0.8228     0.8364     73.7056
13800     0.6334      0.7869     0.8364     74.6690
13900     0.6288      0.8312     0.8364     73.8010
14000     0.6215      0.8059     0.8364     73.9327
14100     0.6164      0.8291     0.8364     72.0022
14200     0.5848      0.8207     0.8364     72.5128
14300     0.6827      0.8059     0.8364     72.5505
14400     0.6287      0.8059     0.8364     73.3781
14500     0.6491      0.8122     0.8364     72.5201
14600     0.6554      0.7975     0.8364     74.8497
14700     0.5866      0.8207     0.8364     73.6937
14800     0.6358      0.8207     0.8364     73.9504
14900     0.6360      0.7954     0.8364     73.6192
15000     0.5908      0.8270     0.8364     74.1848
15100     0.6593      0.8080     0.8364     73.4546
15200     0.5824      0.8397     0.8364     73.7842
15300     0.5875      0.8270     0.8364     72.4271
15400     0.6626      0.7996     0.8364     71.9225
15500     0.7173      0.7848     0.8364     73.6307
15600     0.6462      0.8122     0.8364     72.9434
15700     0.5740      0.8376     0.8364     74.1227
15800     0.7190      0.7848     0.8364     73.9221
15900     0.5868      0.8228     0.8364     73.3312
16000     0.6104      0.8312     0.8364     75.6417
16100     0.7044      0.7996     0.8364     72.1130
16200     0.6095      0.8080     0.8364     73.1780
16300     0.6433      0.8312     0.8364     73.5457
16400     0.5486      0.8354     0.8364     72.5335
16500     0.6531      0.8059     0.8364     73.5892
16600     0.5729      0.8333     0.8364     71.6336
16700     0.5871      0.8228     0.8364     71.6895
16800     0.5878      0.8165     0.8364     72.4412
16900     0.6579      0.8165     0.8364     72.7378
17000     0.6569      0.7996     0.8364     72.0279
17100     0.6513      0.8080     0.8364     72.8256
17200     0.7127      0.7806     0.8364     74.1672
17300     0.7073      0.7975     0.8364     73.7803
17400     0.5988      0.8270     0.8364     73.7443
17500     0.7103      0.7869     0.8364     72.0195
17600     0.6250      0.8186     0.8364     73.8379
17700     0.6429      0.8228     0.8364     73.7412
17800     0.6406      0.8122     0.8364     74.8487
17900     0.6010      0.8270     0.8364     74.0862
18000     0.6172      0.8249     0.8364     73.2904
18100     0.5566      0.8291     0.8364     72.5272
18200     0.6673      0.8059     0.8364     73.9537
18300     0.5721      0.8354     0.8364     75.3296
18400     0.5976      0.8481     0.8389     73.8706
18500     0.6055      0.8207     0.8389     73.7550
18600     0.7485      0.7827     0.8389     74.1021
18700     0.6183      0.8207     0.8389     74.7095
18800     0.5895      0.8312     0.8389     72.8839
18900     0.6254      0.8122     0.8389     72.9071
19000     0.5508      0.8502     0.8389     73.8955
19100     0.5596      0.8418     0.8389     72.5308
19200     0.5779      0.8207     0.8389     73.6541
19300     0.6657      0.8017     0.8389     72.5648
19400     0.6158      0.8186     0.8389     73.5124
19500     0.5376      0.8439     0.8389     73.4442
19600     0.5929      0.8249     0.8389     71.6455
19700     0.6472      0.8186     0.8389     73.8910
19800     0.6953      0.7700     0.8389     74.1485
19900     0.6345      0.8059     0.8389     75.2000
20000     0.6106      0.8165     0.8389     73.2465
20100     0.5899      0.8376     0.8389     72.1522
20199     0.6465      0.8101     0.8389     70.9634
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     0.5717      0.8333     0.8304     10.2881
00100     0.4334      0.8819     0.8339     71.1902
00200     0.5820      0.8376     0.8403     71.5147
00300     0.6141      0.8143     0.8403     72.8355
00400     0.4995      0.8608     0.8403     74.9776
00500     0.5053      0.8502     0.8403     73.9907
00600     0.5403      0.8354     0.8403     75.5069
00700     0.4753      0.8650     0.8403     74.5298
00800     0.4986      0.8460     0.8403     75.2255
00900     0.6012      0.8333     0.8403     74.3931
01000     0.4728      0.8692     0.8403     71.9982
01100     0.5824      0.8165     0.8403     74.8286
01200     0.6277      0.8080     0.8403     74.4434
01300     0.5164      0.8481     0.8403     75.9266
01400     0.5680      0.8397     0.8403     76.7957
01500     0.5621      0.8397     0.8403     74.7629
01600     0.5671      0.8397     0.8403     73.9799
01700     0.5284      0.8586     0.8403     73.5046
01800     0.4483      0.8755     0.8403     72.6074
01900     0.4563      0.8608     0.8403     74.3562
02000     0.5505      0.8439     0.8403     72.0301
02100     0.5654      0.8270     0.8403     73.6119
02200     0.5298      0.8397     0.8411     72.8845
02300     0.5143      0.8502     0.8411     73.8913
02400     0.4210      0.8776     0.8411     72.4119
02500     0.5242      0.8565     0.8411     74.0719
02600     0.6061      0.8207     0.8411     73.1501
02700     0.4298      0.8840     0.8411     73.0954
02800     0.5805      0.8291     0.8411     73.1735
02900     0.5122      0.8312     0.8411     71.2212
03000     0.5221      0.8418     0.8411     72.8054
03100     0.5703      0.8354     0.8411     72.9710
03200     0.4975      0.8439     0.8411     75.5325
03300     0.4764      0.8650     0.8411     74.3171
03400     0.5055      0.8481     0.8411     73.0902
03500     0.6126      0.8165     0.8411     73.6452
03600     0.5035      0.8544     0.8411     72.8351
03700     0.6707      0.7911     0.8411     73.0758
03800     0.4653      0.8544     0.8411     75.1292
03900     0.4749      0.8713     0.8411     73.0775
04000     0.5704      0.8228     0.8411     74.5364
04100     0.5101      0.8586     0.8411     74.2879
04200     0.5363      0.8333     0.8411     74.3041
04300     0.5217      0.8439     0.8411     75.1796
04400     0.4806      0.8460     0.8411     73.2628
04500     0.5799      0.8312     0.8411     72.2630
04600     0.5401      0.8460     0.8411     74.0377
04700     0.5430      0.8439     0.8411     74.2275
04800     0.4729      0.8797     0.8411     74.1411
04900     0.5257      0.8312     0.8411     72.9389
05000     0.5475      0.8481     0.8411     75.0890
05100     0.5126      0.8460     0.8411     73.8987
05200     0.5561      0.8523     0.8411     73.8278
05300     0.5149      0.8544     0.8411     76.9982
05400     0.5499      0.8207     0.8411     74.6965
05500     0.5051      0.8544     0.8411     73.7657
05600     0.5193      0.8586     0.8411     72.8603
05700     0.5883      0.8228     0.8411     74.3230
05800     0.4515      0.8924     0.8411     74.7310
05900     0.5467      0.8502     0.8411     72.5076
06000     0.5121      0.8586     0.8411     71.5463
06100     0.5446      0.8270     0.8411     72.8809
06200     0.4647      0.8755     0.8411     74.0609
06300     0.5279      0.8376     0.8411     74.1022
06400     0.4786      0.8502     0.8411     73.7801
06500     0.5502      0.8481     0.8411     70.6046
06600     0.5221      0.8439     0.8411     71.2718
06700     0.5527      0.8354     0.8411     73.1806
06800     0.5956      0.8186     0.8411     72.2018
06900     0.4627      0.8713     0.8411     72.0548
07000     0.5312      0.8523     0.8437     74.2473
07100     0.6085      0.8122     0.8437     72.2497
07200     0.4940      0.8481     0.8437     73.6794
07300     0.6448      0.8122     0.8437     73.8734
07400     0.5862      0.8249     0.8437     74.9657
07500     0.4847      0.8608     0.8437     72.4999
07600     0.5302      0.8460     0.8437     73.4715
07700     0.5339      0.8502     0.8437     72.3640
07800     0.4989      0.8481     0.8448     73.9817
07900     0.6011      0.8207     0.8448     72.8855
08000     0.5327      0.8376     0.8448     73.9189
08100     0.4739      0.8418     0.8448     73.7412
08200     0.5110      0.8460     0.8448     73.5355
08300     0.4410      0.8713     0.8448     73.9070
08400     0.5079      0.8418     0.8448     73.9573
08500     0.4980      0.8460     0.8448     72.7916
08600     0.5349      0.8418     0.8448     74.6747
08700     0.5383      0.8481     0.8448     73.1686
08800     0.5645      0.8460     0.8448     75.1557
08900     0.5441      0.8418     0.8448     74.6228
09000     0.5381      0.8439     0.8448     73.7587
09100     0.5157      0.8502     0.8448     73.7793
09200     0.5392      0.8418     0.8448     73.2167
09300     0.6081      0.8186     0.8448     73.7117
09400     0.5989      0.8312     0.8448     74.2939
09500     0.5685      0.8481     0.8448     73.5612
09600     0.5693      0.8354     0.8448     74.9181
09700     0.5941      0.8059     0.8448     74.2868
09800     0.5230      0.8523     0.8448     74.7949
09900     0.5087      0.8565     0.8448     73.9250
Start testing:
Test Accuracy: 0.8142
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=7, quant_actNM=7, quant_inp=7, quant_w=7, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
c38c1b37-22ae-4ada-8537-6ab9bc034670
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.7968      0.0949     0.0419     13.8537
00100     1.5043      0.5063     0.5566     73.3796
00200     0.9086      0.7110     0.7324     73.6822
00300     0.7069      0.7890     0.7786     73.2927
00400     0.6606      0.8017     0.8220     74.0259
00500     0.5314      0.8270     0.8354     73.0927
00600     0.4871      0.8565     0.8490     73.2525
00700     0.5495      0.8312     0.8574     73.3651
00800     0.4005      0.8819     0.8678     73.0128
00900     0.4923      0.8608     0.8751     71.8566
01000     0.3957      0.8987     0.8751     72.7643
01100     0.3733      0.8924     0.8805     72.1006
01200     0.3820      0.8882     0.8805     72.1965
01300     0.3654      0.8882     0.8844     73.3190
01400     0.3587      0.8924     0.8844     73.8098
01500     0.3720      0.8903     0.8844     72.4017
01600     0.3561      0.9030     0.8844     73.0948
01700     0.3199      0.9008     0.8925     72.6876
01800     0.3337      0.8945     0.8925     72.4490
01900     0.3519      0.9135     0.8925     72.3965
02000     0.3077      0.8966     0.8925     72.6667
02100     0.3201      0.8987     0.8945     72.7511
02200     0.3221      0.9156     0.8949     73.1612
02300     0.2805      0.9072     0.8949     71.6944
02400     0.3196      0.9135     0.8975     73.5560
02500     0.2934      0.9156     0.9003     73.3003
02600     0.2903      0.9135     0.9003     73.6347
02700     0.2799      0.9177     0.9003     72.2098
02800     0.2579      0.9219     0.9003     72.2137
02900     0.2633      0.9430     0.9021     72.6971
03000     0.2552      0.9219     0.9021     72.9126
03100     0.2745      0.9135     0.9021     73.1063
03200     0.1998      0.9346     0.9021     73.2745
03300     0.2668      0.9198     0.9021     72.4911
03400     0.2815      0.9156     0.9102     73.0087
03500     0.2342      0.9283     0.9102     72.7574
03600     0.1994      0.9515     0.9102     72.8395
03700     0.2222      0.9473     0.9102     72.6354
03800     0.1890      0.9515     0.9102     72.5688
03900     0.2719      0.9198     0.9102     72.9075
04000     0.2401      0.9219     0.9102     73.0062
04100     0.2218      0.9304     0.9102     72.9452
04200     0.1715      0.9578     0.9102     72.5236
04300     0.2663      0.9409     0.9102     72.3359
04400     0.2264      0.9283     0.9102     73.0854
04500     0.1879      0.9515     0.9102     73.3683
04600     0.2983      0.9283     0.9102     73.0613
04700     0.2159      0.9409     0.9102     71.9590
04800     0.2254      0.9346     0.9102     72.7148
04900     0.2152      0.9473     0.9102     72.7475
05000     0.2201      0.9325     0.9102     72.6543
05100     0.2140      0.9430     0.9102     73.3640
05200     0.1920      0.9515     0.9102     72.9370
05300     0.1863      0.9515     0.9102     72.7083
05400     0.1910      0.9388     0.9102     72.6918
05500     0.2322      0.9304     0.9102     72.3064
05600     0.2049      0.9388     0.9102     73.0743
05700     0.2047      0.9515     0.9102     72.2762
05800     0.1754      0.9536     0.9102     73.1067
05900     0.2122      0.9430     0.9102     73.2087
06000     0.2172      0.9494     0.9102     71.8761
06100     0.2239      0.9409     0.9102     72.3648
06200     0.1695      0.9557     0.9102     73.4952
06300     0.1738      0.9578     0.9102     72.9840
06400     0.2128      0.9536     0.9102     72.7705
06500     0.1881      0.9473     0.9102     72.8253
06600     0.2074      0.9409     0.9102     72.7294
06700     0.1981      0.9430     0.9102     72.2062
06800     0.1866      0.9430     0.9102     72.5769
06900     0.1913      0.9557     0.9102     72.3743
07000     0.1722      0.9620     0.9102     73.0625
07100     0.2310      0.9409     0.9102     72.3161
07200     0.1580      0.9451     0.9102     72.8007
07300     0.1661      0.9557     0.9102     73.1754
07400     0.2067      0.9430     0.9102     73.0715
07500     0.2162      0.9557     0.9124     73.2435
07600     0.1901      0.9515     0.9124     73.2295
07700     0.1700      0.9620     0.9124     72.9885
07800     0.1589      0.9620     0.9124     72.8210
07900     0.1611      0.9578     0.9124     73.0338
08000     0.2079      0.9515     0.9124     73.1748
08100     0.2195      0.9473     0.9124     72.6757
08200     0.2238      0.9430     0.9124     73.1766
08300     0.1721      0.9578     0.9124     73.7912
08400     0.2002      0.9473     0.9124     73.2394
08500     0.1573      0.9599     0.9124     72.5061
08600     0.1860      0.9557     0.9124     72.9548
08700     0.2248      0.9451     0.9124     73.6691
08800     0.1579      0.9515     0.9124     73.2707
08900     0.1712      0.9578     0.9124     74.1431
09000     0.1469      0.9705     0.9124     73.5395
09100     0.1305      0.9705     0.9124     73.2838
09200     0.2081      0.9515     0.9124     73.1210
09300     0.1522      0.9599     0.9124     72.4672
09400     0.1960      0.9451     0.9124     73.0981
09500     0.1512      0.9620     0.9124     72.7339
09600     0.1564      0.9599     0.9124     73.2208
09700     0.2319      0.9388     0.9124     72.6451
09800     0.1963      0.9494     0.9124     72.4130
09900     0.1438      0.9705     0.9124     72.9916
10000     0.1455      0.9662     0.9124     73.2596
10100     0.1626      0.9620     0.9124     73.6551
10200     0.1943      0.9473     0.9124     73.7302
10300     0.1535      0.9684     0.9124     73.0711
10400     0.0991      0.9831     0.9124     73.0672
10500     0.1805      0.9578     0.9124     72.6726
10600     0.1151      0.9705     0.9124     72.5475
10700     0.1836      0.9515     0.9124     72.8195
10800     0.1318      0.9684     0.9124     72.8447
10900     0.1451      0.9620     0.9124     72.9307
11000     0.1323      0.9662     0.9124     72.6830
11100     0.1653      0.9620     0.9124     72.8694
11200     0.1509      0.9536     0.9124     72.7189
11300     0.1577      0.9599     0.9124     72.1704
11400     0.1460      0.9662     0.9124     73.0910
11500     0.1455      0.9662     0.9124     72.9598
11600     0.1974      0.9536     0.9124     73.4621
11700     0.1632      0.9578     0.9124     73.1893
11800     0.1364      0.9641     0.9124     72.9046
11900     0.1523      0.9578     0.9124     73.9147
12000     0.2281      0.9430     0.9171     73.1718
12100     0.1558      0.9641     0.9171     73.4864
12200     0.1254      0.9705     0.9171     72.5036
12300     0.1972      0.9578     0.9171     73.3119
12400     0.1088      0.9768     0.9171     72.8561
12500     0.1157      0.9726     0.9171     73.1332
12600     0.1305      0.9684     0.9171     73.2058
12700     0.1476      0.9578     0.9171     73.0623
12800     0.1322      0.9747     0.9171     73.3944
12900     0.1273      0.9662     0.9171     72.6148
13000     0.1344      0.9662     0.9171     72.2988
13100     0.1048      0.9831     0.9171     72.4917
13200     0.1309      0.9662     0.9171     72.5204
13300     0.1685      0.9494     0.9171     72.9576
13400     0.1719      0.9599     0.9171     72.8354
13500     0.0935      0.9873     0.9171     73.0776
13600     0.1598      0.9620     0.9171     72.7180
13700     0.1498      0.9620     0.9171     72.5656
13800     0.1289      0.9726     0.9171     71.6506
13900     0.1296      0.9641     0.9171     72.6322
14000     0.1639      0.9726     0.9171     73.3182
14100     0.1210      0.9705     0.9171     72.6796
14200     0.1356      0.9705     0.9171     72.9192
14300     0.1555      0.9620     0.9171     72.9828
14400     0.1288      0.9705     0.9171     73.2546
14500     0.1044      0.9705     0.9171     72.9775
14600     0.1342      0.9641     0.9171     72.6945
14700     0.1183      0.9726     0.9171     73.1255
14800     0.1197      0.9747     0.9171     72.5976
14900     0.1049      0.9726     0.9171     72.7074
15000     0.1165      0.9705     0.9171     72.7373
15100     0.1191      0.9684     0.9171     72.2966
15200     0.1083      0.9789     0.9171     72.5790
15300     0.1348      0.9641     0.9171     72.6793
15400     0.1455      0.9599     0.9171     73.2049
15500     0.1390      0.9641     0.9171     73.5635
15600     0.1328      0.9662     0.9171     72.6820
15700     0.1775      0.9599     0.9171     72.8813
15800     0.1332      0.9684     0.9171     72.8065
15900     0.1003      0.9831     0.9171     73.4530
16000     0.1452      0.9515     0.9171     73.1770
16100     0.1836      0.9536     0.9171     72.7060
16200     0.1430      0.9599     0.9171     73.1766
16300     0.1275      0.9705     0.9171     73.4453
16400     0.1362      0.9641     0.9171     72.5319
16500     0.1050      0.9768     0.9171     73.0237
16600     0.1269      0.9726     0.9171     72.8250
16700     0.1683      0.9515     0.9171     73.4252
16800     0.1203      0.9726     0.9171     73.2776
16900     0.1181      0.9705     0.9171     72.3589
17000     0.1253      0.9684     0.9171     72.5363
17100     0.1050      0.9726     0.9171     72.1754
17200     0.1218      0.9768     0.9171     73.2298
17300     0.1449      0.9557     0.9171     72.6844
17400     0.1107      0.9726     0.9171     73.4290
17500     0.0966      0.9810     0.9171     72.4232
17600     0.1076      0.9747     0.9171     72.9361
17700     0.1336      0.9641     0.9171     73.5486
17800     0.1596      0.9684     0.9171     73.1452
17900     0.1153      0.9726     0.9171     73.4410
18000     0.1322      0.9662     0.9171     72.6560
18100     0.1130      0.9768     0.9171     73.0886
18200     0.0989      0.9810     0.9171     74.1961
18300     0.1434      0.9662     0.9171     72.1526
18400     0.1058      0.9747     0.9171     73.4530
18500     0.1437      0.9662     0.9171     73.5800
18600     0.1292      0.9747     0.9171     72.6337
18700     0.1447      0.9578     0.9171     73.0343
18800     0.1462      0.9641     0.9171     73.2886
18900     0.1419      0.9747     0.9171     72.6324
19000     0.1312      0.9747     0.9171     72.4945
19100     0.1112      0.9747     0.9171     71.9348
19200     0.1192      0.9578     0.9171     72.5703
19300     0.1428      0.9620     0.9171     72.4157
19400     0.1165      0.9684     0.9171     72.5959
19500     0.1190      0.9726     0.9171     73.5604
19600     0.1205      0.9684     0.9171     72.9926
19700     0.1266      0.9810     0.9171     72.6939
19800     0.1494      0.9705     0.9171     73.3164
19900     0.1274      0.9684     0.9171     73.3103
20000     0.1262      0.9705     0.9171     72.6558
20100     0.1020      0.9747     0.9171     72.5472
20199     0.1258      0.9662     0.9171     71.7718
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     0.1279      0.9684     0.9021     10.4234
00100     0.0969      0.9768     0.9021     72.3557
00200     0.1014      0.9705     0.9025     72.5103
00300     0.0833      0.9831     0.9052     73.7369
00400     0.1076      0.9768     0.9052     72.3536
00500     0.0875      0.9831     0.9052     73.3813
00600     0.1252      0.9662     0.9052     73.1530
00700     0.1137      0.9705     0.9052     72.9332
00800     0.1210      0.9620     0.9052     71.7405
00900     0.1262      0.9747     0.9052     72.3822
01000     0.1135      0.9726     0.9052     72.9928
01100     0.0818      0.9895     0.9052     73.4362
01200     0.0665      0.9895     0.9052     73.2532
01300     0.1187      0.9726     0.9052     72.7444
01400     0.1238      0.9620     0.9052     73.5525
01500     0.1249      0.9662     0.9052     73.5504
01600     0.1161      0.9747     0.9052     73.1956
01700     0.0833      0.9810     0.9052     73.5191
01800     0.1124      0.9810     0.9052     72.2723
01900     0.0864      0.9810     0.9052     72.8627
02000     0.0991      0.9831     0.9052     72.9986
02100     0.0780      0.9831     0.9052     72.8763
02200     0.0895      0.9873     0.9052     72.7141
02300     0.1645      0.9620     0.9052     72.1402
02400     0.0706      0.9916     0.9052     73.0788
02500     0.0729      0.9852     0.9052     73.1101
02600     0.0991      0.9768     0.9052     72.8110
02700     0.1162      0.9768     0.9052     72.9071
02800     0.0970      0.9768     0.9052     72.5125
02900     0.0832      0.9831     0.9052     72.8490
03000     0.1340      0.9747     0.9052     72.5885
03100     0.1092      0.9747     0.9052     71.6653
03200     0.0728      0.9895     0.9052     72.4712
03300     0.1091      0.9684     0.9052     72.6162
03400     0.1501      0.9599     0.9052     72.1689
03500     0.1362      0.9726     0.9052     72.4521
03600     0.0936      0.9768     0.9052     72.5720
03700     0.0806      0.9789     0.9052     71.5852
03800     0.0990      0.9789     0.9052     72.8161
03900     0.0818      0.9852     0.9052     72.3299
04000     0.0994      0.9789     0.9052     72.3320
04100     0.1106      0.9789     0.9052     72.9422
04200     0.1083      0.9789     0.9052     72.6187
04300     0.0770      0.9873     0.9052     72.9196
04400     0.1030      0.9768     0.9052     72.2628
04500     0.0914      0.9852     0.9052     72.4080
04600     0.0912      0.9768     0.9052     72.9805
04700     0.1094      0.9768     0.9052     72.1598
04800     0.0908      0.9852     0.9052     73.0885
04900     0.1167      0.9768     0.9052     73.3209
05000     0.1010      0.9726     0.9052     73.2068
05100     0.1033      0.9768     0.9052     73.8726
05200     0.1020      0.9726     0.9052     72.7614
05300     0.0834      0.9852     0.9052     72.9259
05400     0.1202      0.9726     0.9052     73.5952
05500     0.0949      0.9789     0.9052     72.8905
05600     0.1045      0.9684     0.9052     73.0992
05700     0.0847      0.9873     0.9052     73.1148
05800     0.0808      0.9831     0.9052     72.8662
05900     0.0881      0.9789     0.9052     73.4032
06000     0.1193      0.9662     0.9052     73.2440
06100     0.0677      0.9873     0.9052     72.2028
06200     0.1019      0.9768     0.9052     73.0200
06300     0.0990      0.9768     0.9052     72.3581
06400     0.0769      0.9831     0.9052     72.2829
06500     0.0735      0.9873     0.9052     73.9169
06600     0.1103      0.9747     0.9052     72.0215
06700     0.0974      0.9852     0.9052     73.1437
06800     0.1206      0.9705     0.9052     72.8902
06900     0.0707      0.9852     0.9052     73.1375
07000     0.1104      0.9768     0.9052     73.1507
07100     0.1177      0.9747     0.9052     73.0033
07200     0.1116      0.9852     0.9052     73.1143
07300     0.1121      0.9747     0.9052     72.9477
07400     0.0656      0.9916     0.9052     72.6900
07500     0.1097      0.9726     0.9052     73.5358
07600     0.0849      0.9831     0.9052     72.7393
07700     0.0879      0.9810     0.9052     72.8574
07800     0.0866      0.9852     0.9052     72.7018
07900     0.1289      0.9705     0.9052     72.2295
08000     0.0788      0.9873     0.9052     72.5450
08100     0.1241      0.9726     0.9052     72.3744
08200     0.0780      0.9852     0.9052     72.3286
08300     0.0630      0.9895     0.9052     73.7272
08400     0.0960      0.9895     0.9052     72.5089
08500     0.0863      0.9831     0.9052     72.4886
08600     0.1084      0.9684     0.9052     73.1481
08700     0.1019      0.9747     0.9052     72.7749
08800     0.1071      0.9810     0.9052     72.6897
08900     0.0870      0.9852     0.9052     71.9225
09000     0.0909      0.9810     0.9052     72.3224
09100     0.0995      0.9747     0.9052     72.2136
09200     0.0948      0.9810     0.9052     72.1235
09300     0.1111      0.9684     0.9052     72.5873
09400     0.0997      0.9810     0.9052     72.2723
09500     0.0895      0.9810     0.9052     72.9540
09600     0.1015      0.9768     0.9052     72.2244
09700     0.0864      0.9852     0.9052     73.1034
09800     0.0805      0.9873     0.9052     72.3407
09900     0.0762      0.9831     0.9052     72.7555
Start testing:
Test Accuracy: 0.8979
