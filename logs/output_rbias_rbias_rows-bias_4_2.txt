Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=512, canonical_testing=False, cs=0.5, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, hidden=110, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=0.1, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=6, quant_actNM=8, quant_inp=4, quant_w=None, random_seed=8627169, rows_bias=4, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=640, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
91c8b17f-8766-4632-9f70-97ecf4f23f1e
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 161, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 212, in forward
    gates = quant_pass(pact_a_bmm( quant_pass(pact_a_bmm(part1, self.a12), self.abMVM, self.a12) + quant_pass(pact_a_bmm(part2, self.a13), self.abMVM, self.a13), self.a14), self.abNM, self.a14)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 89, in forward
    x01q =  torch.round(x01 * step_d ) / step_d
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.76 GiB total capacity; 8.50 GiB already allocated; 17.12 MiB free; 9.78 GiB reserved in total by PyTorch)
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=512, canonical_testing=False, cs=0.5, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, hidden=110, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=0.1, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=6, quant_actNM=8, quant_inp=4, quant_w=None, random_seed=8627169, rows_bias=4, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=640, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
1b63ad2a-cab5-4021-8953-e92d34539530
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 161, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 212, in forward
    gates = quant_pass(pact_a_bmm( quant_pass(pact_a_bmm(part1, self.a12), self.abMVM, self.a12) + quant_pass(pact_a_bmm(part2, self.a13), self.abMVM, self.a13), self.a14), self.abNM, self.a14)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 89, in forward
    x01q =  torch.round(x01 * step_d ) / step_d
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.76 GiB total capacity; 8.50 GiB already allocated; 17.12 MiB free; 9.78 GiB reserved in total by PyTorch)
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=512, canonical_testing=False, cs=0.5, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, hidden=110, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=0.1, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=6, quant_actNM=8, quant_inp=4, quant_w=None, random_seed=8627169, rows_bias=4, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=640, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
c3a70a87-ea75-4a77-afb4-5969745c18de
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.6892      0.0703     0.0730     9.9764
00100     1.5669      0.5039     0.5148     68.2891
00200     1.1792      0.6328     0.6791     68.8856
00300     1.0574      0.6855     0.7126     71.9193
00400     0.9449      0.6973     0.7213     69.9571
00500     0.8248      0.7539     0.7599     69.5493
00600     0.8777      0.6953     0.8042     69.6255
00700     0.8324      0.7324     0.8042     69.0075
00800     0.8723      0.7285     0.8042     71.0245
00900     0.7695      0.7871     0.8086     68.8736
01000     0.7308      0.7812     0.8086     70.2597
01100     0.6889      0.7832     0.8086     71.1071
01200     0.8152      0.7461     0.8086     68.8610
01300     0.7413      0.7715     0.8137     69.7768
01400     0.7105      0.7910     0.8137     68.6941
01500     0.6815      0.7812     0.8137     69.8744
01600     0.7385      0.7812     0.8137     68.5563
01700     0.7880      0.7656     0.8137     69.7056
01800     0.7241      0.7617     0.8137     70.1718
01900     0.6794      0.8066     0.8246     70.5733
02000     0.8152      0.7598     0.8246     69.5410
02100     0.7637      0.7715     0.8246     69.6948
02200     0.6633      0.8125     0.8246     69.4534
02300     0.6017      0.8242     0.8246     72.1127
02400     0.6391      0.8027     0.8246     71.7781
02500     0.6525      0.8164     0.8246     70.4543
02600     0.7054      0.7773     0.8323     70.1055
02700     0.6975      0.7773     0.8323     70.1784
02800     0.6681      0.7891     0.8323     69.6140
02900     0.6454      0.8125     0.8323     69.9794
03000     0.6761      0.7988     0.8323     68.7573
03100     0.6601      0.7969     0.8323     71.4660
03200     0.5837      0.8242     0.8323     70.3374
03300     0.5259      0.8359     0.8323     70.6363
03400     0.6266      0.8242     0.8323     71.0505
03500     0.6135      0.8066     0.8323     70.4144
03600     0.6652      0.7852     0.8323     71.9251
03700     0.6468      0.8086     0.8323     71.1637
03800     0.6323      0.8105     0.8464     72.2237
03900     0.6428      0.8008     0.8464     70.3858
04000     0.6038      0.8184     0.8464     70.5140
04100     0.5842      0.8086     0.8464     69.6574
04200     0.6129      0.8008     0.8464     72.3183
04300     0.6423      0.7832     0.8464     70.7458
04400     0.6244      0.8105     0.8464     69.2482
04500     0.5849      0.8242     0.8464     68.7380
04600     0.6046      0.8184     0.8464     69.6834
04700     0.5357      0.8574     0.8464     69.5563
04800     0.5676      0.8340     0.8464     70.0055
04900     0.5816      0.8320     0.8464     70.9680
05000     0.6157      0.8145     0.8464     70.9073
05100     0.5532      0.8359     0.8464     70.6215
05200     0.5547      0.8516     0.8464     72.8339
05300     0.6481      0.7988     0.8464     69.2084
05400     0.5816      0.8184     0.8464     69.3572
05500     0.6020      0.8086     0.8464     71.1175
05600     0.6384      0.8203     0.8464     71.2036
05700     0.4953      0.8496     0.8464     69.7269
05800     0.5731      0.8359     0.8464     68.8448
05900     0.5464      0.8320     0.8464     70.8930
06000     0.5262      0.8516     0.8464     70.5746
06100     0.5303      0.8457     0.8464     71.8506
06200     0.6354      0.8008     0.8464     72.3379
06300     0.5900      0.8145     0.8464     70.3841
06400     0.5130      0.8516     0.8464     68.4097
06500     0.6293      0.7949     0.8464     69.4990
06600     0.5843      0.8262     0.8464     71.4690
06700     0.5751      0.8066     0.8464     70.2852
06800     0.5872      0.8203     0.8464     72.3300
06900     0.5678      0.8242     0.8464     69.9478
07000     0.5894      0.8262     0.8464     71.6825
07100     0.4715      0.8594     0.8464     71.3495
07200     0.5422      0.8438     0.8464     69.9049
07300     0.5771      0.8086     0.8464     70.3684
07400     0.5324      0.8418     0.8464     70.8735
07500     0.5533      0.8398     0.8464     70.5080
07600     0.5555      0.8359     0.8464     71.9666
07700     0.5563      0.8398     0.8489     70.4563
07800     0.5413      0.8359     0.8489     72.0807
07900     0.5334      0.8496     0.8489     71.8877
08000     0.5811      0.8398     0.8489     73.1351
08100     0.5150      0.8457     0.8537     71.9024
08200     0.5789      0.8203     0.8537     69.2280
08300     0.5791      0.8223     0.8537     69.0456
08400     0.4874      0.8535     0.8537     69.7293
08500     0.5078      0.8438     0.8537     70.3432
08600     0.5201      0.8438     0.8537     69.0059
08700     0.5552      0.8477     0.8537     68.9474
08800     0.5877      0.8164     0.8537     70.1119
08900     0.5458      0.8301     0.8537     68.7125
09000     0.5183      0.8535     0.8537     69.8544
09100     0.5374      0.8320     0.8537     69.0388
09200     0.5126      0.8516     0.8537     69.8478
09300     0.5727      0.8242     0.8537     70.4808
09400     0.4928      0.8457     0.8537     70.9314
09500     0.4797      0.8516     0.8537     71.1453
09600     0.5784      0.8242     0.8537     71.3098
09700     0.4535      0.8633     0.8537     70.4553
09800     0.5983      0.8320     0.8537     71.7398
09900     0.5287      0.8398     0.8537     69.6395
10000     0.4636      0.8730     0.8537     72.3378
10100     0.4940      0.8477     0.8537     70.5770
10200     0.4646      0.8652     0.8537     70.3311
10300     0.4936      0.8457     0.8537     70.5440
10400     0.4786      0.8555     0.8537     70.2608
10500     0.5282      0.8203     0.8537     72.2568
10600     0.5177      0.8594     0.8537     70.9674
10700     0.4561      0.8711     0.8537     69.9825
10800     0.4369      0.8691     0.8537     69.1456
10900     0.4383      0.8789     0.8537     70.2905
11000     0.4822      0.8652     0.8537     72.6301
11100     0.4419      0.8770     0.8537     72.8434
11200     0.4703      0.8730     0.8537     67.6862
11300     0.4771      0.8730     0.8537     68.7678
11400     0.4593      0.8730     0.8537     71.2573
11500     0.4168      0.8750     0.8537     71.4360
11600     0.5464      0.8398     0.8537     70.0282
11700     0.4102      0.8848     0.8537     69.2421
11800     0.4747      0.8711     0.8537     71.3932
11900     0.4695      0.8633     0.8537     69.1140
12000     0.4767      0.8613     0.8537     68.8796
12100     0.4711      0.8418     0.8537     70.7198
12200     0.5315      0.8438     0.8537     69.1005
12300     0.4307      0.8691     0.8537     71.6758
12400     0.4395      0.8633     0.8537     70.6912
12500     0.4266      0.8730     0.8537     71.2771
12600     0.4770      0.8652     0.8537     71.2104
12700     0.4394      0.8633     0.8537     69.8087
12800     0.4205      0.8789     0.8537     71.3546
12900     0.4577      0.8730     0.8537     70.0354
13000     0.4326      0.8730     0.8537     71.0705
13100     0.4804      0.8652     0.8537     70.3827
13200     0.4926      0.8535     0.8537     70.3397
13300     0.4769      0.8496     0.8537     70.5035
13400     0.5159      0.8359     0.8537     68.6508
13500     0.4403      0.8711     0.8537     70.3594
13600     0.4191      0.8633     0.8537     73.0931
13700     0.5071      0.8652     0.8537     71.4091
13800     0.5043      0.8516     0.8537     69.7809
13900     0.4329      0.8887     0.8537     68.9169
14000     0.4029      0.8926     0.8537     69.9591
14100     0.4512      0.8750     0.8537     69.8145
14200     0.4818      0.8613     0.8537     69.2721
14300     0.4391      0.8711     0.8537     70.6740
14400     0.4176      0.8926     0.8537     72.7522
14500     0.4228      0.8633     0.8537     72.0405
14600     0.4512      0.8555     0.8537     70.4445
14700     0.3864      0.8887     0.8537     71.6020
14800     0.4349      0.8848     0.8537     71.3527
14900     0.4300      0.8789     0.8537     71.5943
15000     0.5001      0.8496     0.8537     73.9461
15100     0.4695      0.8457     0.8537     69.8908
15200     0.4126      0.8789     0.8537     70.7906
15300     0.4928      0.8535     0.8537     71.4764
15400     0.4172      0.8828     0.8537     71.3741
15500     0.4069      0.8730     0.8537     70.5475
15600     0.4772      0.8730     0.8537     70.1515
15700     0.5229      0.8516     0.8537     70.3515
15800     0.4903      0.8438     0.8537     71.1946
15900     0.5069      0.8574     0.8537     71.1541
16000     0.5025      0.8398     0.8537     70.0117
16100     0.4271      0.8828     0.8537     69.2619
16200     0.4636      0.8535     0.8537     72.0996
16300     0.4710      0.8594     0.8537     69.9065
16400     0.3798      0.9082     0.8537     69.8402
16500     0.4277      0.8672     0.8537     67.0421
16600     0.4368      0.8750     0.8537     71.2819
16700     0.3927      0.9004     0.8537     69.8550
16800     0.4459      0.8770     0.8537     70.2113
16900     0.4525      0.8672     0.8537     71.1122
17000     0.4146      0.8848     0.8537     69.7470
17100     0.3557      0.9082     0.8537     69.4533
17200     0.4419      0.8594     0.8537     68.9538
17300     0.4918      0.8613     0.8537     70.3772
17400     0.4204      0.8750     0.8537     69.5276
17500     0.4287      0.8789     0.8537     67.4039
17600     0.4637      0.8594     0.8537     69.3857
17700     0.4304      0.8750     0.8537     70.1938
17800     0.4972      0.8379     0.8537     69.7388
17900     0.4151      0.8809     0.8537     70.4254
18000     0.4538      0.8730     0.8537     70.7781
18100     0.4056      0.8770     0.8537     68.9491
18200     0.4647      0.8633     0.8537     69.7633
18300     0.4331      0.8828     0.8537     68.9433
18400     0.4509      0.8711     0.8537     69.1898
18500     0.4282      0.8770     0.8537     69.1331
18600     0.3891      0.8691     0.8537     70.8085
18700     0.4249      0.8691     0.8537     68.9718
18800     0.4551      0.8828     0.8537     71.0265
18900     0.4396      0.8672     0.8537     70.4915
19000     0.4826      0.8711     0.8537     70.0814
19100     0.4641      0.8730     0.8537     69.3790
19200     0.3912      0.8926     0.8537     68.9143
19300     0.4443      0.8633     0.8537     69.5244
19400     0.4393      0.8691     0.8537     69.1166
19500     0.4769      0.8730     0.8537     69.2869
19600     0.4550      0.8730     0.8537     71.0786
19700     0.3495      0.9023     0.8537     70.3148
19800     0.4370      0.8770     0.8537     69.2730
19900     0.3952      0.8809     0.8537     67.9708
20000     0.4905      0.8535     0.8537     68.8618
20100     0.4459      0.8652     0.8537     70.5548
20199     0.3970      0.8633     0.8537     68.7132
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     0.7178      0.7734     0.7672     10.1495
00100     0.4693      0.8535     0.8715     69.8981
00200     0.4903      0.8418     0.8830     68.9739
00300     0.4128      0.8770     0.8841     70.0192
00400     0.4287      0.8789     0.8841     68.8273
00500     0.4806      0.8594     0.8854     70.9196
00600     0.3832      0.8926     0.8854     67.9839
00700     0.5496      0.8418     0.8854     67.0879
00800     0.4999      0.8418     0.8854     68.1780
00900     0.4427      0.8633     0.8854     68.4272
01000     0.3843      0.8750     0.8854     68.5924
01100     0.5101      0.8613     0.8854     70.4686
01200     0.4616      0.8828     0.8854     67.9647
01300     0.4208      0.8652     0.8854     68.4736
01400     0.4434      0.8691     0.8854     69.4392
01500     0.4344      0.8691     0.8864     68.3414
01600     0.4360      0.8652     0.8864     67.6554
01700     0.4819      0.8633     0.8864     71.0074
01800     0.4697      0.8730     0.8886     69.3448
01900     0.4655      0.8535     0.8886     69.9107
02000     0.4632      0.8613     0.8886     67.3639
02100     0.4006      0.8770     0.8886     70.1269
02200     0.4876      0.8418     0.8886     68.7733
02300     0.4460      0.8672     0.8886     67.9177
02400     0.4246      0.8555     0.8886     67.4473
02500     0.4231      0.8887     0.8886     68.8567
02600     0.4825      0.8535     0.8886     68.1297
02700     0.4641      0.8594     0.8886     68.6687
02800     0.4625      0.8457     0.8886     68.4177
02900     0.4846      0.8320     0.8886     69.6467
03000     0.4482      0.8691     0.8886     69.7086
03100     0.4731      0.8516     0.8900     69.5912
03200     0.3584      0.8965     0.8900     69.2623
03300     0.4424      0.8555     0.8900     69.6118
03400     0.3810      0.8945     0.8900     68.9408
03500     0.4025      0.8867     0.8900     72.2251
03600     0.4221      0.8848     0.8900     69.6533
03700     0.4414      0.8691     0.8900     69.2085
03800     0.3758      0.8789     0.8900     71.3170
03900     0.4839      0.8574     0.8900     71.2520
04000     0.4496      0.8809     0.8900     71.0582
04100     0.4894      0.8496     0.8900     67.8070
04200     0.4261      0.8867     0.8916     72.4572
04300     0.4174      0.8789     0.8916     71.0994
04400     0.4366      0.8711     0.8916     68.3354
04500     0.4852      0.8457     0.8916     68.0388
04600     0.4816      0.8398     0.8916     69.4452
04700     0.4162      0.8828     0.8916     69.3580
04800     0.3688      0.8965     0.8916     69.9088
04900     0.4210      0.8828     0.8916     71.1974
05000     0.4042      0.8750     0.8916     67.9529
05100     0.4414      0.8574     0.8916     70.9480
05200     0.4592      0.8730     0.8916     68.7174
05300     0.4391      0.8594     0.8916     69.2584
05400     0.4626      0.8613     0.8916     68.3286
05500     0.4359      0.8789     0.8916     70.2644
05600     0.3910      0.8867     0.8916     70.4186
05700     0.3802      0.8984     0.8916     72.2322
05800     0.3968      0.8945     0.8916     70.0337
05900     0.4332      0.8730     0.8916     72.3650
06000     0.5292      0.8359     0.8916     69.5907
06100     0.4184      0.8848     0.8916     70.3800
06200     0.3916      0.8926     0.8916     69.3099
06300     0.4065      0.8867     0.8916     68.2427
06400     0.4051      0.8828     0.8916     71.6406
06500     0.3885      0.8965     0.8916     68.8103
06600     0.4188      0.8730     0.8916     71.1970
06700     0.4502      0.8691     0.8916     69.0894
06800     0.4379      0.8750     0.8916     69.7101
06900     0.4072      0.8809     0.8916     71.0558
07000     0.3607      0.8926     0.8916     71.4912
07100     0.4370      0.8828     0.8916     69.7552
07200     0.4752      0.8613     0.8916     69.1024
07300     0.4291      0.8711     0.8916     70.2238
07400     0.5081      0.8516     0.8916     70.6887
07500     0.3675      0.8965     0.8916     69.6285
07600     0.4445      0.8633     0.8916     69.9138
07700     0.3994      0.8789     0.8916     69.4591
07800     0.4879      0.8477     0.8916     70.8659
07900     0.4282      0.8730     0.8916     67.8880
08000     0.4072      0.8809     0.8916     69.6950
08100     0.3693      0.8945     0.8916     68.6704
08200     0.4082      0.8867     0.8938     70.0111
08300     0.4484      0.8730     0.8938     69.0997
08400     0.4361      0.8789     0.8938     68.1967
08500     0.4277      0.8691     0.8938     69.9651
08600     0.4305      0.8730     0.8938     69.2441
08700     0.4574      0.8750     0.8938     68.9232
08800     0.3654      0.8965     0.8938     70.3222
08900     0.3825      0.8887     0.8938     70.0793
09000     0.4445      0.8574     0.8938     72.1816
09100     0.4229      0.8652     0.8938     69.6323
09200     0.4203      0.8750     0.8945     69.4399
09300     0.3918      0.8848     0.8945     68.7546
09400     0.4301      0.8770     0.8945     69.4815
09500     0.4482      0.8770     0.8945     69.2469
09600     0.3897      0.8770     0.8945     70.3110
09700     0.4071      0.8711     0.8945     72.5355
09800     0.3309      0.9062     0.8945     71.6025
09900     0.3604      0.8984     0.8945     70.8950
Start testing:
Test Accuracy: 0.8863
