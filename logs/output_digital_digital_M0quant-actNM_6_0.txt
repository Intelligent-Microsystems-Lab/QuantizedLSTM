Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=6, quant_actNM=6, quant_inp=6, quant_w=6, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
3c6b6ac8-d8f6-4f0d-890b-e6589fc23113
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, 1.)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 82, in forward
    if len(x_range) > 1:
TypeError: object of type 'float' has no len()
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=6, quant_actNM=6, quant_inp=6, quant_w=6, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
64f5ed4a-87b8-476e-9b08-2744bf71e887
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, torch.tensor([1.]))
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 94, in forward
    x_scaled = x/x_range
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=6, quant_actNM=6, quant_inp=6, quant_w=6, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
7ec23741-17b1-445b-a6d1-0868bcb449e9
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, torch.tensor([1.]).device(input.device))
TypeError: 'torch.device' object is not callable
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=6, quant_actNM=6, quant_inp=6, quant_w=6, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
5a754d3b-1760-4cae-b7fd-e0cd959b4a2d
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.8311      0.0675     0.0955     9.7559
00100     2.3949      0.1667     0.2462     71.3893
00200     2.2711      0.2152     0.2845     71.8497
00300     2.0007      0.3418     0.3670     71.7006
00400     1.7648      0.4114     0.4668     71.9276
00500     1.5699      0.5021     0.5456     71.6583
00600     1.3249      0.5717     0.6015     71.6021
00700     1.2673      0.6245     0.6085     71.1544
00800     1.2523      0.6224     0.6385     72.0570
00900     1.2203      0.6097     0.6539     71.8599
01000     1.2273      0.6013     0.6747     71.8159
01100     1.1329      0.6350     0.6790     72.1812
01200     1.0837      0.6582     0.6987     71.4033
01300     0.9899      0.7215     0.7144     71.7131
01400     1.1232      0.6350     0.7144     70.6697
01500     1.0445      0.6920     0.7167     71.5753
01600     0.9776      0.6983     0.7334     70.7387
01700     0.9029      0.7321     0.7365     70.7265
01800     0.9363      0.7173     0.7390     70.3194
01900     0.9579      0.7089     0.7447     71.4627
02000     0.9243      0.7426     0.7531     70.2309
02100     0.9042      0.7300     0.7531     72.1748
02200     0.9700      0.7110     0.7531     70.5221
02300     0.9339      0.7025     0.7613     70.9908
02400     0.8300      0.7447     0.7629     72.0462
02500     0.9853      0.7025     0.7629     70.9228
02600     0.9945      0.7046     0.7629     71.4187
02700     0.9276      0.7236     0.7629     72.1993
02800     0.8958      0.7384     0.7652     72.1980
02900     1.0580      0.6751     0.7724     71.4778
03000     0.9207      0.7405     0.7724     72.9920
03100     0.9606      0.7278     0.7791     73.7957
03200     0.9299      0.7173     0.7791     72.6750
03300     0.8305      0.7426     0.7791     71.7991
03400     0.8982      0.7278     0.7791     72.0253
03500     0.8700      0.7447     0.7791     71.5234
03600     0.8070      0.7679     0.7808     71.9173
03700     0.8986      0.7426     0.7808     71.9094
03800     0.8413      0.7511     0.7811     71.6464
03900     0.9448      0.7131     0.7811     72.0749
04000     0.7741      0.7679     0.7829     72.4454
04100     0.8101      0.7658     0.7829     71.0900
04200     0.8195      0.7595     0.7829     71.4522
04300     0.8178      0.7679     0.7829     71.9195
04400     0.8660      0.7426     0.7843     71.2990
04500     0.7486      0.7637     0.7843     72.8610
04600     0.8815      0.7342     0.7843     72.0476
04700     0.7613      0.7932     0.7843     71.1243
04800     0.8088      0.7722     0.7843     72.3379
04900     0.8037      0.7489     0.7888     71.2098
05000     0.8341      0.7321     0.7888     71.4938
05100     0.8790      0.7278     0.7888     71.8980
05200     0.8538      0.7553     0.7888     72.0383
05300     0.8852      0.7384     0.7890     71.6709
05400     0.8929      0.7257     0.7890     72.6251
05500     0.8200      0.7511     0.7890     72.3908
05600     0.8292      0.7679     0.7890     72.2285
05700     0.8995      0.7173     0.7890     71.3670
05800     0.8415      0.7447     0.7890     72.3219
05900     0.8192      0.7468     0.7890     72.9318
06000     0.9528      0.7068     0.7890     72.6419
06100     0.7653      0.7637     0.7890     72.2488
06200     0.8735      0.7511     0.7890     72.7749
06300     0.7808      0.7743     0.7890     71.6883
06400     0.7726      0.7700     0.7890     72.3133
06500     0.8347      0.7637     0.7890     71.4891
06600     0.8903      0.7489     0.7934     71.8935
06700     0.8561      0.7405     0.7944     72.2042
06800     0.7963      0.7785     0.7944     72.2148
06900     0.7815      0.7658     0.7944     71.4930
07000     0.8597      0.7489     0.7944     72.0445
07100     0.8705      0.7321     0.7944     71.8180
07200     0.8487      0.7595     0.7944     72.7831
07300     0.8560      0.7489     0.7944     72.0916
07400     0.8224      0.7637     0.7944     71.9832
07500     0.7491      0.7785     0.7944     72.0871
07600     0.8525      0.7300     0.7944     72.0226
07700     0.7412      0.7785     0.7944     71.6144
07800     0.8217      0.7616     0.7944     72.0749
07900     0.8422      0.7637     0.7944     72.4759
08000     0.8858      0.7363     0.7949     71.9096
08100     0.8680      0.7384     0.7949     72.3983
08200     0.7898      0.7700     0.7949     72.5144
08300     0.7856      0.7722     0.7949     72.6757
08400     0.9030      0.7257     0.7949     72.3752
08500     0.8019      0.7764     0.7949     71.5388
08600     0.7896      0.7743     0.7949     72.6001
08700     0.8184      0.7300     0.7949     72.2293
08800     0.8404      0.7616     0.7949     72.0557
08900     0.8021      0.7574     0.7977     73.4366
09000     0.7340      0.7574     0.7977     72.5719
09100     0.7618      0.7848     0.7977     74.8944
09200     0.8211      0.7637     0.7977     72.9063
09300     0.7403      0.7954     0.7977     73.6507
09400     0.8173      0.7574     0.7977     73.0993
09500     0.9548      0.7046     0.7977     73.8262
09600     0.8513      0.7553     0.7977     73.1761
09700     0.8027      0.7637     0.7977     72.3539
09800     0.8609      0.7468     0.7977     71.7510
09900     0.7758      0.7890     0.7977     72.9085
10000     0.7120      0.7890     0.7977     71.9325
10100     0.8822      0.7511     0.7984     72.5885
10200     0.7337      0.7932     0.8002     72.3504
10300     0.7798      0.7658     0.8002     72.3950
10400     0.7830      0.7658     0.8023     72.1598
10500     0.7965      0.7658     0.8088     72.4626
10600     0.7344      0.7658     0.8088     71.9447
10700     0.7844      0.7827     0.8088     72.9781
10800     0.8007      0.7743     0.8088     72.0647
10900     0.8009      0.7532     0.8088     72.3604
11000     0.7200      0.7848     0.8088     72.5933
11100     0.7487      0.7722     0.8088     72.2538
11200     0.6088      0.8270     0.8088     72.6754
11300     0.7920      0.7616     0.8090     73.0406
11400     0.8015      0.7553     0.8090     72.7182
11500     0.7548      0.7975     0.8090     72.6326
11600     0.7933      0.7616     0.8090     72.3843
11700     0.8023      0.7722     0.8090     72.4504
11800     0.8661      0.7152     0.8090     73.0415
11900     0.7738      0.7764     0.8090     72.6468
12000     0.7609      0.7679     0.8090     73.1380
12100     0.7442      0.7679     0.8090     74.4736
12200     0.7399      0.7869     0.8090     72.5307
12300     0.6884      0.8038     0.8092     72.9434
12400     0.7636      0.7743     0.8092     72.3450
12500     0.7638      0.7700     0.8092     71.6617
12600     0.7944      0.7700     0.8092     71.9220
12700     0.7855      0.7764     0.8092     72.3456
12800     0.7242      0.7911     0.8092     73.2686
12900     0.7606      0.7722     0.8110     72.5934
13000     0.7076      0.7911     0.8110     72.1711
13100     0.8077      0.7743     0.8110     73.4771
13200     0.7925      0.7743     0.8139     72.7277
13300     0.7817      0.7637     0.8139     73.8452
13400     0.8257      0.7532     0.8139     72.7379
13500     0.8004      0.7637     0.8139     74.0506
13600     0.7644      0.7722     0.8139     74.1522
13700     0.7879      0.7764     0.8139     73.0683
13800     0.7484      0.7595     0.8139     72.3398
13900     0.7291      0.7764     0.8139     72.8735
14000     0.7747      0.7954     0.8139     73.3043
14100     0.8334      0.7363     0.8139     72.4833
14200     0.6916      0.8017     0.8139     72.6362
14300     0.7924      0.7511     0.8139     72.7860
14400     0.7476      0.7679     0.8139     72.7996
14500     0.8131      0.7489     0.8139     73.3062
14600     0.8022      0.7700     0.8145     73.6186
14700     0.8366      0.7489     0.8145     72.5859
14800     0.8002      0.7679     0.8145     72.5493
14900     0.8181      0.7532     0.8145     73.5536
15000     0.7658      0.7722     0.8145     72.7965
15100     0.7992      0.7658     0.8145     72.5750
15200     0.7769      0.7553     0.8145     72.6457
15300     0.7449      0.7827     0.8145     72.8071
15400     0.7592      0.7679     0.8145     73.2470
15500     0.8030      0.7743     0.8145     72.9132
15600     0.7107      0.8017     0.8145     72.6685
15700     0.7659      0.7700     0.8145     72.9281
15800     0.8807      0.7532     0.8145     73.1266
15900     0.7354      0.7785     0.8145     71.8124
16000     0.6908      0.7975     0.8145     73.7412
16100     0.8846      0.7384     0.8145     72.2745
16200     0.7488      0.7848     0.8145     73.0734
16300     0.7586      0.7743     0.8145     73.5004
16400     0.6459      0.8059     0.8145     72.4672
16500     0.6298      0.8207     0.8145     73.4778
16600     0.7660      0.7700     0.8145     73.4197
16700     0.7477      0.7658     0.8145     72.2791
16800     0.7873      0.7595     0.8145     73.6267
16900     0.7689      0.7764     0.8145     72.9698
17000     0.7760      0.7700     0.8145     72.6187
17100     0.7564      0.7848     0.8145     73.3326
17200     0.7979      0.7785     0.8145     72.6649
17300     0.7876      0.7722     0.8145     73.0938
17400     0.7851      0.7511     0.8145     73.6017
17500     0.8998      0.7300     0.8145     72.8036
17600     0.8047      0.7827     0.8145     72.8480
17700     0.6984      0.8165     0.8145     73.1153
17800     0.7733      0.7722     0.8145     72.4542
17900     0.7419      0.7869     0.8145     73.4286
18000     0.7618      0.7806     0.8145     72.7533
18100     0.8243      0.7574     0.8145     72.3923
18200     0.7590      0.7848     0.8145     72.5867
18300     0.7364      0.7806     0.8145     72.8927
18400     0.7961      0.7764     0.8145     73.1283
18500     0.7005      0.7890     0.8145     73.0960
18600     0.8425      0.7574     0.8145     72.1564
18700     0.7791      0.7700     0.8145     74.5259
18800     0.7238      0.7890     0.8145     72.8712
18900     0.7345      0.7616     0.8145     73.5001
19000     0.6724      0.8165     0.8145     72.5891
19100     0.6854      0.7911     0.8145     73.2830
19200     0.7651      0.7848     0.8145     72.3140
19300     0.7864      0.7616     0.8145     73.1842
19400     0.7359      0.7785     0.8145     74.4562
19500     0.7225      0.8017     0.8145     73.1751
19600     0.7887      0.7553     0.8145     72.2538
19700     0.8076      0.7447     0.8145     74.6436
19800     0.8388      0.7342     0.8145     72.4514
19900     0.8426      0.7532     0.8145     72.8129
20000     0.7379      0.7785     0.8145     72.4127
20100     0.7253      0.7975     0.8145     72.3342
20199     0.7866      0.7743     0.8145     71.2086
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     0.7354      0.8059     0.8112     9.6543
00100     0.5649      0.8376     0.8112     71.7789
00200     0.6612      0.7996     0.8137     71.5050
00300     0.7146      0.7996     0.8137     71.6396
00400     0.5832      0.8376     0.8138     71.2260
00500     0.6226      0.8165     0.8176     71.4184
00600     0.7201      0.8059     0.8176     72.3319
00700     0.6250      0.8080     0.8176     72.0388
00800     0.6257      0.8143     0.8176     71.3893
00900     0.7567      0.7700     0.8176     71.9662
01000     0.6387      0.8333     0.8176     73.2112
01100     0.7298      0.7975     0.8176     71.9630
01200     0.7902      0.7658     0.8176     71.1037
01300     0.6886      0.7975     0.8176     70.6080
01400     0.6996      0.7911     0.8176     71.6611
01500     0.6630      0.8080     0.8176     70.4979
01600     0.6960      0.7932     0.8176     70.8316
01700     0.6466      0.8122     0.8176     71.3714
01800     0.6305      0.8418     0.8176     71.0147
01900     0.6300      0.8165     0.8176     71.7550
02000     0.7278      0.7890     0.8176     71.4387
02100     0.7024      0.8080     0.8176     71.4701
02200     0.6251      0.8207     0.8176     71.4910
02300     0.6628      0.8122     0.8176     71.3050
02400     0.6259      0.8122     0.8176     71.4652
02500     0.6778      0.7869     0.8176     72.9936
02600     0.7758      0.7700     0.8176     70.7419
02700     0.6653      0.8059     0.8176     71.8811
02800     0.7100      0.7848     0.8176     71.7337
02900     0.7340      0.7806     0.8176     71.6541
03000     0.6811      0.8059     0.8176     72.0934
03100     0.6815      0.7954     0.8176     72.1060
03200     0.6532      0.8101     0.8176     72.2114
03300     0.6577      0.7827     0.8176     72.3676
03400     0.6760      0.8017     0.8176     72.2269
03500     0.7723      0.7637     0.8176     72.6575
03600     0.6341      0.8186     0.8176     72.2856
03700     0.7624      0.7616     0.8176     73.2641
03800     0.6551      0.8059     0.8176     71.6760
03900     0.6421      0.8017     0.8176     71.4397
04000     0.6532      0.8059     0.8176     72.6617
04100     0.6634      0.8143     0.8176     72.5021
04200     0.6596      0.8038     0.8176     72.7202
04300     0.6089      0.8270     0.8176     73.3204
04400     0.6407      0.8122     0.8176     72.3998
04500     0.6636      0.7975     0.8176     72.9063
04600     0.7091      0.7722     0.8176     74.3964
04700     0.6981      0.7848     0.8176     72.8283
04800     0.6208      0.8333     0.8176     72.3106
04900     0.6962      0.7932     0.8176     72.3307
05000     0.6890      0.8038     0.8176     72.1404
05100     0.6476      0.8228     0.8176     72.2264
05200     0.6998      0.7911     0.8176     71.8065
05300     0.6204      0.8228     0.8176     74.0334
05400     0.6928      0.8080     0.8176     78.9354
05500     0.6442      0.8038     0.8176     76.9995
05600     0.6127      0.8376     0.8176     76.7343
05700     0.6609      0.8122     0.8176     75.9370
05800     0.6447      0.8143     0.8176     76.8861
05900     0.6705      0.7975     0.8176     76.6076
06000     0.6873      0.7911     0.8176     76.7662
06100     0.7055      0.7785     0.8176     76.2841
06200     0.5987      0.8333     0.8176     74.7949
06300     0.7077      0.8017     0.8205     75.3953
06400     0.6885      0.7869     0.8205     75.3365
06500     0.6872      0.7996     0.8205     75.8083
06600     0.6726      0.8101     0.8205     75.3755
06700     0.6850      0.8249     0.8205     78.6463
06800     0.6917      0.7975     0.8205     76.4241
06900     0.5576      0.8249     0.8205     77.2802
07000     0.5821      0.8291     0.8205     77.6070
07100     0.6454      0.8059     0.8205     77.9335
07200     0.6758      0.8017     0.8205     75.5454
07300     0.7635      0.7679     0.8205     76.4853
07400     0.7495      0.7764     0.8205     75.9615
07500     0.6078      0.8270     0.8205     77.5856
07600     0.6726      0.8059     0.8205     77.6288
07700     0.6793      0.8080     0.8205     77.7578
07800     0.6700      0.7911     0.8205     77.3779
07900     0.7149      0.7743     0.8205     78.1957
08000     0.6444      0.8080     0.8205     76.7140
08100     0.6300      0.8038     0.8205     76.7589
08200     0.6527      0.8143     0.8205     77.3519
08300     0.5831      0.8228     0.8205     78.5531
08400     0.6356      0.7954     0.8205     78.0693
08500     0.6188      0.8312     0.8205     75.9102
08600     0.7373      0.7700     0.8205     77.5808
08700     0.6429      0.8207     0.8205     76.4113
08800     0.6788      0.8017     0.8205     76.1211
08900     0.6454      0.8165     0.8205     76.4338
09000     0.6042      0.8186     0.8205     76.4060
09100     0.6206      0.8228     0.8205     78.2082
09200     0.6572      0.8059     0.8205     77.2753
09300     0.6984      0.8059     0.8205     78.0174
09400     0.6563      0.8101     0.8208     76.5787
09500     0.6672      0.8038     0.8208     76.6790
09600     0.6460      0.8186     0.8208     77.4399
09700     0.7342      0.7764     0.8208     78.3221
09800     0.6574      0.8165     0.8208     79.2571
09900     0.6918      0.7954     0.8208     79.0716
Start testing:
Test Accuracy: 0.8005
