Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=235899598, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=235899598, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 362, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.0, finetuning_epochs=0, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.0005,0.0001,0.00002', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=235899598, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,10000', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
03b35595-9a65-4ea5-bfc1-742742f1b256
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.5350      0.0633     0.0807     11.8354
00100     2.5219      0.0612     0.0811     56.9530
00200     2.5139      0.0696     0.0811     56.9571
00300     2.5096      0.0759     0.0811     57.8601
00400     2.5072      0.0886     0.0811     56.9966
00500     2.5057      0.0970     0.0811     56.9051
00600     2.5047      0.1076     0.0811     57.3418
00700     2.5039      0.0781     0.0811     57.0110
00800     2.5034      0.0844     0.0811     57.6161
00900     2.5029      0.0802     0.0811     57.6906
01000     2.5025      0.0907     0.0811     57.4660
01100     2.5022      0.0992     0.0811     56.7543
01200     2.5020      0.0654     0.0811     56.2182
01300     2.5017      0.1013     0.0811     56.1900
01400     2.5015      0.1013     0.0812     57.2594
01500     2.5014      0.1076     0.0812     57.0954
01600     2.5012      0.0506     0.0812     58.3126
01700     2.5011      0.0464     0.0812     56.7714
01800     2.5010      0.1055     0.0812     57.2501
01900     2.5008      0.0823     0.0812     56.8084
02000     2.5007      0.0823     0.0812     57.3039
02100     2.5007      0.0844     0.0812     56.1667
02200     2.5006      0.0907     0.0812     57.4136
02300     2.5005      0.0717     0.0812     57.1024
02400     2.5004      0.0717     0.0814     58.1125
02500     2.5004      0.0802     0.0814     56.7273
02600     2.5003      0.0633     0.0814     57.0889
02700     2.5003      0.0738     0.0814     57.9168
02800     2.5002      0.0970     0.0814     56.7578
02900     2.5002      0.0844     0.0814     57.1983
03000     2.5001      0.0717     0.0814     57.3171
03100     2.5001      0.0865     0.0814     56.4592
03200     2.5000      0.0823     0.0814     57.9061
03300     2.5000      0.0823     0.0814     56.6293
03400     2.5000      0.0759     0.0814     56.6827
03500     2.4999      0.0907     0.0814     56.9259
03600     2.4999      0.0865     0.0814     57.3563
03700     2.4999      0.0717     0.0814     57.2104
03800     2.4999      0.0970     0.0814     57.3756
03900     2.4998      0.0506     0.0814     56.8307
04000     2.4998      0.0802     0.0814     56.9890
04100     2.4998      0.0738     0.0814     57.1569
04200     2.4998      0.0992     0.0814     58.2188
04300     2.4997      0.0823     0.0814     57.8402
04400     2.4997      0.0823     0.0814     57.1614
04500     2.4997      0.0464     0.0814     57.0849
04600     2.4997      0.0781     0.0814     57.2304
04700     2.4997      0.0928     0.0814     56.8506
04800     2.4996      0.0886     0.0814     57.5192
04900     2.4996      0.0612     0.0814     56.6411
05000     2.4996      0.0844     0.0814     56.9132
05100     2.4996      0.0823     0.0814     57.0254
05200     2.4996      0.1055     0.0814     56.6201
05300     2.4995      0.0717     0.0814     57.0018
05400     2.4995      0.0717     0.0814     57.5796
05500     2.4995      0.0738     0.0814     57.0190
05600     2.4995      0.0738     0.0814     57.1376
05700     2.4995      0.0654     0.0814     56.8198
05800     2.4994      0.0759     0.0814     56.4947
05900     2.4994      0.0759     0.0814     57.4543
06000     2.4994      0.0633     0.0814     56.9630
06100     2.4994      0.0865     0.0814     56.8096
06200     2.4994      0.0717     0.0814     58.0969
06300     2.4993      0.0928     0.0814     56.8804
06400     2.4993      0.0738     0.0814     57.5718
06500     2.4993      0.0675     0.0814     56.8450
06600     2.4993      0.0675     0.0814     56.4399
06700     2.4993      0.0907     0.0814     57.4510
06800     2.4992      0.0759     0.0814     56.2093
06900     2.4992      0.0844     0.0814     56.8895
07000     2.4992      0.0886     0.0814     57.1630
07100     2.4992      0.0781     0.0814     56.8190
07200     2.4992      0.0886     0.0814     57.7052
07300     2.4992      0.0781     0.0814     56.7490
07400     2.4991      0.0781     0.0814     57.0657
07500     2.4991      0.0422     0.0814     57.7364
07600     2.4991      0.0865     0.0814     56.8492
07700     2.4991      0.0949     0.0814     57.1282
07800     2.4991      0.0949     0.0814     57.2423
07900     2.4990      0.0675     0.0814     57.5216
08000     2.4990      0.0781     0.0814     57.5584
08100     2.4990      0.0675     0.0814     57.0602
08200     2.4990      0.0654     0.0814     57.5563
08300     2.4990      0.0949     0.0814     57.6986
08400     2.4989      0.0633     0.0814     57.4279
08500     2.4989      0.0886     0.0814     56.5655
08600     2.4989      0.0738     0.0814     57.3487
08700     2.4989      0.0696     0.0814     56.7503
08800     2.4989      0.0844     0.0814     57.4738
08900     2.4989      0.0865     0.0814     56.4712
09000     2.4988      0.0738     0.0814     56.8751
09100     2.4988      0.0823     0.0814     57.3274
09200     2.4988      0.0738     0.0814     57.0929
09300     2.4988      0.0781     0.0814     57.3414
09400     2.4988      0.0802     0.0814     57.4828
09500     2.4987      0.0654     0.0814     57.1820
09600     2.4987      0.0802     0.0814     58.0843
09700     2.4987      0.0823     0.0814     57.6524
09800     2.4987      0.0865     0.0814     57.0133
09900     2.4987      0.0844     0.0814     57.3543
10000     2.4986      0.0886     0.0814     56.6974
10100     2.4986      0.0717     0.0814     56.7423
10200     2.4986      0.0992     0.0814     58.0575
10300     2.4986      0.0759     0.0814     57.1815
10400     2.4986      0.0886     0.0814     58.0135
10500     2.4986      0.0570     0.0814     57.0133
10600     2.4986      0.0675     0.0814     56.6005
10700     2.4986      0.0738     0.0814     57.2944
10800     2.4986      0.0844     0.0814     56.8586
10900     2.4986      0.0970     0.0814     56.9110
11000     2.4986      0.0759     0.0814     57.9187
11100     2.4986      0.0696     0.0814     57.3393
11200     2.4986      0.0970     0.0814     58.5496
11300     2.4986      0.0907     0.0814     57.3798
11400     2.4986      0.0823     0.0814     57.1511
11500     2.4986      0.0802     0.0814     57.6161
11600     2.4986      0.0970     0.0814     57.9578
11700     2.4986      0.0823     0.0814     57.2264
11800     2.4986      0.0738     0.0814     57.3131
11900     2.4986      0.0802     0.0814     56.5779
12000     2.4986      0.0886     0.0814     57.4272
12100     2.4986      0.1118     0.0814     57.4385
12200     2.4986      0.0654     0.0814     57.3274
12300     2.4986      0.0823     0.0814     57.5048
12400     2.4986      0.1118     0.0814     57.0012
12500     2.4986      0.0759     0.0814     57.3060
12600     2.4986      0.0781     0.0814     57.4760
12700     2.4985      0.0802     0.0814     57.5160
12800     2.4985      0.0865     0.0814     57.4528
12900     2.4985      0.0570     0.0814     57.2700
13000     2.4985      0.0759     0.0814     57.0221
13100     2.4985      0.0844     0.0814     57.8882
13200     2.4985      0.0549     0.0814     56.9561
13300     2.4985      0.0865     0.0814     56.6501
13400     2.4985      0.0781     0.0814     57.7972
13500     2.4985      0.0696     0.0814     57.4619
13600     2.4985      0.0823     0.0814     58.0645
13700     2.4985      0.0717     0.0814     57.3351
13800     2.4985      0.0675     0.0814     57.0177
13900     2.4985      0.0612     0.0814     57.7080
14000     2.4985      0.1076     0.0814     56.9723
14100     2.4985      0.0865     0.0814     56.9727
14200     2.4985      0.0633     0.0814     57.3200
14300     2.4985      0.0865     0.0814     57.7063
14400     2.4985      0.0865     0.0814     57.7075
14500     2.4985      0.0549     0.0814     57.1188
14600     2.4985      0.0907     0.0814     57.0661
14700     2.4985      0.0928     0.0814     57.7640
14800     2.4985      0.0759     0.0814     56.9293
14900     2.4985      0.0970     0.0814     57.3768
15000     2.4985      0.0759     0.0814     58.0333
15100     2.4985      0.1160     0.0814     58.0193
15200     2.4985      0.0865     0.0814     57.6389
15300     2.4985      0.0928     0.0814     57.1404
15400     2.4985      0.0549     0.0814     56.5426
15500     2.4984      0.0781     0.0814     57.2229
15600     2.4984      0.0823     0.0814     56.4365
15700     2.4984      0.0570     0.0814     56.9931
15800     2.4984      0.0823     0.0814     57.3832
15900     2.4984      0.0844     0.0814     56.8669
16000     2.4984      0.0759     0.0814     57.6532
16100     2.4984      0.0654     0.0814     56.7010
16200     2.4984      0.0633     0.0814     57.1910
16300     2.4984      0.0844     0.0814     57.8511
16400     2.4984      0.0823     0.0814     57.4127
16500     2.4984      0.0717     0.0814     57.0191
16600     2.4984      0.0928     0.0814     57.8575
16700     2.4984      0.0696     0.0814     57.3591
16800     2.4984      0.0865     0.0814     57.4591
16900     2.4984      0.0844     0.0814     57.3374
17000     2.4984      0.0612     0.0814     57.3772
17100     2.4984      0.0907     0.0814     58.2857
17200     2.4984      0.0675     0.0814     56.8803
17300     2.4984      0.0886     0.0814     57.0718
17400     2.4984      0.0591     0.0814     58.3123
17500     2.4984      0.0759     0.0814     57.0127
17600     2.4984      0.0907     0.0814     58.1647
17700     2.4984      0.0759     0.0814     56.7572
17800     2.4984      0.0527     0.0814     57.7640
17900     2.4984      0.0865     0.0814     57.7638
18000     2.4984      0.0696     0.0814     57.2170
18100     2.4984      0.0844     0.0814     57.4164
18200     2.4984      0.0612     0.0814     57.8223
18300     2.4983      0.0949     0.0814     57.4514
18400     2.4983      0.0823     0.0814     58.5941
18500     2.4983      0.0886     0.0814     57.6172
18600     2.4983      0.0886     0.0814     57.1521
18700     2.4983      0.0802     0.0814     57.4465
18800     2.4983      0.0675     0.0814     57.3065
18900     2.4983      0.0886     0.0814     57.4534
19000     2.4983      0.0738     0.0814     57.4065
19100     2.4983      0.0738     0.0814     58.0556
19200     2.4983      0.0781     0.0814     58.6774
19300     2.4983      0.0865     0.0814     57.3698
19400     2.4983      0.0949     0.0814     57.0784
19500     2.4983      0.0886     0.0814     57.7784
19600     2.4983      0.0696     0.0814     58.0719
19700     2.4983      0.0823     0.0814     57.3611
19800     2.4983      0.0992     0.0814     57.7419
19900     2.4983      0.0696     0.0814     56.6751
20000     2.4983      0.0633     0.0814     57.6745
20100     2.4983      0.0717     0.0814     57.6852
20200     2.4983      0.1224     0.0814     57.3553
20300     2.4983      0.0570     0.0814     58.5563
20400     2.4983      0.0886     0.0814     59.5378
20500     2.4983      0.0949     0.0814     58.2044
20600     2.4983      0.0928     0.0814     57.8036
20700     2.4983      0.0928     0.0814     57.5759
20800     2.4983      0.0591     0.0814     57.8455
20900     2.4983      0.0781     0.0814     57.2782
21000     2.4983      0.0675     0.0814     57.4547
21100     2.4983      0.0591     0.0814     57.7516
21200     2.4983      0.0675     0.0814     57.5681
21300     2.4983      0.0823     0.0814     57.3646
21400     2.4983      0.0907     0.0814     57.5910
21500     2.4983      0.0949     0.0814     56.5428
21600     2.4983      0.0738     0.0814     58.2706
21700     2.4983      0.1076     0.0814     56.7753
21800     2.4983      0.0970     0.0814     57.0353
21900     2.4983      0.0675     0.0814     58.1016
22000     2.4983      0.0844     0.0814     56.8180
22100     2.4983      0.0675     0.0814     56.9683
22200     2.4983      0.0844     0.0814     57.5977
22300     2.4983      0.0844     0.0814     57.1994
22400     2.4983      0.0823     0.0814     57.8456
22500     2.4983      0.0802     0.0814     57.1232
22600     2.4983      0.0970     0.0814     56.8013
22700     2.4983      0.0696     0.0814     57.5199
22800     2.4983      0.0675     0.0814     56.8236
22900     2.4983      0.0865     0.0814     57.6983
23000     2.4983      0.0802     0.0814     58.1683
23100     2.4983      0.0506     0.0814     57.1800
23200     2.4983      0.0886     0.0814     57.3855
23300     2.4983      0.0717     0.0814     57.3785
23400     2.4983      0.0865     0.0814     57.5392
23500     2.4983      0.0823     0.0814     57.8398
23600     2.4983      0.0823     0.0814     57.6351
23700     2.4983      0.0738     0.0814     57.1333
23800     2.4983      0.0844     0.0814     58.1614
23900     2.4983      0.0823     0.0814     56.9413
24000     2.4983      0.1013     0.0814     57.3984
24100     2.4983      0.0844     0.0814     57.2165
24200     2.4983      0.1013     0.0814     57.1073
24300     2.4983      0.1055     0.0814     57.6930
24400     2.4983      0.0949     0.0814     57.3979
24500     2.4982      0.0675     0.0814     57.0030
24600     2.4982      0.0759     0.0814     57.1488
24700     2.4982      0.0781     0.0814     56.9842
24800     2.4982      0.0865     0.0814     57.9561
24900     2.4982      0.0844     0.0814     57.4554
25000     2.4982      0.0675     0.0814     56.7470
25100     2.4982      0.0781     0.0814     57.6290
25200     2.4982      0.0717     0.0814     57.6093
25300     2.4982      0.0886     0.0814     57.0970
25400     2.4982      0.0591     0.0814     57.7819
25500     2.4982      0.0865     0.0814     57.3159
25600     2.4982      0.0781     0.0814     57.9699
25700     2.4982      0.0717     0.0814     57.4302
25800     2.4982      0.0992     0.0814     58.2866
25900     2.4982      0.0570     0.0814     60.3471
26000     2.4982      0.0549     0.0814     59.6443
26100     2.4982      0.0907     0.0814     58.2597
26200     2.4982      0.0823     0.0814     58.2835
26300     2.4982      0.0928     0.0814     57.6781
26400     2.4982      0.0675     0.0814     57.7924
26500     2.4982      0.0717     0.0814     57.3985
26600     2.4982      0.0844     0.0814     57.1209
26700     2.4982      0.0781     0.0814     57.4529
26800     2.4982      0.0949     0.0814     57.1942
26900     2.4982      0.0781     0.0814     56.9792
27000     2.4982      0.0823     0.0814     58.1849
27100     2.4982      0.0759     0.0814     56.8885
27200     2.4982      0.0928     0.0814     57.9228
27300     2.4982      0.0781     0.0814     57.1738
27400     2.4982      0.0633     0.0814     55.7067
27500     2.4982      0.0781     0.0814     57.6928
27600     2.4982      0.1055     0.0814     57.7875
27700     2.4982      0.0949     0.0814     57.2006
27800     2.4982      0.0654     0.0814     58.1536
27900     2.4982      0.0696     0.0814     57.5519
28000     2.4982      0.0907     0.0814     57.7658
28100     2.4982      0.0865     0.0814     57.1187
28200     2.4982      0.0717     0.0814     57.6453
28300     2.4982      0.0844     0.0814     59.2491
28400     2.4982      0.0802     0.0814     58.1121
28500     2.4982      0.0675     0.0814     58.1294
28600     2.4982      0.0928     0.0814     57.9090
28700     2.4982      0.0675     0.0814     57.4199
28800     2.4982      0.0781     0.0814     57.9606
28900     2.4982      0.0844     0.0814     57.3683
29000     2.4982      0.1076     0.0814     57.4208
29100     2.4982      0.0738     0.0814     58.2558
29200     2.4982      0.0759     0.0814     58.0525
29300     2.4982      0.0591     0.0814     57.0993
29400     2.4982      0.0844     0.0814     58.0650
29500     2.4982      0.0823     0.0814     57.3894
29600     2.4982      0.0844     0.0814     58.2653
29700     2.4982      0.0992     0.0814     58.5576
29800     2.4982      0.0781     0.0814     59.2904
29900     2.4982      0.0886     0.0814     58.6189
29999     2.4982      0.0865     0.0814     57.2039
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Start testing:
Test Accuracy: 0.0789
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=235899598, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
814ae636-e717-4dcf-b9df-c9a5d5622ab7
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.5350      0.0633     0.0806     12.6903
00100     2.5068      0.0928     0.0807     59.4231
00200     2.5033      0.0802     0.0808     57.5849
00300     2.5020      0.0781     0.0808     59.0415
00400     2.5013      0.0928     0.0808     58.1390
00500     2.5008      0.0675     0.0808     59.2663
00600     2.5005      0.0717     0.0808     58.4783
00700     2.5003      0.0570     0.0808     57.0239
00800     2.5001      0.0844     0.0808     58.5276
00900     2.5000      0.0802     0.0808     57.7988
01000     2.4999      0.0949     0.0808     57.9771
01100     2.4998      0.0738     0.0810     59.0524
01200     2.4997      0.0865     0.0810     59.7762
01300     2.4996      0.0696     0.0810     57.8492
01400     2.4995      0.0823     0.0810     57.6540
01500     2.4994      0.0717     0.0810     58.5288
01600     2.4993      0.0865     0.0810     59.3278
01700     2.4993      0.1034     0.0810     57.3476
01800     2.4992      0.0865     0.0810     59.2132
01900     2.4991      0.0506     0.0810     59.3133
02000     2.4990      0.0696     0.0810     57.6796
02100     2.4990      0.0738     0.0810     59.7266
02200     2.4989      0.0907     0.0812     57.8842
02300     2.4988      0.1160     0.0812     58.3146
02400     2.4987      0.0844     0.0812     57.6725
02500     2.4987      0.0570     0.0812     58.0747
02600     2.4986      0.0738     0.0812     57.3758
02700     2.4985      0.0591     0.0812     58.4167
02800     2.4984      0.0717     0.0812     57.2843
02900     2.4984      0.0781     0.0814     57.7752
03000     2.4983      0.0717     0.0814     57.7534
03100     2.4982      0.0928     0.0814     57.9015
03200     2.4982      0.0844     0.0814     59.5670
03300     2.4981      0.0717     0.0814     57.7394
03400     2.4980      0.1076     0.0814     59.8974
03500     2.4979      0.0696     0.0814     58.9698
03600     2.4979      0.0654     0.0814     58.8010
03700     2.4978      0.0865     0.0814     57.6139
03800     2.4977      0.0654     0.0814     57.4313
03900     2.4977      0.0844     0.0814     56.8942
04000     2.4976      0.0844     0.0814     58.6203
04100     2.4975      0.0781     0.0814     56.7897
04200     2.4975      0.0759     0.0814     57.1360
04300     2.4974      0.0612     0.0814     57.8744
04400     2.4973      0.0886     0.0814     57.1313
04500     2.4972      0.0633     0.0814     60.5480
04600     2.4972      0.0823     0.0814     57.5753
04700     2.4971      0.0907     0.0814     58.0462
04800     2.4970      0.0802     0.0814     58.9592
04900     2.4970      0.0907     0.0814     58.3564
05000     2.4969      0.0633     0.0814     60.0685
05100     2.4968      0.0844     0.0814     59.1094
05200     2.4968      0.0844     0.0814     59.0010
05300     2.4967      0.0823     0.0814     58.1499
05400     2.4966      0.0717     0.0814     58.4226
05500     2.4966      0.0675     0.0814     57.8916
05600     2.4965      0.0802     0.0814     57.5763
05700     2.4964      0.0802     0.0814     56.8814
05800     2.4964      0.1076     0.0814     56.5925
05900     2.4963      0.0696     0.0814     58.1655
06000     2.4962      0.0696     0.0814     56.9648
06100     2.4962      0.0738     0.0814     58.2205
06200     2.4961      0.0591     0.0814     57.0496
06300     2.4960      0.0696     0.0814     58.2284
06400     2.4960      0.0844     0.0814     59.4087
06500     2.4959      0.0781     0.0814     59.1135
06600     2.4958      0.0591     0.0814     58.5001
06700     2.4958      0.0907     0.0814     57.9890
06800     2.4957      0.0738     0.0814     57.4901
06900     2.4956      0.0886     0.0814     58.3042
07000     2.4956      0.0591     0.0814     58.1169
07100     2.4955      0.0759     0.0814     59.0779
07200     2.4954      0.0781     0.0814     58.5746
07300     2.4954      0.0823     0.0814     57.9287
07400     2.4953      0.0759     0.0814     56.9921
07500     2.4952      0.0549     0.0814     56.8423
07600     2.4952      0.0823     0.0814     57.2415
07700     2.4951      0.0823     0.0814     57.3952
07800     2.4950      0.0865     0.0814     57.5840
07900     2.4950      0.0949     0.0814     57.2073
08000     2.4949      0.0759     0.0814     57.2965
08100     2.4949      0.1055     0.0814     57.6062
08200     2.4948      0.0675     0.0814     58.1149
08300     2.4947      0.0907     0.0814     57.6302
08400     2.4947      0.0781     0.0814     58.2858
08500     2.4946      0.0781     0.0814     57.5081
08600     2.4945      0.0886     0.0814     58.1070
08700     2.4945      0.0738     0.0814     57.1247
08800     2.4944      0.0865     0.0814     57.9349
08900     2.4943      0.0759     0.0814     57.0602
09000     2.4943      0.0612     0.0814     56.8580
09100     2.4942      0.0717     0.0814     58.7360
09200     2.4942      0.0759     0.0814     57.6164
09300     2.4941      0.0759     0.0814     58.1422
09400     2.4940      0.0591     0.0814     58.7707
09500     2.4940      0.0949     0.0814     57.6394
09600     2.4939      0.0928     0.0814     57.2904
09700     2.4938      0.0949     0.0814     57.7850
09800     2.4938      0.0675     0.0814     57.0714
09900     2.4937      0.0886     0.0814     59.3728
10000     2.4936      0.0781     0.0814     58.1376
10100     2.4936      0.0970     0.0814     58.9619
10200     2.4936      0.0886     0.0814     58.8023
10300     2.4936      0.0907     0.0814     57.3793
10400     2.4936      0.0949     0.0814     58.5243
10500     2.4936      0.0675     0.0814     58.3630
10600     2.4936      0.0844     0.0814     57.4350
10700     2.4935      0.1034     0.0814     58.0185
10800     2.4935      0.0781     0.0814     57.6360
10900     2.4935      0.0633     0.0814     57.9523
11000     2.4935      0.0527     0.0814     58.7910
11100     2.4935      0.0759     0.0814     58.1190
11200     2.4935      0.0928     0.0814     58.5694
11300     2.4934      0.0823     0.0814     57.9045
11400     2.4934      0.0949     0.0814     58.7289
11500     2.4934      0.0865     0.0814     58.6324
11600     2.4934      0.0696     0.0814     56.9491
11700     2.4934      0.0823     0.0814     58.0140
11800     2.4934      0.0823     0.0814     59.2040
11900     2.4933      0.0654     0.0814     57.9322
12000     2.4933      0.0844     0.0814     59.9331
12100     2.4933      0.0633     0.0814     58.8932
12200     2.4933      0.0928     0.0814     59.5001
12300     2.4933      0.0696     0.0814     58.9851
12400     2.4933      0.0823     0.0814     59.5172
12500     2.4933      0.0802     0.0814     58.9809
12600     2.4932      0.0781     0.0814     58.6908
12700     2.4932      0.0886     0.0814     58.4758
12800     2.4932      0.0970     0.0814     58.3157
12900     2.4932      0.0802     0.0814     58.6143
13000     2.4932      0.0844     0.0814     57.1894
13100     2.4932      0.0759     0.0814     58.1688
13200     2.4931      0.0570     0.0814     57.8364
13300     2.4931      0.0759     0.0814     58.5005
13400     2.4931      0.0844     0.0814     57.9146
13500     2.4931      0.1013     0.0814     58.5626
13600     2.4931      0.0738     0.0814     57.8554
13700     2.4931      0.0886     0.0814     57.6596
13800     2.4930      0.0844     0.0814     57.8951
13900     2.4930      0.0696     0.0814     57.1939
14000     2.4930      0.1034     0.0814     58.2548
14100     2.4930      0.0992     0.0814     58.8733
14200     2.4930      0.0823     0.0814     57.6063
14300     2.4930      0.0865     0.0814     56.7905
14400     2.4930      0.0823     0.0814     57.9274
14500     2.4929      0.0570     0.0814     57.8375
14600     2.4929      0.0675     0.0814     61.0597
14700     2.4929      0.0570     0.0814     58.9095
14800     2.4929      0.0949     0.0814     60.7087
14900     2.4929      0.0696     0.0814     57.4466
15000     2.4929      0.0823     0.0814     57.2651
15100     2.4928      0.0781     0.0814     58.3887
15200     2.4928      0.0907     0.0814     59.0434
15300     2.4928      0.0949     0.0814     57.6177
15400     2.4928      0.0823     0.0814     58.0491
15500     2.4928      0.1034     0.0814     60.2906
15600     2.4928      0.0886     0.0814     57.8843
15700     2.4928      0.0865     0.0814     58.7819
15800     2.4927      0.0928     0.0814     57.8924
15900     2.4927      0.0928     0.0814     58.8569
16000     2.4927      0.0802     0.0814     58.9710
16100     2.4927      0.0844     0.0814     58.8025
16200     2.4927      0.0823     0.0814     58.4537
16300     2.4927      0.0738     0.0814     58.5126
16400     2.4926      0.0781     0.0814     59.5000
16500     2.4926      0.0759     0.0814     59.2156
16600     2.4926      0.0696     0.0814     59.2037
16700     2.4926      0.0970     0.0814     57.7190
16800     2.4926      0.0781     0.0814     58.9305
16900     2.4926      0.0823     0.0814     57.8437
17000     2.4925      0.1013     0.0814     59.3797
17100     2.4925      0.0781     0.0814     58.2840
17200     2.4925      0.0802     0.0814     59.5523
17300     2.4925      0.0802     0.0814     58.9188
17400     2.4925      0.0844     0.0814     58.2892
17500     2.4925      0.0823     0.0814     58.9068
17600     2.4925      0.0907     0.0814     59.2100
17700     2.4924      0.0759     0.0814     58.9007
17800     2.4924      0.0886     0.0814     58.4737
17900     2.4924      0.0844     0.0814     57.5132
18000     2.4924      0.0886     0.0814     58.6502
18100     2.4924      0.0654     0.0814     58.8950
18200     2.4924      0.0570     0.0816     58.6988
18300     2.4923      0.0907     0.0816     58.7493
18400     2.4923      0.0759     0.0816     59.9910
18500     2.4923      0.0781     0.0816     58.8214
18600     2.4923      0.0781     0.0816     59.6164
18700     2.4923      0.0759     0.0816     58.3945
18800     2.4923      0.0696     0.0816     58.1744
18900     2.4922      0.0717     0.0816     57.5441
19000     2.4922      0.0865     0.0816     58.8898
19100     2.4922      0.1013     0.0816     59.9372
19200     2.4922      0.0717     0.0816     57.8452
19300     2.4922      0.0907     0.0816     58.1881
19400     2.4922      0.0759     0.0816     57.8256
19500     2.4922      0.1013     0.0816     58.8653
19600     2.4921      0.0717     0.0816     57.4882
19700     2.4921      0.0949     0.0816     58.4919
19800     2.4921      0.0738     0.0816     59.1430
19900     2.4921      0.0949     0.0816     58.2151
20000     2.4921      0.0781     0.0816     59.1193
20100     2.4921      0.0949     0.0816     57.5519
20199     2.4921      0.0759     0.0816     57.3916
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.4921      0.0759     0.0802     9.5113
00100     2.4921      0.0865     0.0803     58.4989
00200     2.4921      0.0802     0.0809     57.0469
00300     2.4921      0.0738     0.0809     58.7574
00400     2.4921      0.0886     0.0809     57.6236
00500     2.4921      0.0759     0.0809     56.8824
00600     2.4921      0.0844     0.0812     56.8739
00700     2.4921      0.0823     0.0812     58.2828
00800     2.4921      0.0823     0.0812     56.8941
00900     2.4920      0.0738     0.0812     56.8561
01000     2.4920      0.0654     0.0812     57.2175
01100     2.4920      0.0802     0.0812     56.6023
01200     2.4920      0.0907     0.0812     57.2465
01300     2.4920      0.0781     0.0812     58.0981
01400     2.4920      0.0738     0.0812     58.6702
01500     2.4920      0.0992     0.0812     58.0342
01600     2.4920      0.0654     0.0812     57.9053
01700     2.4920      0.0738     0.0812     57.5192
01800     2.4920      0.0717     0.0812     56.6006
01900     2.4920      0.0781     0.0812     57.6514
02000     2.4920      0.0802     0.0812     57.4234
02100     2.4920      0.0970     0.0812     57.6490
02200     2.4920      0.0696     0.0812     57.4153
02300     2.4920      0.0970     0.0812     56.8643
02400     2.4920      0.0759     0.0812     57.7431
02500     2.4920      0.0844     0.0812     57.7023
02600     2.4920      0.0823     0.0812     57.0311
02700     2.4920      0.0738     0.0812     56.9007
02800     2.4920      0.0992     0.0812     56.0376
02900     2.4920      0.1055     0.0812     56.6410
03000     2.4920      0.0802     0.0812     58.7518
03100     2.4920      0.0717     0.0812     57.7345
03200     2.4920      0.0844     0.0812     56.4392
03300     2.4920      0.0633     0.0812     57.0930
03400     2.4920      0.0654     0.0812     57.1495
03500     2.4920      0.0865     0.0812     57.3397
03600     2.4920      0.0886     0.0812     57.2569
03700     2.4920      0.0759     0.0812     56.2891
03800     2.4920      0.0781     0.0812     57.2807
03900     2.4920      0.0675     0.0812     55.8924
04000     2.4920      0.0844     0.0812     56.6173
04100     2.4920      0.0802     0.0812     58.9070
04200     2.4920      0.0781     0.0812     56.8084
04300     2.4920      0.0802     0.0812     57.9947
04400     2.4920      0.0696     0.0812     56.4194
04500     2.4920      0.0696     0.0812     57.1666
04600     2.4920      0.1076     0.0812     56.7192
04700     2.4920      0.0928     0.0812     57.3618
04800     2.4920      0.0928     0.0812     56.8812
04900     2.4920      0.0570     0.0812     58.1413
05000     2.4920      0.0949     0.0816     58.0430
05100     2.4919      0.0612     0.0816     58.4377
05200     2.4919      0.0759     0.0816     58.9093
05300     2.4919      0.0759     0.0816     59.3735
05400     2.4919      0.0823     0.0816     56.5853
05500     2.4919      0.0886     0.0816     57.1473
05600     2.4919      0.0992     0.0816     58.3595
05700     2.4919      0.0717     0.0816     56.9236
05800     2.4919      0.0949     0.0816     57.5515
05900     2.4919      0.0992     0.0816     57.0957
06000     2.4919      0.0823     0.0816     56.5940
06100     2.4919      0.0992     0.0816     56.8883
06200     2.4919      0.0865     0.0816     56.5404
06300     2.4919      0.0759     0.0816     56.8125
06400     2.4919      0.1097     0.0816     56.7532
06500     2.4919      0.0759     0.0816     58.2459
06600     2.4919      0.0633     0.0816     58.5839
06700     2.4919      0.1013     0.0816     57.9662
06800     2.4919      0.0633     0.0816     58.2980
06900     2.4919      0.0759     0.0816     58.4463
07000     2.4919      0.0696     0.0816     58.5295
07100     2.4919      0.0738     0.0816     56.6303
07200     2.4919      0.0738     0.0816     58.1859
07300     2.4919      0.0802     0.0816     57.7316
07400     2.4919      0.1013     0.0816     59.3602
07500     2.4919      0.0802     0.0816     57.1848
07600     2.4919      0.0717     0.0816     56.4893
07700     2.4919      0.0781     0.0816     56.1950
07800     2.4919      0.0865     0.0816     56.3768
07900     2.4919      0.0907     0.0816     56.0695
08000     2.4919      0.0781     0.0816     56.9538
08100     2.4919      0.0781     0.0816     57.0493
08200     2.4919      0.0717     0.0816     55.4999
08300     2.4919      0.0759     0.0816     58.5084
08400     2.4919      0.0928     0.0816     58.4284
08500     2.4919      0.0675     0.0816     58.2476
08600     2.4919      0.0591     0.0816     56.8742
08700     2.4919      0.0865     0.0816     56.6757
08800     2.4919      0.0738     0.0816     56.7320
08900     2.4919      0.1034     0.0816     56.7515
09000     2.4919      0.0675     0.0816     57.9764
09100     2.4919      0.0970     0.0816     56.5861
09200     2.4919      0.0696     0.0816     56.8171
09300     2.4918      0.0738     0.0816     57.3638
09400     2.4918      0.0696     0.0816     57.3315
09500     2.4918      0.1013     0.0816     57.3731
09600     2.4918      0.0949     0.0816     57.2652
09700     2.4918      0.0612     0.0816     57.0292
09800     2.4918      0.0738     0.0816     55.7897
09900     2.4918      0.0738     0.0816     61.0477
Start testing:
Test Accuracy: 0.0789
