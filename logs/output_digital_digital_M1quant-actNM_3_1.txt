Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=235899598, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=235899598, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 362, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.0, finetuning_epochs=0, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.0005,0.0001,0.00002', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=235899598, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,10000', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
03b35595-9a65-4ea5-bfc1-742742f1b256
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.5350      0.0633     0.0807     11.8354
00100     2.5219      0.0612     0.0811     56.9530
00200     2.5139      0.0696     0.0811     56.9571
00300     2.5096      0.0759     0.0811     57.8601
00400     2.5072      0.0886     0.0811     56.9966
00500     2.5057      0.0970     0.0811     56.9051
00600     2.5047      0.1076     0.0811     57.3418
00700     2.5039      0.0781     0.0811     57.0110
00800     2.5034      0.0844     0.0811     57.6161
00900     2.5029      0.0802     0.0811     57.6906
01000     2.5025      0.0907     0.0811     57.4660
01100     2.5022      0.0992     0.0811     56.7543
01200     2.5020      0.0654     0.0811     56.2182
01300     2.5017      0.1013     0.0811     56.1900
01400     2.5015      0.1013     0.0812     57.2594
01500     2.5014      0.1076     0.0812     57.0954
01600     2.5012      0.0506     0.0812     58.3126
01700     2.5011      0.0464     0.0812     56.7714
01800     2.5010      0.1055     0.0812     57.2501
01900     2.5008      0.0823     0.0812     56.8084
02000     2.5007      0.0823     0.0812     57.3039
02100     2.5007      0.0844     0.0812     56.1667
02200     2.5006      0.0907     0.0812     57.4136
02300     2.5005      0.0717     0.0812     57.1024
02400     2.5004      0.0717     0.0814     58.1125
02500     2.5004      0.0802     0.0814     56.7273
02600     2.5003      0.0633     0.0814     57.0889
02700     2.5003      0.0738     0.0814     57.9168
02800     2.5002      0.0970     0.0814     56.7578
02900     2.5002      0.0844     0.0814     57.1983
03000     2.5001      0.0717     0.0814     57.3171
03100     2.5001      0.0865     0.0814     56.4592
03200     2.5000      0.0823     0.0814     57.9061
03300     2.5000      0.0823     0.0814     56.6293
03400     2.5000      0.0759     0.0814     56.6827
03500     2.4999      0.0907     0.0814     56.9259
03600     2.4999      0.0865     0.0814     57.3563
03700     2.4999      0.0717     0.0814     57.2104
03800     2.4999      0.0970     0.0814     57.3756
03900     2.4998      0.0506     0.0814     56.8307
04000     2.4998      0.0802     0.0814     56.9890
04100     2.4998      0.0738     0.0814     57.1569
04200     2.4998      0.0992     0.0814     58.2188
04300     2.4997      0.0823     0.0814     57.8402
04400     2.4997      0.0823     0.0814     57.1614
04500     2.4997      0.0464     0.0814     57.0849
04600     2.4997      0.0781     0.0814     57.2304
04700     2.4997      0.0928     0.0814     56.8506
04800     2.4996      0.0886     0.0814     57.5192
04900     2.4996      0.0612     0.0814     56.6411
05000     2.4996      0.0844     0.0814     56.9132
05100     2.4996      0.0823     0.0814     57.0254
05200     2.4996      0.1055     0.0814     56.6201
05300     2.4995      0.0717     0.0814     57.0018
05400     2.4995      0.0717     0.0814     57.5796
05500     2.4995      0.0738     0.0814     57.0190
05600     2.4995      0.0738     0.0814     57.1376
05700     2.4995      0.0654     0.0814     56.8198
05800     2.4994      0.0759     0.0814     56.4947
05900     2.4994      0.0759     0.0814     57.4543
06000     2.4994      0.0633     0.0814     56.9630
06100     2.4994      0.0865     0.0814     56.8096
06200     2.4994      0.0717     0.0814     58.0969
06300     2.4993      0.0928     0.0814     56.8804
06400     2.4993      0.0738     0.0814     57.5718
06500     2.4993      0.0675     0.0814     56.8450
06600     2.4993      0.0675     0.0814     56.4399
06700     2.4993      0.0907     0.0814     57.4510
06800     2.4992      0.0759     0.0814     56.2093
06900     2.4992      0.0844     0.0814     56.8895
07000     2.4992      0.0886     0.0814     57.1630
07100     2.4992      0.0781     0.0814     56.8190
07200     2.4992      0.0886     0.0814     57.7052
07300     2.4992      0.0781     0.0814     56.7490
07400     2.4991      0.0781     0.0814     57.0657
07500     2.4991      0.0422     0.0814     57.7364
07600     2.4991      0.0865     0.0814     56.8492
07700     2.4991      0.0949     0.0814     57.1282
07800     2.4991      0.0949     0.0814     57.2423
07900     2.4990      0.0675     0.0814     57.5216
08000     2.4990      0.0781     0.0814     57.5584
08100     2.4990      0.0675     0.0814     57.0602
08200     2.4990      0.0654     0.0814     57.5563
08300     2.4990      0.0949     0.0814     57.6986
08400     2.4989      0.0633     0.0814     57.4279
08500     2.4989      0.0886     0.0814     56.5655
08600     2.4989      0.0738     0.0814     57.3487
08700     2.4989      0.0696     0.0814     56.7503
08800     2.4989      0.0844     0.0814     57.4738
08900     2.4989      0.0865     0.0814     56.4712
09000     2.4988      0.0738     0.0814     56.8751
09100     2.4988      0.0823     0.0814     57.3274
09200     2.4988      0.0738     0.0814     57.0929
09300     2.4988      0.0781     0.0814     57.3414
09400     2.4988      0.0802     0.0814     57.4828
09500     2.4987      0.0654     0.0814     57.1820
09600     2.4987      0.0802     0.0814     58.0843
09700     2.4987      0.0823     0.0814     57.6524
09800     2.4987      0.0865     0.0814     57.0133
09900     2.4987      0.0844     0.0814     57.3543
10000     2.4986      0.0886     0.0814     56.6974
10100     2.4986      0.0717     0.0814     56.7423
10200     2.4986      0.0992     0.0814     58.0575
10300     2.4986      0.0759     0.0814     57.1815
10400     2.4986      0.0886     0.0814     58.0135
10500     2.4986      0.0570     0.0814     57.0133
10600     2.4986      0.0675     0.0814     56.6005
10700     2.4986      0.0738     0.0814     57.2944
10800     2.4986      0.0844     0.0814     56.8586
10900     2.4986      0.0970     0.0814     56.9110
11000     2.4986      0.0759     0.0814     57.9187
11100     2.4986      0.0696     0.0814     57.3393
11200     2.4986      0.0970     0.0814     58.5496
11300     2.4986      0.0907     0.0814     57.3798
11400     2.4986      0.0823     0.0814     57.1511
11500     2.4986      0.0802     0.0814     57.6161
11600     2.4986      0.0970     0.0814     57.9578
11700     2.4986      0.0823     0.0814     57.2264
11800     2.4986      0.0738     0.0814     57.3131
11900     2.4986      0.0802     0.0814     56.5779
12000     2.4986      0.0886     0.0814     57.4272
12100     2.4986      0.1118     0.0814     57.4385
12200     2.4986      0.0654     0.0814     57.3274
12300     2.4986      0.0823     0.0814     57.5048
12400     2.4986      0.1118     0.0814     57.0012
12500     2.4986      0.0759     0.0814     57.3060
12600     2.4986      0.0781     0.0814     57.4760
12700     2.4985      0.0802     0.0814     57.5160
12800     2.4985      0.0865     0.0814     57.4528
12900     2.4985      0.0570     0.0814     57.2700
13000     2.4985      0.0759     0.0814     57.0221
13100     2.4985      0.0844     0.0814     57.8882
13200     2.4985      0.0549     0.0814     56.9561
13300     2.4985      0.0865     0.0814     56.6501
13400     2.4985      0.0781     0.0814     57.7972
13500     2.4985      0.0696     0.0814     57.4619
13600     2.4985      0.0823     0.0814     58.0645
13700     2.4985      0.0717     0.0814     57.3351
13800     2.4985      0.0675     0.0814     57.0177
13900     2.4985      0.0612     0.0814     57.7080
14000     2.4985      0.1076     0.0814     56.9723
14100     2.4985      0.0865     0.0814     56.9727
14200     2.4985      0.0633     0.0814     57.3200
14300     2.4985      0.0865     0.0814     57.7063
14400     2.4985      0.0865     0.0814     57.7075
14500     2.4985      0.0549     0.0814     57.1188
14600     2.4985      0.0907     0.0814     57.0661
14700     2.4985      0.0928     0.0814     57.7640
14800     2.4985      0.0759     0.0814     56.9293
14900     2.4985      0.0970     0.0814     57.3768
15000     2.4985      0.0759     0.0814     58.0333
15100     2.4985      0.1160     0.0814     58.0193
15200     2.4985      0.0865     0.0814     57.6389
15300     2.4985      0.0928     0.0814     57.1404
15400     2.4985      0.0549     0.0814     56.5426
15500     2.4984      0.0781     0.0814     57.2229
15600     2.4984      0.0823     0.0814     56.4365
15700     2.4984      0.0570     0.0814     56.9931
15800     2.4984      0.0823     0.0814     57.3832
15900     2.4984      0.0844     0.0814     56.8669
16000     2.4984      0.0759     0.0814     57.6532
16100     2.4984      0.0654     0.0814     56.7010
16200     2.4984      0.0633     0.0814     57.1910
16300     2.4984      0.0844     0.0814     57.8511
16400     2.4984      0.0823     0.0814     57.4127
16500     2.4984      0.0717     0.0814     57.0191
16600     2.4984      0.0928     0.0814     57.8575
16700     2.4984      0.0696     0.0814     57.3591
16800     2.4984      0.0865     0.0814     57.4591
16900     2.4984      0.0844     0.0814     57.3374
17000     2.4984      0.0612     0.0814     57.3772
17100     2.4984      0.0907     0.0814     58.2857
17200     2.4984      0.0675     0.0814     56.8803
17300     2.4984      0.0886     0.0814     57.0718
17400     2.4984      0.0591     0.0814     58.3123
17500     2.4984      0.0759     0.0814     57.0127
17600     2.4984      0.0907     0.0814     58.1647
17700     2.4984      0.0759     0.0814     56.7572
17800     2.4984      0.0527     0.0814     57.7640
17900     2.4984      0.0865     0.0814     57.7638
18000     2.4984      0.0696     0.0814     57.2170
18100     2.4984      0.0844     0.0814     57.4164
18200     2.4984      0.0612     0.0814     57.8223
18300     2.4983      0.0949     0.0814     57.4514
18400     2.4983      0.0823     0.0814     58.5941
18500     2.4983      0.0886     0.0814     57.6172
18600     2.4983      0.0886     0.0814     57.1521
18700     2.4983      0.0802     0.0814     57.4465
18800     2.4983      0.0675     0.0814     57.3065
18900     2.4983      0.0886     0.0814     57.4534
19000     2.4983      0.0738     0.0814     57.4065
19100     2.4983      0.0738     0.0814     58.0556
19200     2.4983      0.0781     0.0814     58.6774
19300     2.4983      0.0865     0.0814     57.3698
19400     2.4983      0.0949     0.0814     57.0784
19500     2.4983      0.0886     0.0814     57.7784
19600     2.4983      0.0696     0.0814     58.0719
19700     2.4983      0.0823     0.0814     57.3611
19800     2.4983      0.0992     0.0814     57.7419
19900     2.4983      0.0696     0.0814     56.6751
20000     2.4983      0.0633     0.0814     57.6745
20100     2.4983      0.0717     0.0814     57.6852
20200     2.4983      0.1224     0.0814     57.3553
20300     2.4983      0.0570     0.0814     58.5563
20400     2.4983      0.0886     0.0814     59.5378
20500     2.4983      0.0949     0.0814     58.2044
20600     2.4983      0.0928     0.0814     57.8036
20700     2.4983      0.0928     0.0814     57.5759
20800     2.4983      0.0591     0.0814     57.8455
20900     2.4983      0.0781     0.0814     57.2782
21000     2.4983      0.0675     0.0814     57.4547
21100     2.4983      0.0591     0.0814     57.7516
21200     2.4983      0.0675     0.0814     57.5681
21300     2.4983      0.0823     0.0814     57.3646
21400     2.4983      0.0907     0.0814     57.5910
21500     2.4983      0.0949     0.0814     56.5428
21600     2.4983      0.0738     0.0814     58.2706
21700     2.4983      0.1076     0.0814     56.7753
21800     2.4983      0.0970     0.0814     57.0353
21900     2.4983      0.0675     0.0814     58.1016
22000     2.4983      0.0844     0.0814     56.8180
22100     2.4983      0.0675     0.0814     56.9683
22200     2.4983      0.0844     0.0814     57.5977
22300     2.4983      0.0844     0.0814     57.1994
22400     2.4983      0.0823     0.0814     57.8456
22500     2.4983      0.0802     0.0814     57.1232
22600     2.4983      0.0970     0.0814     56.8013
22700     2.4983      0.0696     0.0814     57.5199
22800     2.4983      0.0675     0.0814     56.8236
22900     2.4983      0.0865     0.0814     57.6983
23000     2.4983      0.0802     0.0814     58.1683
23100     2.4983      0.0506     0.0814     57.1800
23200     2.4983      0.0886     0.0814     57.3855
23300     2.4983      0.0717     0.0814     57.3785
23400     2.4983      0.0865     0.0814     57.5392
23500     2.4983      0.0823     0.0814     57.8398
23600     2.4983      0.0823     0.0814     57.6351
23700     2.4983      0.0738     0.0814     57.1333
23800     2.4983      0.0844     0.0814     58.1614
23900     2.4983      0.0823     0.0814     56.9413
24000     2.4983      0.1013     0.0814     57.3984
24100     2.4983      0.0844     0.0814     57.2165
24200     2.4983      0.1013     0.0814     57.1073
24300     2.4983      0.1055     0.0814     57.6930
24400     2.4983      0.0949     0.0814     57.3979
24500     2.4982      0.0675     0.0814     57.0030
24600     2.4982      0.0759     0.0814     57.1488
24700     2.4982      0.0781     0.0814     56.9842
24800     2.4982      0.0865     0.0814     57.9561
24900     2.4982      0.0844     0.0814     57.4554
25000     2.4982      0.0675     0.0814     56.7470
25100     2.4982      0.0781     0.0814     57.6290
25200     2.4982      0.0717     0.0814     57.6093
25300     2.4982      0.0886     0.0814     57.0970
25400     2.4982      0.0591     0.0814     57.7819
25500     2.4982      0.0865     0.0814     57.3159
25600     2.4982      0.0781     0.0814     57.9699
25700     2.4982      0.0717     0.0814     57.4302
25800     2.4982      0.0992     0.0814     58.2866
25900     2.4982      0.0570     0.0814     60.3471
26000     2.4982      0.0549     0.0814     59.6443
26100     2.4982      0.0907     0.0814     58.2597
26200     2.4982      0.0823     0.0814     58.2835
26300     2.4982      0.0928     0.0814     57.6781
26400     2.4982      0.0675     0.0814     57.7924
26500     2.4982      0.0717     0.0814     57.3985
26600     2.4982      0.0844     0.0814     57.1209
26700     2.4982      0.0781     0.0814     57.4529
26800     2.4982      0.0949     0.0814     57.1942
26900     2.4982      0.0781     0.0814     56.9792
27000     2.4982      0.0823     0.0814     58.1849
27100     2.4982      0.0759     0.0814     56.8885
27200     2.4982      0.0928     0.0814     57.9228
27300     2.4982      0.0781     0.0814     57.1738
27400     2.4982      0.0633     0.0814     55.7067
27500     2.4982      0.0781     0.0814     57.6928
27600     2.4982      0.1055     0.0814     57.7875
27700     2.4982      0.0949     0.0814     57.2006
27800     2.4982      0.0654     0.0814     58.1536
27900     2.4982      0.0696     0.0814     57.5519
28000     2.4982      0.0907     0.0814     57.7658
28100     2.4982      0.0865     0.0814     57.1187
28200     2.4982      0.0717     0.0814     57.6453
28300     2.4982      0.0844     0.0814     59.2491
28400     2.4982      0.0802     0.0814     58.1121
28500     2.4982      0.0675     0.0814     58.1294
28600     2.4982      0.0928     0.0814     57.9090
28700     2.4982      0.0675     0.0814     57.4199
28800     2.4982      0.0781     0.0814     57.9606
28900     2.4982      0.0844     0.0814     57.3683
29000     2.4982      0.1076     0.0814     57.4208
29100     2.4982      0.0738     0.0814     58.2558
29200     2.4982      0.0759     0.0814     58.0525
29300     2.4982      0.0591     0.0814     57.0993
29400     2.4982      0.0844     0.0814     58.0650
29500     2.4982      0.0823     0.0814     57.3894
29600     2.4982      0.0844     0.0814     58.2653
29700     2.4982      0.0992     0.0814     58.5576
29800     2.4982      0.0781     0.0814     59.2904
29900     2.4982      0.0886     0.0814     58.6189
29999     2.4982      0.0865     0.0814     57.2039
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Start testing:
Test Accuracy: 0.0789
