Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=512, canonical_testing=False, cs=0.5, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, hidden=107, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=0.1, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=6, quant_actNM=8, quant_inp=4, quant_w=None, random_seed=8627169, rows_bias=7, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=640, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
9ce8b1c9-42db-4af0-82ed-5b3ee3fed3d9
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 161, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 221, in forward
    forget_gate_out = quant_pass(pact_a_bmm(torch.sigmoid(f), self.a3), self.abNM, self.a3)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 89, in forward
    x01q =  torch.round(x01 * step_d ) / step_d
RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 10.76 GiB total capacity; 8.38 GiB already allocated; 3.12 MiB free; 9.80 GiB reserved in total by PyTorch)
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=512, canonical_testing=False, cs=0.5, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, hidden=107, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=0.1, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=6, quant_actNM=8, quant_inp=4, quant_w=None, random_seed=8627169, rows_bias=7, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=640, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
aa1084c6-2858-459e-ac13-3925fff982e1
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 161, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 221, in forward
    forget_gate_out = quant_pass(pact_a_bmm(torch.sigmoid(f), self.a3), self.abNM, self.a3)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 89, in forward
    x01q =  torch.round(x01 * step_d ) / step_d
RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 10.76 GiB total capacity; 8.38 GiB already allocated; 3.12 MiB free; 9.80 GiB reserved in total by PyTorch)
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.5, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, hidden=107, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=0.1, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=6, quant_actNM=8, quant_inp=4, quant_w=None, random_seed=8627169, rows_bias=7, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=640, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
8d10d134-f6cc-484e-bdf7-c5e51163a2b2
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     3.2702      0.0633     0.0704     10.7273
00100     1.7598      0.4156     0.4300     70.1986
00200     1.2832      0.5675     0.6042     69.1975
00300     1.0584      0.6160     0.6747     69.7397
00400     1.0675      0.6203     0.7002     69.3092
00500     0.9330      0.6772     0.7468     68.7663
00600     0.8305      0.7257     0.7550     68.6917
00700     0.7773      0.7194     0.7701     68.9820
00800     0.6747      0.7658     0.7701     70.1054
00900     0.7265      0.7468     0.7701     69.5436
01000     0.7411      0.7447     0.7930     70.5738
01100     0.6946      0.7511     0.7930     70.0771
01200     0.6969      0.7194     0.7930     69.6533
01300     0.7587      0.7194     0.7930     69.9512
01400     0.6746      0.7637     0.7930     69.2489
01500     0.6893      0.7574     0.7930     69.9267
01600     0.6364      0.7722     0.7972     69.9084
01700     0.5250      0.8186     0.8079     68.8068
01800     0.6357      0.7679     0.8079     69.8997
01900     0.6054      0.7700     0.8079     69.7163
02000     0.5799      0.7806     0.8157     69.8950
02100     0.6398      0.7679     0.8157     70.3748
02200     0.5625      0.7996     0.8157     69.0508
02300     0.5807      0.7911     0.8157     70.3680
02400     0.5556      0.7827     0.8157     70.5430
02500     0.5291      0.8017     0.8157     69.1808
02600     0.6396      0.7785     0.8157     69.4423
02700     0.5961      0.7785     0.8157     69.4279
02800     0.5329      0.8059     0.8157     70.0971
02900     0.5449      0.7932     0.8157     70.7103
03000     0.6525      0.7743     0.8159     69.7347
03100     0.5733      0.7827     0.8228     70.4150
03200     0.5608      0.8038     0.8237     70.1883
03300     0.6226      0.7869     0.8237     69.9395
03400     0.5667      0.7954     0.8237     70.0382
03500     0.5347      0.7996     0.8237     69.8613
03600     0.5702      0.7911     0.8237     69.9373
03700     0.5922      0.7827     0.8248     70.2030
03800     0.5434      0.7806     0.8248     69.5491
03900     0.5908      0.7743     0.8248     69.8396
04000     0.5309      0.8038     0.8248     70.2097
04100     0.5893      0.7890     0.8281     69.6653
04200     0.5619      0.7848     0.8318     69.3507
04300     0.5325      0.7996     0.8318     68.7819
04400     0.4863      0.8080     0.8318     69.4197
04500     0.5468      0.8038     0.8318     69.4680
04600     0.4967      0.8101     0.8318     69.9200
04700     0.5547      0.7975     0.8318     71.3470
04800     0.5142      0.7890     0.8318     69.9053
04900     0.4966      0.8228     0.8318     69.8800
05000     0.5217      0.8249     0.8318     70.1271
05100     0.5624      0.7890     0.8318     70.2856
05200     0.5640      0.7785     0.8318     70.7604
05300     0.4996      0.8143     0.8318     70.3066
05400     0.5367      0.8059     0.8318     70.3174
05500     0.4786      0.8186     0.8318     69.7149
05600     0.4907      0.8312     0.8318     70.6175
05700     0.5155      0.8207     0.8318     69.8631
05800     0.5093      0.7996     0.8318     69.0754
05900     0.5408      0.8143     0.8318     70.6385
06000     0.5222      0.8122     0.8318     69.8071
06100     0.5471      0.7975     0.8356     70.5529
06200     0.4982      0.8165     0.8356     70.9357
06300     0.4328      0.8270     0.8411     70.3692
06400     0.4799      0.8165     0.8411     69.8493
06500     0.4621      0.8376     0.8497     71.2545
06600     0.4778      0.8249     0.8497     69.9890
06700     0.4488      0.8186     0.8497     70.6038
06800     0.3735      0.8713     0.8497     71.0178
06900     0.4911      0.8080     0.8497     70.7276
07000     0.4316      0.8291     0.8497     70.3146
07100     0.4362      0.8523     0.8497     70.8775
07200     0.4438      0.8312     0.8497     69.8852
07300     0.4518      0.8165     0.8497     70.2055
07400     0.4121      0.8502     0.8497     70.4785
07500     0.4333      0.8418     0.8497     70.0303
07600     0.4636      0.8376     0.8497     70.1854
07700     0.4306      0.8312     0.8497     69.7688
07800     0.4531      0.8333     0.8497     69.7844
07900     0.4927      0.8080     0.8497     70.2905
08000     0.4536      0.8249     0.8497     69.9803
08100     0.4794      0.8270     0.8497     69.7176
08200     0.4789      0.8122     0.8497     69.8730
08300     0.4592      0.8460     0.8497     71.2453
08400     0.4033      0.8608     0.8497     70.3413
08500     0.4620      0.8312     0.8497     69.4651
08600     0.4219      0.8397     0.8497     70.6465
08700     0.3953      0.8565     0.8497     70.1808
08800     0.3682      0.8481     0.8497     69.6352
08900     0.4451      0.8354     0.8497     70.3395
09000     0.3586      0.8629     0.8497     70.1514
09100     0.4622      0.8207     0.8497     70.5574
09200     0.4257      0.8376     0.8497     69.9780
09300     0.3277      0.8755     0.8497     70.0461
09400     0.4424      0.8460     0.8497     70.5474
09500     0.3950      0.8481     0.8497     70.3155
09600     0.4612      0.8291     0.8497     70.6721
09700     0.4020      0.8629     0.8497     70.8078
09800     0.5261      0.7996     0.8497     70.8031
09900     0.4736      0.8312     0.8497     70.5663
10000     0.4778      0.8270     0.8497     70.8335
10100     0.4460      0.8397     0.8497     70.6258
10200     0.4073      0.8376     0.8497     70.7786
10300     0.2670      0.8924     0.8497     70.5544
10400     0.4224      0.8376     0.8497     69.9903
10500     0.4500      0.8228     0.8497     71.0402
10600     0.3273      0.8671     0.8497     70.8644
10700     0.3672      0.8502     0.8497     70.5077
10800     0.3621      0.8608     0.8497     70.3382
10900     0.4119      0.8460     0.8497     69.7913
11000     0.3847      0.8502     0.8497     70.3460
11100     0.3700      0.8586     0.8497     71.1306
11200     0.4608      0.8354     0.8497     70.4961
11300     0.4042      0.8460     0.8497     71.2949
11400     0.3447      0.8586     0.8497     71.6588
11500     0.4406      0.8439     0.8497     70.8375
11600     0.3558      0.8629     0.8497     71.9701
11700     0.4084      0.8397     0.8497     71.4948
11800     0.2800      0.9051     0.8497     71.6927
11900     0.3523      0.8565     0.8497     71.4938
12000     0.3742      0.8671     0.8497     71.7988
12100     0.4121      0.8460     0.8497     71.8140
12200     0.3749      0.8608     0.8497     71.3517
12300     0.3489      0.8544     0.8497     70.2191
12400     0.3150      0.8819     0.8497     71.0014
12500     0.3580      0.8523     0.8497     71.6278
12600     0.3790      0.8608     0.8497     71.4721
12700     0.3424      0.8671     0.8497     71.0517
12800     0.3677      0.8544     0.8497     72.2958
12900     0.3806      0.8544     0.8497     72.2960
13000     0.3765      0.8608     0.8497     72.4325
13100     0.3159      0.8840     0.8497     71.9451
13200     0.3301      0.8797     0.8497     71.7992
13300     0.2674      0.8987     0.8497     70.8759
13400     0.3397      0.8523     0.8497     71.7527
13500     0.3389      0.8713     0.8497     71.2375
13600     0.3463      0.8544     0.8497     70.9245
13700     0.3742      0.8523     0.8497     71.7797
13800     0.4230      0.8291     0.8497     70.4486
13900     0.3814      0.8544     0.8497     70.8727
14000     0.2850      0.8882     0.8497     71.0779
14100     0.3784      0.8586     0.8497     69.3419
14200     0.3166      0.8903     0.8497     68.6004
14300     0.2994      0.8671     0.8497     68.7108
14400     0.3784      0.8671     0.8497     69.0369
14500     0.3609      0.8586     0.8497     69.4818
14600     0.3836      0.8460     0.8497     69.9750
14700     0.3228      0.8671     0.8497     69.5391
14800     0.3035      0.8797     0.8497     69.6731
14900     0.4153      0.8291     0.8497     70.4267
15000     0.3303      0.8692     0.8497     68.8716
15100     0.3708      0.8629     0.8497     69.4934
15200     0.3122      0.8755     0.8497     68.9242
15300     0.3482      0.8713     0.8497     69.5863
15400     0.3768      0.8586     0.8497     68.8016
15500     0.3259      0.8608     0.8497     68.7386
15600     0.4499      0.8502     0.8497     69.5795
15700     0.3261      0.8734     0.8497     69.0256
15800     0.3223      0.8797     0.8497     69.0342
15900     0.3508      0.8608     0.8497     69.0600
16000     0.3814      0.8650     0.8497     68.6118
16100     0.3553      0.8776     0.8497     68.6107
16200     0.4072      0.8354     0.8497     68.6180
16300     0.3737      0.8586     0.8497     68.5672
16400     0.3429      0.8734     0.8497     69.6071
16500     0.3389      0.8819     0.8497     69.2207
16600     0.2834      0.8945     0.8497     68.6544
16700     0.3651      0.8397     0.8497     68.9009
16800     0.3564      0.8586     0.8497     68.4461
16900     0.3841      0.8586     0.8497     69.1397
17000     0.3758      0.8650     0.8497     68.4790
17100     0.3619      0.8608     0.8497     69.0118
17200     0.3130      0.8755     0.8497     69.1334
17300     0.2891      0.8924     0.8497     68.2900
17400     0.3641      0.8586     0.8497     68.9593
17500     0.3409      0.8755     0.8497     68.9656
17600     0.3544      0.8565     0.8497     68.1643
17700     0.3255      0.8713     0.8497     68.1354
17800     0.2985      0.8755     0.8497     68.2944
17900     0.3501      0.8650     0.8497     69.1612
18000     0.3677      0.8671     0.8497     69.0757
18100     0.3449      0.8608     0.8497     68.9275
18200     0.3731      0.8629     0.8497     68.9828
18300     0.3596      0.8502     0.8497     69.2271
18400     0.3321      0.8734     0.8497     69.1343
18500     0.2998      0.8776     0.8497     68.8511
18600     0.4193      0.8354     0.8497     68.6720
18700     0.4252      0.8460     0.8497     68.6204
18800     0.3355      0.8882     0.8497     69.4931
18900     0.3804      0.8523     0.8497     68.7760
19000     0.3919      0.8502     0.8497     68.1631
19100     0.3431      0.8734     0.8497     68.8089
19200     0.3326      0.8608     0.8497     69.0307
19300     0.3593      0.8523     0.8497     68.4920
19400     0.3422      0.8671     0.8497     68.9499
19500     0.3309      0.8755     0.8497     68.6925
19600     0.3976      0.8523     0.8497     69.2597
19700     0.3334      0.8608     0.8497     69.0043
19800     0.3673      0.8713     0.8497     69.0641
19900     0.3268      0.8608     0.8497     69.2256
20000     0.3288      0.8713     0.8497     68.8218
20100     0.3789      0.8544     0.8497     69.3736
20199     0.3687      0.8523     0.8497     67.8432
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     0.6967      0.7595     0.7632     9.8147
00100     0.3642      0.8608     0.8719     68.2520
00200     0.3929      0.8523     0.8728     69.4332
00300     0.4857      0.8122     0.8775     68.9848
00400     0.3645      0.8776     0.8775     68.6613
00500     0.3519      0.8650     0.8791     67.8767
00600     0.3430      0.8755     0.8791     69.0573
00700     0.3565      0.8734     0.8791     69.2185
00800     0.3567      0.8692     0.8833     68.4813
00900     0.3911      0.8713     0.8833     69.5392
01000     0.3052      0.8608     0.8833     68.6124
01100     0.3486      0.8776     0.8833     68.7953
01200     0.4010      0.8608     0.8833     69.0360
01300     0.3951      0.8460     0.8833     68.8073
01400     0.2824      0.8840     0.8845     69.1291
01500     0.3675      0.8586     0.8845     68.7080
01600     0.3739      0.8755     0.8845     68.9762
01700     0.3539      0.8671     0.8845     69.4842
01800     0.3608      0.8586     0.8845     68.1742
01900     0.3252      0.8840     0.8845     68.1364
02000     0.3829      0.8608     0.8845     68.9691
02100     0.3342      0.8692     0.8845     68.3325
02200     0.3078      0.8903     0.8845     69.1286
02300     0.3418      0.8713     0.8845     68.2152
02400     0.3249      0.8819     0.8845     68.2962
02500     0.4031      0.8354     0.8845     69.3658
02600     0.3555      0.8565     0.8845     68.3136
02700     0.2986      0.8713     0.8852     67.9275
02800     0.3624      0.8629     0.8852     68.8570
02900     0.3884      0.8608     0.8852     68.1591
03000     0.3761      0.8502     0.8852     68.5048
03100     0.4353      0.8376     0.8852     68.5721
03200     0.3284      0.8671     0.8852     69.1791
03300     0.2897      0.8755     0.8852     68.8820
03400     0.3908      0.8354     0.8852     67.7311
03500     0.3727      0.8586     0.8852     69.3486
03600     0.2439      0.9114     0.8852     68.6228
03700     0.3307      0.8692     0.8852     69.5103
03800     0.3715      0.8586     0.8852     68.4306
03900     0.3628      0.8523     0.8852     68.5279
04000     0.4235      0.8376     0.8852     68.6595
04100     0.3222      0.8755     0.8852     70.0274
04200     0.3643      0.8608     0.8852     68.9514
04300     0.3103      0.8713     0.8852     68.6729
04400     0.3346      0.8671     0.8852     67.7538
04500     0.3454      0.8586     0.8852     68.1007
04600     0.2660      0.8987     0.8852     68.6105
04700     0.3053      0.8797     0.8852     68.6899
04800     0.3332      0.8819     0.8863     69.5806
04900     0.3216      0.8650     0.8863     68.4897
05000     0.3505      0.8755     0.8863     68.4093
05100     0.4170      0.8565     0.8863     68.8812
05200     0.3458      0.8565     0.8863     68.5256
05300     0.2952      0.8861     0.8863     68.3218
05400     0.3913      0.8629     0.8863     68.5392
05500     0.3594      0.8797     0.8863     69.0969
05600     0.3162      0.8713     0.8869     69.0214
05700     0.4116      0.8544     0.8869     68.3362
05800     0.3431      0.8671     0.8869     67.6422
05900     0.3492      0.8713     0.8869     68.7612
06000     0.3263      0.8776     0.8869     68.4512
06100     0.3506      0.8502     0.8869     68.7690
06200     0.3459      0.8439     0.8869     68.6471
06300     0.3112      0.8734     0.8869     68.4660
06400     0.3563      0.8840     0.8885     68.5198
06500     0.4257      0.8523     0.8885     68.7360
06600     0.3627      0.8586     0.8885     69.1176
06700     0.3675      0.8650     0.8885     69.2873
06800     0.3914      0.8397     0.8885     68.6768
06900     0.3499      0.8692     0.8885     68.9241
07000     0.3074      0.8882     0.8885     68.7543
07100     0.3200      0.8586     0.8885     69.0803
07200     0.4252      0.8333     0.8885     68.7306
07300     0.3775      0.8671     0.8885     68.2172
07400     0.3577      0.8734     0.8885     68.9473
07500     0.3328      0.8692     0.8885     69.4748
07600     0.4177      0.8354     0.8885     69.0533
07700     0.3379      0.8819     0.8885     69.0230
07800     0.3838      0.8439     0.8885     68.5879
07900     0.3252      0.8650     0.8885     68.7244
08000     0.3567      0.8734     0.8885     69.2668
08100     0.3465      0.8608     0.8885     68.3429
08200     0.3028      0.8797     0.8885     68.7785
08300     0.3520      0.8734     0.8885     68.3494
08400     0.3272      0.8671     0.8885     68.1283
08500     0.3965      0.8481     0.8885     69.1778
08600     0.3008      0.8755     0.8885     69.6666
08700     0.3515      0.8629     0.8885     68.7797
08800     0.4336      0.8439     0.8885     67.9116
08900     0.3480      0.8755     0.8885     68.0759
09000     0.3860      0.8418     0.8885     68.8163
09100     0.3218      0.8734     0.8885     68.8255
09200     0.3653      0.8629     0.8885     68.6200
09300     0.3830      0.8333     0.8885     68.1768
09400     0.3638      0.8629     0.8885     68.5989
09500     0.3469      0.8565     0.8885     69.4709
09600     0.3699      0.8734     0.8885     68.8148
09700     0.3285      0.8776     0.8885     69.3191
09800     0.3540      0.8629     0.8885     69.2448
09900     0.3626      0.8565     0.8885     68.5068
Start testing:
Test Accuracy: 0.8769
