Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=9, quant_actNM=9, quant_inp=9, quant_w=9, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
86042be6-e2cf-440e-a769-17cc8f7936f5
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, 1.)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 82, in forward
    if len(x_range) > 1:
TypeError: object of type 'float' has no len()
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=9, quant_actNM=9, quant_inp=9, quant_w=9, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
1192ea87-9306-4869-b8e4-dd66ab49485a
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, torch.tensor([1.]))
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 94, in forward
    x_scaled = x/x_range
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=9, quant_actNM=9, quant_inp=9, quant_w=9, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
d6116b5e-cece-4644-962b-cc0052437cb4
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, torch.tensor([1.]).device(input.device))
TypeError: 'torch.device' object is not callable
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=9, quant_actNM=9, quant_inp=9, quant_w=9, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
f3d59b59-f296-41e2-bc9e-3ac3fd250f7b
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.8226      0.0738     0.0876     9.7697
00100     2.3679      0.2173     0.2548     67.6565
00200     1.8556      0.3819     0.4247     68.3693
00300     1.5161      0.5042     0.5308     67.9150
00400     1.3947      0.5591     0.6106     67.6879
00500     1.1697      0.6181     0.6550     67.3886
00600     1.1055      0.6772     0.6809     67.2068
00700     0.9934      0.7089     0.7056     66.9744
00800     0.9944      0.6878     0.7235     67.5473
00900     1.0187      0.6899     0.7330     66.5955
01000     0.9411      0.7110     0.7465     67.2067
01100     0.9501      0.7363     0.7465     66.9515
01200     0.8823      0.7574     0.7604     67.7742
01300     0.8612      0.7321     0.7610     66.4725
01400     0.9051      0.7489     0.7691     66.7661
01500     0.8415      0.7384     0.7760     66.7970
01600     0.8383      0.7342     0.7829     67.0329
01700     0.7697      0.7658     0.7840     66.9379
01800     0.8284      0.7553     0.7840     66.6075
01900     0.7104      0.7932     0.7873     67.4511
02000     0.8062      0.7700     0.7873     66.8680
02100     0.7123      0.7764     0.7899     66.6587
02200     0.7084      0.7722     0.7899     66.6402
02300     0.7454      0.7637     0.7899     66.1562
02400     0.6891      0.8080     0.7918     67.3442
02500     0.7912      0.7743     0.8026     67.1487
02600     0.7616      0.7764     0.8026     66.7292
02700     0.6928      0.7954     0.8026     67.3539
02800     0.7641      0.7722     0.8026     66.8892
02900     0.8331      0.7489     0.8026     67.5301
03000     0.7150      0.7806     0.8026     67.7838
03100     0.7593      0.7679     0.8053     67.5096
03200     0.7217      0.7743     0.8053     67.9797
03300     0.6498      0.8059     0.8053     66.7223
03400     0.7536      0.7574     0.8053     66.8426
03500     0.7176      0.7785     0.8057     68.1216
03600     0.7137      0.7869     0.8057     67.0848
03700     0.7185      0.7932     0.8057     66.9496
03800     0.6968      0.7932     0.8129     67.3572
03900     0.7230      0.7869     0.8129     66.6791
04000     0.6760      0.8059     0.8129     67.5266
04100     0.6266      0.8143     0.8129     67.5054
04200     0.6676      0.8038     0.8129     67.9786
04300     0.7041      0.7975     0.8129     67.0938
04400     0.6858      0.8186     0.8129     67.4511
04500     0.6144      0.8101     0.8129     66.8586
04600     0.7244      0.7722     0.8129     67.2329
04700     0.6024      0.8249     0.8129     65.1649
04800     0.6671      0.8101     0.8129     65.6568
04900     0.6549      0.7954     0.8135     66.4915
05000     0.6625      0.7890     0.8173     66.8342
05100     0.6984      0.7911     0.8173     66.1783
05200     0.6589      0.8143     0.8204     66.4114
05300     0.6993      0.8122     0.8204     65.9688
05400     0.6623      0.7996     0.8204     66.7321
05500     0.6659      0.8207     0.8204     66.8535
05600     0.5782      0.8186     0.8213     67.0361
05700     0.6939      0.7911     0.8213     67.5227
05800     0.7035      0.7869     0.8215     66.5011
05900     0.6215      0.8122     0.8215     66.7278
06000     0.6726      0.7932     0.8215     66.0398
06100     0.6831      0.7785     0.8215     66.1179
06200     0.6735      0.8038     0.8215     66.5945
06300     0.6076      0.8186     0.8215     66.3070
06400     0.6330      0.8101     0.8224     66.3703
06500     0.6702      0.8038     0.8224     65.8985
06600     0.6616      0.7954     0.8250     66.6220
06700     0.6648      0.8059     0.8250     67.0222
06800     0.5963      0.8122     0.8250     67.1275
06900     0.5838      0.8291     0.8250     67.0151
07000     0.6770      0.7975     0.8250     67.0402
07100     0.6520      0.8059     0.8250     66.1424
07200     0.6780      0.7911     0.8250     66.8427
07300     0.7068      0.7954     0.8250     66.4137
07400     0.6412      0.8080     0.8250     66.6379
07500     0.5856      0.8376     0.8250     66.6560
07600     0.7107      0.7890     0.8250     67.0446
07700     0.5253      0.8481     0.8250     66.7367
07800     0.6457      0.8038     0.8295     66.9896
07900     0.7319      0.7806     0.8295     66.5981
08000     0.6190      0.8354     0.8295     66.5772
08100     0.6295      0.8080     0.8295     67.3052
08200     0.5302      0.8460     0.8323     66.6547
08300     0.5663      0.8186     0.8323     67.3506
08400     0.6180      0.8017     0.8323     66.9016
08500     0.6461      0.8186     0.8323     66.6128
08600     0.6516      0.8080     0.8323     66.3325
08700     0.6492      0.8038     0.8323     67.0653
08800     0.6806      0.7932     0.8323     67.6715
08900     0.6678      0.7954     0.8323     67.0617
09000     0.5844      0.8270     0.8323     66.7806
09100     0.5899      0.8228     0.8323     67.3225
09200     0.6420      0.8101     0.8323     66.8801
09300     0.6248      0.8059     0.8323     67.3662
09400     0.6709      0.7848     0.8323     67.1781
09500     0.6459      0.8017     0.8323     67.7618
09600     0.5468      0.8376     0.8323     66.9748
09700     0.5697      0.8249     0.8323     67.4083
09800     0.5533      0.8418     0.8323     67.3227
09900     0.5717      0.8228     0.8323     67.1452
10000     0.5893      0.8228     0.8323     67.0913
10100     0.6203      0.8228     0.8323     67.9463
10200     0.5474      0.8270     0.8323     67.3794
10300     0.5538      0.8249     0.8346     67.5052
10400     0.5636      0.8249     0.8346     67.5486
10500     0.5081      0.8565     0.8346     66.9604
10600     0.5786      0.8186     0.8346     66.8939
10700     0.5646      0.8143     0.8346     68.0433
10800     0.5621      0.8460     0.8346     67.4064
10900     0.6127      0.8017     0.8346     68.6392
11000     0.4938      0.8523     0.8346     67.5144
11100     0.5503      0.8249     0.8346     67.5370
11200     0.4940      0.8460     0.8346     67.2982
11300     0.5960      0.8165     0.8346     68.8048
11400     0.5855      0.8228     0.8346     66.5905
11500     0.5948      0.8354     0.8346     67.4440
11600     0.6581      0.7975     0.8352     67.6646
11700     0.6293      0.8122     0.8352     67.1504
11800     0.6112      0.7975     0.8352     66.6525
11900     0.5729      0.8312     0.8352     66.9224
12000     0.5002      0.8565     0.8357     67.9795
12100     0.5458      0.8439     0.8357     67.3209
12200     0.5509      0.8312     0.8357     68.2077
12300     0.5604      0.8418     0.8357     68.5636
12400     0.6114      0.8207     0.8357     66.6690
12500     0.6047      0.7996     0.8357     67.3934
12600     0.5974      0.8165     0.8357     67.7964
12700     0.5821      0.8228     0.8357     67.6868
12800     0.5605      0.8376     0.8357     66.9312
12900     0.5528      0.8333     0.8357     67.3006
13000     0.5804      0.8165     0.8367     67.0784
13100     0.5317      0.8565     0.8395     67.0770
13200     0.5559      0.8523     0.8395     67.3730
13300     0.5327      0.8565     0.8395     68.2462
13400     0.5026      0.8565     0.8395     67.4787
13500     0.6157      0.7996     0.8395     67.8234
13600     0.5718      0.8291     0.8410     68.6520
13700     0.5694      0.8291     0.8410     67.2539
13800     0.5952      0.8376     0.8410     66.6866
13900     0.5453      0.8439     0.8410     68.3817
14000     0.6142      0.8270     0.8410     67.8382
14100     0.5397      0.8397     0.8410     66.9252
14200     0.5136      0.8481     0.8410     66.9884
14300     0.5370      0.8460     0.8410     66.9352
14400     0.6051      0.8038     0.8410     68.2414
14500     0.5867      0.8418     0.8410     66.7139
14600     0.5694      0.8312     0.8410     66.5435
14700     0.5944      0.8165     0.8410     67.1826
14800     0.5899      0.8228     0.8410     67.8197
14900     0.5828      0.8186     0.8410     67.2865
15000     0.5290      0.8418     0.8410     67.7566
15100     0.5282      0.8565     0.8410     67.9762
15200     0.5663      0.8397     0.8410     68.0098
15300     0.5499      0.8165     0.8410     67.4342
15400     0.5413      0.8207     0.8410     67.4116
15500     0.7060      0.7954     0.8410     68.3981
15600     0.5972      0.8460     0.8410     68.1538
15700     0.5918      0.8439     0.8410     68.0887
15800     0.6763      0.7890     0.8410     67.3488
15900     0.5530      0.8312     0.8410     67.0361
16000     0.5527      0.8354     0.8410     67.6997
16100     0.6371      0.7975     0.8410     66.6129
16200     0.6389      0.8143     0.8410     67.2130
16300     0.5755      0.8165     0.8410     67.4417
16400     0.5506      0.8249     0.8410     67.1411
16500     0.4753      0.8671     0.8410     67.9183
16600     0.5946      0.8059     0.8410     67.5169
16700     0.4568      0.8692     0.8410     67.8221
16800     0.5274      0.8249     0.8467     68.1765
16900     0.6410      0.8143     0.8467     66.7947
17000     0.6818      0.7890     0.8467     66.3998
17100     0.5321      0.8544     0.8467     67.7447
17200     0.5883      0.8312     0.8467     67.0237
17300     0.6356      0.8059     0.8467     67.3801
17400     0.5718      0.8165     0.8467     67.8232
17500     0.6464      0.8101     0.8467     68.6273
17600     0.6260      0.8101     0.8467     67.3889
17700     0.5467      0.8376     0.8467     67.6160
17800     0.5939      0.8207     0.8467     67.2636
17900     0.5768      0.8270     0.8467     67.3601
18000     0.5664      0.8270     0.8467     68.6199
18100     0.5111      0.8481     0.8467     67.5443
18200     0.5481      0.8228     0.8467     68.1622
18300     0.5745      0.8312     0.8467     68.2337
18400     0.5547      0.8418     0.8467     67.5664
18500     0.4638      0.8586     0.8467     66.1516
18600     0.6371      0.8165     0.8467     66.6411
18700     0.5761      0.8354     0.8467     67.6963
18800     0.5463      0.8565     0.8467     67.2003
18900     0.6279      0.8143     0.8467     67.2382
19000     0.4757      0.8650     0.8467     67.5082
19100     0.4617      0.8776     0.8467     67.3913
19200     0.5161      0.8460     0.8467     68.3045
19300     0.6195      0.8291     0.8467     67.3306
19400     0.5083      0.8586     0.8467     67.6072
19500     0.5207      0.8650     0.8467     67.6064
19600     0.5947      0.8143     0.8467     67.0843
19700     0.5622      0.8228     0.8467     67.2681
19800     0.6339      0.8101     0.8467     67.4325
19900     0.5928      0.8080     0.8467     67.2458
20000     0.5794      0.8354     0.8467     67.3814
20100     0.5428      0.8291     0.8467     67.5093
20199     0.5813      0.8249     0.8467     66.2207
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     0.5367      0.8312     0.8345     9.4145
00100     0.3665      0.8903     0.8483     65.0717
00200     0.4996      0.8502     0.8483     66.9898
00300     0.5078      0.8460     0.8483     66.0662
00400     0.4200      0.8734     0.8483     65.8621
00500     0.4490      0.8692     0.8483     66.8238
00600     0.4245      0.8861     0.8483     66.6064
00700     0.4392      0.8629     0.8483     67.4658
00800     0.4407      0.8692     0.8483     67.0290
00900     0.5505      0.8354     0.8483     66.6192
01000     0.4347      0.8797     0.8483     66.8729
01100     0.4776      0.8586     0.8483     66.8264
01200     0.5377      0.8481     0.8483     66.3217
01300     0.4722      0.8608     0.8483     66.5312
01400     0.5192      0.8376     0.8490     66.3174
01500     0.4870      0.8671     0.8490     66.2824
01600     0.5253      0.8418     0.8490     66.2575
01700     0.4782      0.8713     0.8490     67.6840
01800     0.3926      0.9008     0.8490     66.5690
01900     0.3923      0.8903     0.8490     66.3791
02000     0.5127      0.8460     0.8490     67.4938
02100     0.4504      0.8755     0.8490     65.9356
02200     0.4566      0.8755     0.8500     66.5928
02300     0.4618      0.8819     0.8500     66.2395
02400     0.3964      0.9008     0.8500     66.8104
02500     0.4835      0.8586     0.8500     67.2365
02600     0.4928      0.8439     0.8500     67.1993
02700     0.4272      0.8840     0.8500     67.0267
02800     0.4733      0.8502     0.8500     67.6572
02900     0.4408      0.8819     0.8500     66.7848
03000     0.5134      0.8376     0.8500     67.4249
03100     0.4970      0.8755     0.8500     68.0612
03200     0.4602      0.8734     0.8500     66.9156
03300     0.4611      0.8418     0.8500     67.6840
03400     0.4323      0.8586     0.8500     66.9333
03500     0.5245      0.8523     0.8500     67.3310
03600     0.4291      0.8755     0.8500     66.9315
03700     0.5729      0.8270     0.8500     66.6569
03800     0.4391      0.8713     0.8500     66.9828
03900     0.4280      0.8776     0.8500     66.2611
04000     0.5503      0.8249     0.8500     66.2080
04100     0.4601      0.8797     0.8500     66.7469
04200     0.4464      0.8713     0.8500     66.3333
04300     0.4479      0.8797     0.8500     67.1942
04400     0.4585      0.8586     0.8500     67.1727
04500     0.4627      0.8586     0.8500     67.0256
04600     0.4830      0.8692     0.8500     66.9457
04700     0.4946      0.8544     0.8500     66.7976
04800     0.4155      0.8987     0.8500     66.8154
04900     0.4798      0.8586     0.8500     67.2571
05000     0.4850      0.8460     0.8500     66.8515
05100     0.4122      0.8840     0.8500     67.6325
05200     0.5089      0.8629     0.8512     67.2493
05300     0.4374      0.8650     0.8512     65.7032
05400     0.4472      0.8565     0.8512     67.8090
05500     0.4539      0.8650     0.8512     67.3094
05600     0.4748      0.8565     0.8512     66.3022
05700     0.5035      0.8502     0.8512     67.7245
05800     0.4300      0.8819     0.8512     66.8850
05900     0.4898      0.8523     0.8512     67.2255
06000     0.4572      0.8629     0.8512     67.0944
06100     0.4840      0.8565     0.8512     67.1119
06200     0.4327      0.8861     0.8512     68.5067
06300     0.4884      0.8608     0.8512     67.0946
06400     0.4169      0.8924     0.8512     67.7417
06500     0.4886      0.8629     0.8512     67.4835
06600     0.4805      0.8692     0.8512     67.6917
06700     0.5484      0.8460     0.8512     67.3313
06800     0.5383      0.8397     0.8512     68.2351
06900     0.3561      0.8924     0.8512     67.2985
07000     0.4802      0.8629     0.8512     67.9358
07100     0.5234      0.8397     0.8512     67.5823
07200     0.4563      0.8650     0.8512     68.3584
07300     0.5448      0.8523     0.8512     68.1010
07400     0.5216      0.8439     0.8512     67.3931
07500     0.3997      0.8840     0.8512     67.5355
07600     0.4948      0.8481     0.8512     67.1960
07700     0.4217      0.8734     0.8512     68.1964
07800     0.4595      0.8481     0.8512     67.9685
07900     0.4986      0.8333     0.8512     67.1458
08000     0.4817      0.8734     0.8512     68.0205
08100     0.4725      0.8544     0.8512     67.7626
08200     0.4464      0.8629     0.8512     66.7954
08300     0.4288      0.8797     0.8512     67.7898
08400     0.4231      0.8713     0.8512     67.4232
08500     0.4165      0.8840     0.8512     67.0879
08600     0.5038      0.8460     0.8512     67.9595
08700     0.4509      0.8755     0.8512     67.0098
08800     0.4709      0.8650     0.8512     66.3388
08900     0.4510      0.8671     0.8512     67.0883
09000     0.4866      0.8481     0.8512     67.6191
09100     0.4427      0.8713     0.8512     67.6109
09200     0.4499      0.8650     0.8512     66.9908
09300     0.4540      0.8629     0.8512     67.8907
09400     0.5019      0.8544     0.8512     68.6999
09500     0.4472      0.8755     0.8512     68.1031
09600     0.4785      0.8586     0.8559     67.9148
09700     0.5521      0.8312     0.8559     67.7733
09800     0.4574      0.8734     0.8559     67.7637
09900     0.4370      0.8608     0.8559     68.0386
Start testing:
Test Accuracy: 0.8374
