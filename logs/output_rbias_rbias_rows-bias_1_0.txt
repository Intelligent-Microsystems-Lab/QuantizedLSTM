Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=512, canonical_testing=False, cs=0.5, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, hidden=113, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=0.1, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=6, quant_actNM=8, quant_inp=4, quant_w=None, random_seed=193012823, rows_bias=1, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=640, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
91bad9b9-e4d5-4430-b5f4-a21e1556f8f0
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 161, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 228, in forward
    activated_input = quant_pass(pact_a_bmm(input_gate_out * activation_out, self.a8), self.abNM, self.a8)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 124, in pact_a_bmm
    return torch.sign(x) * .5 * (torch.abs(x) - torch.abs(torch.abs(x) - a) + a)
RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 10.76 GiB total capacity; 8.69 GiB already allocated; 3.12 MiB free; 9.80 GiB reserved in total by PyTorch)
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=512, canonical_testing=False, cs=0.5, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, hidden=113, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=0.1, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=6, quant_actNM=8, quant_inp=4, quant_w=None, random_seed=193012823, rows_bias=1, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=640, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
00ea9188-8714-4137-849e-b1050798793b
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 161, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 228, in forward
    activated_input = quant_pass(pact_a_bmm(input_gate_out * activation_out, self.a8), self.abNM, self.a8)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 124, in pact_a_bmm
    return torch.sign(x) * .5 * (torch.abs(x) - torch.abs(torch.abs(x) - a) + a)
RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 10.76 GiB total capacity; 8.69 GiB already allocated; 3.12 MiB free; 9.80 GiB reserved in total by PyTorch)
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=512, canonical_testing=False, cs=0.5, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, hidden=113, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=0.1, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=6, quant_actNM=8, quant_inp=4, quant_w=None, random_seed=193012823, rows_bias=1, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=640, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
d240a6dd-e29c-4668-bbf3-bacaa6f2eae1
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.6615      0.0781     0.0714     10.7708
00100     1.5871      0.4961     0.5301     53.9333
00200     1.2119      0.6055     0.6780     53.8256
00300     1.0894      0.6562     0.7205     54.3237
00400     0.8999      0.7305     0.7495     53.5993
00500     0.8548      0.7559     0.8016     53.9663
00600     0.7828      0.7715     0.8016     54.5980
00700     0.8414      0.7480     0.8016     54.2103
00800     0.7674      0.7637     0.8052     54.5710
00900     0.7832      0.7363     0.8052     53.8749
01000     0.7440      0.7695     0.8052     53.3774
01100     0.8498      0.7148     0.8052     53.7900
01200     0.7787      0.7598     0.8311     53.4742
01300     0.8196      0.7480     0.8311     54.4018
01400     0.8521      0.7363     0.8375     54.0207
01500     0.7104      0.7793     0.8375     53.4560
01600     0.6312      0.8164     0.8375     53.7753
01700     0.6938      0.7949     0.8375     53.7391
01800     0.7291      0.7754     0.8375     53.8237
01900     0.7142      0.7656     0.8375     54.1989
02000     0.6390      0.8008     0.8375     58.2775
02100     0.7437      0.7637     0.8375     60.9782
02200     0.6808      0.7754     0.8375     60.2841
02300     0.7110      0.7734     0.8417     54.0348
02400     0.6427      0.8047     0.8417     55.0046
02500     0.6186      0.8066     0.8417     54.0163
02600     0.7129      0.7754     0.8417     54.5045
02700     0.6259      0.8047     0.8417     53.5547
02800     0.6191      0.8027     0.8417     53.8362
02900     0.6227      0.8125     0.8417     53.6321
03000     0.6042      0.8320     0.8417     53.6444
03100     0.6412      0.7988     0.8417     53.8746
03200     0.5984      0.8027     0.8417     54.3440
03300     0.6828      0.8125     0.8417     54.1307
03400     0.6155      0.7930     0.8431     54.1878
03500     0.6002      0.8164     0.8431     53.3215
03600     0.6568      0.8125     0.8431     54.0103
03700     0.5683      0.8418     0.8431     54.5206
03800     0.6361      0.7930     0.8431     53.7590
03900     0.5315      0.8496     0.8431     55.1886
04000     0.5546      0.8262     0.8528     54.7919
04100     0.5947      0.8242     0.8528     55.1571
04200     0.6054      0.7832     0.8528     54.4378
04300     0.6510      0.8027     0.8528     53.5353
04400     0.6062      0.7910     0.8528     53.5411
04500     0.5743      0.8242     0.8544     54.3341
04600     0.5828      0.8223     0.8544     54.4890
04700     0.6200      0.8105     0.8544     54.6988
04800     0.6022      0.8086     0.8544     53.5992
04900     0.5643      0.8223     0.8544     53.5731
05000     0.6652      0.8066     0.8544     54.1374
05100     0.5397      0.8477     0.8544     53.8727
05200     0.6552      0.7930     0.8544     54.6439
05300     0.5356      0.8281     0.8544     54.0252
05400     0.5282      0.8496     0.8544     54.0321
05500     0.6738      0.8008     0.8544     54.7541
05600     0.6082      0.8086     0.8544     53.9160
05700     0.5007      0.8594     0.8544     53.6778
05800     0.5511      0.8418     0.8544     54.6148
05900     0.5908      0.8301     0.8544     54.4260
06000     0.6114      0.8066     0.8544     54.1403
06100     0.5659      0.8438     0.8544     54.1456
06200     0.5510      0.8320     0.8544     53.6341
06300     0.5986      0.8184     0.8544     54.2748
06400     0.5645      0.8203     0.8544     54.1074
06500     0.5081      0.8457     0.8544     55.1328
06600     0.5892      0.8320     0.8544     54.1412
06700     0.5039      0.8594     0.8544     54.1219
06800     0.5260      0.8359     0.8544     54.9420
06900     0.5750      0.8242     0.8544     54.1499
07000     0.5719      0.8281     0.8544     53.7079
07100     0.5755      0.8418     0.8544     54.0584
07200     0.5995      0.8301     0.8544     54.6034
07300     0.5179      0.8418     0.8544     54.6304
07400     0.6294      0.8066     0.8544     54.9096
07500     0.5087      0.8516     0.8544     54.0272
07600     0.5745      0.8320     0.8544     54.6491
07700     0.4993      0.8262     0.8544     54.2621
07800     0.5490      0.8398     0.8544     54.1270
07900     0.5682      0.8164     0.8596     55.2447
08000     0.5426      0.8438     0.8596     55.6413
08100     0.5851      0.8359     0.8596     55.7074
08200     0.5400      0.8418     0.8596     54.9960
08300     0.4477      0.8672     0.8596     55.1237
08400     0.6153      0.8105     0.8596     55.2631
08500     0.5200      0.8418     0.8596     54.0316
08600     0.5329      0.8398     0.8596     53.9343
08700     0.5932      0.8047     0.8596     55.0523
08800     0.4555      0.8652     0.8596     54.8768
08900     0.5767      0.8105     0.8596     55.7053
09000     0.5536      0.8438     0.8596     53.7836
09100     0.5925      0.8125     0.8596     54.6640
09200     0.5044      0.8594     0.8596     54.9081
09300     0.5149      0.8477     0.8596     54.2097
09400     0.5637      0.8242     0.8596     54.2342
09500     0.5155      0.8477     0.8596     56.0193
09600     0.4919      0.8691     0.8596     54.8622
09700     0.4688      0.8711     0.8596     56.2544
09800     0.5088      0.8516     0.8596     55.6928
09900     0.5223      0.8555     0.8596     54.7143
10000     0.5013      0.8555     0.8596     55.6271
10100     0.4587      0.8574     0.8596     54.5628
10200     0.4813      0.8691     0.8596     53.9047
10300     0.3825      0.8809     0.8596     54.3385
10400     0.4258      0.8730     0.8596     54.3328
10500     0.4440      0.8711     0.8596     54.6762
10600     0.4787      0.8633     0.8596     53.7759
10700     0.4675      0.8594     0.8596     53.8054
10800     0.4003      0.8887     0.8596     54.2953
10900     0.4441      0.8730     0.8596     54.5736
11000     0.4604      0.8691     0.8596     53.9507
11100     0.4401      0.8750     0.8596     54.3422
11200     0.4539      0.8594     0.8596     53.3767
11300     0.3620      0.9043     0.8596     54.1506
11400     0.5452      0.8340     0.8596     54.0679
11500     0.3947      0.8906     0.8596     54.0383
11600     0.4208      0.8789     0.8596     54.2713
11700     0.4124      0.8750     0.8596     53.8082
11800     0.4844      0.8574     0.8596     53.8323
11900     0.4093      0.8828     0.8596     54.3610
12000     0.4546      0.8594     0.8596     53.6105
12100     0.4703      0.8496     0.8596     54.5273
12200     0.4539      0.8613     0.8596     53.5811
12300     0.4188      0.8809     0.8596     54.6475
12400     0.4483      0.8730     0.8596     54.5677
12500     0.4719      0.8711     0.8596     53.9633
12600     0.4924      0.8496     0.8596     53.3754
12700     0.4095      0.8828     0.8596     54.3275
12800     0.4580      0.8789     0.8596     55.0868
12900     0.4474      0.8672     0.8596     54.6014
13000     0.4264      0.8613     0.8596     53.6052
13100     0.4309      0.8691     0.8596     54.3831
13200     0.4626      0.8789     0.8596     54.7316
13300     0.4818      0.8691     0.8596     54.3239
13400     0.3951      0.8828     0.8596     54.2698
13500     0.4057      0.8809     0.8596     54.1985
13600     0.4475      0.8535     0.8596     54.0165
13700     0.3825      0.8848     0.8596     54.1699
13800     0.4593      0.8574     0.8596     54.3703
13900     0.4559      0.8652     0.8596     54.3270
14000     0.4546      0.8633     0.8596     54.7782
14100     0.4292      0.8848     0.8596     54.3299
14200     0.4927      0.8574     0.8596     54.2158
14300     0.3790      0.8867     0.8596     55.8856
14400     0.5051      0.8555     0.8596     55.4941
14500     0.4289      0.8672     0.8596     54.6638
14600     0.3916      0.9023     0.8596     55.0257
14700     0.4770      0.8496     0.8596     54.8325
14800     0.3791      0.8848     0.8596     54.0362
14900     0.5074      0.8535     0.8596     53.3061
15000     0.4818      0.8691     0.8596     53.9207
15100     0.4891      0.8672     0.8596     54.1758
15200     0.4845      0.8438     0.8596     54.2041
15300     0.4258      0.8789     0.8596     54.6631
15400     0.4062      0.8887     0.8596     55.8390
15500     0.4412      0.8672     0.8596     54.8959
15600     0.4063      0.8672     0.8596     55.4384
15700     0.4313      0.8711     0.8596     55.0210
15800     0.4508      0.8711     0.8596     54.5627
15900     0.4341      0.8691     0.8596     55.1406
16000     0.4434      0.8848     0.8596     54.3400
16100     0.5221      0.8457     0.8596     55.8126
16200     0.4144      0.8711     0.8606     54.3441
16300     0.4600      0.8672     0.8606     55.7178
16400     0.4010      0.8809     0.8606     55.8360
16500     0.4189      0.8867     0.8606     54.8750
16600     0.3894      0.9004     0.8606     55.8570
16700     0.3930      0.8906     0.8606     55.8620
16800     0.4003      0.8730     0.8606     55.5690
16900     0.4585      0.8750     0.8606     55.9733
17000     0.4241      0.8691     0.8606     54.7855
17100     0.4409      0.8730     0.8606     56.3652
17200     0.4402      0.8809     0.8606     56.2268
17300     0.4626      0.8691     0.8606     55.4682
17400     0.4420      0.8730     0.8606     56.5874
17500     0.4351      0.8672     0.8606     56.3975
17600     0.3947      0.8945     0.8606     54.6076
17700     0.4470      0.8730     0.8606     55.3557
17800     0.4324      0.8711     0.8606     55.2131
17900     0.4683      0.8672     0.8606     54.0907
18000     0.3851      0.8984     0.8606     54.0868
18100     0.4638      0.8594     0.8606     53.5902
18200     0.4730      0.8574     0.8606     53.4611
18300     0.3795      0.8926     0.8606     54.2501
18400     0.4241      0.8711     0.8606     53.9203
18500     0.4205      0.8828     0.8606     54.1199
18600     0.4470      0.8633     0.8606     54.9614
18700     0.3514      0.9160     0.8606     55.3354
18800     0.4361      0.8672     0.8606     56.3190
18900     0.4301      0.8770     0.8606     55.2223
19000     0.4367      0.8730     0.8606     55.2621
19100     0.3870      0.8906     0.8606     55.6089
19200     0.3647      0.9023     0.8606     54.4568
19300     0.3842      0.8887     0.8606     55.5631
19400     0.4240      0.8652     0.8606     54.5025
19500     0.4219      0.8828     0.8606     54.1676
19600     0.4765      0.8496     0.8606     55.5595
19700     0.3673      0.8965     0.8606     54.9035
19800     0.4575      0.8594     0.8606     54.1821
19900     0.3905      0.8848     0.8606     55.1837
20000     0.4213      0.8730     0.8606     53.9156
20100     0.4766      0.8711     0.8606     54.0513
20199     0.4436      0.8730     0.8606     53.6159
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     0.6720      0.8184     0.7808     8.8574
00100     0.4128      0.8555     0.8610     53.8344
00200     0.4099      0.8770     0.8750     54.4027
00300     0.4983      0.8398     0.8750     53.9236
00400     0.4312      0.8789     0.8750     54.2757
00500     0.4122      0.8906     0.8798     53.5789
00600     0.4015      0.8848     0.8798     53.5085
00700     0.4430      0.8691     0.8798     53.6947
00800     0.4434      0.8750     0.8798     53.9611
00900     0.4078      0.8711     0.8798     54.8857
01000     0.4461      0.8594     0.8798     53.8493
01100     0.3716      0.8984     0.8798     54.1078
01200     0.4168      0.8750     0.8798     54.0631
01300     0.3889      0.8867     0.8798     53.6392
01400     0.4033      0.8750     0.8798     53.9514
01500     0.4821      0.8457     0.8798     54.2018
01600     0.4232      0.8730     0.8798     53.7320
01700     0.4100      0.8848     0.8861     54.0288
01800     0.4263      0.8730     0.8861     53.8937
01900     0.5491      0.8438     0.8861     53.6104
02000     0.4031      0.8770     0.8861     54.4847
02100     0.4452      0.8672     0.8861     54.9101
02200     0.4583      0.8711     0.8861     57.1783
02300     0.4796      0.8633     0.8861     54.2816
02400     0.4432      0.8711     0.8861     54.6284
02500     0.4167      0.8574     0.8861     55.1124
02600     0.4590      0.8770     0.8861     56.6083
02700     0.3690      0.8984     0.8861     54.7330
02800     0.4516      0.8516     0.8861     54.1406
02900     0.4038      0.8887     0.8861     53.5683
03000     0.4849      0.8438     0.8861     54.6673
03100     0.4041      0.8809     0.8861     54.1374
03200     0.3927      0.8887     0.8861     53.6175
03300     0.3827      0.9023     0.8861     54.9254
03400     0.3557      0.9004     0.8861     54.2889
03500     0.4163      0.8906     0.8861     53.8373
03600     0.4416      0.8711     0.8861     53.2132
03700     0.4370      0.8887     0.8861     53.6598
03800     0.3765      0.8906     0.8861     54.1299
03900     0.4604      0.8613     0.8861     53.5811
04000     0.3807      0.8887     0.8861     54.2058
04100     0.4465      0.8750     0.8887     54.4584
04200     0.3535      0.9043     0.8887     54.0290
04300     0.3699      0.8789     0.8887     54.5150
04400     0.4382      0.8809     0.8887     54.2842
04500     0.3923      0.8887     0.8887     53.9561
04600     0.4428      0.8711     0.8887     54.0671
04700     0.4592      0.8672     0.8887     53.9022
04800     0.4143      0.8867     0.8887     54.6707
04900     0.4466      0.8594     0.8887     54.7947
05000     0.4719      0.8535     0.8887     54.9459
05100     0.4699      0.8594     0.8887     55.4925
05200     0.4740      0.8438     0.8887     55.4896
05300     0.4516      0.8555     0.8887     54.8944
05400     0.4458      0.8730     0.8887     55.0278
05500     0.4471      0.8594     0.8887     54.7462
05600     0.4113      0.8770     0.8887     55.8506
05700     0.4561      0.8613     0.8887     54.1163
05800     0.4111      0.8848     0.8889     53.6714
05900     0.3997      0.8770     0.8889     53.8191
06000     0.4605      0.8750     0.8889     53.5609
06100     0.4129      0.8750     0.8889     54.1583
06200     0.4083      0.8730     0.8889     53.8810
06300     0.4860      0.8652     0.8889     54.3463
06400     0.3969      0.8926     0.8889     54.1831
06500     0.3925      0.8750     0.8889     53.8709
06600     0.4075      0.8828     0.8889     54.1751
06700     0.4029      0.8770     0.8889     54.1902
06800     0.3883      0.8828     0.8889     53.5856
06900     0.3942      0.8750     0.8889     54.1907
07000     0.3379      0.9062     0.8889     53.3699
07100     0.3971      0.8848     0.8889     53.2625
07200     0.4320      0.8730     0.8889     54.2358
07300     0.3868      0.8789     0.8889     53.9885
07400     0.4448      0.8496     0.8889     53.9639
07500     0.3556      0.8965     0.8889     54.0520
07600     0.4211      0.8867     0.8889     53.9999
07700     0.4554      0.8574     0.8889     55.8111
07800     0.3977      0.8809     0.8889     54.7909
07900     0.4512      0.8691     0.8889     54.3220
08000     0.3715      0.8926     0.8889     54.6640
08100     0.4434      0.8652     0.8889     53.8306
08200     0.4075      0.8809     0.8889     53.6880
08300     0.4060      0.8574     0.8889     54.0123
08400     0.4628      0.8633     0.8889     54.0990
08500     0.4347      0.8652     0.8889     55.2620
08600     0.4060      0.8848     0.8889     55.5215
08700     0.4168      0.8809     0.8889     55.7016
08800     0.4531      0.8574     0.8889     56.4785
08900     0.4329      0.8711     0.8889     55.0825
09000     0.4814      0.8613     0.8889     56.0546
09100     0.4026      0.8809     0.8889     55.4624
09200     0.3997      0.8848     0.8889     55.4742
09300     0.3673      0.9004     0.8889     56.1240
09400     0.3792      0.8945     0.8889     55.8683
09500     0.3521      0.9004     0.8889     54.7304
09600     0.4230      0.8594     0.8889     56.2525
09700     0.3928      0.8730     0.8889     55.3307
09800     0.3849      0.8926     0.8889     55.0255
09900     0.4608      0.8574     0.8889     54.5481
Start testing:
Test Accuracy: 0.8855
