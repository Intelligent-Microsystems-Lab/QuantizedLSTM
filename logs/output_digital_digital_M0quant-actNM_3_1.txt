Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=235899598, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=235899598, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 279, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.0, finetuning_epochs=0, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.0005,0.0001,0.00002', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=235899598, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,10000', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
ca4493a4-56bd-4b72-bb16-1eba98b866b1
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.5257      0.1076     0.0998     15.6977
00100     2.5256      0.0844     0.1002     73.6885
00200     2.5256      0.1203     0.1008     74.8147
00300     2.5256      0.0970     0.1008     74.4685
00400     2.5256      0.0949     0.1008     73.7143
00500     2.5256      0.1076     0.1008     73.3977
00600     2.5255      0.0970     0.1008     74.2951
00700     2.5255      0.1055     0.1008     74.1213
00800     2.5255      0.1160     0.1008     73.8697
00900     2.5255      0.1118     0.1008     72.1938
01000     2.5254      0.0928     0.1008     75.0578
01100     2.5254      0.0907     0.1008     74.2687
01200     2.5254      0.0928     0.1008     76.3198
01300     2.5254      0.1055     0.1008     73.0629
01400     2.5254      0.0654     0.1008     73.9144
01500     2.5253      0.0865     0.1008     74.5711
01600     2.5253      0.1055     0.1008     75.2307
01700     2.5253      0.0865     0.1008     73.3972
01800     2.5253      0.0970     0.1008     75.0280
01900     2.5252      0.1076     0.1008     73.7746
02000     2.5252      0.1013     0.1008     74.6738
02100     2.5252      0.1013     0.1008     74.8948
02200     2.5252      0.0907     0.1008     75.3412
02300     2.5252      0.0949     0.1008     75.0632
02400     2.5251      0.0928     0.1008     75.9496
02500     2.5251      0.1034     0.1008     75.7411
02600     2.5251      0.0970     0.1008     74.7283
02700     2.5251      0.1013     0.1008     77.2395
02800     2.5251      0.1097     0.1008     76.2268
02900     2.5250      0.0992     0.1008     76.0522
03000     2.5250      0.0886     0.1008     75.8058
03100     2.5250      0.0970     0.1008     75.3407
03200     2.5250      0.0886     0.1008     74.4631
03300     2.5249      0.1013     0.1008     75.4854
03400     2.5249      0.0886     0.1008     75.1676
03500     2.5249      0.0992     0.1008     76.2132
03600     2.5249      0.1013     0.1008     75.4377
03700     2.5249      0.0928     0.1008     75.7887
03800     2.5248      0.0886     0.1008     76.2295
03900     2.5248      0.0696     0.1008     75.0220
04000     2.5248      0.0907     0.1008     74.8593
04100     2.5248      0.1097     0.1008     75.1648
04200     2.5248      0.0907     0.1008     75.7538
04300     2.5247      0.1139     0.1008     75.2240
04400     2.5247      0.0928     0.1008     75.6214
04500     2.5247      0.1034     0.1008     76.1152
04600     2.5247      0.1224     0.1008     75.8733
04700     2.5247      0.0992     0.1008     74.9634
04800     2.5246      0.1181     0.1008     76.5270
04900     2.5246      0.1181     0.1008     74.6659
05000     2.5246      0.1139     0.1008     76.6511
05100     2.5246      0.0970     0.1008     76.5515
05200     2.5246      0.1118     0.1008     74.0081
05300     2.5245      0.0886     0.1008     74.6141
05400     2.5245      0.0802     0.1008     76.1482
05500     2.5245      0.1013     0.1008     75.3944
05600     2.5245      0.0886     0.1008     76.7879
05700     2.5245      0.1034     0.1008     74.2812
05800     2.5244      0.1245     0.1008     74.0630
05900     2.5244      0.1034     0.1008     75.1056
06000     2.5244      0.1118     0.1008     75.0061
06100     2.5244      0.1097     0.1008     75.4327
06200     2.5244      0.1034     0.1008     76.3367
06300     2.5243      0.0886     0.1008     76.3666
06400     2.5243      0.0907     0.1008     76.1445
06500     2.5243      0.0949     0.1008     75.8407
06600     2.5243      0.1076     0.1008     74.7400
06700     2.5243      0.1013     0.1008     75.6072
06800     2.5242      0.1097     0.1008     76.0046
06900     2.5242      0.0970     0.1008     75.8765
07000     2.5242      0.1160     0.1008     75.0828
07100     2.5242      0.0759     0.1008     75.4850
07200     2.5242      0.0928     0.1008     77.5771
07300     2.5242      0.0949     0.1008     75.5000
07400     2.5241      0.0970     0.1008     75.6068
07500     2.5241      0.0928     0.1008     76.8165
07600     2.5241      0.1118     0.1008     75.5782
07700     2.5241      0.1055     0.1008     75.1379
07800     2.5241      0.0949     0.1008     76.3847
07900     2.5240      0.1245     0.1008     77.3204
08000     2.5240      0.0823     0.1008     76.1493
08100     2.5240      0.0992     0.1008     75.3833
08200     2.5240      0.0823     0.1008     75.2907
08300     2.5240      0.0823     0.1008     75.2922
08400     2.5239      0.1013     0.1008     73.8328
08500     2.5239      0.1287     0.1008     76.5399
08600     2.5239      0.0886     0.1008     74.8219
08700     2.5239      0.1097     0.1008     76.2652
08800     2.5239      0.1118     0.1008     76.5005
08900     2.5239      0.1160     0.1008     76.7266
09000     2.5238      0.1034     0.1008     75.8508
09100     2.5238      0.1013     0.1008     75.5672
09200     2.5238      0.0886     0.1008     74.5335
09300     2.5238      0.0992     0.1008     75.8979
09400     2.5238      0.0823     0.1008     74.2932
09500     2.5237      0.0865     0.1008     76.0563
09600     2.5237      0.0717     0.1008     74.5960
09700     2.5237      0.0907     0.1008     74.9379
09800     2.5237      0.0992     0.1008     75.9275
09900     2.5237      0.1013     0.1008     74.6698
10000     2.5236      0.1055     0.1008     76.1435
10100     2.5236      0.1076     0.1008     76.0319
10200     2.5236      0.0759     0.1008     75.1397
10300     2.5236      0.1097     0.1008     75.6806
10400     2.5236      0.1245     0.1008     77.6201
10500     2.5236      0.0844     0.1008     74.1984
10600     2.5236      0.0992     0.1008     74.2164
10700     2.5236      0.1245     0.1008     75.4233
10800     2.5236      0.1097     0.1008     76.6326
10900     2.5236      0.0844     0.1008     76.8292
11000     2.5236      0.0823     0.1008     76.9016
11100     2.5236      0.1245     0.1008     76.7612
11200     2.5236      0.0612     0.1008     77.5012
11300     2.5236      0.0992     0.1008     75.1404
11400     2.5236      0.1203     0.1008     75.7825
11500     2.5236      0.0907     0.1008     75.6932
11600     2.5236      0.1013     0.1008     75.2945
11700     2.5236      0.0928     0.1008     74.8975
11800     2.5236      0.1181     0.1008     75.4769
11900     2.5236      0.0992     0.1008     76.2974
12000     2.5236      0.0970     0.1008     76.1650
12100     2.5236      0.0781     0.1008     76.3008
12200     2.5236      0.0928     0.1008     76.2790
12300     2.5236      0.0970     0.1008     75.6921
12400     2.5236      0.0928     0.1008     74.3826
12500     2.5236      0.1139     0.1008     75.5826
12600     2.5236      0.1034     0.1008     75.9131
12700     2.5235      0.0865     0.1008     75.2177
12800     2.5235      0.0970     0.1008     76.4514
12900     2.5235      0.1013     0.1008     74.0813
13000     2.5235      0.0970     0.1008     73.9331
13100     2.5235      0.0992     0.1008     75.2959
13200     2.5235      0.1181     0.1008     75.6677
13300     2.5235      0.1076     0.1008     74.9520
13400     2.5235      0.1329     0.1008     74.7433
13500     2.5235      0.0823     0.1008     75.0277
13600     2.5235      0.0928     0.1008     74.9939
13700     2.5235      0.0759     0.1008     74.7322
13800     2.5235      0.1139     0.1008     76.7786
13900     2.5235      0.0865     0.1008     74.8124
14000     2.5235      0.1076     0.1008     73.6012
14100     2.5235      0.1371     0.1008     74.3046
14200     2.5235      0.1013     0.1008     76.1488
14300     2.5235      0.0886     0.1008     75.0815
14400     2.5235      0.1076     0.1008     76.8096
14500     2.5235      0.0970     0.1008     74.6892
14600     2.5235      0.1013     0.1008     75.6798
14700     2.5235      0.1203     0.1008     76.3701
14800     2.5235      0.1203     0.1008     75.0683
14900     2.5235      0.0802     0.1008     75.5794
15000     2.5235      0.1266     0.1008     77.1624
15100     2.5235      0.0992     0.1008     77.7790
15200     2.5235      0.0928     0.1008     77.2858
15300     2.5235      0.0949     0.1008     75.3076
15400     2.5235      0.0992     0.1008     76.3445
15500     2.5234      0.1350     0.1008     76.6702
15600     2.5234      0.0802     0.1008     75.9594
15700     2.5234      0.0675     0.1008     75.4763
15800     2.5234      0.1034     0.1008     78.5121
15900     2.5234      0.1034     0.1008     76.9788
16000     2.5234      0.1076     0.1008     77.3490
16100     2.5234      0.1203     0.1008     74.9022
16200     2.5234      0.0802     0.1008     76.2071
16300     2.5234      0.0907     0.1008     76.9348
16400     2.5234      0.1203     0.1008     75.1368
16500     2.5234      0.0992     0.1008     75.8057
16600     2.5234      0.1013     0.1008     75.0764
16700     2.5234      0.1055     0.1008     77.0521
16800     2.5234      0.0823     0.1009     75.5859
16900     2.5234      0.0949     0.1009     76.7107
17000     2.5234      0.1055     0.1009     75.2096
17100     2.5234      0.0886     0.1009     76.8192
17200     2.5234      0.1203     0.1009     76.3639
17300     2.5234      0.1097     0.1009     76.1589
17400     2.5234      0.1266     0.1009     75.8912
17500     2.5234      0.0992     0.1009     76.5101
17600     2.5234      0.1118     0.1009     78.1172
17700     2.5234      0.0591     0.1009     77.1107
17800     2.5234      0.0970     0.1009     76.3877
17900     2.5234      0.1013     0.1009     74.8327
18000     2.5234      0.1203     0.1009     74.9773
18100     2.5234      0.0907     0.1009     75.8192
18200     2.5234      0.0907     0.1009     74.5267
18300     2.5233      0.1097     0.1009     74.7758
18400     2.5233      0.1034     0.1009     77.9718
18500     2.5233      0.0759     0.1009     74.7822
18600     2.5233      0.0928     0.1009     74.7454
18700     2.5233      0.1034     0.1009     76.5599
18800     2.5233      0.0992     0.1009     77.8384
18900     2.5233      0.0781     0.1009     78.1242
19000     2.5233      0.0781     0.1009     75.5479
19100     2.5233      0.1055     0.1009     74.7509
19200     2.5233      0.0970     0.1009     76.2661
19300     2.5233      0.0970     0.1016     75.0182
19400     2.5233      0.0970     0.1016     74.9444
19500     2.5233      0.0886     0.1016     76.7201
19600     2.5233      0.0949     0.1016     75.4459
19700     2.5233      0.1034     0.1016     76.0136
19800     2.5233      0.0992     0.1016     74.6429
19900     2.5233      0.0781     0.1016     76.2232
20000     2.5233      0.1013     0.1016     75.6214
20100     2.5233      0.1181     0.1016     75.3411
20200     2.5233      0.1118     0.1016     76.7857
20300     2.5233      0.1139     0.1016     77.7652
20400     2.5233      0.0717     0.1016     76.5748
20500     2.5233      0.0844     0.1016     74.4682
20600     2.5233      0.1160     0.1016     75.4152
20700     2.5233      0.1034     0.1016     76.0898
20800     2.5233      0.0907     0.1016     75.9198
20900     2.5233      0.1181     0.1016     77.8172
21000     2.5233      0.1329     0.1016     76.4244
21100     2.5233      0.1245     0.1016     76.9809
21200     2.5233      0.1245     0.1016     75.2377
21300     2.5233      0.0823     0.1016     74.6740
21400     2.5233      0.0949     0.1016     74.8345
21500     2.5233      0.1055     0.1016     74.6662
21600     2.5233      0.1097     0.1016     76.4788
21700     2.5233      0.1181     0.1016     74.8680
21800     2.5233      0.0970     0.1016     75.5438
21900     2.5233      0.0928     0.1016     76.6545
22000     2.5233      0.0949     0.1016     74.8076
22100     2.5233      0.0992     0.1016     77.1215
22200     2.5233      0.0928     0.1016     74.7466
22300     2.5233      0.1055     0.1016     75.7223
22400     2.5233      0.0907     0.1016     75.0566
22500     2.5233      0.0886     0.1016     78.2576
22600     2.5233      0.1203     0.1016     75.2155
22700     2.5233      0.0886     0.1016     76.0799
22800     2.5233      0.0992     0.1016     74.9932
22900     2.5233      0.0992     0.1016     76.7139
23000     2.5233      0.1013     0.1016     76.7971
23100     2.5233      0.1013     0.1016     75.8359
23200     2.5233      0.1139     0.1016     78.0354
23300     2.5233      0.1013     0.1016     76.7655
23400     2.5233      0.1139     0.1016     75.6607
23500     2.5233      0.0759     0.1016     76.2513
23600     2.5233      0.1160     0.1016     76.3020
23700     2.5233      0.1076     0.1016     74.9514
23800     2.5233      0.0970     0.1016     77.2408
23900     2.5233      0.1034     0.1016     75.2455
24000     2.5233      0.1076     0.1016     76.0607
24100     2.5233      0.0970     0.1016     76.2176
24200     2.5233      0.1013     0.1016     75.6464
24300     2.5233      0.0907     0.1016     76.3758
24400     2.5233      0.0886     0.1016     75.7344
24500     2.5232      0.1013     0.1016     75.5030
24600     2.5232      0.1287     0.1016     76.1113
24700     2.5232      0.1055     0.1016     76.4529
24800     2.5232      0.0886     0.1016     76.7391
24900     2.5232      0.0992     0.1016     75.5471
25000     2.5232      0.1181     0.1016     75.1115
25100     2.5232      0.1013     0.1016     77.4233
25200     2.5232      0.1097     0.1016     75.3480
25300     2.5232      0.1076     0.1016     76.1594
25400     2.5232      0.0992     0.1016     78.6954
25500     2.5232      0.1160     0.1016     75.3864
25600     2.5232      0.1055     0.1016     75.0907
25700     2.5232      0.0781     0.1016     75.8334
25800     2.5232      0.0823     0.1016     75.1869
25900     2.5232      0.1392     0.1016     76.2033
26000     2.5232      0.1118     0.1016     76.6053
26100     2.5232      0.1160     0.1016     76.8599
26200     2.5232      0.1181     0.1016     75.0183
26300     2.5232      0.1034     0.1016     75.4154
26400     2.5232      0.1055     0.1016     76.4728
26500     2.5232      0.1097     0.1016     74.5595
26600     2.5232      0.1203     0.1016     75.4521
26700     2.5232      0.1097     0.1016     75.9008
26800     2.5232      0.1203     0.1016     75.8065
26900     2.5232      0.1266     0.1016     77.2882
27000     2.5232      0.1013     0.1016     75.7529
27100     2.5232      0.1118     0.1016     75.8474
27200     2.5232      0.1076     0.1016     73.7371
27300     2.5232      0.0970     0.1016     74.3844
27400     2.5232      0.0865     0.1016     75.3358
27500     2.5232      0.1076     0.1016     76.4912
27600     2.5232      0.1097     0.1016     76.1100
27700     2.5232      0.1013     0.1016     76.9501
27800     2.5232      0.1160     0.1016     76.4730
27900     2.5232      0.0844     0.1016     74.4941
28000     2.5232      0.1013     0.1016     76.9737
28100     2.5232      0.0781     0.1016     74.0298
28200     2.5232      0.0865     0.1016     76.0423
28300     2.5232      0.0802     0.1016     75.7785
28400     2.5232      0.0949     0.1016     76.3558
28500     2.5232      0.1034     0.1016     77.5318
28600     2.5232      0.0992     0.1016     76.7736
28700     2.5232      0.0907     0.1016     74.8127
28800     2.5232      0.0823     0.1016     77.9815
28900     2.5232      0.0865     0.1016     74.8298
29000     2.5232      0.0992     0.1016     75.9865
29100     2.5232      0.1160     0.1016     76.4281
29200     2.5232      0.1181     0.1016     76.2198
29300     2.5232      0.1118     0.1016     74.4580
29400     2.5232      0.0949     0.1016     75.3482
29500     2.5232      0.0907     0.1016     73.5868
29600     2.5232      0.0992     0.1016     80.2258
29700     2.5232      0.1013     0.1016     76.7578
29800     2.5232      0.1097     0.1016     76.3231
29900     2.5232      0.1160     0.1016     74.9458
29999     2.5232      0.0928     0.1016     74.6306
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Start testing:
Test Accuracy: 0.1002
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=235899598, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
4a1e702a-56eb-4671-8f01-7194236111a6
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.5257      0.0949     0.1003     11.7236
00100     2.5256      0.0865     0.1003     58.6915
00200     2.5255      0.1350     0.1004     59.7461
00300     2.5254      0.1097     0.1004     59.4263
00400     2.5253      0.1055     0.1005     59.8458
00500     2.5252      0.0844     0.1005     58.7410
00600     2.5251      0.1013     0.1005     60.8295
00700     2.5251      0.1013     0.1005     59.4321
00800     2.5250      0.0907     0.1005     59.0466
00900     2.5249      0.0949     0.1005     59.5352
01000     2.5248      0.0759     0.1005     59.4372
01100     2.5247      0.1097     0.1005     59.5797
01200     2.5247      0.0865     0.1005     58.7212
01300     2.5246      0.1076     0.1005     59.7855
01400     2.5245      0.0928     0.1005     60.5922
01500     2.5244      0.0992     0.1006     60.8131
01600     2.5243      0.1055     0.1006     60.0455
01700     2.5243      0.1055     0.1006     59.1011
01800     2.5242      0.0907     0.1006     59.7984
01900     2.5241      0.1013     0.1006     59.8240
02000     2.5240      0.1034     0.1006     59.4481
02100     2.5240      0.1181     0.1006     59.6820
02200     2.5239      0.1055     0.1006     60.6653
02300     2.5238      0.0823     0.1006     59.0178
02400     2.5237      0.1076     0.1007     59.9333
02500     2.5237      0.1181     0.1007     59.1555
02600     2.5236      0.0949     0.1007     60.9780
02700     2.5235      0.0928     0.1007     61.8511
02800     2.5234      0.0865     0.1007     59.9779
02900     2.5234      0.0886     0.1007     59.9505
03000     2.5233      0.0992     0.1007     61.2776
03100     2.5232      0.0865     0.1007     59.8279
03200     2.5232      0.1013     0.1007     59.9371
03300     2.5231      0.1034     0.1007     59.1301
03400     2.5230      0.1160     0.1007     59.1346
03500     2.5229      0.0949     0.1007     62.1846
03600     2.5229      0.1013     0.1007     60.2443
03700     2.5228      0.0949     0.1007     60.3523
03800     2.5227      0.0970     0.1007     61.6599
03900     2.5227      0.1034     0.1007     60.1795
04000     2.5226      0.0928     0.1007     60.7834
04100     2.5225      0.0970     0.1007     60.7011
04200     2.5225      0.0970     0.1007     59.6497
04300     2.5224      0.1160     0.1007     61.2873
04400     2.5223      0.0612     0.1011     58.7509
04500     2.5222      0.1013     0.1011     60.2755
04600     2.5222      0.1013     0.1011     60.5512
04700     2.5221      0.1055     0.1011     60.6557
04800     2.5220      0.0928     0.1011     59.9537
04900     2.5220      0.0886     0.1011     58.5125
05000     2.5219      0.1013     0.1011     60.6532
05100     2.5218      0.0886     0.1011     60.3738
05200     2.5218      0.1160     0.1011     59.5041
05300     2.5217      0.1034     0.1011     59.9713
05400     2.5216      0.1097     0.1011     61.0347
05500     2.5216      0.0992     0.1011     59.8348
05600     2.5215      0.0992     0.1011     61.6982
05700     2.5214      0.1160     0.1011     59.9771
05800     2.5214      0.1329     0.1011     61.4006
05900     2.5213      0.1055     0.1011     60.9860
06000     2.5212      0.0886     0.1011     60.6481
06100     2.5212      0.1034     0.1011     59.2331
06200     2.5211      0.0949     0.1011     59.7494
06300     2.5210      0.0992     0.1011     58.8326
06400     2.5210      0.1055     0.1011     59.4805
06500     2.5209      0.1034     0.1011     58.8070
06600     2.5208      0.0886     0.1011     60.3329
06700     2.5208      0.1266     0.1011     59.9334
06800     2.5207      0.0949     0.1011     59.7227
06900     2.5206      0.1055     0.1011     60.5816
07000     2.5206      0.0802     0.1011     60.9878
07100     2.5205      0.0949     0.1011     61.9053
07200     2.5204      0.0949     0.1011     60.6318
07300     2.5204      0.0781     0.1011     61.2050
07400     2.5203      0.0928     0.1011     63.1264
07500     2.5202      0.0928     0.1011     61.6207
07600     2.5202      0.1076     0.1011     61.3222
07700     2.5201      0.0865     0.1011     61.3932
07800     2.5200      0.1034     0.1011     60.9883
07900     2.5200      0.0886     0.1011     60.6861
08000     2.5199      0.0907     0.1011     59.7586
08100     2.5199      0.1118     0.1011     58.3928
08200     2.5198      0.0928     0.1011     58.3233
08300     2.5197      0.0970     0.1011     58.0267
08400     2.5197      0.1097     0.1011     57.8700
08500     2.5196      0.0696     0.1011     58.3474
08600     2.5195      0.1308     0.1011     58.3126
08700     2.5195      0.1097     0.1011     56.8233
08800     2.5194      0.1034     0.1011     56.6718
08900     2.5193      0.0865     0.1011     57.2175
09000     2.5193      0.0907     0.1011     56.4359
09100     2.5192      0.0717     0.1011     57.7794
09200     2.5192      0.0865     0.1011     56.8180
09300     2.5191      0.1013     0.1011     57.0349
09400     2.5190      0.1139     0.1011     56.9086
09500     2.5190      0.1139     0.1011     56.8871
09600     2.5189      0.1224     0.1011     57.4212
09700     2.5188      0.0886     0.1011     56.0957
09800     2.5188      0.1203     0.1011     56.5657
09900     2.5187      0.1034     0.1011     57.2477
10000     2.5186      0.1034     0.1011     56.6240
10100     2.5186      0.0928     0.1011     56.1754
10200     2.5186      0.0823     0.1011     57.0082
10300     2.5186      0.1118     0.1011     56.9081
10400     2.5186      0.0970     0.1011     57.3207
10500     2.5186      0.0992     0.1011     56.7457
10600     2.5186      0.1308     0.1011     56.9382
10700     2.5185      0.0992     0.1011     57.8290
10800     2.5185      0.1097     0.1011     57.6836
10900     2.5185      0.1287     0.1011     59.5786
11000     2.5185      0.1203     0.1011     58.3554
11100     2.5185      0.0844     0.1011     57.2443
11200     2.5185      0.0759     0.1011     57.5570
11300     2.5184      0.0886     0.1011     57.4483
11400     2.5184      0.0970     0.1011     56.9550
11500     2.5184      0.1013     0.1011     57.9245
11600     2.5184      0.0949     0.1011     56.8555
11700     2.5184      0.1097     0.1011     56.7576
11800     2.5184      0.0886     0.1011     56.6452
11900     2.5183      0.0696     0.1011     56.3701
12000     2.5183      0.1055     0.1011     58.1572
12100     2.5183      0.1181     0.1011     56.4707
12200     2.5183      0.0886     0.1011     56.8092
12300     2.5183      0.0886     0.1011     56.8073
12400     2.5183      0.0928     0.1011     56.9132
12500     2.5183      0.0823     0.1011     56.0632
12600     2.5182      0.0928     0.1011     56.9733
12700     2.5182      0.1139     0.1011     57.0368
12800     2.5182      0.0907     0.1011     57.9368
12900     2.5182      0.0865     0.1011     56.3795
13000     2.5182      0.0823     0.1011     56.6211
13100     2.5182      0.1160     0.1011     56.7671
13200     2.5181      0.0949     0.1011     57.7682
13300     2.5181      0.0949     0.1011     56.5108
13400     2.5181      0.0970     0.1011     57.6657
13500     2.5181      0.1266     0.1011     56.9110
13600     2.5181      0.1097     0.1011     58.3450
13700     2.5181      0.0970     0.1011     57.6869
13800     2.5180      0.1055     0.1011     57.2996
13900     2.5180      0.0738     0.1011     58.7308
14000     2.5180      0.1076     0.1011     57.2077
14100     2.5180      0.1013     0.1011     57.9877
14200     2.5180      0.1287     0.1011     57.7905
14300     2.5180      0.1034     0.1011     56.9720
14400     2.5180      0.1414     0.1011     57.8926
14500     2.5179      0.0823     0.1011     57.2621
14600     2.5179      0.1013     0.1011     58.0818
14700     2.5179      0.0865     0.1011     57.5490
14800     2.5179      0.1118     0.1011     57.4676
14900     2.5179      0.1181     0.1011     56.6045
15000     2.5179      0.0802     0.1011     57.5637
15100     2.5178      0.0781     0.1011     58.4466
15200     2.5178      0.0928     0.1011     58.3720
15300     2.5178      0.1055     0.1011     55.7026
15400     2.5178      0.0844     0.1011     58.1352
15500     2.5178      0.0970     0.1011     56.9148
15600     2.5178      0.1076     0.1011     58.1473
15700     2.5178      0.1034     0.1011     57.5127
15800     2.5177      0.1013     0.1011     58.1330
15900     2.5177      0.0949     0.1011     57.8370
16000     2.5177      0.1013     0.1011     57.4674
16100     2.5177      0.0992     0.1011     56.2865
16200     2.5177      0.1034     0.1011     58.3694
16300     2.5177      0.1203     0.1011     57.5561
16400     2.5176      0.1076     0.1011     57.1782
16500     2.5176      0.0970     0.1011     56.9451
16600     2.5176      0.0949     0.1011     56.8467
16700     2.5176      0.0970     0.1011     57.6169
16800     2.5176      0.0949     0.1011     58.1351
16900     2.5176      0.1013     0.1011     56.8300
17000     2.5175      0.1076     0.1011     56.1804
17100     2.5175      0.1013     0.1011     56.6038
17200     2.5175      0.0844     0.1011     56.5471
17300     2.5175      0.0907     0.1011     56.4066
17400     2.5175      0.0970     0.1011     57.1181
17500     2.5175      0.0949     0.1011     57.5041
17600     2.5175      0.0928     0.1011     58.0316
17700     2.5174      0.0992     0.1011     57.6586
17800     2.5174      0.1013     0.1011     57.9615
17900     2.5174      0.1097     0.1011     57.6948
18000     2.5174      0.0970     0.1011     56.7873
18100     2.5174      0.1224     0.1011     56.2956
18200     2.5174      0.0970     0.1011     57.1112
18300     2.5173      0.0928     0.1011     56.7332
18400     2.5173      0.1034     0.1011     56.7239
18500     2.5173      0.0992     0.1011     56.9372
18600     2.5173      0.0992     0.1011     57.4651
18700     2.5173      0.1139     0.1011     58.5709
18800     2.5173      0.1245     0.1011     57.1262
18900     2.5172      0.0823     0.1011     57.0719
19000     2.5172      0.1097     0.1011     57.4106
19100     2.5172      0.0992     0.1011     56.8079
19200     2.5172      0.0949     0.1011     57.5187
19300     2.5172      0.1160     0.1011     56.7193
19400     2.5172      0.0970     0.1011     57.4398
19500     2.5172      0.1055     0.1011     59.4612
19600     2.5171      0.0865     0.1011     56.7233
19700     2.5171      0.0907     0.1011     56.7079
19800     2.5171      0.0865     0.1011     57.0990
19900     2.5171      0.0949     0.1011     56.5711
20000     2.5171      0.0781     0.1011     57.7479
20100     2.5171      0.1118     0.1011     56.8570
20199     2.5171      0.1034     0.1011     57.2732
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.5171      0.0738     0.0915     9.3664
00100     2.5168      0.0865     0.0915     55.9962
00200     2.5172      0.0654     0.0915     55.5613
00300     2.5171      0.0759     0.0915     55.3709
00400     2.5171      0.0823     0.0915     55.1136
00500     2.5171      0.0675     0.0915     55.9097
00600     2.5170      0.0696     0.0915     55.7306
00700     2.5170      0.0907     0.0915     55.7784
00800     2.5170      0.0696     0.0915     55.6211
00900     2.5171      0.1034     0.0915     55.6101
01000     2.5170      0.0928     0.0915     55.0413
01100     2.5169      0.1076     0.0915     55.8687
01200     2.5169      0.1097     0.0915     54.9543
01300     2.5169      0.1160     0.0915     56.7578
01400     2.5171      0.0759     0.0915     56.5617
01500     2.5170      0.0865     0.0915     54.9625
01600     2.5168      0.0907     0.0915     55.6179
01700     2.5171      0.1076     0.0915     55.9964
01800     2.5169      0.0907     0.0915     56.0402
01900     2.5171      0.0759     0.0915     56.1646
02000     2.5170      0.0781     0.0915     55.3463
02100     2.5168      0.0844     0.0915     55.5001
02200     2.5170      0.0907     0.0915     56.0569
02300     2.5170      0.0823     0.0915     55.1202
02400     2.5169      0.0738     0.0915     55.4007
02500     2.5170      0.0886     0.0915     55.3354
02600     2.5172      0.0654     0.0915     55.7936
02700     2.5172      0.0633     0.0915     56.3781
02800     2.5169      0.0992     0.0915     55.8605
02900     2.5171      0.0675     0.0915     55.2955
03000     2.5171      0.0696     0.0915     56.2453
03100     2.5170      0.0696     0.0915     55.1029
03200     2.5171      0.0865     0.0915     56.4110
03300     2.5170      0.0865     0.0915     56.5627
03400     2.5170      0.0970     0.0915     55.6017
03500     2.5170      0.0928     0.0915     56.4661
03600     2.5168      0.0738     0.0915     55.9210
03700     2.5170      0.0717     0.0915     55.8141
03800     2.5170      0.0570     0.0915     56.2011
03900     2.5170      0.0844     0.0915     55.7368
04000     2.5170      0.0696     0.0915     56.6393
04100     2.5170      0.0738     0.0915     56.7287
04200     2.5169      0.0696     0.0915     55.4718
04300     2.5169      0.0886     0.0915     56.2123
04400     2.5170      0.0865     0.0915     55.3068
04500     2.5170      0.0654     0.0915     55.6594
04600     2.5169      0.0781     0.0915     55.9651
04700     2.5169      0.1013     0.0915     55.6631
04800     2.5170      0.0717     0.0915     55.9268
04900     2.5169      0.1097     0.0915     56.0101
05000     2.5170      0.0802     0.0915     55.0899
05100     2.5169      0.0865     0.0915     55.9963
05200     2.5169      0.0844     0.0915     55.2401
05300     2.5169      0.0970     0.0915     55.0239
05400     2.5169      0.0802     0.0915     56.0452
05500     2.5169      0.0886     0.0915     56.1274
05600     2.5169      0.0612     0.0915     56.3519
05700     2.5170      0.0802     0.0915     56.1153
05800     2.5170      0.0738     0.0915     56.0993
05900     2.5169      0.0696     0.0915     57.4607
06000     2.5169      0.0738     0.0915     56.2949
06100     2.5169      0.0759     0.0915     55.7495
06200     2.5170      0.0970     0.0915     56.6798
06300     2.5169      0.0781     0.0915     55.5417
06400     2.5168      0.1013     0.0915     54.6446
06500     2.5171      0.0570     0.0915     55.7354
06600     2.5169      0.0865     0.0915     56.2309
06700     2.5168      0.0907     0.0915     57.0994
06800     2.5170      0.0928     0.0915     55.3149
06900     2.5170      0.0802     0.0915     56.9253
07000     2.5169      0.0759     0.0915     56.9155
07100     2.5168      0.0970     0.0915     57.0196
07200     2.5170      0.0865     0.0915     57.2451
07300     2.5169      0.0802     0.0915     56.4594
07400     2.5168      0.1245     0.0962     56.5169
07500     2.5169      0.0886     0.0962     57.1322
07600     2.5169      0.0549     0.0968     55.8655
07700     2.5167      0.1181     0.1032     56.2579
07800     2.5169      0.1055     0.1032     55.9943
07900     2.5169      0.0844     0.1032     55.0856
08000     2.5168      0.1245     0.1032     55.7476
08100     2.5169      0.0844     0.1032     55.8489
08200     2.5168      0.1118     0.1032     55.3320
08300     2.5168      0.0759     0.1032     55.6833
08400     2.5170      0.0823     0.1032     55.4717
08500     2.5168      0.0992     0.1032     55.5063
08600     2.5168      0.1013     0.1032     56.5481
08700     2.5169      0.1013     0.1032     56.1789
08800     2.5168      0.1076     0.1032     55.1650
08900     2.5169      0.0591     0.1032     56.3276
09000     2.5167      0.0781     0.1032     55.9992
09100     2.5170      0.0612     0.1032     56.1917
09200     2.5169      0.0759     0.1032     56.8064
09300     2.5169      0.0633     0.1032     55.9168
09400     2.5168      0.0802     0.1032     56.6024
09500     2.5169      0.0654     0.1032     55.9949
09600     2.5168      0.0865     0.1032     55.8875
09700     2.5168      0.0802     0.1032     56.4839
09800     2.5169      0.0781     0.1032     56.0231
09900     2.5169      0.0654     0.1032     56.6661
Start testing:
Test Accuracy: 0.0911
