Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=235899598, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=235899598, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 279, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.0, finetuning_epochs=0, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.0005,0.0001,0.00002', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=235899598, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,10000', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
ca4493a4-56bd-4b72-bb16-1eba98b866b1
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.5257      0.1076     0.0998     15.6977
00100     2.5256      0.0844     0.1002     73.6885
00200     2.5256      0.1203     0.1008     74.8147
00300     2.5256      0.0970     0.1008     74.4685
00400     2.5256      0.0949     0.1008     73.7143
00500     2.5256      0.1076     0.1008     73.3977
00600     2.5255      0.0970     0.1008     74.2951
00700     2.5255      0.1055     0.1008     74.1213
00800     2.5255      0.1160     0.1008     73.8697
00900     2.5255      0.1118     0.1008     72.1938
01000     2.5254      0.0928     0.1008     75.0578
01100     2.5254      0.0907     0.1008     74.2687
01200     2.5254      0.0928     0.1008     76.3198
01300     2.5254      0.1055     0.1008     73.0629
01400     2.5254      0.0654     0.1008     73.9144
01500     2.5253      0.0865     0.1008     74.5711
01600     2.5253      0.1055     0.1008     75.2307
01700     2.5253      0.0865     0.1008     73.3972
01800     2.5253      0.0970     0.1008     75.0280
01900     2.5252      0.1076     0.1008     73.7746
02000     2.5252      0.1013     0.1008     74.6738
02100     2.5252      0.1013     0.1008     74.8948
02200     2.5252      0.0907     0.1008     75.3412
02300     2.5252      0.0949     0.1008     75.0632
02400     2.5251      0.0928     0.1008     75.9496
02500     2.5251      0.1034     0.1008     75.7411
02600     2.5251      0.0970     0.1008     74.7283
02700     2.5251      0.1013     0.1008     77.2395
02800     2.5251      0.1097     0.1008     76.2268
02900     2.5250      0.0992     0.1008     76.0522
03000     2.5250      0.0886     0.1008     75.8058
03100     2.5250      0.0970     0.1008     75.3407
03200     2.5250      0.0886     0.1008     74.4631
03300     2.5249      0.1013     0.1008     75.4854
03400     2.5249      0.0886     0.1008     75.1676
03500     2.5249      0.0992     0.1008     76.2132
03600     2.5249      0.1013     0.1008     75.4377
03700     2.5249      0.0928     0.1008     75.7887
03800     2.5248      0.0886     0.1008     76.2295
03900     2.5248      0.0696     0.1008     75.0220
04000     2.5248      0.0907     0.1008     74.8593
04100     2.5248      0.1097     0.1008     75.1648
04200     2.5248      0.0907     0.1008     75.7538
04300     2.5247      0.1139     0.1008     75.2240
04400     2.5247      0.0928     0.1008     75.6214
04500     2.5247      0.1034     0.1008     76.1152
04600     2.5247      0.1224     0.1008     75.8733
04700     2.5247      0.0992     0.1008     74.9634
04800     2.5246      0.1181     0.1008     76.5270
04900     2.5246      0.1181     0.1008     74.6659
05000     2.5246      0.1139     0.1008     76.6511
05100     2.5246      0.0970     0.1008     76.5515
05200     2.5246      0.1118     0.1008     74.0081
05300     2.5245      0.0886     0.1008     74.6141
05400     2.5245      0.0802     0.1008     76.1482
05500     2.5245      0.1013     0.1008     75.3944
05600     2.5245      0.0886     0.1008     76.7879
05700     2.5245      0.1034     0.1008     74.2812
05800     2.5244      0.1245     0.1008     74.0630
05900     2.5244      0.1034     0.1008     75.1056
06000     2.5244      0.1118     0.1008     75.0061
06100     2.5244      0.1097     0.1008     75.4327
06200     2.5244      0.1034     0.1008     76.3367
06300     2.5243      0.0886     0.1008     76.3666
06400     2.5243      0.0907     0.1008     76.1445
06500     2.5243      0.0949     0.1008     75.8407
06600     2.5243      0.1076     0.1008     74.7400
06700     2.5243      0.1013     0.1008     75.6072
06800     2.5242      0.1097     0.1008     76.0046
06900     2.5242      0.0970     0.1008     75.8765
07000     2.5242      0.1160     0.1008     75.0828
07100     2.5242      0.0759     0.1008     75.4850
07200     2.5242      0.0928     0.1008     77.5771
07300     2.5242      0.0949     0.1008     75.5000
07400     2.5241      0.0970     0.1008     75.6068
07500     2.5241      0.0928     0.1008     76.8165
07600     2.5241      0.1118     0.1008     75.5782
07700     2.5241      0.1055     0.1008     75.1379
07800     2.5241      0.0949     0.1008     76.3847
07900     2.5240      0.1245     0.1008     77.3204
08000     2.5240      0.0823     0.1008     76.1493
08100     2.5240      0.0992     0.1008     75.3833
08200     2.5240      0.0823     0.1008     75.2907
08300     2.5240      0.0823     0.1008     75.2922
08400     2.5239      0.1013     0.1008     73.8328
08500     2.5239      0.1287     0.1008     76.5399
08600     2.5239      0.0886     0.1008     74.8219
08700     2.5239      0.1097     0.1008     76.2652
08800     2.5239      0.1118     0.1008     76.5005
08900     2.5239      0.1160     0.1008     76.7266
09000     2.5238      0.1034     0.1008     75.8508
09100     2.5238      0.1013     0.1008     75.5672
09200     2.5238      0.0886     0.1008     74.5335
09300     2.5238      0.0992     0.1008     75.8979
09400     2.5238      0.0823     0.1008     74.2932
09500     2.5237      0.0865     0.1008     76.0563
09600     2.5237      0.0717     0.1008     74.5960
09700     2.5237      0.0907     0.1008     74.9379
09800     2.5237      0.0992     0.1008     75.9275
09900     2.5237      0.1013     0.1008     74.6698
10000     2.5236      0.1055     0.1008     76.1435
10100     2.5236      0.1076     0.1008     76.0319
10200     2.5236      0.0759     0.1008     75.1397
10300     2.5236      0.1097     0.1008     75.6806
10400     2.5236      0.1245     0.1008     77.6201
10500     2.5236      0.0844     0.1008     74.1984
10600     2.5236      0.0992     0.1008     74.2164
10700     2.5236      0.1245     0.1008     75.4233
10800     2.5236      0.1097     0.1008     76.6326
10900     2.5236      0.0844     0.1008     76.8292
11000     2.5236      0.0823     0.1008     76.9016
11100     2.5236      0.1245     0.1008     76.7612
11200     2.5236      0.0612     0.1008     77.5012
11300     2.5236      0.0992     0.1008     75.1404
11400     2.5236      0.1203     0.1008     75.7825
11500     2.5236      0.0907     0.1008     75.6932
11600     2.5236      0.1013     0.1008     75.2945
11700     2.5236      0.0928     0.1008     74.8975
11800     2.5236      0.1181     0.1008     75.4769
11900     2.5236      0.0992     0.1008     76.2974
12000     2.5236      0.0970     0.1008     76.1650
12100     2.5236      0.0781     0.1008     76.3008
12200     2.5236      0.0928     0.1008     76.2790
12300     2.5236      0.0970     0.1008     75.6921
12400     2.5236      0.0928     0.1008     74.3826
12500     2.5236      0.1139     0.1008     75.5826
12600     2.5236      0.1034     0.1008     75.9131
12700     2.5235      0.0865     0.1008     75.2177
12800     2.5235      0.0970     0.1008     76.4514
12900     2.5235      0.1013     0.1008     74.0813
13000     2.5235      0.0970     0.1008     73.9331
13100     2.5235      0.0992     0.1008     75.2959
13200     2.5235      0.1181     0.1008     75.6677
13300     2.5235      0.1076     0.1008     74.9520
13400     2.5235      0.1329     0.1008     74.7433
13500     2.5235      0.0823     0.1008     75.0277
13600     2.5235      0.0928     0.1008     74.9939
13700     2.5235      0.0759     0.1008     74.7322
13800     2.5235      0.1139     0.1008     76.7786
13900     2.5235      0.0865     0.1008     74.8124
14000     2.5235      0.1076     0.1008     73.6012
14100     2.5235      0.1371     0.1008     74.3046
14200     2.5235      0.1013     0.1008     76.1488
14300     2.5235      0.0886     0.1008     75.0815
14400     2.5235      0.1076     0.1008     76.8096
14500     2.5235      0.0970     0.1008     74.6892
14600     2.5235      0.1013     0.1008     75.6798
14700     2.5235      0.1203     0.1008     76.3701
14800     2.5235      0.1203     0.1008     75.0683
14900     2.5235      0.0802     0.1008     75.5794
15000     2.5235      0.1266     0.1008     77.1624
15100     2.5235      0.0992     0.1008     77.7790
15200     2.5235      0.0928     0.1008     77.2858
15300     2.5235      0.0949     0.1008     75.3076
15400     2.5235      0.0992     0.1008     76.3445
15500     2.5234      0.1350     0.1008     76.6702
15600     2.5234      0.0802     0.1008     75.9594
15700     2.5234      0.0675     0.1008     75.4763
15800     2.5234      0.1034     0.1008     78.5121
15900     2.5234      0.1034     0.1008     76.9788
16000     2.5234      0.1076     0.1008     77.3490
16100     2.5234      0.1203     0.1008     74.9022
16200     2.5234      0.0802     0.1008     76.2071
16300     2.5234      0.0907     0.1008     76.9348
16400     2.5234      0.1203     0.1008     75.1368
16500     2.5234      0.0992     0.1008     75.8057
16600     2.5234      0.1013     0.1008     75.0764
16700     2.5234      0.1055     0.1008     77.0521
16800     2.5234      0.0823     0.1009     75.5859
16900     2.5234      0.0949     0.1009     76.7107
17000     2.5234      0.1055     0.1009     75.2096
17100     2.5234      0.0886     0.1009     76.8192
17200     2.5234      0.1203     0.1009     76.3639
17300     2.5234      0.1097     0.1009     76.1589
17400     2.5234      0.1266     0.1009     75.8912
17500     2.5234      0.0992     0.1009     76.5101
17600     2.5234      0.1118     0.1009     78.1172
17700     2.5234      0.0591     0.1009     77.1107
17800     2.5234      0.0970     0.1009     76.3877
17900     2.5234      0.1013     0.1009     74.8327
18000     2.5234      0.1203     0.1009     74.9773
18100     2.5234      0.0907     0.1009     75.8192
18200     2.5234      0.0907     0.1009     74.5267
18300     2.5233      0.1097     0.1009     74.7758
18400     2.5233      0.1034     0.1009     77.9718
18500     2.5233      0.0759     0.1009     74.7822
18600     2.5233      0.0928     0.1009     74.7454
18700     2.5233      0.1034     0.1009     76.5599
18800     2.5233      0.0992     0.1009     77.8384
18900     2.5233      0.0781     0.1009     78.1242
19000     2.5233      0.0781     0.1009     75.5479
19100     2.5233      0.1055     0.1009     74.7509
19200     2.5233      0.0970     0.1009     76.2661
19300     2.5233      0.0970     0.1016     75.0182
19400     2.5233      0.0970     0.1016     74.9444
19500     2.5233      0.0886     0.1016     76.7201
19600     2.5233      0.0949     0.1016     75.4459
19700     2.5233      0.1034     0.1016     76.0136
19800     2.5233      0.0992     0.1016     74.6429
19900     2.5233      0.0781     0.1016     76.2232
20000     2.5233      0.1013     0.1016     75.6214
20100     2.5233      0.1181     0.1016     75.3411
20200     2.5233      0.1118     0.1016     76.7857
20300     2.5233      0.1139     0.1016     77.7652
20400     2.5233      0.0717     0.1016     76.5748
20500     2.5233      0.0844     0.1016     74.4682
20600     2.5233      0.1160     0.1016     75.4152
20700     2.5233      0.1034     0.1016     76.0898
20800     2.5233      0.0907     0.1016     75.9198
20900     2.5233      0.1181     0.1016     77.8172
21000     2.5233      0.1329     0.1016     76.4244
21100     2.5233      0.1245     0.1016     76.9809
21200     2.5233      0.1245     0.1016     75.2377
21300     2.5233      0.0823     0.1016     74.6740
21400     2.5233      0.0949     0.1016     74.8345
21500     2.5233      0.1055     0.1016     74.6662
21600     2.5233      0.1097     0.1016     76.4788
21700     2.5233      0.1181     0.1016     74.8680
21800     2.5233      0.0970     0.1016     75.5438
21900     2.5233      0.0928     0.1016     76.6545
22000     2.5233      0.0949     0.1016     74.8076
22100     2.5233      0.0992     0.1016     77.1215
22200     2.5233      0.0928     0.1016     74.7466
22300     2.5233      0.1055     0.1016     75.7223
22400     2.5233      0.0907     0.1016     75.0566
22500     2.5233      0.0886     0.1016     78.2576
22600     2.5233      0.1203     0.1016     75.2155
22700     2.5233      0.0886     0.1016     76.0799
22800     2.5233      0.0992     0.1016     74.9932
22900     2.5233      0.0992     0.1016     76.7139
23000     2.5233      0.1013     0.1016     76.7971
23100     2.5233      0.1013     0.1016     75.8359
23200     2.5233      0.1139     0.1016     78.0354
23300     2.5233      0.1013     0.1016     76.7655
23400     2.5233      0.1139     0.1016     75.6607
23500     2.5233      0.0759     0.1016     76.2513
23600     2.5233      0.1160     0.1016     76.3020
23700     2.5233      0.1076     0.1016     74.9514
23800     2.5233      0.0970     0.1016     77.2408
23900     2.5233      0.1034     0.1016     75.2455
24000     2.5233      0.1076     0.1016     76.0607
24100     2.5233      0.0970     0.1016     76.2176
24200     2.5233      0.1013     0.1016     75.6464
24300     2.5233      0.0907     0.1016     76.3758
24400     2.5233      0.0886     0.1016     75.7344
24500     2.5232      0.1013     0.1016     75.5030
24600     2.5232      0.1287     0.1016     76.1113
24700     2.5232      0.1055     0.1016     76.4529
24800     2.5232      0.0886     0.1016     76.7391
24900     2.5232      0.0992     0.1016     75.5471
25000     2.5232      0.1181     0.1016     75.1115
25100     2.5232      0.1013     0.1016     77.4233
25200     2.5232      0.1097     0.1016     75.3480
25300     2.5232      0.1076     0.1016     76.1594
25400     2.5232      0.0992     0.1016     78.6954
25500     2.5232      0.1160     0.1016     75.3864
25600     2.5232      0.1055     0.1016     75.0907
25700     2.5232      0.0781     0.1016     75.8334
25800     2.5232      0.0823     0.1016     75.1869
25900     2.5232      0.1392     0.1016     76.2033
26000     2.5232      0.1118     0.1016     76.6053
26100     2.5232      0.1160     0.1016     76.8599
26200     2.5232      0.1181     0.1016     75.0183
26300     2.5232      0.1034     0.1016     75.4154
26400     2.5232      0.1055     0.1016     76.4728
26500     2.5232      0.1097     0.1016     74.5595
26600     2.5232      0.1203     0.1016     75.4521
26700     2.5232      0.1097     0.1016     75.9008
26800     2.5232      0.1203     0.1016     75.8065
26900     2.5232      0.1266     0.1016     77.2882
27000     2.5232      0.1013     0.1016     75.7529
27100     2.5232      0.1118     0.1016     75.8474
27200     2.5232      0.1076     0.1016     73.7371
27300     2.5232      0.0970     0.1016     74.3844
27400     2.5232      0.0865     0.1016     75.3358
27500     2.5232      0.1076     0.1016     76.4912
27600     2.5232      0.1097     0.1016     76.1100
27700     2.5232      0.1013     0.1016     76.9501
27800     2.5232      0.1160     0.1016     76.4730
27900     2.5232      0.0844     0.1016     74.4941
28000     2.5232      0.1013     0.1016     76.9737
28100     2.5232      0.0781     0.1016     74.0298
28200     2.5232      0.0865     0.1016     76.0423
28300     2.5232      0.0802     0.1016     75.7785
28400     2.5232      0.0949     0.1016     76.3558
28500     2.5232      0.1034     0.1016     77.5318
28600     2.5232      0.0992     0.1016     76.7736
28700     2.5232      0.0907     0.1016     74.8127
28800     2.5232      0.0823     0.1016     77.9815
28900     2.5232      0.0865     0.1016     74.8298
29000     2.5232      0.0992     0.1016     75.9865
29100     2.5232      0.1160     0.1016     76.4281
29200     2.5232      0.1181     0.1016     76.2198
29300     2.5232      0.1118     0.1016     74.4580
29400     2.5232      0.0949     0.1016     75.3482
29500     2.5232      0.0907     0.1016     73.5868
29600     2.5232      0.0992     0.1016     80.2258
29700     2.5232      0.1013     0.1016     76.7578
29800     2.5232      0.1097     0.1016     76.3231
29900     2.5232      0.1160     0.1016     74.9458
29999     2.5232      0.0928     0.1016     74.6306
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Start testing:
Test Accuracy: 0.1002
