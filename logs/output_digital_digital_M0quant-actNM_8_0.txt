Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=8, quant_actNM=8, quant_inp=8, quant_w=8, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
8342a8de-b620-4269-9465-dad2dda94318
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, 1.)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 82, in forward
    if len(x_range) > 1:
TypeError: object of type 'float' has no len()
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=8, quant_actNM=8, quant_inp=8, quant_w=8, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
d5dcb4e3-131f-498e-8aaa-d163f3201cb9
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, torch.tensor([1.]))
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 94, in forward
    x_scaled = x/x_range
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=8, quant_actNM=8, quant_inp=8, quant_w=8, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
3cfa8923-826c-456a-a2a1-55650d8a3bbe
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, torch.tensor([1.]).device(input.device))
TypeError: 'torch.device' object is not callable
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=8, quant_actNM=8, quant_inp=8, quant_w=8, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
e44e80e8-2cdd-4b64-a960-5862b254daa7
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.8273      0.0654     0.0861     10.0853
00100     2.3726      0.2046     0.2559     71.3265
00200     1.9040      0.3671     0.4035     71.5434
00300     1.5486      0.5084     0.5152     73.9687
00400     1.4003      0.5802     0.5885     73.0563
00500     1.3171      0.6013     0.6312     72.5587
00600     1.1429      0.6287     0.6668     72.5438
00700     1.0205      0.7046     0.7140     73.0755
00800     1.0101      0.7004     0.7215     73.8708
00900     1.0479      0.6835     0.7405     71.7809
01000     0.9115      0.7257     0.7488     72.3951
01100     0.9275      0.7300     0.7488     72.7038
01200     0.8669      0.7468     0.7610     72.8465
01300     0.8617      0.7511     0.7610     72.6056
01400     0.8545      0.7532     0.7610     71.9604
01500     0.8987      0.7300     0.7753     72.3937
01600     0.7813      0.7658     0.7753     74.3561
01700     0.7375      0.7848     0.7753     73.0635
01800     0.8155      0.7574     0.7769     72.7825
01900     0.7529      0.7743     0.7817     73.8951
02000     0.7476      0.7848     0.7824     72.6179
02100     0.7129      0.7890     0.7850     72.9932
02200     0.7323      0.7764     0.7899     72.8234
02300     0.7541      0.7595     0.7899     74.0905
02400     0.7095      0.7785     0.7899     73.2571
02500     0.8273      0.7679     0.7935     72.3853
02600     0.7525      0.7848     0.7935     72.6298
02700     0.7684      0.7890     0.7935     73.0533
02800     0.7382      0.7806     0.7973     73.7021
02900     0.8447      0.7616     0.7973     72.2010
03000     0.7493      0.7743     0.7973     73.2548
03100     0.7264      0.8017     0.8052     72.8022
03200     0.6967      0.8080     0.8052     73.7781
03300     0.6359      0.8165     0.8052     72.6479
03400     0.6976      0.7996     0.8084     72.5760
03500     0.7112      0.7848     0.8086     72.9922
03600     0.6871      0.8101     0.8096     73.5144
03700     0.7080      0.7932     0.8114     75.9033
03800     0.6829      0.7890     0.8115     72.7815
03900     0.7829      0.7574     0.8115     74.5881
04000     0.6869      0.8059     0.8115     74.0453
04100     0.6814      0.8038     0.8151     72.0854
04200     0.6743      0.7932     0.8188     74.4244
04300     0.6217      0.8249     0.8188     72.9789
04400     0.6927      0.7848     0.8188     73.9801
04500     0.6001      0.8207     0.8188     73.2355
04600     0.6986      0.7975     0.8188     73.2489
04700     0.5164      0.8692     0.8188     72.4187
04800     0.6322      0.8017     0.8188     74.0849
04900     0.6586      0.8038     0.8210     72.4073
05000     0.5971      0.8101     0.8210     74.9716
05100     0.6768      0.7954     0.8210     72.7137
05200     0.7145      0.7785     0.8210     73.7500
05300     0.6315      0.7954     0.8216     75.1205
05400     0.6517      0.7954     0.8324     74.3690
05500     0.6673      0.8059     0.8324     74.7251
05600     0.5867      0.8249     0.8324     74.4101
05700     0.6708      0.7722     0.8324     72.8991
05800     0.6727      0.7996     0.8324     71.7251
05900     0.6331      0.8291     0.8324     72.1248
06000     0.6800      0.8122     0.8324     72.9874
06100     0.7249      0.7806     0.8324     72.7800
06200     0.6593      0.8207     0.8324     75.6717
06300     0.5457      0.8460     0.8324     72.8547
06400     0.5946      0.7975     0.8324     76.0488
06500     0.6005      0.8228     0.8324     73.0832
06600     0.6778      0.7975     0.8324     73.2013
06700     0.6975      0.7975     0.8324     76.0302
06800     0.5793      0.8207     0.8324     72.7697
06900     0.5853      0.8333     0.8324     73.7519
07000     0.7203      0.7996     0.8324     73.5366
07100     0.6291      0.8207     0.8324     73.3544
07200     0.6265      0.8059     0.8324     73.5959
07300     0.7361      0.7890     0.8324     74.5670
07400     0.6031      0.8143     0.8324     74.9481
07500     0.5660      0.8207     0.8324     72.2912
07600     0.7059      0.7827     0.8324     72.9063
07700     0.5299      0.8460     0.8324     73.3956
07800     0.6547      0.8038     0.8324     74.1083
07900     0.7562      0.7743     0.8324     73.2303
08000     0.6866      0.7869     0.8324     72.7430
08100     0.6327      0.8122     0.8324     76.6118
08200     0.5791      0.8397     0.8324     73.5376
08300     0.5067      0.8650     0.8324     74.7748
08400     0.6669      0.8017     0.8324     72.1595
08500     0.5967      0.8165     0.8324     75.2598
08600     0.6368      0.8143     0.8324     73.2556
08700     0.5920      0.8017     0.8324     71.8882
08800     0.7127      0.7890     0.8324     72.4404
08900     0.6320      0.8207     0.8324     72.2907
09000     0.6198      0.8038     0.8324     74.1291
09100     0.6422      0.7996     0.8324     73.2654
09200     0.6466      0.7996     0.8324     73.5633
09300     0.6257      0.8228     0.8324     74.6938
09400     0.6446      0.8017     0.8324     73.7947
09500     0.6722      0.7932     0.8324     72.6533
09600     0.5458      0.8418     0.8324     72.1880
09700     0.5880      0.8228     0.8329     74.8636
09800     0.5889      0.8312     0.8338     73.9692
09900     0.5887      0.8207     0.8338     72.7921
10000     0.5690      0.8312     0.8338     72.9307
10100     0.6532      0.8165     0.8359     73.7955
10200     0.6032      0.8207     0.8403     73.4154
10300     0.5671      0.8397     0.8408     73.2601
10400     0.5543      0.8270     0.8408     72.4376
10500     0.5614      0.8312     0.8408     73.6440
10600     0.5736      0.8376     0.8408     73.1012
10700     0.6299      0.7975     0.8408     73.3616
10800     0.5836      0.8207     0.8408     73.7441
10900     0.6314      0.8228     0.8408     72.5503
11000     0.4907      0.8586     0.8408     72.7211
11100     0.5599      0.8249     0.8408     74.0327
11200     0.5294      0.8418     0.8408     74.8200
11300     0.6240      0.8186     0.8413     74.0275
11400     0.5810      0.8312     0.8414     72.7922
11500     0.5995      0.8333     0.8414     73.2811
11600     0.6713      0.8017     0.8414     74.0059
11700     0.6489      0.7869     0.8414     74.1562
11800     0.5798      0.8291     0.8414     74.0823
11900     0.6462      0.8017     0.8414     73.3196
12000     0.5199      0.8502     0.8414     72.3262
12100     0.5537      0.8312     0.8419     71.3890
12200     0.5456      0.8354     0.8438     71.6207
12300     0.5545      0.8333     0.8438     72.8773
12400     0.5852      0.8291     0.8438     74.0351
12500     0.5690      0.8354     0.8438     73.1420
12600     0.5261      0.8523     0.8438     73.1176
12700     0.6163      0.8080     0.8438     71.2579
12800     0.5837      0.8418     0.8438     73.0366
12900     0.5505      0.8460     0.8438     72.3600
13000     0.5586      0.8376     0.8438     72.8988
13100     0.5537      0.8439     0.8438     73.9810
13200     0.5553      0.8249     0.8438     74.1933
13300     0.5461      0.8376     0.8438     73.3567
13400     0.6317      0.8038     0.8438     71.2503
13500     0.6245      0.8143     0.8440     74.4701
13600     0.5523      0.8291     0.8440     74.3846
13700     0.6108      0.8291     0.8440     72.6182
13800     0.5972      0.8481     0.8440     72.1240
13900     0.5085      0.8481     0.8440     73.1643
14000     0.5400      0.8523     0.8440     71.7380
14100     0.5855      0.8228     0.8440     72.5177
14200     0.4976      0.8376     0.8440     72.9789
14300     0.6050      0.8122     0.8442     71.2940
14400     0.5363      0.8376     0.8457     73.7114
14500     0.6493      0.8186     0.8457     74.6336
14600     0.5685      0.8354     0.8457     73.2667
14700     0.5494      0.8333     0.8490     71.5743
14800     0.6139      0.8186     0.8490     72.7233
14900     0.6067      0.8143     0.8490     72.6862
15000     0.5124      0.8460     0.8490     71.9966
15100     0.5614      0.8165     0.8490     74.7307
15200     0.5840      0.8354     0.8490     75.7617
15300     0.5341      0.8439     0.8490     74.1227
15400     0.5979      0.7996     0.8490     72.3510
15500     0.6434      0.8101     0.8490     73.5329
15600     0.5447      0.8502     0.8490     72.6102
15700     0.5675      0.8291     0.8490     72.4595
15800     0.6454      0.8080     0.8490     72.8766
15900     0.5430      0.8228     0.8490     73.4889
16000     0.5573      0.8376     0.8490     71.7703
16100     0.7100      0.7806     0.8490     71.4329
16200     0.6309      0.8249     0.8490     74.2285
16300     0.6265      0.8101     0.8490     73.7316
16400     0.5320      0.8460     0.8490     74.4704
16500     0.5136      0.8523     0.8490     74.3699
16600     0.5300      0.8397     0.8490     73.0399
16700     0.5021      0.8565     0.8490     75.2462
16800     0.5194      0.8544     0.8490     72.8475
16900     0.6346      0.8291     0.8490     74.3921
17000     0.6896      0.7679     0.8490     72.2009
17100     0.5642      0.8101     0.8490     72.7332
17200     0.6289      0.8165     0.8490     73.2295
17300     0.6244      0.8017     0.8490     73.2881
17400     0.6095      0.8249     0.8490     73.5441
17500     0.6693      0.7932     0.8490     73.1321
17600     0.5957      0.8080     0.8490     75.3208
17700     0.5264      0.8397     0.8490     73.6365
17800     0.6014      0.8249     0.8490     74.2163
17900     0.5451      0.8481     0.8490     72.9905
18000     0.5559      0.8439     0.8490     73.1092
18100     0.5468      0.8439     0.8490     72.8473
18200     0.5763      0.8439     0.8490     76.5760
18300     0.5484      0.8291     0.8490     73.2090
18400     0.5600      0.8354     0.8490     76.0186
18500     0.5017      0.8692     0.8490     73.1725
18600     0.6760      0.7975     0.8490     73.2226
18700     0.5711      0.8333     0.8490     72.7620
18800     0.5522      0.8354     0.8490     74.5770
18900     0.6354      0.8207     0.8490     74.3261
19000     0.5264      0.8333     0.8490     76.0402
19100     0.5462      0.8397     0.8490     72.4391
19200     0.5790      0.8165     0.8490     74.1756
19300     0.5597      0.8565     0.8490     72.9197
19400     0.5253      0.8376     0.8490     73.7554
19500     0.4913      0.8586     0.8490     74.5506
19600     0.6098      0.8080     0.8490     73.2227
19700     0.6092      0.8207     0.8490     73.5686
19800     0.6379      0.7911     0.8490     72.6992
19900     0.6018      0.8143     0.8490     72.2153
20000     0.5357      0.8523     0.8490     71.6623
20100     0.5298      0.8376     0.8490     72.4474
20199     0.5773      0.8228     0.8490     71.8675
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     0.5297      0.8608     0.8480     9.8475
00100     0.3734      0.8987     0.8480     72.3262
00200     0.4928      0.8460     0.8480     71.7943
00300     0.5524      0.8418     0.8484     74.0842
00400     0.3895      0.8903     0.8484     72.9749
00500     0.4525      0.8586     0.8484     74.5457
00600     0.4739      0.8671     0.8484     72.7716
00700     0.4353      0.8713     0.8498     73.0510
00800     0.4100      0.8819     0.8498     72.7191
00900     0.5729      0.8418     0.8498     73.1533
01000     0.4794      0.8734     0.8498     72.6759
01100     0.4734      0.8650     0.8498     74.5905
01200     0.5510      0.8312     0.8498     74.6475
01300     0.4769      0.8629     0.8498     75.5197
01400     0.5249      0.8312     0.8498     71.1855
01500     0.4680      0.8608     0.8498     73.0102
01600     0.5212      0.8523     0.8498     73.2753
01700     0.5260      0.8565     0.8498     73.0483
01800     0.4144      0.8945     0.8498     71.6677
01900     0.3800      0.8987     0.8498     72.2599
02000     0.5246      0.8418     0.8498     72.7272
02100     0.4453      0.8650     0.8498     70.4835
02200     0.4417      0.8713     0.8511     72.3851
02300     0.4650      0.8797     0.8511     72.8990
02400     0.4509      0.8565     0.8511     73.1650
02500     0.5350      0.8354     0.8511     74.8644
02600     0.5088      0.8502     0.8511     72.8212
02700     0.4312      0.8776     0.8515     73.6662
02800     0.5166      0.8650     0.8515     72.4420
02900     0.4618      0.8671     0.8515     73.3359
03000     0.4825      0.8523     0.8515     71.9560
03100     0.4971      0.8523     0.8515     72.6856
03200     0.4622      0.8629     0.8515     72.5570
03300     0.4311      0.8650     0.8515     73.0478
03400     0.4955      0.8608     0.8515     74.1052
03500     0.4995      0.8439     0.8515     73.8715
03600     0.4189      0.8755     0.8515     75.2753
03700     0.5419      0.8376     0.8515     72.4747
03800     0.4280      0.8776     0.8515     73.4010
03900     0.4576      0.8650     0.8515     72.6462
04000     0.5018      0.8650     0.8515     73.4056
04100     0.4934      0.8502     0.8515     73.0051
04200     0.4625      0.8608     0.8515     74.0472
04300     0.4491      0.8692     0.8515     72.1474
04400     0.4464      0.8671     0.8515     74.8199
04500     0.4597      0.8586     0.8515     72.9369
04600     0.4588      0.8692     0.8515     73.5850
04700     0.4796      0.8650     0.8515     74.4496
04800     0.4259      0.8755     0.8515     73.6669
04900     0.4770      0.8565     0.8515     72.5100
05000     0.4861      0.8502     0.8515     72.4811
05100     0.4191      0.8755     0.8515     72.5304
05200     0.5231      0.8354     0.8515     71.4455
05300     0.4422      0.8819     0.8515     74.1526
05400     0.5141      0.8544     0.8515     73.8590
05500     0.4557      0.8734     0.8531     72.4809
05600     0.4417      0.8840     0.8531     74.1879
05700     0.5088      0.8460     0.8531     73.9429
05800     0.4335      0.8840     0.8531     73.1350
05900     0.4569      0.8692     0.8531     72.5727
06000     0.4531      0.8586     0.8531     71.4081
06100     0.5093      0.8439     0.8531     73.8945
06200     0.4663      0.8819     0.8531     71.9671
06300     0.5015      0.8608     0.8531     71.7656
06400     0.4605      0.8671     0.8531     72.5644
06500     0.5190      0.8544     0.8531     73.5683
06600     0.4704      0.8713     0.8531     72.5525
06700     0.5177      0.8565     0.8531     73.5701
06800     0.5227      0.8481     0.8531     71.6066
06900     0.3899      0.8776     0.8531     72.2874
07000     0.4433      0.8755     0.8531     72.0884
07100     0.5022      0.8544     0.8531     70.3007
07200     0.4477      0.8840     0.8531     70.0800
07300     0.5410      0.8523     0.8531     70.5066
07400     0.5009      0.8481     0.8531     70.6015
07500     0.3995      0.8903     0.8531     69.2433
07600     0.5158      0.8354     0.8531     69.3750
07700     0.4464      0.8755     0.8531     70.1840
07800     0.4346      0.8797     0.8531     70.6817
07900     0.5068      0.8586     0.8531     70.1658
08000     0.4665      0.8650     0.8531     70.1819
08100     0.4659      0.8523     0.8531     70.5662
08200     0.4154      0.8861     0.8531     68.9480
08300     0.4440      0.8776     0.8531     71.2407
08400     0.4662      0.8629     0.8531     70.1235
08500     0.4209      0.8734     0.8531     70.0353
08600     0.5087      0.8608     0.8531     70.7883
08700     0.4454      0.8945     0.8531     69.3029
08800     0.4722      0.8629     0.8531     69.2922
08900     0.5203      0.8544     0.8537     70.9221
09000     0.4433      0.8945     0.8537     69.3803
09100     0.4493      0.8755     0.8540     69.7466
09200     0.4579      0.8734     0.8540     68.8138
09300     0.5154      0.8397     0.8540     70.5397
09400     0.4999      0.8608     0.8565     70.0455
09500     0.4545      0.8502     0.8565     71.3652
09600     0.4813      0.8608     0.8565     71.4751
09700     0.5647      0.8502     0.8565     70.4106
09800     0.4850      0.8692     0.8565     69.9461
09900     0.5066      0.8460     0.8565     70.4737
Start testing:
Test Accuracy: 0.8396
