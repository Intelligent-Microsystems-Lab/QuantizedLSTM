Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=8, quant_actNM=8, quant_inp=8, quant_w=8, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
8342a8de-b620-4269-9465-dad2dda94318
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, 1.)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 82, in forward
    if len(x_range) > 1:
TypeError: object of type 'float' has no len()
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=8, quant_actNM=8, quant_inp=8, quant_w=8, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
d5dcb4e3-131f-498e-8aaa-d163f3201cb9
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, torch.tensor([1.]))
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 94, in forward
    x_scaled = x/x_range
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=8, quant_actNM=8, quant_inp=8, quant_w=8, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
3cfa8923-826c-456a-a2a1-55650d8a3bbe
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, torch.tensor([1.]).device(input.device))
TypeError: 'torch.device' object is not callable
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=8, quant_actNM=8, quant_inp=8, quant_w=8, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
e44e80e8-2cdd-4b64-a960-5862b254daa7
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.8273      0.0654     0.0861     10.0853
00100     2.3726      0.2046     0.2559     71.3265
00200     1.9040      0.3671     0.4035     71.5434
00300     1.5486      0.5084     0.5152     73.9687
00400     1.4003      0.5802     0.5885     73.0563
00500     1.3171      0.6013     0.6312     72.5587
00600     1.1429      0.6287     0.6668     72.5438
00700     1.0205      0.7046     0.7140     73.0755
00800     1.0101      0.7004     0.7215     73.8708
00900     1.0479      0.6835     0.7405     71.7809
01000     0.9115      0.7257     0.7488     72.3951
01100     0.9275      0.7300     0.7488     72.7038
01200     0.8669      0.7468     0.7610     72.8465
01300     0.8617      0.7511     0.7610     72.6056
01400     0.8545      0.7532     0.7610     71.9604
01500     0.8987      0.7300     0.7753     72.3937
01600     0.7813      0.7658     0.7753     74.3561
01700     0.7375      0.7848     0.7753     73.0635
01800     0.8155      0.7574     0.7769     72.7825
01900     0.7529      0.7743     0.7817     73.8951
02000     0.7476      0.7848     0.7824     72.6179
02100     0.7129      0.7890     0.7850     72.9932
02200     0.7323      0.7764     0.7899     72.8234
02300     0.7541      0.7595     0.7899     74.0905
02400     0.7095      0.7785     0.7899     73.2571
02500     0.8273      0.7679     0.7935     72.3853
02600     0.7525      0.7848     0.7935     72.6298
02700     0.7684      0.7890     0.7935     73.0533
02800     0.7382      0.7806     0.7973     73.7021
02900     0.8447      0.7616     0.7973     72.2010
03000     0.7493      0.7743     0.7973     73.2548
03100     0.7264      0.8017     0.8052     72.8022
03200     0.6967      0.8080     0.8052     73.7781
03300     0.6359      0.8165     0.8052     72.6479
03400     0.6976      0.7996     0.8084     72.5760
03500     0.7112      0.7848     0.8086     72.9922
03600     0.6871      0.8101     0.8096     73.5144
03700     0.7080      0.7932     0.8114     75.9033
03800     0.6829      0.7890     0.8115     72.7815
03900     0.7829      0.7574     0.8115     74.5881
04000     0.6869      0.8059     0.8115     74.0453
04100     0.6814      0.8038     0.8151     72.0854
04200     0.6743      0.7932     0.8188     74.4244
04300     0.6217      0.8249     0.8188     72.9789
04400     0.6927      0.7848     0.8188     73.9801
04500     0.6001      0.8207     0.8188     73.2355
04600     0.6986      0.7975     0.8188     73.2489
04700     0.5164      0.8692     0.8188     72.4187
04800     0.6322      0.8017     0.8188     74.0849
04900     0.6586      0.8038     0.8210     72.4073
05000     0.5971      0.8101     0.8210     74.9716
05100     0.6768      0.7954     0.8210     72.7137
05200     0.7145      0.7785     0.8210     73.7500
05300     0.6315      0.7954     0.8216     75.1205
05400     0.6517      0.7954     0.8324     74.3690
05500     0.6673      0.8059     0.8324     74.7251
05600     0.5867      0.8249     0.8324     74.4101
05700     0.6708      0.7722     0.8324     72.8991
05800     0.6727      0.7996     0.8324     71.7251
05900     0.6331      0.8291     0.8324     72.1248
06000     0.6800      0.8122     0.8324     72.9874
06100     0.7249      0.7806     0.8324     72.7800
06200     0.6593      0.8207     0.8324     75.6717
06300     0.5457      0.8460     0.8324     72.8547
06400     0.5946      0.7975     0.8324     76.0488
06500     0.6005      0.8228     0.8324     73.0832
06600     0.6778      0.7975     0.8324     73.2013
06700     0.6975      0.7975     0.8324     76.0302
06800     0.5793      0.8207     0.8324     72.7697
06900     0.5853      0.8333     0.8324     73.7519
07000     0.7203      0.7996     0.8324     73.5366
07100     0.6291      0.8207     0.8324     73.3544
07200     0.6265      0.8059     0.8324     73.5959
07300     0.7361      0.7890     0.8324     74.5670
07400     0.6031      0.8143     0.8324     74.9481
07500     0.5660      0.8207     0.8324     72.2912
07600     0.7059      0.7827     0.8324     72.9063
07700     0.5299      0.8460     0.8324     73.3956
07800     0.6547      0.8038     0.8324     74.1083
07900     0.7562      0.7743     0.8324     73.2303
08000     0.6866      0.7869     0.8324     72.7430
08100     0.6327      0.8122     0.8324     76.6118
08200     0.5791      0.8397     0.8324     73.5376
08300     0.5067      0.8650     0.8324     74.7748
08400     0.6669      0.8017     0.8324     72.1595
08500     0.5967      0.8165     0.8324     75.2598
08600     0.6368      0.8143     0.8324     73.2556
08700     0.5920      0.8017     0.8324     71.8882
08800     0.7127      0.7890     0.8324     72.4404
08900     0.6320      0.8207     0.8324     72.2907
09000     0.6198      0.8038     0.8324     74.1291
09100     0.6422      0.7996     0.8324     73.2654
09200     0.6466      0.7996     0.8324     73.5633
09300     0.6257      0.8228     0.8324     74.6938
09400     0.6446      0.8017     0.8324     73.7947
09500     0.6722      0.7932     0.8324     72.6533
09600     0.5458      0.8418     0.8324     72.1880
09700     0.5880      0.8228     0.8329     74.8636
09800     0.5889      0.8312     0.8338     73.9692
09900     0.5887      0.8207     0.8338     72.7921
10000     0.5690      0.8312     0.8338     72.9307
10100     0.6532      0.8165     0.8359     73.7955
10200     0.6032      0.8207     0.8403     73.4154
10300     0.5671      0.8397     0.8408     73.2601
10400     0.5543      0.8270     0.8408     72.4376
10500     0.5614      0.8312     0.8408     73.6440
10600     0.5736      0.8376     0.8408     73.1012
10700     0.6299      0.7975     0.8408     73.3616
10800     0.5836      0.8207     0.8408     73.7441
10900     0.6314      0.8228     0.8408     72.5503
11000     0.4907      0.8586     0.8408     72.7211
11100     0.5599      0.8249     0.8408     74.0327
11200     0.5294      0.8418     0.8408     74.8200
11300     0.6240      0.8186     0.8413     74.0275
11400     0.5810      0.8312     0.8414     72.7922
11500     0.5995      0.8333     0.8414     73.2811
11600     0.6713      0.8017     0.8414     74.0059
11700     0.6489      0.7869     0.8414     74.1562
11800     0.5798      0.8291     0.8414     74.0823
11900     0.6462      0.8017     0.8414     73.3196
12000     0.5199      0.8502     0.8414     72.3262
12100     0.5537      0.8312     0.8419     71.3890
12200     0.5456      0.8354     0.8438     71.6207
12300     0.5545      0.8333     0.8438     72.8773
12400     0.5852      0.8291     0.8438     74.0351
12500     0.5690      0.8354     0.8438     73.1420
12600     0.5261      0.8523     0.8438     73.1176
12700     0.6163      0.8080     0.8438     71.2579
12800     0.5837      0.8418     0.8438     73.0366
12900     0.5505      0.8460     0.8438     72.3600
13000     0.5586      0.8376     0.8438     72.8988
13100     0.5537      0.8439     0.8438     73.9810
13200     0.5553      0.8249     0.8438     74.1933
13300     0.5461      0.8376     0.8438     73.3567
13400     0.6317      0.8038     0.8438     71.2503
13500     0.6245      0.8143     0.8440     74.4701
13600     0.5523      0.8291     0.8440     74.3846
13700     0.6108      0.8291     0.8440     72.6182
13800     0.5972      0.8481     0.8440     72.1240
13900     0.5085      0.8481     0.8440     73.1643
14000     0.5400      0.8523     0.8440     71.7380
14100     0.5855      0.8228     0.8440     72.5177
14200     0.4976      0.8376     0.8440     72.9789
14300     0.6050      0.8122     0.8442     71.2940
14400     0.5363      0.8376     0.8457     73.7114
14500     0.6493      0.8186     0.8457     74.6336
14600     0.5685      0.8354     0.8457     73.2667
14700     0.5494      0.8333     0.8490     71.5743
14800     0.6139      0.8186     0.8490     72.7233
14900     0.6067      0.8143     0.8490     72.6862
15000     0.5124      0.8460     0.8490     71.9966
15100     0.5614      0.8165     0.8490     74.7307
15200     0.5840      0.8354     0.8490     75.7617
15300     0.5341      0.8439     0.8490     74.1227
15400     0.5979      0.7996     0.8490     72.3510
15500     0.6434      0.8101     0.8490     73.5329
15600     0.5447      0.8502     0.8490     72.6102
15700     0.5675      0.8291     0.8490     72.4595
15800     0.6454      0.8080     0.8490     72.8766
15900     0.5430      0.8228     0.8490     73.4889
16000     0.5573      0.8376     0.8490     71.7703
16100     0.7100      0.7806     0.8490     71.4329
16200     0.6309      0.8249     0.8490     74.2285
16300     0.6265      0.8101     0.8490     73.7316
16400     0.5320      0.8460     0.8490     74.4704
16500     0.5136      0.8523     0.8490     74.3699
16600     0.5300      0.8397     0.8490     73.0399
16700     0.5021      0.8565     0.8490     75.2462
16800     0.5194      0.8544     0.8490     72.8475
16900     0.6346      0.8291     0.8490     74.3921
17000     0.6896      0.7679     0.8490     72.2009
17100     0.5642      0.8101     0.8490     72.7332
17200     0.6289      0.8165     0.8490     73.2295
17300     0.6244      0.8017     0.8490     73.2881
17400     0.6095      0.8249     0.8490     73.5441
17500     0.6693      0.7932     0.8490     73.1321
17600     0.5957      0.8080     0.8490     75.3208
17700     0.5264      0.8397     0.8490     73.6365
17800     0.6014      0.8249     0.8490     74.2163
17900     0.5451      0.8481     0.8490     72.9905
18000     0.5559      0.8439     0.8490     73.1092
18100     0.5468      0.8439     0.8490     72.8473
18200     0.5763      0.8439     0.8490     76.5760
18300     0.5484      0.8291     0.8490     73.2090
18400     0.5600      0.8354     0.8490     76.0186
18500     0.5017      0.8692     0.8490     73.1725
18600     0.6760      0.7975     0.8490     73.2226
18700     0.5711      0.8333     0.8490     72.7620
18800     0.5522      0.8354     0.8490     74.5770
18900     0.6354      0.8207     0.8490     74.3261
19000     0.5264      0.8333     0.8490     76.0402
19100     0.5462      0.8397     0.8490     72.4391
19200     0.5790      0.8165     0.8490     74.1756
19300     0.5597      0.8565     0.8490     72.9197
19400     0.5253      0.8376     0.8490     73.7554
19500     0.4913      0.8586     0.8490     74.5506
19600     0.6098      0.8080     0.8490     73.2227
19700     0.6092      0.8207     0.8490     73.5686
19800     0.6379      0.7911     0.8490     72.6992
19900     0.6018      0.8143     0.8490     72.2153
20000     0.5357      0.8523     0.8490     71.6623
20100     0.5298      0.8376     0.8490     72.4474
20199     0.5773      0.8228     0.8490     71.8675
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     0.5297      0.8608     0.8480     9.8475
00100     0.3734      0.8987     0.8480     72.3262
00200     0.4928      0.8460     0.8480     71.7943
00300     0.5524      0.8418     0.8484     74.0842
00400     0.3895      0.8903     0.8484     72.9749
00500     0.4525      0.8586     0.8484     74.5457
00600     0.4739      0.8671     0.8484     72.7716
00700     0.4353      0.8713     0.8498     73.0510
00800     0.4100      0.8819     0.8498     72.7191
00900     0.5729      0.8418     0.8498     73.1533
01000     0.4794      0.8734     0.8498     72.6759
01100     0.4734      0.8650     0.8498     74.5905
01200     0.5510      0.8312     0.8498     74.6475
01300     0.4769      0.8629     0.8498     75.5197
01400     0.5249      0.8312     0.8498     71.1855
01500     0.4680      0.8608     0.8498     73.0102
01600     0.5212      0.8523     0.8498     73.2753
01700     0.5260      0.8565     0.8498     73.0483
01800     0.4144      0.8945     0.8498     71.6677
01900     0.3800      0.8987     0.8498     72.2599
02000     0.5246      0.8418     0.8498     72.7272
02100     0.4453      0.8650     0.8498     70.4835
02200     0.4417      0.8713     0.8511     72.3851
02300     0.4650      0.8797     0.8511     72.8990
02400     0.4509      0.8565     0.8511     73.1650
02500     0.5350      0.8354     0.8511     74.8644
02600     0.5088      0.8502     0.8511     72.8212
02700     0.4312      0.8776     0.8515     73.6662
02800     0.5166      0.8650     0.8515     72.4420
02900     0.4618      0.8671     0.8515     73.3359
03000     0.4825      0.8523     0.8515     71.9560
03100     0.4971      0.8523     0.8515     72.6856
03200     0.4622      0.8629     0.8515     72.5570
03300     0.4311      0.8650     0.8515     73.0478
03400     0.4955      0.8608     0.8515     74.1052
03500     0.4995      0.8439     0.8515     73.8715
03600     0.4189      0.8755     0.8515     75.2753
03700     0.5419      0.8376     0.8515     72.4747
03800     0.4280      0.8776     0.8515     73.4010
03900     0.4576      0.8650     0.8515     72.6462
04000     0.5018      0.8650     0.8515     73.4056
04100     0.4934      0.8502     0.8515     73.0051
04200     0.4625      0.8608     0.8515     74.0472
04300     0.4491      0.8692     0.8515     72.1474
04400     0.4464      0.8671     0.8515     74.8199
04500     0.4597      0.8586     0.8515     72.9369
04600     0.4588      0.8692     0.8515     73.5850
04700     0.4796      0.8650     0.8515     74.4496
04800     0.4259      0.8755     0.8515     73.6669
04900     0.4770      0.8565     0.8515     72.5100
05000     0.4861      0.8502     0.8515     72.4811
05100     0.4191      0.8755     0.8515     72.5304
05200     0.5231      0.8354     0.8515     71.4455
05300     0.4422      0.8819     0.8515     74.1526
05400     0.5141      0.8544     0.8515     73.8590
05500     0.4557      0.8734     0.8531     72.4809
05600     0.4417      0.8840     0.8531     74.1879
05700     0.5088      0.8460     0.8531     73.9429
05800     0.4335      0.8840     0.8531     73.1350
05900     0.4569      0.8692     0.8531     72.5727
06000     0.4531      0.8586     0.8531     71.4081
06100     0.5093      0.8439     0.8531     73.8945
06200     0.4663      0.8819     0.8531     71.9671
06300     0.5015      0.8608     0.8531     71.7656
06400     0.4605      0.8671     0.8531     72.5644
06500     0.5190      0.8544     0.8531     73.5683
06600     0.4704      0.8713     0.8531     72.5525
06700     0.5177      0.8565     0.8531     73.5701
06800     0.5227      0.8481     0.8531     71.6066
06900     0.3899      0.8776     0.8531     72.2874
07000     0.4433      0.8755     0.8531     72.0884
07100     0.5022      0.8544     0.8531     70.3007
07200     0.4477      0.8840     0.8531     70.0800
07300     0.5410      0.8523     0.8531     70.5066
07400     0.5009      0.8481     0.8531     70.6015
07500     0.3995      0.8903     0.8531     69.2433
07600     0.5158      0.8354     0.8531     69.3750
07700     0.4464      0.8755     0.8531     70.1840
07800     0.4346      0.8797     0.8531     70.6817
07900     0.5068      0.8586     0.8531     70.1658
08000     0.4665      0.8650     0.8531     70.1819
08100     0.4659      0.8523     0.8531     70.5662
08200     0.4154      0.8861     0.8531     68.9480
08300     0.4440      0.8776     0.8531     71.2407
08400     0.4662      0.8629     0.8531     70.1235
08500     0.4209      0.8734     0.8531     70.0353
08600     0.5087      0.8608     0.8531     70.7883
08700     0.4454      0.8945     0.8531     69.3029
08800     0.4722      0.8629     0.8531     69.2922
08900     0.5203      0.8544     0.8537     70.9221
09000     0.4433      0.8945     0.8537     69.3803
09100     0.4493      0.8755     0.8540     69.7466
09200     0.4579      0.8734     0.8540     68.8138
09300     0.5154      0.8397     0.8540     70.5397
09400     0.4999      0.8608     0.8565     70.0455
09500     0.4545      0.8502     0.8565     71.3652
09600     0.4813      0.8608     0.8565     71.4751
09700     0.5647      0.8502     0.8565     70.4106
09800     0.4850      0.8692     0.8565     69.9461
09900     0.5066      0.8460     0.8565     70.4737
Start testing:
Test Accuracy: 0.8396
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=8, quant_actNM=8, quant_inp=8, quant_w=8, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
70d83ae6-ccda-454e-be1b-00238aadc70a
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.8008      0.0823     0.1539     11.3503
00100     1.4026      0.5253     0.5865     71.0703
00200     0.8898      0.7131     0.7627     72.7749
00300     0.6834      0.8101     0.8018     73.8537
00400     0.6201      0.8101     0.8293     73.8558
00500     0.5608      0.8143     0.8433     74.0202
00600     0.4887      0.8650     0.8555     74.1672
00700     0.4659      0.8608     0.8587     74.5893
00800     0.4251      0.8776     0.8671     74.1425
00900     0.4177      0.8903     0.8731     75.5423
01000     0.3370      0.9219     0.8774     75.8898
01100     0.3859      0.8966     0.8798     75.5504
01200     0.4121      0.8776     0.8798     76.4164
01300     0.3057      0.9135     0.8890     75.2079
01400     0.3052      0.9114     0.8890     74.9508
01500     0.3458      0.9030     0.8890     75.8230
01600     0.2900      0.9177     0.8890     75.7421
01700     0.2799      0.9325     0.8890     75.3486
01800     0.2776      0.9304     0.8894     73.9490
01900     0.3277      0.9114     0.8895     75.3976
02000     0.3079      0.9030     0.8968     74.3680
02100     0.2734      0.9262     0.8968     75.4113
02200     0.2878      0.9283     0.8968     75.2449
02300     0.2434      0.9304     0.8986     75.0251
02400     0.3036      0.9093     0.9051     74.3959
02500     0.2505      0.9262     0.9051     75.0034
02600     0.2459      0.9198     0.9051     75.4858
02700     0.2292      0.9325     0.9051     74.1040
02800     0.2482      0.9325     0.9051     75.3051
02900     0.2673      0.9346     0.9051     75.9232
03000     0.2382      0.9262     0.9051     76.2895
03100     0.2517      0.9388     0.9051     74.2640
03200     0.1806      0.9578     0.9051     75.6746
03300     0.2576      0.9135     0.9051     77.1693
03400     0.2439      0.9388     0.9088     76.6630
03500     0.2089      0.9494     0.9088     77.7342
03600     0.1719      0.9536     0.9088     74.5550
03700     0.2171      0.9451     0.9088     75.6523
03800     0.1743      0.9515     0.9088     76.4061
03900     0.2462      0.9304     0.9099     76.4392
04000     0.1857      0.9536     0.9099     74.8160
04100     0.1821      0.9515     0.9099     74.1900
04200     0.1819      0.9557     0.9099     76.9697
04300     0.2590      0.9346     0.9099     77.3644
04400     0.2130      0.9451     0.9099     76.3164
04500     0.1692      0.9536     0.9112     77.0428
04600     0.2695      0.9325     0.9112     77.5493
04700     0.2043      0.9388     0.9160     76.9911
04800     0.1966      0.9451     0.9160     77.2010
04900     0.1941      0.9578     0.9160     76.2261
05000     0.1863      0.9557     0.9160     75.7887
05100     0.1588      0.9599     0.9160     76.5499
05200     0.1844      0.9536     0.9160     75.5240
05300     0.1843      0.9494     0.9160     77.7557
05400     0.1449      0.9641     0.9160     76.9553
05500     0.2013      0.9409     0.9160     76.2775
05600     0.2014      0.9409     0.9160     78.2608
05700     0.1882      0.9494     0.9160     76.8633
05800     0.1699      0.9515     0.9160     76.8303
05900     0.1871      0.9557     0.9160     78.3469
06000     0.2179      0.9451     0.9160     76.3655
06100     0.1904      0.9557     0.9160     75.9247
06200     0.1849      0.9451     0.9160     77.2425
06300     0.1714      0.9641     0.9160     77.3633
06400     0.1956      0.9515     0.9160     80.2025
06500     0.1857      0.9536     0.9160     78.5766
06600     0.2326      0.9367     0.9160     79.1458
06700     0.1571      0.9557     0.9160     77.4405
06800     0.1849      0.9557     0.9160     77.7139
06900     0.1975      0.9536     0.9160     77.9205
07000     0.1621      0.9620     0.9160     76.0617
07100     0.1886      0.9536     0.9160     78.0677
07200     0.1343      0.9662     0.9160     79.0109
07300     0.1619      0.9620     0.9160     76.2864
07400     0.1908      0.9578     0.9160     77.5705
07500     0.1999      0.9409     0.9160     80.5894
07600     0.1892      0.9557     0.9160     79.0405
07700     0.1613      0.9599     0.9160     76.9369
07800     0.1438      0.9620     0.9160     77.5444
07900     0.1259      0.9726     0.9160     77.6795
08000     0.2389      0.9388     0.9160     76.7992
08100     0.1885      0.9536     0.9160     76.6487
08200     0.2072      0.9409     0.9160     75.9737
08300     0.1779      0.9515     0.9160     77.5775
08400     0.1683      0.9494     0.9160     77.8950
08500     0.1569      0.9641     0.9160     77.9468
08600     0.2007      0.9494     0.9160     76.6871
08700     0.1568      0.9557     0.9160     79.0135
08800     0.1756      0.9557     0.9160     79.8923
08900     0.1762      0.9536     0.9160     79.0468
09000     0.1532      0.9557     0.9160     78.7246
09100     0.1737      0.9599     0.9160     77.3158
09200     0.1966      0.9473     0.9160     79.9474
09300     0.1515      0.9641     0.9160     77.5310
09400     0.1929      0.9620     0.9160     76.1831
09500     0.1451      0.9705     0.9160     79.6195
09600     0.1495      0.9620     0.9160     81.1145
09700     0.1723      0.9578     0.9160     79.3733
09800     0.1844      0.9515     0.9160     80.3195
09900     0.1187      0.9726     0.9160     80.5285
10000     0.1339      0.9662     0.9160     80.2009
10100     0.1177      0.9747     0.9160     80.4034
10200     0.1562      0.9599     0.9160     80.2713
10300     0.1366      0.9726     0.9160     80.5226
10400     0.1022      0.9789     0.9160     79.6189
10500     0.1458      0.9705     0.9160     76.2866
10600     0.1164      0.9747     0.9160     74.1068
10700     0.1720      0.9705     0.9160     75.8312
10800     0.1350      0.9705     0.9160     77.3110
10900     0.1622      0.9599     0.9174     75.2240
11000     0.1422      0.9662     0.9174     75.0078
11100     0.1308      0.9747     0.9174     76.4063
11200     0.1490      0.9599     0.9197     76.1269
11300     0.1386      0.9662     0.9197     75.6946
11400     0.1055      0.9747     0.9197     80.5067
11500     0.1579      0.9494     0.9197     79.1745
11600     0.1667      0.9620     0.9197     78.6702
11700     0.1330      0.9641     0.9197     80.3214
11800     0.1279      0.9705     0.9197     79.4563
11900     0.1307      0.9620     0.9197     79.9613
12000     0.1756      0.9557     0.9197     81.8587
12100     0.1592      0.9578     0.9197     80.3971
12200     0.1237      0.9684     0.9197     80.6859
12300     0.1695      0.9494     0.9197     80.5184
12400     0.1216      0.9705     0.9197     81.1903
12500     0.1154      0.9726     0.9197     80.0523
12600     0.1113      0.9641     0.9197     81.6706
12700     0.1175      0.9705     0.9197     80.2140
12800     0.1695      0.9620     0.9197     79.9125
12900     0.1181      0.9684     0.9197     81.7020
13000     0.1273      0.9726     0.9197     79.7941
13100     0.1308      0.9662     0.9197     80.5386
13200     0.1279      0.9705     0.9197     79.2353
13300     0.1475      0.9536     0.9197     80.2221
13400     0.1539      0.9641     0.9197     81.4470
13500     0.1017      0.9831     0.9197     79.7598
13600     0.1608      0.9578     0.9197     79.7774
13700     0.1279      0.9705     0.9197     81.0572
13800     0.1189      0.9641     0.9197     80.4372
13900     0.1228      0.9684     0.9197     81.5118
14000     0.1519      0.9684     0.9197     81.0993
14100     0.1145      0.9831     0.9197     80.2399
14200     0.1281      0.9747     0.9197     80.6527
14300     0.1233      0.9747     0.9197     79.1125
14400     0.1117      0.9705     0.9197     80.7880
14500     0.1167      0.9726     0.9197     79.6019
14600     0.1318      0.9726     0.9197     81.1338
14700     0.1208      0.9768     0.9197     80.2501
14800     0.1056      0.9747     0.9197     80.4432
14900     0.1022      0.9768     0.9197     79.8289
15000     0.1100      0.9789     0.9197     81.4898
15100     0.1205      0.9768     0.9197     79.7788
15200     0.1236      0.9768     0.9197     81.0539
15300     0.1099      0.9768     0.9197     81.2557
15400     0.1199      0.9705     0.9197     80.2136
15500     0.1001      0.9768     0.9197     80.4867
15600     0.0930      0.9831     0.9197     80.8559
15700     0.1394      0.9662     0.9197     80.4676
15800     0.1285      0.9726     0.9197     80.9687
15900     0.1046      0.9768     0.9197     80.3317
16000     0.1184      0.9705     0.9197     80.1386
16100     0.1753      0.9620     0.9197     80.9999
16200     0.1307      0.9662     0.9197     81.0476
16300     0.1034      0.9768     0.9197     81.1216
16400     0.1445      0.9684     0.9197     80.0511
16500     0.0899      0.9873     0.9197     79.9717
16600     0.1240      0.9662     0.9197     80.6526
16700     0.1466      0.9557     0.9197     80.0024
16800     0.1162      0.9705     0.9197     80.8820
16900     0.1354      0.9726     0.9197     80.9684
17000     0.0985      0.9789     0.9197     79.9206
17100     0.1183      0.9705     0.9197     81.0601
17200     0.1215      0.9684     0.9197     79.8327
17300     0.1167      0.9641     0.9197     79.7335
17400     0.1216      0.9747     0.9197     81.0102
17500     0.1105      0.9705     0.9197     81.4071
17600     0.1057      0.9726     0.9197     81.3121
17700     0.1434      0.9726     0.9197     80.1847
17800     0.1264      0.9747     0.9197     80.7742
17900     0.1276      0.9726     0.9197     81.5266
18000     0.1353      0.9726     0.9197     80.2056
18100     0.1121      0.9662     0.9197     80.0361
18200     0.0699      0.9916     0.9197     81.0144
18300     0.1153      0.9726     0.9197     80.2488
18400     0.1207      0.9705     0.9197     81.2236
18500     0.1069      0.9810     0.9197     80.1679
18600     0.1189      0.9599     0.9197     80.7318
18700     0.1301      0.9641     0.9197     79.6486
18800     0.1050      0.9768     0.9197     81.5447
18900     0.1438      0.9578     0.9197     80.3609
19000     0.0943      0.9810     0.9197     79.8756
19100     0.0896      0.9852     0.9197     80.3947
19200     0.0951      0.9789     0.9197     81.4331
19300     0.1397      0.9705     0.9197     81.5292
19400     0.1176      0.9684     0.9197     81.2762
19500     0.0923      0.9852     0.9197     81.0815
19600     0.1199      0.9768     0.9197     80.9060
19700     0.1333      0.9705     0.9197     80.9037
19800     0.1208      0.9768     0.9197     80.8440
19900     0.1085      0.9726     0.9197     79.8915
20000     0.1012      0.9831     0.9197     80.5923
20100     0.1051      0.9768     0.9197     75.0871
20199     0.1422      0.9641     0.9197     75.3409
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     0.1265      0.9641     0.9060     10.2473
00100     0.0958      0.9810     0.9060     76.2041
00200     0.0918      0.9789     0.9061     72.5774
00300     0.0676      0.9895     0.9061     73.4120
00400     0.0774      0.9895     0.9061     74.1923
00500     0.0941      0.9789     0.9061     74.7780
00600     0.1018      0.9810     0.9064     73.2681
00700     0.1158      0.9789     0.9064     75.4178
00800     0.0894      0.9831     0.9064     74.2395
00900     0.0956      0.9831     0.9064     74.3174
01000     0.0960      0.9810     0.9064     74.3469
01100     0.0604      0.9937     0.9064     76.4327
01200     0.0861      0.9810     0.9064     74.2320
01300     0.0970      0.9810     0.9064     73.0948
01400     0.1171      0.9726     0.9064     75.7045
01500     0.1206      0.9747     0.9064     77.2033
01600     0.1132      0.9747     0.9064     77.2428
01700     0.0772      0.9852     0.9064     75.1618
01800     0.0848      0.9789     0.9065     77.8139
01900     0.0856      0.9810     0.9065     77.8154
02000     0.0990      0.9726     0.9065     77.1876
02100     0.0848      0.9810     0.9065     77.3692
02200     0.0827      0.9852     0.9065     77.7192
02300     0.1283      0.9705     0.9065     79.5725
02400     0.0694      0.9916     0.9065     76.4703
02500     0.0721      0.9937     0.9065     78.1116
02600     0.0828      0.9852     0.9065     79.7007
02700     0.1027      0.9789     0.9066     81.5604
02800     0.1068      0.9747     0.9066     79.7316
02900     0.0872      0.9852     0.9066     80.2416
03000     0.0834      0.9789     0.9066     80.2871
03100     0.0862      0.9852     0.9066     77.1045
03200     0.0810      0.9831     0.9066     78.0913
03300     0.1048      0.9768     0.9066     80.2516
03400     0.1186      0.9747     0.9066     79.8392
03500     0.1482      0.9684     0.9066     80.2145
03600     0.0780      0.9831     0.9066     80.2992
03700     0.0761      0.9852     0.9066     80.3530
03800     0.0733      0.9873     0.9066     79.2409
03900     0.0747      0.9895     0.9066     79.4675
04000     0.1019      0.9768     0.9066     77.8071
04100     0.0887      0.9831     0.9066     77.2023
04200     0.1078      0.9747     0.9066     76.6918
04300     0.0707      0.9873     0.9066     77.9691
04400     0.0789      0.9895     0.9066     78.6175
04500     0.0836      0.9852     0.9066     77.8051
04600     0.0914      0.9789     0.9066     78.3986
04700     0.0955      0.9831     0.9066     80.3175
04800     0.0842      0.9831     0.9066     80.8004
04900     0.0859      0.9852     0.9066     79.7036
05000     0.0899      0.9831     0.9066     80.1843
05100     0.1081      0.9831     0.9066     80.2425
05200     0.0802      0.9831     0.9066     80.5158
05300     0.0786      0.9852     0.9066     79.9042
05400     0.0998      0.9768     0.9066     78.4810
05500     0.0680      0.9873     0.9066     73.7106
05600     0.0703      0.9873     0.9066     75.2259
05700     0.0787      0.9895     0.9066     75.5200
05800     0.0706      0.9873     0.9066     75.5175
05900     0.0800      0.9852     0.9066     77.0499
06000     0.1012      0.9747     0.9066     76.9417
06100     0.0742      0.9852     0.9066     75.1621
06200     0.0978      0.9831     0.9066     75.7034
06300     0.1008      0.9789     0.9066     77.2997
06400     0.0788      0.9852     0.9066     76.8247
06500     0.0672      0.9895     0.9066     76.4190
06600     0.0839      0.9810     0.9066     76.6146
06700     0.1063      0.9789     0.9066     76.8730
06800     0.0972      0.9789     0.9066     76.9826
06900     0.0703      0.9873     0.9066     77.2762
07000     0.0818      0.9831     0.9066     77.8416
07100     0.0920      0.9831     0.9066     77.3902
07200     0.0903      0.9810     0.9066     77.6397
07300     0.0991      0.9789     0.9066     78.2222
07400     0.0735      0.9873     0.9066     77.9719
07500     0.1068      0.9747     0.9066     78.8987
07600     0.0755      0.9852     0.9066     77.8331
07700     0.0912      0.9852     0.9066     77.5718
07800     0.0821      0.9831     0.9066     79.7370
07900     0.1265      0.9768     0.9066     78.7182
08000     0.0948      0.9789     0.9066     78.4577
08100     0.1282      0.9684     0.9066     79.2417
08200     0.0733      0.9895     0.9066     78.9820
08300     0.0555      0.9937     0.9066     78.8557
08400     0.0931      0.9831     0.9066     78.7182
08500     0.0900      0.9831     0.9066     78.3153
08600     0.0853      0.9831     0.9066     78.5224
08700     0.0661      0.9873     0.9066     77.5491
08800     0.1208      0.9747     0.9066     78.5169
08900     0.0845      0.9852     0.9066     78.2289
09000     0.0860      0.9768     0.9066     78.2753
09100     0.0915      0.9789     0.9066     78.4572
09200     0.0785      0.9852     0.9066     78.8664
09300     0.0852      0.9831     0.9066     78.4100
09400     0.1163      0.9768     0.9066     79.3620
09500     0.0808      0.9810     0.9066     79.6858
09600     0.0942      0.9831     0.9066     78.0875
09700     0.0837      0.9852     0.9066     77.4185
09800     0.0957      0.9684     0.9066     78.6539
09900     0.0704      0.9831     0.9066     79.5333
Start testing:
Test Accuracy: 0.8918
