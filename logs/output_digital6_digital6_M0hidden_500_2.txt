Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=494, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=6, quant_actNM=6, quant_inp=6, quant_w=6, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
c226e5cb-849f-47bf-85d3-aaea507f36c9
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 290, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 211, in forward
    part2 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(hx, self.a11), self.ib, self.a11), self.weight_hh * w_mask, self.bias_hh, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 146, in forward
    wq = quant_pass(weight, wb, torch.tensor([1.]).to(input.device))
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 94, in forward
    x_scaled = x/x_range
RuntimeError: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 10.76 GiB total capacity; 8.81 GiB already allocated; 3.12 MiB free; 9.80 GiB reserved in total by PyTorch)
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=100, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=494, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=6, quant_actNM=6, quant_inp=6, quant_w=6, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
b410b5c4-cf19-4e7e-ac8a-753f5bdfa39e
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 167, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 290, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 211, in forward
    part2 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(hx, self.a11), self.ib, self.a11), self.weight_hh * w_mask, self.bias_hh, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 146, in forward
    wq = quant_pass(weight, wb, torch.tensor([1.]).to(input.device))
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 98, in forward
    x01q =  torch.round(x01 * step_d ) / step_d
RuntimeError: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 10.76 GiB total capacity; 8.64 GiB already allocated; 11.12 MiB free; 9.79 GiB reserved in total by PyTorch)
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=100, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=494, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=6, quant_actNM=6, quant_inp=6, quant_w=6, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
09b35af0-7f92-4b1e-a8c9-91c3ecb5a964
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.9998      0.0500     0.0850     19.1376
00100     1.5130      0.5200     0.0969     66.8944
00200     0.9340      0.7100     0.1040     66.5975
00300     1.0827      0.6700     0.1040     66.0523
00400     0.7090      0.8000     0.1040     65.6052
00500     0.7894      0.7200     0.1115     70.4360
00600     0.7302      0.7500     0.1115     67.6539
00700     0.6310      0.8400     0.1115     66.9327
00800     0.4364      0.8500     0.1115     65.6544
00900     0.4994      0.8200     0.1115     66.7429
01000     0.4417      0.8600     0.1141     70.3276
01100     0.6012      0.8400     0.1141     67.8584
01200     0.5644      0.8500     0.1213     65.6440
01300     0.5459      0.8800     0.1231     68.0472
01400     0.3958      0.8900     0.1231     69.3609
01500     0.4380      0.8500     0.1231     65.0056
01600     0.4997      0.8800     0.1269     66.8677
01700     0.3907      0.8800     0.1269     65.6142
01800     0.3718      0.8900     0.1269     64.6849
01900     0.3614      0.8700     0.1269     67.9726
02000     0.4636      0.8600     0.1429     65.7323
02100     0.2993      0.9400     0.1553     67.0857
02200     0.6974      0.8200     0.1553     65.3276
02300     0.4098      0.8600     0.1553     68.6309
02400     0.3683      0.8900     0.1553     69.3174
02500     0.3947      0.8800     0.1553     67.3625
02600     0.5380      0.8700     0.1553     69.2501
02700     0.3771      0.8800     0.1553     66.1306
02800     0.4274      0.8800     0.1553     69.0012
02900     0.3335      0.9000     0.1553     66.0230
03000     0.3623      0.8800     0.1557     66.5369
03100     0.3490      0.9200     0.1570     67.5000
03200     0.4339      0.8700     0.1570     69.7173
03300     0.4455      0.9100     0.1650     66.6665
03400     0.3554      0.9100     0.1650     65.3126
03500     0.4144      0.8800     0.1676     69.4457
03600     0.3067      0.9100     0.1676     65.8304
03700     0.2955      0.9200     0.1676     67.7711
03800     0.4417      0.8900     0.1676     69.2237
03900     0.2626      0.9400     0.1802     65.7589
04000     0.3626      0.9100     0.1802     67.6424
04100     0.3387      0.8900     0.1802     66.5239
04200     0.2324      0.9200     0.1802     66.2656
04300     0.2443      0.9400     0.1802     67.5336
04400     0.1860      0.9600     0.2043     68.2933
04500     0.2646      0.9200     0.2043     66.6739
04600     0.3545      0.9000     0.2058     67.9367
04700     0.1943      0.9600     0.2058     66.9476
04800     0.2507      0.9300     0.2159     67.5441
04900     0.2450      0.9100     0.2159     67.1890
05000     0.2045      0.9300     0.2159     66.3169
05100     0.2184      0.9300     0.2159     68.0310
05200     0.3630      0.9000     0.2159     69.4901
05300     0.2308      0.9200     0.2316     67.3173
05400     0.2429      0.9200     0.2316     68.2838
05500     0.3621      0.8800     0.2370     68.4043
05600     0.3416      0.9000     0.2370     67.2895
05700     0.2814      0.9100     0.2370     66.5307
05800     0.3317      0.9000     0.2370     67.3333
05900     0.2956      0.9300     0.2370     67.2907
06000     0.2486      0.9400     0.2502     68.3265
06100     0.2135      0.9300     0.2588     67.9196
06200     0.2190      0.9500     0.2682     68.5033
06300     0.2248      0.9600     0.2682     67.2332
06400     0.2404      0.9200     0.2682     67.6808
06500     0.1823      0.9500     0.2682     69.4134
06600     0.3120      0.9100     0.2777     69.3659
06700     0.2033      0.9500     0.2777     67.2144
06800     0.3360      0.8900     0.2777     67.5152
06900     0.4027      0.8700     0.2777     66.8965
07000     0.3315      0.8900     0.2777     66.9103
07100     0.1585      0.9500     0.2777     67.3492
07200     0.3273      0.9200     0.2848     67.7690
07300     0.2200      0.9500     0.2848     70.0170
07400     0.3565      0.8900     0.2848     66.5418
07500     0.3019      0.8900     0.3147     66.5715
07600     0.2690      0.9200     0.3147     66.4089
07700     0.1363      0.9600     0.3147     68.1781
07800     0.2623      0.9300     0.3147     68.0237
07900     0.2890      0.9100     0.3147     68.1413
08000     0.3796      0.9100     0.3147     66.4635
08100     0.2357      0.9600     0.3147     68.7068
08200     0.1632      0.9600     0.3147     68.7771
08300     0.2474      0.9500     0.3695     67.5255
08400     0.2458      0.9500     0.3695     68.1268
08500     0.3600      0.9200     0.3695     67.4092
08600     0.3568      0.8900     0.3695     69.9160
08700     0.2380      0.9300     0.3695     67.0940
08800     0.2207      0.9500     0.3695     68.4155
08900     0.1458      0.9800     0.3695     68.3549
09000     0.2274      0.9100     0.3695     67.4192
09100     0.1533      0.9600     0.3695     69.0820
09200     0.2500      0.9200     0.3695     67.0860
09300     0.3137      0.9000     0.3695     66.3117
09400     0.4002      0.8700     0.3854     67.7062
09500     0.1988      0.9400     0.3854     67.7023
09600     0.3267      0.9000     0.4150     67.8664
09700     0.2307      0.9500     0.4150     66.4387
09800     0.3222      0.9200     0.4150     66.0267
09900     0.1787      0.9400     0.4150     66.6642
10000     0.1783      0.9700     0.4150     67.2563
10100     0.1601      0.9700     0.4150     66.9832
10200     0.1600      0.9400     0.4150     66.6664
10300     0.1795      0.9500     0.4160     69.0322
10400     0.2639      0.9200     0.4377     68.0721
10500     0.2055      0.9300     0.4377     66.9872
10600     0.1705      0.9500     0.4377     68.2052
10700     0.1078      0.9800     0.4377     67.9494
10800     0.1884      0.9600     0.4377     69.5246
10900     0.1768      0.9400     0.4399     67.3438
11000     0.1871      0.9600     0.4399     67.3029
11100     0.1598      0.9500     0.4399     66.8003
11200     0.1995      0.9500     0.4399     67.0000
11300     0.1893      0.9400     0.4399     67.1970
11400     0.1483      0.9600     0.4399     68.6831
11500     0.1437      0.9700     0.4399     66.4664
11600     0.1040      0.9800     0.4399     66.9768
11700     0.0833      0.9900     0.4399     67.2560
11800     0.1548      0.9500     0.4399     68.9730
11900     0.1929      0.9400     0.4399     67.9630
12000     0.1566      0.9600     0.4399     66.3056
12100     0.1244      0.9700     0.4399     69.4007
12200     0.1092      0.9700     0.4399     68.0308
12300     0.1882      0.9500     0.4399     68.3873
12400     0.2163      0.9500     0.4399     68.2026
12500     0.2097      0.9500     0.4399     67.6306
12600     0.1718      0.9600     0.4399     66.8188
12700     0.1076      0.9800     0.4441     67.6066
12800     0.1881      0.9300     0.4441     66.9217
12900     0.1968      0.9300     0.4441     65.2217
13000     0.1090      0.9700     0.4441     66.7533
13100     0.1453      0.9700     0.4441     68.2778
13200     0.0948      0.9800     0.4441     66.8707
13300     0.1628      0.9400     0.4441     67.6038
13400     0.1055      0.9700     0.4441     67.4412
13500     0.2518      0.9300     0.4441     67.7292
13600     0.3318      0.9100     0.4441     69.3910
13700     0.2529      0.9500     0.4441     68.2489
13800     0.2017      0.9600     0.4441     68.2027
13900     0.0727      0.9900     0.4441     68.8358
14000     0.1898      0.9600     0.4441     67.1168
14100     0.1581      0.9700     0.4441     66.3526
14200     0.1329      0.9700     0.4441     69.7152
14300     0.1077      0.9700     0.4441     67.4382
14400     0.1965      0.9300     0.4441     68.8839
14500     0.1258      0.9700     0.4441     67.0236
14600     0.1259      0.9600     0.4441     66.7074
14700     0.1318      0.9700     0.4441     66.4522
14800     0.1856      0.9500     0.4441     70.8449
14900     0.1221      0.9800     0.4441     67.1117
15000     0.1104      0.9800     0.4441     67.3959
15100     0.1455      0.9600     0.4441     67.2487
15200     0.1522      0.9600     0.4441     70.7582
15300     0.1402      0.9600     0.4441     67.8614
15400     0.1311      0.9700     0.4441     67.7156
15500     0.2353      0.9400     0.4441     66.8323
15600     0.0738      0.9900     0.4441     66.9568
15700     0.1837      0.9400     0.4441     67.4534
15800     0.1494      0.9700     0.4441     66.1055
15900     0.0961      0.9800     0.4441     66.9816
16000     0.2263      0.9300     0.4441     67.6424
16100     0.2243      0.9400     0.4441     67.1206
16200     0.1206      0.9800     0.4441     66.1868
16300     0.0983      0.9800     0.4441     65.9471
16400     0.0628      0.9900     0.4441     66.8939
16500     0.2081      0.9300     0.4441     67.0805
16600     0.1167      0.9600     0.4441     66.9331
16700     0.1702      0.9700     0.4441     68.9749
16800     0.0633      0.9900     0.4441     66.5039
16900     0.0755      0.9900     0.4441     67.6942
17000     0.1311      0.9800     0.4441     65.9984
17100     0.2082      0.9700     0.4441     69.0695
17200     0.2571      0.9300     0.4441     67.8908
17300     0.1474      0.9700     0.4441     68.2089
17400     0.1370      0.9700     0.4441     67.4313
17500     0.1819      0.9600     0.4441     68.1082
17600     0.1547      0.9700     0.4441     66.8753
17700     0.1286      0.9500     0.4441     66.5377
17800     0.1985      0.9600     0.4441     67.8706
17900     0.1955      0.9500     0.4441     67.5787
18000     0.0994      0.9900     0.4441     66.7855
18100     0.0909      0.9900     0.4441     66.5491
18200     0.0687      0.9900     0.4441     66.2001
18300     0.0950      0.9800     0.4441     67.8529
18400     0.0947      0.9800     0.4441     66.0322
18500     0.1483      0.9700     0.4441     66.8001
18600     0.1173      0.9700     0.4441     67.1971
18700     0.1083      0.9800     0.4441     66.5956
18800     0.1255      0.9800     0.4441     66.1449
18900     0.0845      0.9800     0.4441     67.7493
19000     0.1570      0.9600     0.4441     67.6188
19100     0.1399      0.9700     0.4441     66.2691
19200     0.1563      0.9500     0.4441     66.3666
19300     0.1856      0.9600     0.4441     66.5215
19400     0.1363      0.9700     0.4441     66.3799
19500     0.1033      0.9700     0.4441     66.6524
19600     0.1318      0.9700     0.4441     66.2890
19700     0.0922      0.9800     0.4441     67.3316
19800     0.1580      0.9800     0.4441     67.1679
19900     0.1383      0.9600     0.4441     67.3484
20000     0.3027      0.9500     0.4441     66.8241
20100     0.1338      0.9700     0.4441     65.8553
20199     0.1135      0.9900     0.4441     66.1540
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     3.4692      0.3700     0.4100     19.6097
00100     1.4572      0.7100     0.6553     67.6835
00200     0.9991      0.7500     0.7300     67.8388
00300     1.3016      0.6800     0.7543     66.8831
00400     0.7002      0.8100     0.7624     68.5138
00500     0.8990      0.7200     0.7917     68.5653
00600     0.9371      0.7600     0.7974     67.7138
00700     0.7993      0.8100     0.8017     68.5414
00800     0.9083      0.7700     0.8017     65.9800
00900     0.6060      0.8200     0.8107     66.7114
01000     1.2171      0.7500     0.8118     69.5323
01100     0.7790      0.8200     0.8195     67.9109
01200     0.8332      0.7600     0.8249     68.7507
01300     0.7602      0.8200     0.8249     67.9215
01400     0.6634      0.8400     0.8249     69.4666
01500     0.4939      0.8500     0.8249     66.4569
01600     0.5390      0.8900     0.8302     68.3409
01700     0.9684      0.7300     0.8302     70.0497
01800     0.8217      0.8400     0.8302     65.7101
01900     0.4416      0.8600     0.8302     67.4050
02000     0.5625      0.8700     0.8366     67.3505
02100     0.7316      0.7800     0.8366     67.2561
02200     0.6577      0.8400     0.8366     69.2629
02300     0.3164      0.8900     0.8366     67.3230
02400     0.8778      0.8100     0.8366     66.6092
02500     0.4995      0.8600     0.8366     67.5053
02600     0.4358      0.9000     0.8439     67.0116
02700     0.4315      0.8900     0.8439     67.5266
02800     0.5530      0.8300     0.8439     66.4485
02900     0.7622      0.8400     0.8439     66.0650
03000     0.7064      0.8400     0.8459     66.6185
03100     0.7090      0.8000     0.8478     68.0420
03200     0.7268      0.8100     0.8478     66.7095
03300     0.4775      0.8700     0.8481     68.2440
03400     0.5220      0.8700     0.8481     67.1467
03500     0.6559      0.8400     0.8481     68.4447
03600     0.6212      0.8500     0.8481     66.8431
03700     0.4986      0.8800     0.8481     67.5644
03800     0.4373      0.8600     0.8492     67.3679
03900     0.4950      0.8700     0.8492     67.4618
04000     0.4341      0.8900     0.8542     67.3551
04100     0.5045      0.8700     0.8542     66.2234
04200     0.3642      0.8600     0.8542     66.6878
04300     0.7707      0.7900     0.8542     66.6622
04400     0.5888      0.8400     0.8542     68.0566
04500     0.8421      0.8300     0.8542     67.4175
04600     0.2783      0.9300     0.8542     68.6714
04700     0.3878      0.8900     0.8542     67.7510
04800     0.6170      0.8800     0.8542     65.8232
04900     0.4395      0.8600     0.8542     64.5784
05000     0.7674      0.8100     0.8542     64.7906
05100     0.2518      0.9100     0.8542     66.7420
05200     0.4775      0.8500     0.8542     62.9406
05300     0.5068      0.8500     0.8596     62.0637
05400     0.7299      0.8200     0.8596     66.0998
05500     0.3731      0.8800     0.8596     60.3480
05600     0.5674      0.8600     0.8596     62.5409
05700     0.4337      0.8500     0.8596     69.8096
05800     0.3725      0.8900     0.8596     61.7446
05900     0.7602      0.7800     0.8596     59.3270
06000     0.5097      0.8400     0.8596     60.4663
06100     0.4535      0.8500     0.8596     64.6368
06200     0.3507      0.9000     0.8596     59.9621
06300     0.3553      0.9100     0.8596     59.4811
06400     0.5124      0.8800     0.8609     60.0903
06500     0.7649      0.8100     0.8609     62.8608
06600     0.4464      0.8900     0.8609     62.3609
06700     0.6494      0.8200     0.8650     60.1807
06800     0.4487      0.8500     0.8650     62.3000
06900     0.6133      0.8000     0.8650     63.6220
07000     0.6494      0.8400     0.8650     62.9671
07100     0.3804      0.8900     0.8708     61.6439
07200     0.2627      0.9500     0.8708     59.3705
07300     0.4662      0.8800     0.8708     61.0444
07400     0.5083      0.8900     0.8708     62.2519
07500     0.1927      0.9600     0.8708     67.6624
07600     0.5875      0.8400     0.8708     65.0100
07700     0.6114      0.8400     0.8708     63.6591
07800     0.6434      0.8700     0.8708     63.1529
07900     0.3200      0.9200     0.8708     60.9190
08000     0.3360      0.9000     0.8708     59.1811
08100     0.4215      0.8600     0.8708     65.2699
08200     0.4028      0.8800     0.8708     68.3108
08300     0.4563      0.8500     0.8708     68.4974
08400     0.6930      0.8200     0.8708     68.0942
08500     0.3333      0.9300     0.8710     60.7102
08600     0.4862      0.9100     0.8710     62.8529
08700     0.4368      0.8900     0.8710     65.0955
08800     0.4552      0.8900     0.8710     63.9226
08900     0.3781      0.8900     0.8710     63.7658
09000     0.5967      0.8500     0.8710     65.0801
09100     0.2644      0.9300     0.8710     63.2188
09200     0.2668      0.9100     0.8710     62.9104
09300     0.5688      0.8400     0.8710     65.0610
09400     0.3118      0.9100     0.8710     62.3836
09500     0.2917      0.8900     0.8710     64.1627
09600     0.5906      0.8600     0.8710     61.7530
09700     0.4035      0.9100     0.8710     61.7961
09800     0.6132      0.8100     0.8710     62.1251
09900     0.3060      0.9200     0.8710     62.5221
Start testing:
Test Accuracy: 0.8981
