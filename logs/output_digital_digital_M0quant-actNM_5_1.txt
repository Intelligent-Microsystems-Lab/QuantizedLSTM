Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=5, quant_actNM=5, quant_inp=5, quant_w=5, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
35c93f18-ff1f-4f92-8b50-22a009d04fee
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, 1.)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 82, in forward
    if len(x_range) > 1:
TypeError: object of type 'float' has no len()
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=5, quant_actNM=5, quant_inp=5, quant_w=5, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
444d00dc-4cc6-496e-887b-792be4964416
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, torch.tensor([1.]))
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 94, in forward
    x_scaled = x/x_range
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=5, quant_actNM=5, quant_inp=5, quant_w=5, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
db7bfd4d-11ea-4ec2-98a5-f725feb3ff7d
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, torch.tensor([1.]).to(input.device))
TypeError: 'torch.device' object is not callable
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=5, quant_actNM=5, quant_inp=5, quant_w=5, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
09b5b70c-433e-4ead-b4f1-54f49f434518
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.7480      0.0844     0.0785     11.1480
00100     2.3802      0.1835     0.1995     75.2000
00200     2.2043      0.2827     0.3206     72.2734
00300     1.8401      0.4072     0.4124     74.3922
00400     1.8455      0.4135     0.4770     70.9782
00500     1.6817      0.4388     0.4997     69.6205
00600     1.6250      0.4705     0.5154     70.8901
00700     1.4028      0.5464     0.5468     68.3580
00800     1.4282      0.5422     0.5654     69.8317
00900     1.2549      0.5738     0.5769     69.7728
01000     1.3656      0.5570     0.5851     67.5196
01100     1.3823      0.5527     0.6081     68.9624
01200     1.3640      0.5717     0.6234     71.1960
01300     1.2977      0.6013     0.6234     70.7974
01400     1.2746      0.5907     0.6330     69.2427
01500     1.1557      0.6097     0.6330     70.4826
01600     1.3256      0.5844     0.6518     68.5743
01700     1.2686      0.6055     0.6518     68.9842
01800     1.2181      0.6055     0.6518     69.9059
01900     1.2525      0.5865     0.6526     70.7544
02000     1.1388      0.6477     0.6567     70.7743
02100     1.1203      0.6224     0.6567     69.8201
02200     1.1620      0.6371     0.6650     68.7368
02300     1.1326      0.6498     0.6650     68.7460
02400     1.1696      0.6582     0.6928     70.0055
02500     1.1720      0.6414     0.6928     68.7432
02600     1.1099      0.6751     0.6980     70.3713
02700     1.2027      0.6224     0.6980     69.7751
02800     1.1822      0.6540     0.6980     69.9713
02900     1.1566      0.6456     0.6980     69.5106
03000     1.1616      0.6224     0.6980     70.2167
03100     1.1701      0.6371     0.6980     69.5023
03200     1.0940      0.6751     0.6990     69.3944
03300     1.1282      0.6709     0.6990     69.1545
03400     1.1323      0.6646     0.7023     70.0596
03500     1.1453      0.6477     0.7023     69.3109
03600     1.1874      0.6329     0.7023     69.5248
03700     1.0856      0.6624     0.7023     71.0458
03800     1.0457      0.7131     0.7023     68.6107
03900     1.1832      0.6308     0.7023     67.2831
04000     0.9881      0.6857     0.7037     69.1526
04100     1.1421      0.6435     0.7037     68.9889
04200     1.0859      0.6582     0.7037     69.0862
04300     1.1235      0.6582     0.7072     71.9757
04400     0.9697      0.6814     0.7072     70.4588
04500     1.0957      0.6646     0.7072     68.8906
04600     1.0054      0.6603     0.7132     69.6617
04700     1.1818      0.6519     0.7132     68.9197
04800     1.1679      0.6582     0.7132     69.2261
04900     1.1600      0.6477     0.7132     70.4962
05000     1.0594      0.6793     0.7132     68.2291
05100     1.0856      0.6561     0.7194     69.2586
05200     1.0261      0.6814     0.7194     69.9501
05300     1.0990      0.6646     0.7194     70.1324
05400     1.1832      0.6266     0.7194     68.3540
05500     1.0759      0.6540     0.7194     70.3887
05600     1.0685      0.6709     0.7194     69.8865
05700     1.0498      0.6730     0.7194     70.0884
05800     1.0165      0.6835     0.7194     70.8103
05900     1.1254      0.6456     0.7194     69.5543
06000     1.0678      0.6878     0.7194     69.6789
06100     1.0656      0.6730     0.7194     69.1828
06200     1.0977      0.6730     0.7225     69.0539
06300     1.0331      0.6814     0.7252     68.6317
06400     1.0156      0.6857     0.7252     69.5714
06500     1.0406      0.6772     0.7252     69.6575
06600     1.0271      0.6835     0.7252     69.5508
06700     1.0058      0.6751     0.7252     69.8640
06800     1.1076      0.6477     0.7252     69.1331
06900     1.1453      0.6519     0.7252     71.1070
07000     1.0236      0.6878     0.7252     70.8704
07100     1.0748      0.6814     0.7252     69.8064
07200     1.0598      0.6582     0.7262     70.3249
07300     0.9581      0.7046     0.7262     68.7347
07400     1.0854      0.6667     0.7262     69.7756
07500     1.1803      0.6456     0.7262     69.4891
07600     1.1408      0.6561     0.7262     68.6064
07700     1.0866      0.6709     0.7262     69.6726
07800     1.0974      0.6519     0.7262     69.3960
07900     1.1007      0.6920     0.7262     70.9409
08000     1.0485      0.6793     0.7262     71.0997
08100     1.0277      0.6772     0.7262     68.8546
08200     0.9728      0.6814     0.7262     69.0944
08300     0.8911      0.7384     0.7274     68.3702
08400     1.0851      0.6857     0.7274     70.5087
08500     1.0799      0.6498     0.7274     70.1799
08600     1.0425      0.6857     0.7274     71.4817
08700     1.0790      0.6624     0.7324     68.3324
08800     0.9787      0.6962     0.7324     70.3121
08900     1.0278      0.6835     0.7324     70.7878
09000     1.0608      0.6814     0.7327     68.6179
09100     1.0835      0.6456     0.7350     70.2773
09200     0.9640      0.7046     0.7350     68.7033
09300     1.0235      0.7046     0.7350     69.6192
09400     1.0092      0.6772     0.7350     70.6011
09500     0.9771      0.7194     0.7350     71.1160
09600     1.0511      0.6941     0.7350     69.2275
09700     1.0544      0.6857     0.7399     69.8362
09800     1.1257      0.6456     0.7399     68.8006
09900     1.0069      0.6983     0.7399     68.8002
10000     1.0258      0.6772     0.7399     68.9116
10100     0.9999      0.7194     0.7399     68.5965
10200     1.0814      0.6709     0.7399     69.5707
10300     1.1967      0.6350     0.7399     68.8093
10400     1.0610      0.6941     0.7399     68.2821
10500     0.9192      0.7215     0.7399     68.7444
10600     1.0402      0.7004     0.7410     68.6291
10700     1.0209      0.6941     0.7410     68.6882
10800     1.0306      0.6814     0.7464     68.5747
10900     1.0446      0.6857     0.7464     68.7339
11000     0.9575      0.7173     0.7464     68.7652
11100     0.9897      0.6899     0.7464     70.5936
11200     1.0324      0.6878     0.7464     69.2468
11300     1.0043      0.7089     0.7464     68.0410
11400     1.0021      0.6709     0.7464     68.9246
11500     1.0097      0.6920     0.7464     68.5562
11600     0.9269      0.7300     0.7464     69.8278
11700     0.9721      0.7110     0.7464     70.0581
11800     1.0056      0.6962     0.7498     69.4162
11900     1.0539      0.6814     0.7498     68.9165
12000     0.9862      0.7194     0.7498     71.1815
12100     1.0035      0.6983     0.7498     68.9634
12200     1.1005      0.6498     0.7498     70.1431
12300     1.0731      0.6688     0.7498     71.8178
12400     0.9997      0.6941     0.7498     69.1966
12500     1.0769      0.6688     0.7498     68.8031
12600     0.9570      0.7089     0.7498     70.5244
12700     1.0782      0.6667     0.7498     69.6197
12800     0.9681      0.7152     0.7498     69.5316
12900     1.0049      0.6751     0.7498     68.7688
13000     1.0041      0.7046     0.7498     69.9097
13100     1.0483      0.6835     0.7498     68.1677
13200     1.0733      0.6519     0.7498     70.2818
13300     1.0057      0.7004     0.7498     70.4930
13400     1.0255      0.6878     0.7498     71.1874
13500     0.9774      0.7173     0.7498     71.0760
13600     0.9968      0.6920     0.7498     69.8478
13700     0.9993      0.6899     0.7524     70.1328
13800     0.9525      0.7278     0.7524     68.0101
13900     1.0782      0.6582     0.7524     69.4350
14000     1.1175      0.6751     0.7524     69.6855
14100     1.0324      0.6878     0.7524     70.4780
14200     0.9817      0.7152     0.7524     71.0810
14300     1.0489      0.6941     0.7524     69.9999
14400     0.9543      0.7004     0.7524     70.9276
14500     0.9044      0.7194     0.7524     70.9875
14600     0.9344      0.7300     0.7524     69.7600
14700     1.1212      0.6751     0.7524     70.8137
14800     1.0018      0.7131     0.7524     70.2114
14900     1.0158      0.7089     0.7524     69.3212
15000     1.0046      0.7068     0.7524     71.0398
15100     1.0618      0.6435     0.7524     69.6116
15200     1.0118      0.6814     0.7524     69.4128
15300     0.9515      0.7236     0.7524     70.5266
15400     0.9505      0.7300     0.7524     68.7143
15500     0.9592      0.6962     0.7524     71.9088
15600     1.0466      0.6962     0.7524     68.9426
15700     0.9340      0.7110     0.7524     68.5269
15800     0.9436      0.7110     0.7524     69.5026
15900     1.0073      0.7025     0.7524     70.7180
16000     1.0145      0.6857     0.7524     69.9562
16100     1.0039      0.7025     0.7524     69.8300
16200     0.9443      0.7321     0.7524     72.2746
16300     1.0595      0.6772     0.7524     70.8225
16400     1.0265      0.6793     0.7524     68.4218
16500     1.0594      0.6857     0.7524     72.3278
16600     1.0725      0.6772     0.7524     71.6546
16700     0.9248      0.7131     0.7524     71.9285
16800     0.9621      0.7110     0.7524     69.2029
16900     1.0445      0.6962     0.7524     69.6297
17000     1.0086      0.7152     0.7524     71.2047
17100     1.0882      0.6814     0.7524     72.8432
17200     1.0011      0.6962     0.7524     71.1867
17300     1.0977      0.6751     0.7524     73.0018
17400     1.0030      0.6920     0.7524     71.3204
17500     0.9969      0.6983     0.7524     71.1692
17600     1.0073      0.6962     0.7524     72.1367
17700     1.0355      0.6899     0.7524     70.2435
17800     0.9694      0.6941     0.7524     69.8578
17900     1.0865      0.6477     0.7524     70.1529
18000     1.0358      0.6962     0.7524     70.6567
18100     0.9862      0.7004     0.7524     69.6393
18200     0.9704      0.7321     0.7524     69.4019
18300     0.9810      0.6962     0.7524     70.3055
18400     1.1039      0.6709     0.7524     70.9500
18500     1.0256      0.7046     0.7524     68.4342
18600     0.9863      0.6793     0.7524     70.6568
18700     1.0444      0.6772     0.7524     70.4956
18800     1.0656      0.6561     0.7524     70.9954
18900     1.0287      0.6624     0.7524     70.4062
19000     0.9723      0.7152     0.7524     72.2923
19100     1.0030      0.7110     0.7524     72.4883
19200     1.0265      0.6835     0.7524     69.9625
19300     0.9401      0.7089     0.7524     73.4626
19400     0.9928      0.7068     0.7524     71.9617
19500     1.0186      0.6709     0.7524     72.0468
19600     1.0008      0.6899     0.7524     71.7004
19700     1.0240      0.6878     0.7524     70.4685
19800     0.8610      0.7489     0.7524     70.3210
19900     1.0013      0.6899     0.7524     70.0741
20000     1.0702      0.6899     0.7524     69.2874
20100     0.9141      0.7342     0.7524     69.5602
20199     1.0245      0.7068     0.7524     71.1396
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     0.9065      0.7194     0.7323     11.3327
00100     0.9667      0.7321     0.7521     72.3812
00200     0.9956      0.7068     0.7521     71.1524
00300     0.9734      0.7384     0.7521     70.7253
00400     0.9840      0.6920     0.7521     70.9471
00500     0.9011      0.7511     0.7521     70.6370
00600     0.9981      0.7004     0.7521     70.6376
00700     0.9962      0.7046     0.7521     69.2180
00800     1.0226      0.6878     0.7560     69.8534
00900     0.8103      0.7532     0.7560     69.9334
01000     0.8779      0.7405     0.7560     70.6371
01100     0.9996      0.6751     0.7560     70.6851
01200     0.9950      0.7046     0.7560     70.3988
01300     0.9246      0.7342     0.7560     68.7224
01400     0.9105      0.7384     0.7560     70.1726
01500     0.9874      0.7068     0.7560     68.6162
01600     0.9305      0.7511     0.7560     70.3966
01700     0.8244      0.7405     0.7560     69.5790
01800     0.8430      0.7426     0.7560     68.3321
01900     0.9197      0.7257     0.7576     70.6073
02000     0.9147      0.7257     0.7576     68.5167
02100     0.9646      0.7194     0.7576     69.0844
02200     1.0333      0.6624     0.7576     68.6642
02300     0.9763      0.6857     0.7576     69.1787
02400     0.9603      0.7131     0.7576     68.7250
02500     0.9118      0.7257     0.7576     70.2992
02600     0.8151      0.7764     0.7576     69.2398
02700     0.9149      0.7046     0.7576     70.7848
02800     0.9879      0.6878     0.7576     71.2204
02900     0.9841      0.7046     0.7576     69.9036
03000     0.9354      0.7089     0.7576     70.0472
03100     0.8692      0.7342     0.7576     72.1897
03200     0.8820      0.7363     0.7576     68.6354
03300     0.7960      0.7553     0.7576     69.6633
03400     0.9197      0.7215     0.7576     70.6734
03500     0.8921      0.7257     0.7576     69.6943
03600     0.9694      0.6983     0.7576     69.6834
03700     0.8544      0.7278     0.7576     69.5019
03800     0.8809      0.7743     0.7576     70.1541
03900     0.9732      0.6983     0.7576     71.1323
04000     0.9230      0.7131     0.7576     69.9563
04100     0.8554      0.7595     0.7576     70.4141
04200     0.9114      0.7447     0.7576     70.0364
04300     0.8533      0.7532     0.7576     73.0023
04400     0.9182      0.7131     0.7576     72.8239
04500     0.9038      0.7489     0.7576     70.1069
04600     0.9200      0.7215     0.7576     70.5043
04700     0.8535      0.7321     0.7576     70.8943
04800     0.9055      0.7278     0.7576     70.0022
04900     0.8987      0.7384     0.7576     69.8072
05000     0.9621      0.7173     0.7576     69.2651
05100     0.8707      0.7363     0.7576     71.8058
05200     0.9689      0.7194     0.7576     71.7955
05300     0.9327      0.7194     0.7576     70.7205
05400     0.9479      0.7089     0.7576     65.4927
05500     0.9728      0.7089     0.7576     68.0566
05600     0.8974      0.7257     0.7580     70.4955
05700     1.0062      0.6878     0.7580     69.4142
05800     1.0685      0.6624     0.7580     70.1568
05900     0.9520      0.7004     0.7580     70.3375
06000     0.9122      0.7152     0.7580     69.8521
06100     0.9830      0.7215     0.7580     70.0886
06200     0.9762      0.7131     0.7580     71.7956
06300     0.8860      0.7131     0.7580     70.4687
06400     0.9090      0.7194     0.7580     70.0199
06500     1.0169      0.6793     0.7580     72.3814
06600     0.9878      0.6751     0.7587     69.5264
06700     0.9272      0.7363     0.7587     72.0884
06800     1.0027      0.6920     0.7630     73.1905
06900     0.9021      0.7194     0.7630     70.7532
07000     0.8911      0.7342     0.7630     69.4361
07100     0.9776      0.7152     0.7630     69.9570
07200     0.9261      0.7173     0.7630     69.0931
07300     0.9275      0.7257     0.7630     69.3727
07400     0.9506      0.7321     0.7630     70.5619
07500     0.9457      0.7025     0.7630     69.0210
07600     1.0380      0.6941     0.7630     71.5982
07700     0.9446      0.7152     0.7630     70.5258
07800     0.9496      0.7110     0.7630     71.4907
07900     0.9148      0.7278     0.7630     70.1704
08000     0.9468      0.7236     0.7630     72.3008
08100     0.8110      0.7637     0.7630     70.2255
08200     0.8713      0.7468     0.7630     72.5585
08300     0.8965      0.7363     0.7630     72.0395
08400     0.9179      0.6962     0.7630     70.5479
08500     0.9601      0.7173     0.7630     70.8881
08600     0.9661      0.7025     0.7630     71.5934
08700     0.8860      0.7278     0.7630     71.2322
08800     1.0083      0.6920     0.7630     73.6516
08900     0.8708      0.7468     0.7630     71.8737
09000     0.9219      0.7278     0.7630     71.5917
09100     0.9122      0.7278     0.7630     70.6036
09200     0.9109      0.7468     0.7630     69.9817
09300     0.9111      0.7532     0.7630     70.9504
09400     0.8912      0.7384     0.7630     69.5585
09500     0.9243      0.7342     0.7630     70.9565
09600     0.8852      0.7511     0.7630     70.0514
09700     0.9783      0.7068     0.7630     69.0734
09800     0.9005      0.7194     0.7636     69.2550
09900     1.0219      0.6962     0.7636     70.7120
Start testing:
Test Accuracy: 0.7302
