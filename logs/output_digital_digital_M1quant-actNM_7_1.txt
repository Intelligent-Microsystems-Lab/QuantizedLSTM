Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=7, quant_actNM=7, quant_inp=7, quant_w=7, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
ca5712ff-6b56-4df8-bd7c-5e1bbfcf5c05
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 373, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, 1.)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 82, in forward
    if len(x_range) > 1:
TypeError: object of type 'float' has no len()
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=7, quant_actNM=7, quant_inp=7, quant_w=7, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
b637509d-fa12-454d-b06e-3ec070927893
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 373, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, torch.tensor([1.]).device(input.device))
TypeError: 'torch.device' object is not callable
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=7, quant_actNM=7, quant_inp=7, quant_w=7, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
5e6b6a13-5488-40dd-b454-6f2e1a0b4864
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     3.4663      0.1034     0.1341     11.0276
00100     2.3046      0.2257     0.2609     69.6738
00200     1.8073      0.3797     0.4201     71.7505
00300     1.4890      0.4810     0.4963     70.9075
00400     1.4142      0.5148     0.5690     71.0201
00500     1.2115      0.5992     0.5808     71.3116
00600     1.2515      0.5759     0.6373     70.2047
00700     1.0573      0.6392     0.6595     70.9562
00800     1.0271      0.6709     0.6844     71.7180
00900     1.1119      0.5970     0.7036     72.3726
01000     1.0487      0.6540     0.7123     70.7766
01100     0.9968      0.6688     0.7263     70.9664
01200     0.9366      0.6962     0.7263     71.1208
01300     0.9879      0.6561     0.7321     72.0374
01400     0.8561      0.7152     0.7461     70.4210
01500     0.8506      0.7089     0.7465     71.3615
01600     0.8769      0.7194     0.7521     71.8379
01700     0.8895      0.7046     0.7601     70.3344
01800     0.7849      0.7300     0.7601     69.4763
01900     0.7393      0.7574     0.7642     69.3510
02000     0.8413      0.7342     0.7642     69.8740
02100     0.8866      0.7300     0.7642     71.2195
02200     0.8916      0.7173     0.7651     70.8564
02300     0.8385      0.7236     0.7651     69.9040
02400     0.8639      0.7426     0.7717     69.5649
02500     0.7364      0.7447     0.7722     69.3064
02600     0.7376      0.7511     0.7722     68.1471
02700     0.8435      0.7553     0.7756     67.5856
02800     0.6834      0.7743     0.7761     69.6087
02900     0.8490      0.7131     0.7761     69.9328
03000     0.7527      0.7616     0.7763     68.1963
03100     0.7974      0.7321     0.7798     67.5239
03200     0.8380      0.7300     0.7833     68.7331
03300     0.7805      0.7595     0.7854     68.5700
03400     0.7231      0.7616     0.7907     69.5061
03500     0.7579      0.7553     0.7907     68.8787
03600     0.7179      0.7743     0.7909     68.6945
03700     0.6818      0.7637     0.7909     68.0388
03800     0.7384      0.7553     0.7909     67.9112
03900     0.6982      0.7932     0.7909     67.7772
04000     0.7460      0.7595     0.7909     68.3525
04100     0.7330      0.7658     0.8008     67.9553
04200     0.7437      0.7447     0.8008     68.5916
04300     0.6718      0.7911     0.8008     70.7174
04400     0.6354      0.7996     0.8008     68.4870
04500     0.6771      0.7722     0.8008     68.5926
04600     0.6547      0.7954     0.8008     70.1054
04700     0.6916      0.7616     0.8008     68.0399
04800     0.6859      0.7806     0.8059     68.6099
04900     0.6849      0.7722     0.8109     69.1176
05000     0.7602      0.7426     0.8109     67.9362
05100     0.7110      0.7595     0.8109     69.0988
05200     0.7007      0.7679     0.8109     68.3054
05300     0.7076      0.7722     0.8109     68.8360
05400     0.6644      0.7869     0.8109     68.8114
05500     0.6353      0.7806     0.8109     68.1280
05600     0.6410      0.7869     0.8167     68.9923
05700     0.6724      0.7637     0.8167     68.0525
05800     0.6277      0.8017     0.8167     67.6038
05900     0.6087      0.7932     0.8207     68.1735
06000     0.6147      0.7954     0.8207     69.1149
06100     0.7035      0.7616     0.8207     68.7706
06200     0.6777      0.7848     0.8220     67.6352
06300     0.5687      0.7932     0.8220     68.6565
06400     0.7303      0.7679     0.8244     68.3527
06500     0.6928      0.7890     0.8244     67.9311
06600     0.5841      0.8165     0.8244     69.2510
06700     0.6980      0.7911     0.8244     67.7092
06800     0.6488      0.7806     0.8244     68.2584
06900     0.7179      0.7806     0.8244     68.5181
07000     0.6168      0.8059     0.8244     69.2677
07100     0.6201      0.7869     0.8244     68.8422
07200     0.6239      0.8059     0.8244     68.9212
07300     0.6581      0.7827     0.8244     67.5352
07400     0.5686      0.8101     0.8244     67.9206
07500     0.6297      0.8101     0.8244     67.6654
07600     0.6447      0.7679     0.8244     68.3245
07700     0.6172      0.8165     0.8244     68.6765
07800     0.5664      0.7996     0.8244     68.0189
07900     0.5557      0.8270     0.8244     69.5932
08000     0.6208      0.7869     0.8244     68.4933
08100     0.6103      0.8080     0.8244     69.3576
08200     0.6261      0.7827     0.8244     69.3880
08300     0.6440      0.8101     0.8244     68.3257
08400     0.6326      0.7806     0.8244     67.3788
08500     0.7506      0.7405     0.8244     68.2436
08600     0.6563      0.7975     0.8244     68.0500
08700     0.5445      0.8502     0.8244     67.3611
08800     0.5849      0.7975     0.8270     69.1117
08900     0.5663      0.8207     0.8270     67.6999
09000     0.6142      0.7764     0.8270     68.0397
09100     0.5885      0.8017     0.8270     68.3376
09200     0.6235      0.7890     0.8270     67.1373
09300     0.6725      0.7785     0.8270     66.8625
09400     0.6176      0.8101     0.8270     68.0005
09500     0.6816      0.7764     0.8270     67.8926
09600     0.6649      0.7932     0.8270     68.1108
09700     0.5180      0.8333     0.8270     68.0158
09800     0.5174      0.8312     0.8270     68.8119
09900     0.5581      0.8080     0.8270     67.7596
10000     0.6369      0.8143     0.8270     68.0936
10100     0.6167      0.8038     0.8270     68.1623
10200     0.5594      0.8228     0.8270     67.9539
10300     0.5827      0.8017     0.8270     68.3467
10400     0.5812      0.7975     0.8270     67.6841
10500     0.5222      0.8502     0.8270     67.9626
10600     0.5845      0.8207     0.8316     67.8915
10700     0.5983      0.8165     0.8316     69.0533
10800     0.5890      0.8017     0.8316     68.2126
10900     0.5605      0.7996     0.8316     68.2517
11000     0.6069      0.8038     0.8316     68.1404
11100     0.5665      0.8270     0.8316     68.3077
11200     0.6024      0.8122     0.8316     69.1685
11300     0.6159      0.8122     0.8316     68.1342
11400     0.4708      0.8502     0.8316     67.9010
11500     0.6813      0.7679     0.8316     67.7055
11600     0.5570      0.8291     0.8316     69.8151
11700     0.4904      0.8354     0.8358     68.3213
11800     0.5164      0.8291     0.8358     67.9661
11900     0.5871      0.7996     0.8358     67.7429
12000     0.5317      0.8228     0.8358     69.4606
12100     0.5917      0.8207     0.8358     70.0176
12200     0.5845      0.8312     0.8358     69.6712
12300     0.5907      0.7975     0.8358     67.6401
12400     0.6099      0.8017     0.8358     67.4804
12500     0.5503      0.8228     0.8358     68.5903
12600     0.6012      0.8059     0.8358     68.4619
12700     0.6097      0.7954     0.8358     68.5578
12800     0.5196      0.8418     0.8358     68.4113
12900     0.4456      0.8481     0.8358     68.5568
13000     0.5346      0.8291     0.8358     69.4832
13100     0.5465      0.8397     0.8358     68.4648
13200     0.5500      0.8291     0.8358     69.4356
13300     0.5294      0.8418     0.8358     68.1467
13400     0.6111      0.7932     0.8365     69.2292
13500     0.6336      0.7932     0.8365     67.7524
13600     0.5783      0.8165     0.8365     68.5361
13700     0.5694      0.8270     0.8365     68.0311
13800     0.5249      0.8354     0.8365     68.2155
13900     0.5971      0.8101     0.8365     68.5206
14000     0.6439      0.7869     0.8365     67.1876
14100     0.4915      0.8291     0.8365     69.5165
14200     0.5481      0.8270     0.8365     67.9956
14300     0.5744      0.8143     0.8365     67.7681
14400     0.5575      0.8122     0.8365     68.3225
14500     0.4571      0.8523     0.8365     67.3630
14600     0.5875      0.7954     0.8365     67.8339
14700     0.5875      0.7954     0.8365     67.8793
14800     0.5694      0.8143     0.8365     68.5193
14900     0.6259      0.7932     0.8365     70.6247
15000     0.5036      0.8122     0.8365     67.9679
15100     0.6041      0.7954     0.8365     68.3196
15200     0.6017      0.8122     0.8365     68.7317
15300     0.5935      0.8038     0.8365     67.9922
15400     0.5755      0.8101     0.8365     70.9179
15500     0.5962      0.7932     0.8365     68.1776
15600     0.5294      0.8481     0.8365     69.1447
15700     0.5142      0.8143     0.8365     68.1251
15800     0.5769      0.8228     0.8365     68.8675
15900     0.6734      0.7785     0.8365     67.7058
16000     0.4654      0.8565     0.8365     68.1194
16100     0.4645      0.8418     0.8365     67.6626
16200     0.5295      0.8059     0.8365     67.8151
16300     0.5301      0.8291     0.8365     69.2493
16400     0.5543      0.7975     0.8365     68.1779
16500     0.6140      0.8101     0.8365     67.3988
16600     0.4743      0.8291     0.8365     67.3826
16700     0.5122      0.8270     0.8365     67.7291
16800     0.5079      0.8312     0.8365     68.8042
16900     0.5191      0.8312     0.8365     69.1989
17000     0.5725      0.8228     0.8365     69.1036
17100     0.5719      0.8017     0.8365     67.3292
17200     0.5207      0.8270     0.8365     68.1809
17300     0.5773      0.8143     0.8365     68.0826
17400     0.5747      0.7996     0.8365     68.2977
17500     0.6381      0.7806     0.8365     69.4484
17600     0.5253      0.8565     0.8365     69.0857
17700     0.5075      0.8165     0.8365     68.0997
17800     0.5419      0.8080     0.8365     68.2484
17900     0.5973      0.8038     0.8365     67.9452
18000     0.5434      0.8376     0.8365     68.1229
18100     0.5411      0.8186     0.8365     70.7422
18200     0.5296      0.8249     0.8365     67.0755
18300     0.5995      0.8122     0.8365     68.7112
18400     0.5818      0.8333     0.8365     67.8849
18500     0.5183      0.8376     0.8365     69.4094
18600     0.5485      0.8101     0.8365     67.9484
18700     0.5724      0.8038     0.8365     69.2378
18800     0.5249      0.8460     0.8365     66.8901
18900     0.5496      0.8059     0.8365     67.9416
19000     0.5670      0.8270     0.8365     67.8429
19100     0.4958      0.8481     0.8365     67.9210
19200     0.5701      0.8249     0.8365     68.8420
19300     0.5574      0.8333     0.8365     69.1235
19400     0.5401      0.8101     0.8365     67.7195
19500     0.6397      0.7954     0.8365     67.7073
19600     0.5746      0.7975     0.8365     68.2153
19700     0.5445      0.8143     0.8365     67.7656
19800     0.5710      0.8207     0.8365     68.6775
19900     0.5062      0.8186     0.8365     68.7841
20000     0.5382      0.8291     0.8365     68.8316
20100     0.4715      0.8523     0.8365     68.7984
20199     0.5474      0.8143     0.8365     67.9876
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     0.4224      0.8629     0.8324     10.0148
00100     0.4656      0.8439     0.8324     67.7953
00200     0.5849      0.8143     0.8359     68.0446
00300     0.4679      0.8544     0.8362     67.7414
00400     0.5049      0.8354     0.8362     68.6764
00500     0.4740      0.8460     0.8362     67.5953
00600     0.5048      0.8418     0.8362     67.2568
00700     0.4366      0.8523     0.8362     68.7501
00800     0.4239      0.8629     0.8362     68.5452
00900     0.4269      0.8608     0.8362     67.3999
01000     0.4892      0.8333     0.8362     69.5806
01100     0.4246      0.8671     0.8362     67.9175
01200     0.4220      0.8629     0.8362     68.5047
01300     0.5026      0.8207     0.8369     68.5777
01400     0.3970      0.8713     0.8369     67.2019
01500     0.4487      0.8523     0.8369     68.4903
01600     0.4809      0.8312     0.8369     68.8971
01700     0.5342      0.8270     0.8369     68.5851
01800     0.4731      0.8460     0.8369     67.7946
01900     0.5847      0.8270     0.8369     68.7245
02000     0.3958      0.8755     0.8369     67.5608
02100     0.5023      0.8354     0.8369     68.8455
02200     0.4819      0.8650     0.8383     68.0499
02300     0.4660      0.8523     0.8383     67.9851
02400     0.4809      0.8481     0.8383     68.5559
02500     0.5083      0.8312     0.8383     69.4363
02600     0.5206      0.8376     0.8383     68.0225
02700     0.5580      0.8270     0.8383     68.5284
02800     0.5155      0.8312     0.8383     67.5276
02900     0.4311      0.8713     0.8383     67.6752
03000     0.4431      0.8608     0.8383     69.1885
03100     0.5783      0.7932     0.8383     67.5336
03200     0.5056      0.8354     0.8383     68.3602
03300     0.4902      0.8418     0.8383     68.3654
03400     0.4512      0.8544     0.8383     69.1967
03500     0.5145      0.8397     0.8383     70.0856
03600     0.4538      0.8544     0.8383     67.3579
03700     0.4877      0.8333     0.8383     69.1799
03800     0.4837      0.8354     0.8383     69.2459
03900     0.4583      0.8418     0.8383     67.6156
04000     0.5343      0.8439     0.8383     70.5230
04100     0.4420      0.8502     0.8383     68.1953
04200     0.5093      0.8439     0.8383     68.6497
04300     0.4832      0.8354     0.8383     70.2740
04400     0.4337      0.8734     0.8384     70.8375
04500     0.4846      0.8460     0.8384     70.2515
04600     0.5111      0.8376     0.8384     68.4834
04700     0.4965      0.8333     0.8384     71.3873
04800     0.4250      0.8523     0.8384     71.1670
04900     0.4766      0.8481     0.8384     68.4032
05000     0.4839      0.8502     0.8384     68.5346
05100     0.4761      0.8333     0.8384     69.4828
05200     0.4818      0.8544     0.8384     68.7006
05300     0.4363      0.8523     0.8384     70.4189
05400     0.5257      0.8207     0.8384     68.1845
05500     0.5398      0.8354     0.8384     69.6541
05600     0.4382      0.8671     0.8384     69.1162
05700     0.4589      0.8650     0.8384     67.6881
05800     0.4111      0.8819     0.8384     68.0392
05900     0.4618      0.8333     0.8384     68.3193
06000     0.4554      0.8544     0.8384     68.5982
06100     0.4437      0.8481     0.8384     68.5978
06200     0.4541      0.8418     0.8384     68.8477
06300     0.4400      0.8713     0.8384     70.6991
06400     0.4548      0.8333     0.8384     67.8707
06500     0.3776      0.8755     0.8384     68.6013
06600     0.4689      0.8713     0.8384     68.7225
06700     0.4330      0.8671     0.8384     71.6270
06800     0.5148      0.8207     0.8384     70.0618
06900     0.4217      0.8650     0.8386     68.7476
07000     0.4238      0.8776     0.8386     68.7905
07100     0.5470      0.8059     0.8386     67.5382
07200     0.5172      0.8312     0.8386     67.9323
07300     0.5305      0.8207     0.8386     68.1481
07400     0.4859      0.8460     0.8386     67.1983
07500     0.5084      0.8418     0.8386     68.3610
07600     0.4350      0.8586     0.8386     70.4315
07700     0.4420      0.8671     0.8386     69.3432
07800     0.4021      0.8692     0.8386     69.1722
07900     0.4989      0.8460     0.8386     70.2531
08000     0.5332      0.8228     0.8386     69.2726
08100     0.5094      0.8376     0.8386     68.0996
08200     0.4626      0.8439     0.8386     70.6732
08300     0.5199      0.8249     0.8386     68.3260
08400     0.4489      0.8544     0.8386     68.0507
08500     0.5347      0.8122     0.8386     68.3614
08600     0.5146      0.8376     0.8386     69.4946
08700     0.4709      0.8460     0.8390     68.6741
08800     0.4427      0.8481     0.8390     68.2296
08900     0.4944      0.8439     0.8390     69.9039
09000     0.5316      0.8143     0.8390     68.1623
09100     0.4847      0.8418     0.8390     67.9319
09200     0.5132      0.8544     0.8390     68.3747
09300     0.5173      0.8397     0.8390     68.2197
09400     0.4954      0.8249     0.8390     68.0895
09500     0.4924      0.8354     0.8390     68.7666
09600     0.5146      0.8354     0.8390     68.2831
09700     0.3794      0.8861     0.8390     67.7897
09800     0.5298      0.8333     0.8390     69.8506
09900     0.4628      0.8333     0.8390     70.1476
Start testing:
Test Accuracy: 0.8252
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
