Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=235899598, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 278, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=235899598, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 110, in <module>
    model = KWS_LSTM_bmm(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 279, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.0, finetuning_epochs=0, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.0005,0.0001,0.00002', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=235899598, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,10000', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
f2fc7d9f-c10a-4d9f-9dce-4b5715363500
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.5257      0.1076     0.0998     12.4654
00100     2.5256      0.0844     0.1002     56.1230
00200     2.5256      0.1203     0.1008     56.4847
00300     2.5256      0.0970     0.1008     56.4018
00400     2.5256      0.0949     0.1008     56.2719
00500     2.5256      0.1076     0.1008     56.8120
00600     2.5255      0.0970     0.1008     57.1573
00700     2.5255      0.1055     0.1008     56.2083
00800     2.5255      0.1160     0.1008     56.9679
00900     2.5255      0.1118     0.1008     57.1393
01000     2.5254      0.0928     0.1008     56.8615
01100     2.5254      0.0907     0.1008     57.3207
01200     2.5254      0.0928     0.1008     57.6869
01300     2.5254      0.1055     0.1008     57.0282
01400     2.5254      0.0654     0.1008     57.3704
01500     2.5253      0.0865     0.1008     57.4468
01600     2.5253      0.1055     0.1008     57.3887
01700     2.5253      0.0865     0.1008     56.4666
01800     2.5253      0.0970     0.1008     57.2580
01900     2.5252      0.1076     0.1008     57.2572
02000     2.5252      0.1013     0.1008     56.5632
02100     2.5252      0.1013     0.1008     56.2715
02200     2.5252      0.0907     0.1008     57.8943
02300     2.5252      0.0949     0.1008     58.5024
02400     2.5251      0.0928     0.1008     61.9515
02500     2.5251      0.1034     0.1008     60.1099
02600     2.5251      0.0970     0.1008     60.6253
02700     2.5251      0.1013     0.1008     62.0198
02800     2.5251      0.1097     0.1008     60.9126
02900     2.5250      0.0992     0.1008     61.0241
03000     2.5250      0.0886     0.1008     62.2961
03100     2.5250      0.0970     0.1008     60.1692
03200     2.5250      0.0886     0.1008     61.8618
03300     2.5249      0.1013     0.1008     60.2050
03400     2.5249      0.0886     0.1008     61.4958
03500     2.5249      0.0992     0.1008     58.9895
03600     2.5249      0.1013     0.1008     60.8943
03700     2.5249      0.0928     0.1008     60.4846
03800     2.5248      0.0886     0.1008     60.3279
03900     2.5248      0.0696     0.1008     60.6568
04000     2.5248      0.0907     0.1008     60.5492
04100     2.5248      0.1097     0.1008     60.4763
04200     2.5248      0.0907     0.1008     60.7099
04300     2.5247      0.1139     0.1008     60.3667
04400     2.5247      0.0928     0.1008     60.2715
04500     2.5247      0.1034     0.1008     59.7896
04600     2.5247      0.1224     0.1008     60.2362
04700     2.5247      0.0992     0.1008     60.0179
04800     2.5246      0.1181     0.1008     60.8931
04900     2.5246      0.1181     0.1008     59.2133
05000     2.5246      0.1139     0.1008     59.5471
05100     2.5246      0.0970     0.1008     60.0161
05200     2.5246      0.1118     0.1008     58.7616
05300     2.5245      0.0886     0.1008     59.1294
05400     2.5245      0.0802     0.1008     58.9623
05500     2.5245      0.1013     0.1008     59.7362
05600     2.5245      0.0886     0.1008     60.1669
05700     2.5245      0.1034     0.1008     59.4903
05800     2.5244      0.1245     0.1008     59.7185
05900     2.5244      0.1034     0.1008     61.0054
06000     2.5244      0.1118     0.1008     61.0589
06100     2.5244      0.1097     0.1008     60.0367
06200     2.5244      0.1034     0.1008     60.1297
06300     2.5243      0.0886     0.1008     59.6366
06400     2.5243      0.0907     0.1008     59.6228
06500     2.5243      0.0949     0.1008     60.3762
06600     2.5243      0.1076     0.1008     59.3360
06700     2.5243      0.1013     0.1008     59.6809
06800     2.5242      0.1097     0.1008     58.7425
06900     2.5242      0.0970     0.1008     59.4047
07000     2.5242      0.1160     0.1008     59.5449
07100     2.5242      0.0759     0.1008     59.6857
07200     2.5242      0.0928     0.1008     59.3902
07300     2.5242      0.0949     0.1008     58.9089
07400     2.5241      0.0970     0.1008     59.8557
07500     2.5241      0.0928     0.1008     60.0735
07600     2.5241      0.1118     0.1008     59.4662
07700     2.5241      0.1055     0.1008     59.3798
07800     2.5241      0.0949     0.1008     59.9000
07900     2.5240      0.1245     0.1008     59.6649
08000     2.5240      0.0823     0.1008     60.1911
08100     2.5240      0.0992     0.1008     58.9341
08200     2.5240      0.0823     0.1008     60.4469
08300     2.5240      0.0823     0.1008     59.4767
08400     2.5239      0.1013     0.1008     59.2861
08500     2.5239      0.1287     0.1008     59.6476
08600     2.5239      0.0886     0.1008     59.8372
08700     2.5239      0.1097     0.1008     60.5046
08800     2.5239      0.1118     0.1008     60.4336
08900     2.5239      0.1160     0.1008     61.1455
09000     2.5238      0.1034     0.1008     59.6944
09100     2.5238      0.1013     0.1008     60.2684
09200     2.5238      0.0886     0.1008     59.3389
09300     2.5238      0.0992     0.1008     59.9637
09400     2.5238      0.0823     0.1008     60.4020
09500     2.5237      0.0865     0.1008     59.8017
09600     2.5237      0.0717     0.1008     61.2871
09700     2.5237      0.0907     0.1008     58.8118
09800     2.5237      0.0992     0.1008     59.3145
09900     2.5237      0.1013     0.1008     61.0407
10000     2.5236      0.1055     0.1008     59.4557
10100     2.5236      0.1076     0.1008     58.6975
10200     2.5236      0.0759     0.1008     59.4844
10300     2.5236      0.1097     0.1008     60.0144
10400     2.5236      0.1245     0.1008     60.4162
10500     2.5236      0.0844     0.1008     59.1684
10600     2.5236      0.0992     0.1008     59.9415
10700     2.5236      0.1245     0.1008     59.4671
10800     2.5236      0.1097     0.1008     60.1540
10900     2.5236      0.0844     0.1008     59.0451
11000     2.5236      0.0823     0.1008     60.0170
11100     2.5236      0.1245     0.1008     59.1376
11200     2.5236      0.0612     0.1008     60.0742
11300     2.5236      0.0992     0.1008     58.6632
11400     2.5236      0.1203     0.1008     58.9242
11500     2.5236      0.0907     0.1008     59.2021
11600     2.5236      0.1013     0.1008     58.0768
11700     2.5236      0.0928     0.1008     59.4205
11800     2.5236      0.1181     0.1008     59.6168
11900     2.5236      0.0992     0.1008     59.0861
12000     2.5236      0.0970     0.1008     59.5539
12100     2.5236      0.0781     0.1008     59.4687
12200     2.5236      0.0928     0.1008     59.5488
12300     2.5236      0.0970     0.1008     59.8736
12400     2.5236      0.0928     0.1008     58.2460
12500     2.5236      0.1139     0.1008     59.5560
12600     2.5236      0.1034     0.1008     58.8559
12700     2.5235      0.0865     0.1008     58.9998
12800     2.5235      0.0970     0.1008     59.3922
12900     2.5235      0.1013     0.1008     59.2607
13000     2.5235      0.0970     0.1008     60.0180
13100     2.5235      0.0992     0.1008     59.8556
13200     2.5235      0.1181     0.1008     59.1971
13300     2.5235      0.1076     0.1008     60.0605
13400     2.5235      0.1329     0.1008     59.8107
13500     2.5235      0.0823     0.1008     58.8225
13600     2.5235      0.0928     0.1008     59.7047
13700     2.5235      0.0759     0.1008     58.7501
13800     2.5235      0.1139     0.1008     59.4480
13900     2.5235      0.0865     0.1008     59.9364
14000     2.5235      0.1076     0.1008     59.7062
14100     2.5235      0.1371     0.1008     59.4923
14200     2.5235      0.1013     0.1008     59.1621
14300     2.5235      0.0886     0.1008     60.6491
14400     2.5235      0.1076     0.1008     60.7044
14500     2.5235      0.0970     0.1008     59.6359
14600     2.5235      0.1013     0.1008     59.6325
14700     2.5235      0.1203     0.1008     59.8648
14800     2.5235      0.1203     0.1008     59.9034
14900     2.5235      0.0802     0.1008     59.8931
15000     2.5235      0.1266     0.1008     59.6786
15100     2.5235      0.0992     0.1008     59.3030
15200     2.5235      0.0928     0.1008     59.4525
15300     2.5235      0.0949     0.1008     59.0595
15400     2.5235      0.0992     0.1008     59.9724
15500     2.5234      0.1350     0.1008     61.2199
15600     2.5234      0.0802     0.1008     60.1259
15700     2.5234      0.0675     0.1008     60.7836
15800     2.5234      0.1034     0.1008     61.2051
15900     2.5234      0.1034     0.1008     60.4627
16000     2.5234      0.1076     0.1008     59.8426
16100     2.5234      0.1203     0.1008     59.5487
16200     2.5234      0.0802     0.1008     62.3383
16300     2.5234      0.0907     0.1008     60.2818
16400     2.5234      0.1203     0.1008     60.1014
16500     2.5234      0.0992     0.1008     59.0788
16600     2.5234      0.1013     0.1008     59.8126
16700     2.5234      0.1055     0.1008     59.0159
16800     2.5234      0.0823     0.1009     60.1779
16900     2.5234      0.0949     0.1009     59.2620
17000     2.5234      0.1055     0.1009     58.8022
17100     2.5234      0.0886     0.1009     59.3899
17200     2.5234      0.1203     0.1009     59.0136
17300     2.5234      0.1097     0.1009     60.0064
17400     2.5234      0.1266     0.1009     58.8049
17500     2.5234      0.0992     0.1009     59.4497
17600     2.5234      0.1118     0.1009     59.8520
17700     2.5234      0.0591     0.1009     59.4170
17800     2.5234      0.0970     0.1009     59.3614
17900     2.5234      0.1013     0.1009     60.5255
18000     2.5234      0.1203     0.1009     60.0444
18100     2.5234      0.0907     0.1009     60.3975
18200     2.5234      0.0907     0.1009     60.8647
18300     2.5233      0.1097     0.1009     61.6452
18400     2.5233      0.1034     0.1009     61.5006
18500     2.5233      0.0759     0.1009     60.2844
18600     2.5233      0.0928     0.1009     59.9617
18700     2.5233      0.1034     0.1009     59.8298
18800     2.5233      0.0992     0.1009     59.1475
18900     2.5233      0.0781     0.1009     59.1028
19000     2.5233      0.0781     0.1009     59.8840
19100     2.5233      0.1055     0.1009     59.5036
19200     2.5233      0.0970     0.1009     60.4004
19300     2.5233      0.0970     0.1016     59.0338
19400     2.5233      0.0970     0.1016     59.1011
19500     2.5233      0.0886     0.1016     59.2355
19600     2.5233      0.0949     0.1016     59.4633
19700     2.5233      0.1034     0.1016     59.1882
19800     2.5233      0.0992     0.1016     59.9210
19900     2.5233      0.0781     0.1016     59.4337
20000     2.5233      0.1013     0.1016     59.6335
20100     2.5233      0.1181     0.1016     59.3908
20200     2.5233      0.1118     0.1016     60.0394
20300     2.5233      0.1139     0.1016     60.4793
20400     2.5233      0.0717     0.1016     59.4140
20500     2.5233      0.0844     0.1016     58.8828
20600     2.5233      0.1160     0.1016     59.9384
20700     2.5233      0.1034     0.1016     60.1763
20800     2.5233      0.0907     0.1016     60.8648
20900     2.5233      0.1181     0.1016     59.6590
21000     2.5233      0.1329     0.1016     59.8152
21100     2.5233      0.1245     0.1016     60.8454
21200     2.5233      0.1245     0.1016     60.2594
21300     2.5233      0.0823     0.1016     59.8352
21400     2.5233      0.0949     0.1016     60.1182
21500     2.5233      0.1055     0.1016     60.1133
21600     2.5233      0.1097     0.1016     60.4958
21700     2.5233      0.1181     0.1016     59.6834
21800     2.5233      0.0970     0.1016     60.2187
21900     2.5233      0.0928     0.1016     60.0257
22000     2.5233      0.0949     0.1016     59.7860
22100     2.5233      0.0992     0.1016     60.0546
22200     2.5233      0.0928     0.1016     60.4543
22300     2.5233      0.1055     0.1016     59.2920
22400     2.5233      0.0907     0.1016     61.8698
22500     2.5233      0.0886     0.1016     60.8307
22600     2.5233      0.1203     0.1016     60.9257
22700     2.5233      0.0886     0.1016     60.5711
22800     2.5233      0.0992     0.1016     60.3687
22900     2.5233      0.0992     0.1016     59.9578
23000     2.5233      0.1013     0.1016     59.5815
23100     2.5233      0.1013     0.1016     59.4680
23200     2.5233      0.1139     0.1016     60.4209
23300     2.5233      0.1013     0.1016     60.3285
23400     2.5233      0.1139     0.1016     61.2736
23500     2.5233      0.0759     0.1016     60.3416
23600     2.5233      0.1160     0.1016     60.0262
23700     2.5233      0.1076     0.1016     60.1050
23800     2.5233      0.0970     0.1016     61.5142
23900     2.5233      0.1034     0.1016     59.5593
24000     2.5233      0.1076     0.1016     60.7316
24100     2.5233      0.0970     0.1016     61.2335
24200     2.5233      0.1013     0.1016     60.1108
24300     2.5233      0.0907     0.1016     59.9712
24400     2.5233      0.0886     0.1016     59.6373
24500     2.5232      0.1013     0.1016     59.6617
24600     2.5232      0.1287     0.1016     60.0452
24700     2.5232      0.1055     0.1016     59.8362
24800     2.5232      0.0886     0.1016     60.9446
24900     2.5232      0.0992     0.1016     59.3448
25000     2.5232      0.1181     0.1016     58.9095
25100     2.5232      0.1013     0.1016     59.4497
25200     2.5232      0.1097     0.1016     58.4836
25300     2.5232      0.1076     0.1016     59.2516
25400     2.5232      0.0992     0.1016     59.6878
25500     2.5232      0.1160     0.1016     58.8913
25600     2.5232      0.1055     0.1016     60.0597
25700     2.5232      0.0781     0.1016     58.8746
25800     2.5232      0.0823     0.1016     58.9843
25900     2.5232      0.1392     0.1016     59.9695
26000     2.5232      0.1118     0.1016     58.9933
26100     2.5232      0.1160     0.1016     59.7075
26200     2.5232      0.1181     0.1016     60.9613
26300     2.5232      0.1034     0.1016     60.6608
26400     2.5232      0.1055     0.1016     60.9142
26500     2.5232      0.1097     0.1016     59.8585
26600     2.5232      0.1203     0.1016     59.9472
26700     2.5232      0.1097     0.1016     59.4371
26800     2.5232      0.1203     0.1016     59.9533
26900     2.5232      0.1266     0.1016     60.0137
27000     2.5232      0.1013     0.1016     59.4646
27100     2.5232      0.1118     0.1016     59.1103
27200     2.5232      0.1076     0.1016     60.9119
27300     2.5232      0.0970     0.1016     59.9170
27400     2.5232      0.0865     0.1016     60.3856
27500     2.5232      0.1076     0.1016     60.3578
27600     2.5232      0.1097     0.1016     60.5094
27700     2.5232      0.1013     0.1016     61.1241
27800     2.5232      0.1160     0.1016     60.9656
27900     2.5232      0.0844     0.1016     56.3084
28000     2.5232      0.1013     0.1016     58.7771
28100     2.5232      0.0781     0.1016     58.7280
28200     2.5232      0.0865     0.1016     58.6430
28300     2.5232      0.0802     0.1016     59.8454
28400     2.5232      0.0949     0.1016     60.0145
28500     2.5232      0.1034     0.1016     61.2458
28600     2.5232      0.0992     0.1016     59.2791
28700     2.5232      0.0907     0.1016     58.9752
28800     2.5232      0.0823     0.1016     60.2660
28900     2.5232      0.0865     0.1016     58.7335
29000     2.5232      0.0992     0.1016     58.3086
29100     2.5232      0.1160     0.1016     58.5915
29200     2.5232      0.1181     0.1016     57.9602
29300     2.5232      0.1118     0.1016     59.6366
29400     2.5232      0.0949     0.1016     60.5041
29500     2.5232      0.0907     0.1016     58.1325
29600     2.5232      0.0992     0.1016     59.2220
29700     2.5232      0.1013     0.1016     58.7586
29800     2.5232      0.1097     0.1016     59.9801
29900     2.5232      0.1160     0.1016     59.7442
29999     2.5232      0.0928     0.1016     58.4989
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Start testing:
Test Accuracy: 0.1002
