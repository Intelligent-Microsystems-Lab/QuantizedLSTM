Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=11, quant_actNM=11, quant_inp=11, quant_w=11, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
cfe971ac-d676-4c14-a7e8-bdcc456555a0
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 373, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, 1.)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 82, in forward
    if len(x_range) > 1:
TypeError: object of type 'float' has no len()
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=11, quant_actNM=11, quant_inp=11, quant_w=11, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
3718c82c-9c02-4363-a60e-865ef61e8458
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 373, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, torch.tensor([1.]).device(input.device))
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 94, in forward
    x_scaled = x/x_range
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=11, quant_actNM=11, quant_inp=11, quant_w=11, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
d1013a3c-7967-4ce0-961d-fc8cc9554f0c
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     3.5380      0.0949     0.0908     9.4716
00100     2.2285      0.2321     0.2394     56.3485
00200     1.7570      0.4177     0.4308     56.8420
00300     1.5464      0.4916     0.5311     55.9103
00400     1.2862      0.6160     0.6097     55.4883
00500     1.2002      0.6477     0.6485     55.6074
00600     0.9912      0.7025     0.6868     56.0359
00700     1.1421      0.6477     0.7121     55.3614
00800     1.0226      0.6730     0.7190     56.4564
00900     0.9516      0.6751     0.7254     55.9183
01000     1.0180      0.6688     0.7377     54.3299
01100     0.8482      0.7215     0.7448     57.8201
01200     0.8653      0.7342     0.7448     55.3009
01300     0.9433      0.6941     0.7572     54.8590
01400     0.8155      0.7489     0.7572     55.2261
01500     0.9356      0.6962     0.7671     54.7450
01600     0.8214      0.7426     0.7702     55.2624
01700     0.8508      0.7173     0.7723     54.1362
01800     0.7824      0.7511     0.7723     54.6692
01900     0.8545      0.7194     0.7737     55.4326
02000     0.7584      0.7700     0.7801     54.8190
02100     0.8108      0.7363     0.7825     54.6908
02200     0.7687      0.7426     0.7825     54.5885
02300     0.6795      0.7848     0.7879     54.7881
02400     0.7197      0.7764     0.8028     54.8502
02500     0.7111      0.7595     0.8028     54.6922
02600     0.7202      0.7679     0.8028     54.5055
02700     0.7270      0.7637     0.8028     55.1761
02800     0.6772      0.7869     0.8097     55.4418
02900     0.7343      0.7637     0.8097     54.8266
03000     0.7191      0.7932     0.8097     54.8981
03100     0.6910      0.7658     0.8097     54.5242
03200     0.7360      0.7869     0.8111     54.8923
03300     0.6443      0.7848     0.8111     54.3100
03400     0.8063      0.7384     0.8111     54.5827
03500     0.6562      0.7848     0.8111     55.2862
03600     0.6186      0.8017     0.8126     54.8101
03700     0.6398      0.7911     0.8126     55.8400
03800     0.7009      0.7722     0.8240     55.3003
03900     0.6196      0.8059     0.8240     54.5163
04000     0.5683      0.8101     0.8240     55.0270
04100     0.5596      0.8101     0.8240     54.9530
04200     0.6536      0.7932     0.8240     55.9921
04300     0.6898      0.7848     0.8240     54.6775
04400     0.6090      0.8186     0.8240     55.9841
04500     0.6396      0.7890     0.8240     55.1132
04600     0.6066      0.8101     0.8240     55.5097
04700     0.6310      0.8038     0.8240     55.0495
04800     0.6496      0.8165     0.8240     55.0913
04900     0.6296      0.8101     0.8240     54.5671
05000     0.6435      0.7806     0.8240     54.7366
05100     0.6581      0.7890     0.8243     55.8089
05200     0.5674      0.8228     0.8243     54.7625
05300     0.6419      0.8143     0.8243     54.7072
05400     0.5841      0.8038     0.8243     54.6880
05500     0.6808      0.7911     0.8243     54.1644
05600     0.5878      0.8291     0.8243     56.4264
05700     0.6767      0.7827     0.8243     55.2970
05800     0.7487      0.7637     0.8243     54.9663
05900     0.5596      0.8333     0.8253     54.7954
06000     0.5532      0.8354     0.8253     55.8651
06100     0.6119      0.8228     0.8253     54.2954
06200     0.6016      0.8080     0.8365     54.4509
06300     0.5151      0.8312     0.8365     54.8702
06400     0.6114      0.8038     0.8365     54.8200
06500     0.5727      0.8376     0.8365     54.4009
06600     0.5894      0.8080     0.8365     54.1468
06700     0.5829      0.8270     0.8365     56.0670
06800     0.4991      0.8439     0.8365     54.6849
06900     0.5289      0.8312     0.8365     55.3988
07000     0.5786      0.8312     0.8365     55.0188
07100     0.5595      0.8122     0.8365     55.5180
07200     0.6245      0.8080     0.8365     54.8416
07300     0.6125      0.7890     0.8365     54.4887
07400     0.5370      0.8207     0.8365     55.3291
07500     0.6635      0.7996     0.8365     54.5347
07600     0.6201      0.8017     0.8365     54.2274
07700     0.6235      0.7975     0.8365     54.2284
07800     0.5684      0.8207     0.8365     54.2345
07900     0.6227      0.8017     0.8365     54.4825
08000     0.6121      0.8122     0.8365     54.3406
08100     0.5893      0.8228     0.8365     55.6118
08200     0.5862      0.8101     0.8365     55.1147
08300     0.5952      0.8186     0.8365     55.0385
08400     0.5518      0.8143     0.8365     54.6721
08500     0.5863      0.8101     0.8365     54.8185
08600     0.5515      0.8333     0.8365     55.1675
08700     0.5457      0.8312     0.8365     53.9387
08800     0.5843      0.7975     0.8365     54.8869
08900     0.5056      0.8418     0.8365     54.2391
09000     0.6250      0.7996     0.8365     54.9521
09100     0.5665      0.8038     0.8365     54.6349
09200     0.5830      0.8165     0.8365     54.5439
09300     0.5478      0.8291     0.8365     54.1677
09400     0.6020      0.8080     0.8365     55.7040
09500     0.5821      0.8186     0.8365     56.3497
09600     0.5202      0.8333     0.8365     55.8143
09700     0.5402      0.8207     0.8386     55.7951
09800     0.4507      0.8629     0.8386     54.5682
09900     0.5774      0.8291     0.8386     55.3891
10000     0.5395      0.8291     0.8386     54.3878
10100     0.5391      0.8481     0.8386     55.2116
10200     0.5327      0.8270     0.8386     55.3995
10300     0.4830      0.8502     0.8386     54.9708
10400     0.5307      0.8186     0.8386     55.6562
10500     0.5552      0.8017     0.8386     55.2537
10600     0.5358      0.8376     0.8386     55.0931
10700     0.5532      0.8080     0.8386     55.7270
10800     0.5279      0.8376     0.8389     54.4800
10900     0.5826      0.7996     0.8389     56.3689
11000     0.5102      0.8397     0.8430     55.8128
11100     0.5656      0.8270     0.8430     55.1998
11200     0.5333      0.8207     0.8430     55.2306
11300     0.4679      0.8523     0.8430     55.1923
11400     0.5249      0.8439     0.8430     54.2612
11500     0.4586      0.8418     0.8430     56.9188
11600     0.4930      0.8333     0.8430     54.4319
11700     0.5152      0.8270     0.8430     54.8138
11800     0.5048      0.8439     0.8430     54.1329
11900     0.5750      0.8207     0.8430     54.2036
12000     0.4748      0.8565     0.8430     55.1915
12100     0.5200      0.8165     0.8430     54.6851
12200     0.5538      0.8143     0.8430     54.8808
12300     0.5460      0.8249     0.8430     55.1764
12400     0.4822      0.8608     0.8430     54.2385
12500     0.5270      0.8333     0.8430     54.0860
12600     0.5053      0.8460     0.8430     55.0315
12700     0.4979      0.8376     0.8430     54.2446
12800     0.4927      0.8439     0.8430     54.6361
12900     0.5198      0.8312     0.8487     54.4885
13000     0.5865      0.8165     0.8487     53.5808
13100     0.4023      0.8797     0.8487     54.7404
13200     0.5126      0.8312     0.8487     53.9198
13300     0.4752      0.8565     0.8487     55.7513
13400     0.4714      0.8608     0.8487     54.8475
13500     0.4696      0.8608     0.8487     54.1450
13600     0.4212      0.8713     0.8487     55.0188
13700     0.4573      0.8608     0.8487     54.2422
13800     0.5361      0.8354     0.8487     55.5129
13900     0.4751      0.8333     0.8487     55.6137
14000     0.5030      0.8354     0.8487     54.5668
14100     0.4652      0.8481     0.8487     54.8538
14200     0.5014      0.8207     0.8487     55.1862
14300     0.4311      0.8671     0.8487     54.4777
14400     0.5046      0.8418     0.8487     54.8128
14500     0.5109      0.8418     0.8487     54.7790
14600     0.4309      0.8650     0.8487     54.3380
14700     0.5003      0.8481     0.8487     55.0175
14800     0.5080      0.8228     0.8487     55.6868
14900     0.4939      0.8544     0.8487     55.3903
15000     0.6336      0.8038     0.8487     56.0857
15100     0.5282      0.8333     0.8487     54.1070
15200     0.4530      0.8460     0.8487     55.4211
15300     0.5352      0.8312     0.8487     54.7677
15400     0.5218      0.8143     0.8487     56.9057
15500     0.5386      0.8418     0.8487     55.9704
15600     0.4748      0.8354     0.8514     55.0068
15700     0.5807      0.8207     0.8514     54.3568
15800     0.5261      0.8418     0.8514     56.4227
15900     0.5542      0.8122     0.8514     54.7818
16000     0.5146      0.8354     0.8514     55.7246
16100     0.5258      0.8312     0.8514     54.9872
16200     0.5104      0.8418     0.8514     54.2497
16300     0.5802      0.8312     0.8514     55.3512
16400     0.4814      0.8523     0.8514     54.5655
16500     0.6213      0.7954     0.8514     54.3352
16600     0.5015      0.8439     0.8514     55.0736
16700     0.4972      0.8312     0.8514     55.4631
16800     0.5103      0.8418     0.8514     57.4236
16900     0.5000      0.8397     0.8516     55.6771
17000     0.4872      0.8270     0.8516     54.9743
17100     0.5058      0.8354     0.8516     55.2869
17200     0.4410      0.8481     0.8516     55.4970
17300     0.5269      0.8249     0.8516     53.9389
17400     0.4938      0.8523     0.8516     55.4425
17500     0.5630      0.8228     0.8516     54.3717
17600     0.4608      0.8523     0.8516     55.4229
17700     0.5325      0.8249     0.8516     54.9722
17800     0.5346      0.8270     0.8516     55.3342
17900     0.5117      0.8460     0.8516     55.4136
18000     0.4550      0.8481     0.8516     53.9903
18100     0.5939      0.8207     0.8516     54.6979
18200     0.5608      0.8249     0.8516     55.7005
18300     0.4480      0.8629     0.8516     54.3477
18400     0.5351      0.8249     0.8516     56.6086
18500     0.5017      0.8439     0.8516     55.0188
18600     0.5292      0.8397     0.8516     55.0098
18700     0.5061      0.8460     0.8516     55.6750
18800     0.5137      0.8312     0.8516     54.7802
18900     0.4579      0.8460     0.8516     55.9876
19000     0.4967      0.8354     0.8516     57.1550
19100     0.4798      0.8565     0.8516     55.5151
19200     0.4670      0.8544     0.8516     55.8268
19300     0.5554      0.8143     0.8516     55.9274
19400     0.4728      0.8270     0.8516     55.1462
19500     0.4984      0.8502     0.8516     55.2852
19600     0.4935      0.8418     0.8516     56.0053
19700     0.4360      0.8671     0.8516     54.9548
19800     0.5038      0.8270     0.8516     55.8584
19900     0.5492      0.8059     0.8516     55.6787
20000     0.4731      0.8439     0.8516     53.4776
20100     0.5192      0.8186     0.8516     55.5772
20199     0.4622      0.8439     0.8516     54.6792
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     0.4646      0.8439     0.8403     8.6056
00100     0.4316      0.8608     0.8534     56.1555
00200     0.4640      0.8439     0.8534     54.8333
00300     0.4484      0.8776     0.8534     55.7137
00400     0.4010      0.8650     0.8534     53.8461
00500     0.3843      0.8840     0.8534     54.9534
00600     0.3789      0.8861     0.8535     58.5986
00700     0.4662      0.8460     0.8535     55.3111
00800     0.4204      0.8797     0.8535     55.0912
00900     0.4902      0.8291     0.8535     55.1933
01000     0.3989      0.8755     0.8547     55.0845
01100     0.3493      0.8966     0.8547     56.5060
01200     0.4090      0.8629     0.8553     55.0949
01300     0.4404      0.8692     0.8553     54.3021
01400     0.4348      0.8629     0.8553     55.7441
01500     0.5005      0.8312     0.8553     54.8757
01600     0.5033      0.8354     0.8553     56.1534
01700     0.3667      0.8840     0.8553     55.3252
01800     0.4406      0.8629     0.8553     54.5407
01900     0.4296      0.8650     0.8557     55.1622
02000     0.4263      0.8734     0.8557     55.2773
02100     0.4239      0.8755     0.8571     56.2424
02200     0.3844      0.8692     0.8571     56.0364
02300     0.4038      0.8840     0.8571     54.3838
02400     0.4081      0.8713     0.8571     55.9849
02500     0.3676      0.8861     0.8571     55.4371
02600     0.4177      0.8650     0.8571     56.2796
02700     0.4068      0.8692     0.8571     55.9427
02800     0.4735      0.8523     0.8571     54.9498
02900     0.4891      0.8481     0.8571     57.4339
03000     0.4170      0.8692     0.8571     56.6566
03100     0.4316      0.8565     0.8571     55.4632
03200     0.4670      0.8439     0.8571     55.0168
03300     0.4652      0.8650     0.8571     54.9168
03400     0.4546      0.8734     0.8571     55.0463
03500     0.3698      0.8945     0.8571     55.6829
03600     0.4081      0.8713     0.8571     55.0944
03700     0.4520      0.8629     0.8571     54.6126
03800     0.3970      0.8713     0.8571     55.7482
03900     0.4596      0.8439     0.8571     58.6671
04000     0.3892      0.8861     0.8571     56.8076
04100     0.4142      0.8671     0.8571     56.4293
04200     0.4489      0.8671     0.8571     55.4572
04300     0.3950      0.8882     0.8571     57.9396
04400     0.4042      0.8734     0.8571     57.6119
04500     0.3706      0.8903     0.8575     55.9817
04600     0.4712      0.8629     0.8575     55.5374
04700     0.4435      0.8608     0.8575     55.7658
04800     0.4444      0.8776     0.8575     56.2720
04900     0.4736      0.8586     0.8577     55.0936
05000     0.4114      0.8650     0.8577     55.2461
05100     0.4140      0.8819     0.8577     56.7136
05200     0.4706      0.8418     0.8577     55.7975
05300     0.4201      0.8903     0.8577     55.3257
05400     0.4196      0.8755     0.8582     55.9959
05500     0.4530      0.8397     0.8582     55.2705
05600     0.4709      0.8565     0.8582     57.2784
05700     0.4143      0.8692     0.8582     56.0411
05800     0.4060      0.8797     0.8582     55.9668
05900     0.3921      0.8713     0.8582     55.8693
06000     0.3508      0.8966     0.8587     56.1437
06100     0.4039      0.8861     0.8587     56.7357
06200     0.5070      0.8397     0.8598     55.8275
06300     0.4143      0.8840     0.8598     55.4529
06400     0.4435      0.8650     0.8598     55.5384
06500     0.4833      0.8502     0.8598     56.4750
06600     0.3443      0.8861     0.8598     55.7754
06700     0.4928      0.8460     0.8598     55.2353
06800     0.3776      0.8882     0.8598     54.7301
06900     0.3549      0.8903     0.8598     55.7477
07000     0.4441      0.8755     0.8598     56.5018
07100     0.4164      0.8840     0.8598     54.7631
07200     0.3804      0.8713     0.8598     55.6218
07300     0.4304      0.8671     0.8598     56.3032
07400     0.4591      0.8544     0.8598     55.2364
07500     0.5160      0.8249     0.8610     55.4441
07600     0.4361      0.8565     0.8610     55.6217
07700     0.3503      0.8924     0.8610     54.5644
07800     0.4009      0.8924     0.8610     55.0454
07900     0.3476      0.8861     0.8610     54.7045
08000     0.4165      0.8734     0.8610     55.4600
08100     0.3981      0.8776     0.8610     55.9507
08200     0.4371      0.8692     0.8610     55.1416
08300     0.4286      0.8713     0.8610     56.6977
08400     0.4408      0.8692     0.8610     55.6075
08500     0.3971      0.8776     0.8610     55.4272
08600     0.4017      0.8565     0.8610     56.6566
08700     0.4610      0.8544     0.8610     54.9320
08800     0.4295      0.8586     0.8610     55.5533
08900     0.4162      0.8819     0.8610     56.3517
09000     0.4111      0.8629     0.8610     55.4065
09100     0.3615      0.8966     0.8610     55.8236
09200     0.4475      0.8608     0.8610     55.6752
09300     0.4527      0.8608     0.8610     55.3867
09400     0.4624      0.8523     0.8610     56.3072
09500     0.3960      0.8776     0.8610     56.0509
09600     0.3597      0.9008     0.8610     58.4973
09700     0.4251      0.8586     0.8610     56.6661
09800     0.4438      0.8544     0.8610     55.4477
09900     0.4329      0.8629     0.8610     55.4930
Start testing:
Test Accuracy: 0.8399
