Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=12, quant_actNM=12, quant_inp=12, quant_w=12, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
2db53f7b-9dc1-4cfb-96ae-b4729fdf06fc
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 373, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, 1.)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 82, in forward
    if len(x_range) > 1:
TypeError: object of type 'float' has no len()
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=12, quant_actNM=12, quant_inp=12, quant_w=12, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
2c97830b-b844-4855-aba1-2a5fa630fe29
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 373, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, torch.tensor([1.]).device(input.device))
TypeError: 'torch.device' object is not callable
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=12, quant_actNM=12, quant_inp=12, quant_w=12, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
45cfa000-0436-4239-b55c-178658c536bc
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     3.4639      0.1034     0.1334     11.2539
00100     2.1646      0.2384     0.3136     75.2954
00200     1.5873      0.5105     0.5110     74.4506
00300     1.3369      0.5759     0.5982     73.6942
00400     1.2069      0.6076     0.6507     73.7971
00500     1.0400      0.6772     0.6844     73.6981
00600     1.0358      0.6772     0.7053     74.0960
00700     0.9310      0.7004     0.7246     74.0035
00800     0.9334      0.7110     0.7464     73.5099
00900     0.9603      0.6709     0.7501     74.0683
01000     0.8604      0.7342     0.7565     73.1026
01100     0.8236      0.7489     0.7712     74.2254
01200     0.8439      0.7553     0.7712     73.7162
01300     0.8561      0.7468     0.7809     73.8328
01400     0.7610      0.7257     0.7836     73.8887
01500     0.6686      0.7764     0.7856     72.9756
01600     0.7065      0.7911     0.7952     73.3465
01700     0.7091      0.7658     0.7952     72.6655
01800     0.6863      0.7806     0.8034     73.8790
01900     0.6202      0.7975     0.8034     73.0263
02000     0.6922      0.7806     0.8041     73.4838
02100     0.7586      0.7679     0.8041     72.7784
02200     0.7092      0.7869     0.8115     73.1618
02300     0.7122      0.7764     0.8115     73.1778
02400     0.6616      0.7806     0.8238     73.9573
02500     0.6017      0.7975     0.8238     73.4604
02600     0.5540      0.8207     0.8238     73.3228
02700     0.6267      0.8270     0.8303     72.8574
02800     0.5743      0.8122     0.8303     73.2815
02900     0.6851      0.7806     0.8303     72.8114
03000     0.6224      0.8249     0.8303     73.5590
03100     0.6989      0.7743     0.8303     73.7304
03200     0.5882      0.7932     0.8303     73.7553
03300     0.7241      0.7743     0.8303     72.7830
03400     0.6527      0.8101     0.8303     73.0167
03500     0.5922      0.7911     0.8303     73.7441
03600     0.5264      0.8502     0.8303     72.9406
03700     0.6342      0.7996     0.8303     73.3192
03800     0.6410      0.7806     0.8303     73.7864
03900     0.5369      0.8439     0.8317     73.5298
04000     0.5631      0.8165     0.8317     73.9956
04100     0.5489      0.8270     0.8317     72.9577
04200     0.6589      0.7869     0.8317     74.4118
04300     0.5515      0.8207     0.8362     73.5107
04400     0.5318      0.8397     0.8362     74.0557
04500     0.6360      0.7932     0.8362     73.6256
04600     0.5372      0.8291     0.8362     73.7183
04700     0.5901      0.8165     0.8362     76.7011
04800     0.5846      0.8207     0.8362     74.9412
04900     0.6191      0.8059     0.8381     73.4504
05000     0.6120      0.7954     0.8381     73.0274
05100     0.5268      0.8270     0.8381     73.8814
05200     0.5951      0.8143     0.8381     73.2906
05300     0.4979      0.8333     0.8381     73.0454
05400     0.5220      0.8460     0.8381     72.9921
05500     0.5360      0.8291     0.8381     71.9782
05600     0.5659      0.8059     0.8381     72.9366
05700     0.5397      0.8291     0.8381     74.0216
05800     0.5877      0.8122     0.8381     73.8549
05900     0.4454      0.8523     0.8381     73.1049
06000     0.5494      0.8101     0.8394     73.8007
06100     0.5869      0.8186     0.8394     73.4671
06200     0.6005      0.8165     0.8394     73.1365
06300     0.5232      0.8439     0.8394     73.0916
06400     0.5583      0.8186     0.8394     73.9766
06500     0.5241      0.8333     0.8394     73.5762
06600     0.5555      0.8312     0.8394     73.6814
06700     0.5951      0.8059     0.8408     73.1059
06800     0.5227      0.8650     0.8408     73.7228
06900     0.5563      0.8397     0.8424     73.4760
07000     0.4561      0.8481     0.8424     73.3955
07100     0.4947      0.8481     0.8424     72.9593
07200     0.5518      0.8207     0.8424     73.6718
07300     0.6274      0.7954     0.8424     73.4204
07400     0.4533      0.8418     0.8424     73.5547
07500     0.5882      0.7996     0.8424     74.0303
07600     0.6083      0.8080     0.8429     73.7353
07700     0.5530      0.8249     0.8429     73.4382
07800     0.5206      0.8333     0.8429     74.1240
07900     0.4709      0.8376     0.8429     73.5544
08000     0.5757      0.8101     0.8429     73.9537
08100     0.5316      0.8312     0.8429     73.9936
08200     0.4739      0.8481     0.8429     74.2812
08300     0.5189      0.8354     0.8429     74.2160
08400     0.5423      0.8101     0.8429     73.6892
08500     0.5585      0.8122     0.8429     74.1164
08600     0.5427      0.8249     0.8429     74.1003
08700     0.4726      0.8291     0.8429     73.2048
08800     0.5250      0.8397     0.8444     73.7515
08900     0.5010      0.8376     0.8444     77.7516
09000     0.4756      0.8439     0.8444     76.2468
09100     0.4530      0.8460     0.8444     74.2550
09200     0.5201      0.8333     0.8444     73.5972
09300     0.5112      0.8397     0.8446     74.4435
09400     0.4879      0.8418     0.8446     74.0086
09500     0.6322      0.7932     0.8456     73.9261
09600     0.5143      0.8312     0.8456     74.4186
09700     0.5018      0.8291     0.8476     75.2022
09800     0.4655      0.8481     0.8476     74.3040
09900     0.5071      0.8333     0.8476     74.0663
10000     0.6042      0.8270     0.8476     74.0130
10100     0.5159      0.8439     0.8476     75.0260
10200     0.4609      0.8565     0.8476     74.2548
10300     0.4912      0.8418     0.8476     75.1971
10400     0.5387      0.8228     0.8499     74.9117
10500     0.4773      0.8692     0.8499     74.3196
10600     0.5332      0.8460     0.8530     74.7212
10700     0.4451      0.8650     0.8530     74.3530
10800     0.4761      0.8460     0.8530     74.3458
10900     0.4503      0.8650     0.8538     73.8816
11000     0.5431      0.8186     0.8538     73.6608
11100     0.5261      0.8291     0.8538     74.4553
11200     0.4931      0.8502     0.8538     74.8657
11300     0.4763      0.8565     0.8538     73.8344
11400     0.4366      0.8460     0.8538     74.4894
11500     0.4728      0.8544     0.8538     74.4109
11600     0.4827      0.8523     0.8538     73.6051
11700     0.4333      0.8439     0.8538     74.8009
11800     0.4663      0.8502     0.8538     73.8314
11900     0.4528      0.8544     0.8538     73.9016
12000     0.5010      0.8418     0.8538     73.8803
12100     0.5115      0.8460     0.8538     74.1649
12200     0.4695      0.8565     0.8538     73.8264
12300     0.5436      0.8207     0.8549     73.8689
12400     0.4879      0.8439     0.8549     74.0987
12500     0.5047      0.8586     0.8553     74.3590
12600     0.4904      0.8397     0.8560     73.9642
12700     0.4940      0.8291     0.8560     73.6564
12800     0.5233      0.8418     0.8568     73.9559
12900     0.4125      0.8692     0.8572     73.5725
13000     0.4542      0.8502     0.8593     74.4720
13100     0.5358      0.8376     0.8593     74.1251
13200     0.4559      0.8502     0.8593     74.2848
13300     0.4528      0.8713     0.8593     74.2010
13400     0.4758      0.8565     0.8593     74.0970
13500     0.4872      0.8418     0.8593     73.8621
13600     0.5092      0.8249     0.8593     73.6460
13700     0.5050      0.8502     0.8593     73.4319
13800     0.5137      0.8291     0.8593     74.2862
13900     0.4831      0.8418     0.8593     73.8141
14000     0.5011      0.8397     0.8593     74.3705
14100     0.5026      0.8376     0.8593     74.0116
14200     0.4383      0.8861     0.8593     73.8847
14300     0.5467      0.8165     0.8593     73.7329
14400     0.4718      0.8502     0.8593     73.9252
14500     0.4378      0.8544     0.8593     73.4174
14600     0.4867      0.8312     0.8593     73.8962
14700     0.5235      0.8397     0.8593     74.2616
14800     0.4468      0.8460     0.8593     74.0735
14900     0.5013      0.8333     0.8593     74.7875
15000     0.4247      0.8755     0.8593     74.4197
15100     0.5266      0.8249     0.8593     74.1802
15200     0.5004      0.8397     0.8607     73.8484
15300     0.5036      0.8439     0.8607     74.1987
15400     0.4854      0.8586     0.8607     74.7504
15500     0.5093      0.8249     0.8607     73.7670
15600     0.4516      0.8565     0.8607     74.1477
15700     0.4862      0.8502     0.8607     74.6536
15800     0.4775      0.8523     0.8607     74.7505
15900     0.5608      0.8165     0.8635     73.6972
16000     0.4032      0.8819     0.8635     74.1312
16100     0.3884      0.8903     0.8635     74.3489
16200     0.4274      0.8713     0.8635     74.3867
16300     0.5646      0.8291     0.8635     74.1326
16400     0.4728      0.8502     0.8635     74.4814
16500     0.5193      0.8397     0.8635     73.8439
16600     0.4066      0.8671     0.8635     73.9448
16700     0.3797      0.8903     0.8635     74.1480
16800     0.4567      0.8629     0.8635     74.3059
16900     0.5054      0.8439     0.8635     73.5349
17000     0.4827      0.8397     0.8635     73.6360
17100     0.4647      0.8376     0.8635     73.7509
17200     0.4147      0.8608     0.8635     74.3480
17300     0.4776      0.8544     0.8635     74.0179
17400     0.4598      0.8565     0.8635     73.7883
17500     0.4697      0.8333     0.8635     74.2106
17600     0.4455      0.8565     0.8635     73.5559
17700     0.4425      0.8586     0.8635     72.6546
17800     0.4523      0.8523     0.8635     73.5772
17900     0.4548      0.8629     0.8635     74.8477
18000     0.4303      0.8692     0.8635     74.8685
18100     0.4517      0.8565     0.8635     74.9159
18200     0.4110      0.8671     0.8635     75.0091
18300     0.5055      0.8333     0.8635     74.0502
18400     0.4172      0.8671     0.8635     74.6422
18500     0.5133      0.8207     0.8635     73.7155
18600     0.5240      0.8376     0.8635     73.7673
18700     0.4175      0.8608     0.8635     73.2372
18800     0.4953      0.8418     0.8635     73.9004
18900     0.4483      0.8481     0.8635     73.7739
19000     0.4274      0.8692     0.8635     73.6593
19100     0.4772      0.8460     0.8635     73.7983
19200     0.4531      0.8608     0.8635     74.6341
19300     0.4614      0.8544     0.8635     74.8294
19400     0.5020      0.8312     0.8635     74.5192
19500     0.4688      0.8502     0.8635     74.4864
19600     0.5020      0.8354     0.8635     74.4978
19700     0.4726      0.8608     0.8635     74.6346
19800     0.5054      0.8544     0.8635     73.7823
19900     0.4548      0.8586     0.8635     74.5560
20000     0.5094      0.8291     0.8635     73.8574
20100     0.3849      0.8692     0.8635     74.0533
20199     0.4769      0.8713     0.8635     72.7755
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     0.3777      0.8945     0.8633     10.1883
00100     0.4126      0.8713     0.8634     73.4821
00200     0.4301      0.8565     0.8665     73.9879
00300     0.4073      0.8734     0.8665     73.4278
00400     0.4143      0.8650     0.8665     73.3719
00500     0.4347      0.8586     0.8665     74.6914
00600     0.4357      0.8713     0.8665     73.5784
00700     0.3817      0.8882     0.8665     73.3617
00800     0.3644      0.8903     0.8665     73.4472
00900     0.3391      0.8840     0.8665     73.3563
01000     0.3637      0.8776     0.8665     74.4863
01100     0.3891      0.8819     0.8665     73.0826
01200     0.3694      0.8903     0.8665     74.2466
01300     0.4087      0.8671     0.8665     74.3255
01400     0.3517      0.8861     0.8665     73.5142
01500     0.3857      0.8819     0.8665     74.2511
01600     0.3848      0.8755     0.8665     73.8954
01700     0.4593      0.8523     0.8665     73.6726
01800     0.3837      0.8776     0.8672     74.4697
01900     0.4543      0.8629     0.8672     73.0356
02000     0.3508      0.8797     0.8672     73.2097
02100     0.3630      0.8882     0.8672     74.6628
02200     0.3534      0.8903     0.8672     73.6302
02300     0.3738      0.8861     0.8674     73.6133
02400     0.3984      0.8734     0.8674     73.5992
02500     0.3874      0.8776     0.8674     73.5293
02600     0.4835      0.8439     0.8676     74.1281
02700     0.4662      0.8460     0.8679     73.7361
02800     0.4211      0.8671     0.8679     73.5968
02900     0.3745      0.8755     0.8679     74.5346
03000     0.3634      0.8924     0.8679     74.0592
03100     0.4463      0.8460     0.8679     73.3929
03200     0.4141      0.8797     0.8679     73.6810
03300     0.3481      0.8966     0.8679     73.9705
03400     0.3690      0.8713     0.8679     73.8468
03500     0.3592      0.8776     0.8679     72.7989
03600     0.3917      0.8755     0.8679     73.5236
03700     0.3748      0.8924     0.8695     74.0327
03800     0.3907      0.8671     0.8695     73.1739
03900     0.3777      0.8882     0.8695     73.0123
04000     0.4142      0.8586     0.8695     73.1356
04100     0.3167      0.9093     0.8695     73.2897
04200     0.3992      0.8797     0.8695     73.7972
04300     0.3673      0.8861     0.8695     73.8824
04400     0.3618      0.8987     0.8695     73.9283
04500     0.3875      0.8945     0.8695     74.4792
04600     0.4006      0.8692     0.8695     73.5603
04700     0.4296      0.8755     0.8695     73.6644
04800     0.3532      0.8840     0.8695     73.6289
04900     0.3426      0.8903     0.8695     73.6426
05000     0.3896      0.8945     0.8696     74.8239
05100     0.4260      0.8776     0.8705     75.0007
05200     0.3834      0.8797     0.8705     74.5491
05300     0.3926      0.8755     0.8706     74.9025
05400     0.4120      0.8755     0.8706     73.8072
05500     0.4448      0.8692     0.8706     73.8585
05600     0.3766      0.8819     0.8706     74.1745
05700     0.3961      0.8755     0.8706     73.1930
05800     0.3512      0.8945     0.8706     74.0002
05900     0.3916      0.8734     0.8706     73.7123
06000     0.3744      0.8650     0.8706     73.4941
06100     0.3488      0.9030     0.8706     73.4099
06200     0.3789      0.8903     0.8706     73.7918
06300     0.3534      0.8882     0.8706     73.4122
06400     0.3262      0.8966     0.8706     73.4021
06500     0.3421      0.8924     0.8706     73.0219
06600     0.3624      0.8945     0.8706     74.4275
06700     0.3537      0.8861     0.8706     74.1702
06800     0.4307      0.8755     0.8706     73.7073
06900     0.3738      0.8840     0.8706     74.2431
07000     0.3527      0.9008     0.8706     73.5670
07100     0.4757      0.8586     0.8706     74.3287
07200     0.4144      0.8608     0.8706     74.6069
07300     0.4483      0.8523     0.8706     73.2732
07400     0.4220      0.8586     0.8706     74.9555
07500     0.3725      0.8797     0.8706     73.7455
07600     0.3584      0.8819     0.8706     74.0204
07700     0.4098      0.8819     0.8706     75.3714
07800     0.3570      0.8734     0.8706     73.6091
07900     0.3840      0.8903     0.8706     73.9221
08000     0.3948      0.8671     0.8706     74.0083
08100     0.4136      0.8608     0.8706     73.8052
08200     0.3088      0.9008     0.8706     74.4460
08300     0.4390      0.8650     0.8706     73.4157
08400     0.3601      0.8903     0.8706     73.3782
08500     0.4326      0.8565     0.8706     73.9161
08600     0.4257      0.8713     0.8706     73.5296
08700     0.3650      0.8692     0.8706     73.1752
08800     0.3862      0.8924     0.8706     74.9832
08900     0.3653      0.8819     0.8706     73.8422
09000     0.3724      0.8945     0.8706     74.5872
09100     0.3389      0.8840     0.8706     73.6414
09200     0.4385      0.8755     0.8706     73.3729
09300     0.3876      0.8755     0.8706     73.5002
09400     0.4517      0.8481     0.8706     74.2810
09500     0.3971      0.8840     0.8706     73.7353
09600     0.3462      0.8987     0.8706     73.9355
09700     0.3361      0.9030     0.8706     73.5706
09800     0.3981      0.8840     0.8706     73.8508
09900     0.3631      0.8861     0.8706     73.1774
Start testing:
Test Accuracy: 0.8515
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
