Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=12, quant_actNM=12, quant_inp=12, quant_w=12, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
5fac4de4-9065-4922-b868-e8a91febd08b
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 373, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, 1.)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 82, in forward
    if len(x_range) > 1:
TypeError: object of type 'float' has no len()
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=12, quant_actNM=12, quant_inp=12, quant_w=12, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
e19920ac-27e6-4723-a6bf-9783f28c808b
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 373, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, torch.tensor([1.]).device(input.device))
TypeError: 'torch.device' object is not callable
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=12, quant_actNM=12, quant_inp=12, quant_w=12, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
d6f9c1e3-3c9e-45e0-9239-7391e28a598b
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     3.5764      0.0633     0.0688     9.9026
00100     2.0815      0.3101     0.3455     55.6970
00200     1.5989      0.4747     0.5135     55.9075
00300     1.3421      0.5865     0.5853     56.7493
00400     1.2255      0.6245     0.6414     56.1801
00500     1.1440      0.6350     0.6802     56.6034
00600     0.9590      0.7089     0.6802     56.7390
00700     0.9499      0.7110     0.7061     55.9658
00800     0.8788      0.7257     0.7221     56.5654
00900     0.8787      0.7447     0.7425     56.7751
01000     0.7499      0.7553     0.7521     56.7186
01100     0.8554      0.7110     0.7606     57.0517
01200     0.8264      0.7278     0.7606     56.3600
01300     0.8185      0.7468     0.7703     55.8501
01400     0.7955      0.7616     0.7778     56.5382
01500     0.7888      0.7426     0.7823     56.6222
01600     0.7714      0.7511     0.7823     56.8646
01700     0.6834      0.7848     0.7855     55.4648
01800     0.6341      0.8080     0.7875     56.4499
01900     0.6698      0.7785     0.7875     56.5966
02000     0.7170      0.7869     0.7944     56.1955
02100     0.6649      0.7890     0.8036     56.4802
02200     0.7711      0.7321     0.8036     57.0842
02300     0.6354      0.7996     0.8037     56.7938
02400     0.6491      0.7975     0.8037     57.3855
02500     0.7016      0.7743     0.8120     57.3636
02600     0.6669      0.7996     0.8120     56.2152
02700     0.6509      0.7932     0.8120     57.2386
02800     0.6498      0.7827     0.8120     56.0708
02900     0.6558      0.8038     0.8168     56.1673
03000     0.6570      0.7911     0.8168     56.9301
03100     0.6250      0.8017     0.8168     55.7467
03200     0.6666      0.7911     0.8168     56.8326
03300     0.7154      0.7785     0.8168     56.7863
03400     0.6000      0.8059     0.8192     56.3611
03500     0.6654      0.8038     0.8192     56.8908
03600     0.6614      0.7911     0.8192     56.1518
03700     0.6137      0.8080     0.8192     56.1139
03800     0.6161      0.8207     0.8192     55.7467
03900     0.5769      0.8101     0.8192     55.8487
04000     0.6361      0.8059     0.8203     56.4423
04100     0.6098      0.7954     0.8247     55.9102
04200     0.6546      0.8017     0.8258     55.9337
04300     0.5653      0.8270     0.8258     57.8475
04400     0.6203      0.8186     0.8258     57.5771
04500     0.6776      0.7743     0.8258     57.2801
04600     0.6268      0.7890     0.8258     56.6818
04700     0.5233      0.8249     0.8286     56.7679
04800     0.5627      0.8122     0.8323     58.0126
04900     0.6120      0.8143     0.8323     55.9021
05000     0.6438      0.8080     0.8323     56.3594
05100     0.5550      0.8080     0.8323     58.9732
05200     0.5948      0.8101     0.8323     57.8279
05300     0.5747      0.8101     0.8323     57.9508
05400     0.5350      0.8228     0.8342     58.9081
05500     0.6583      0.7996     0.8361     57.9739
05600     0.6684      0.7954     0.8361     57.9339
05700     0.5386      0.8354     0.8361     57.8193
05800     0.6228      0.7996     0.8361     56.9270
05900     0.5883      0.8291     0.8361     59.3073
06000     0.5035      0.8481     0.8361     57.3334
06100     0.6024      0.7975     0.8361     58.0389
06200     0.5394      0.8354     0.8361     57.7180
06300     0.5881      0.8080     0.8361     57.8595
06400     0.5783      0.8080     0.8361     58.0018
06500     0.5673      0.8080     0.8361     56.4687
06600     0.5567      0.8418     0.8361     56.5918
06700     0.5412      0.8502     0.8361     56.3685
06800     0.6276      0.7975     0.8361     56.1265
06900     0.5109      0.8312     0.8361     55.9813
07000     0.6340      0.8059     0.8361     56.5151
07100     0.5469      0.8249     0.8361     56.5070
07200     0.5912      0.8038     0.8361     56.6349
07300     0.6701      0.7869     0.8361     55.7917
07400     0.4589      0.8397     0.8361     56.4774
07500     0.4854      0.8439     0.8361     56.5704
07600     0.5849      0.8122     0.8361     56.4771
07700     0.5807      0.8270     0.8361     55.7225
07800     0.5129      0.8418     0.8361     56.5692
07900     0.5240      0.8502     0.8361     56.3737
08000     0.5444      0.8354     0.8361     56.6137
08100     0.5107      0.8312     0.8361     55.8698
08200     0.5720      0.7932     0.8361     56.1909
08300     0.5657      0.8270     0.8361     56.7839
08400     0.5746      0.8059     0.8361     56.4369
08500     0.5412      0.8228     0.8411     56.4396
08600     0.5435      0.8249     0.8411     56.0657
08700     0.4623      0.8460     0.8411     55.9439
08800     0.5959      0.7932     0.8411     57.0265
08900     0.4610      0.8629     0.8411     56.1729
09000     0.5217      0.8165     0.8411     56.7074
09100     0.5479      0.8186     0.8411     56.2923
09200     0.5653      0.8228     0.8412     55.7585
09300     0.5792      0.8186     0.8412     57.6687
09400     0.5560      0.8249     0.8422     57.5527
09500     0.4770      0.8502     0.8422     58.5923
09600     0.5280      0.8312     0.8422     58.6284
09700     0.4965      0.8439     0.8422     58.0239
09800     0.6014      0.7996     0.8471     57.6675
09900     0.4991      0.8354     0.8471     58.4200
10000     0.5494      0.8165     0.8471     57.6680
10100     0.5037      0.8418     0.8471     57.1057
10200     0.4807      0.8608     0.8471     56.3226
10300     0.5072      0.8397     0.8471     57.0320
10400     0.4768      0.8460     0.8481     56.9721
10500     0.4931      0.8333     0.8481     55.9557
10600     0.4607      0.8481     0.8482     56.7737
10700     0.4645      0.8819     0.8482     57.1962
10800     0.5129      0.8397     0.8482     57.0524
10900     0.5136      0.8312     0.8482     56.9214
11000     0.5215      0.8312     0.8487     56.9518
11100     0.4748      0.8586     0.8487     56.0793
11200     0.4636      0.8418     0.8506     56.4869
11300     0.4297      0.8608     0.8506     55.7741
11400     0.4438      0.8671     0.8506     56.3791
11500     0.4088      0.8819     0.8506     56.5726
11600     0.5276      0.8291     0.8506     55.8423
11700     0.5135      0.8059     0.8506     56.2514
11800     0.5464      0.8228     0.8506     56.2789
11900     0.5152      0.8460     0.8506     55.6824
12000     0.4879      0.8333     0.8506     56.5756
12100     0.5447      0.8418     0.8506     56.4310
12200     0.5401      0.8354     0.8506     55.9833
12300     0.4493      0.8650     0.8506     56.8231
12400     0.5114      0.8122     0.8509     55.9590
12500     0.4357      0.8586     0.8509     56.0627
12600     0.4804      0.8586     0.8509     56.4816
12700     0.4530      0.8608     0.8509     56.4510
12800     0.4920      0.8333     0.8509     56.1066
12900     0.4535      0.8565     0.8509     56.1060
13000     0.4718      0.8608     0.8509     55.4776
13100     0.5035      0.8418     0.8537     56.5152
13200     0.4575      0.8565     0.8537     55.9498
13300     0.4537      0.8544     0.8537     56.4286
13400     0.4572      0.8692     0.8537     56.2607
13500     0.5331      0.8270     0.8550     55.8226
13600     0.5308      0.8291     0.8550     56.3202
13700     0.5346      0.8397     0.8550     55.6302
13800     0.5121      0.8439     0.8550     55.8300
13900     0.3933      0.8734     0.8550     56.3146
14000     0.4485      0.8544     0.8550     56.0020
14100     0.4645      0.8523     0.8551     56.6956
14200     0.5144      0.8418     0.8551     57.0435
14300     0.3612      0.8755     0.8551     55.7036
14400     0.4328      0.8629     0.8551     57.0625
14500     0.4267      0.8650     0.8551     56.3563
14600     0.4582      0.8671     0.8551     56.2359
14700     0.4980      0.8439     0.8551     56.2817
14800     0.4801      0.8376     0.8551     56.0398
14900     0.4851      0.8439     0.8551     55.8503
15000     0.4941      0.8397     0.8551     56.4779
15100     0.4575      0.8608     0.8551     56.7202
15200     0.5345      0.8376     0.8551     58.2690
15300     0.5181      0.8418     0.8551     56.8914
15400     0.5365      0.8122     0.8551     56.0432
15500     0.4765      0.8418     0.8551     56.5772
15600     0.4717      0.8586     0.8551     56.2614
15700     0.5361      0.8481     0.8551     56.3216
15800     0.4440      0.8544     0.8551     56.8429
15900     0.4616      0.8502     0.8551     55.9327
16000     0.4738      0.8481     0.8562     56.0813
16100     0.5046      0.8418     0.8562     55.8430
16200     0.4661      0.8354     0.8562     58.2175
16300     0.4931      0.8333     0.8562     58.4091
16400     0.6046      0.8333     0.8562     58.4811
16500     0.3502      0.8966     0.8562     58.3330
16600     0.4221      0.8544     0.8562     58.8908
16700     0.4580      0.8502     0.8562     57.9867
16800     0.4948      0.8544     0.8562     58.9712
16900     0.5363      0.8165     0.8562     58.8823
17000     0.4447      0.8713     0.8562     58.7759
17100     0.5096      0.8354     0.8573     58.2110
17200     0.4670      0.8523     0.8573     57.3300
17300     0.5113      0.8523     0.8573     55.9901
17400     0.4694      0.8544     0.8573     56.7098
17500     0.5475      0.8228     0.8573     56.2688
17600     0.5249      0.8291     0.8573     56.5823
17700     0.4308      0.8819     0.8573     56.1479
17800     0.5340      0.8312     0.8573     56.9648
17900     0.4843      0.8460     0.8573     56.2280
18000     0.4234      0.8776     0.8573     57.4555
18100     0.4800      0.8460     0.8573     58.0163
18200     0.3776      0.8840     0.8573     56.9384
18300     0.4814      0.8523     0.8573     56.5092
18400     0.4220      0.8797     0.8573     57.1862
18500     0.4687      0.8376     0.8573     58.4585
18600     0.4548      0.8671     0.8573     56.2159
18700     0.4791      0.8397     0.8573     56.9933
18800     0.4818      0.8354     0.8573     55.7608
18900     0.5089      0.8481     0.8574     56.7717
19000     0.4744      0.8291     0.8574     56.5820
19100     0.4650      0.8608     0.8574     56.1906
19200     0.5447      0.8101     0.8574     57.2366
19300     0.4837      0.8586     0.8574     58.0403
19400     0.4889      0.8333     0.8574     57.2065
19500     0.4675      0.8481     0.8574     58.0845
19600     0.4997      0.8397     0.8574     56.6100
19700     0.5622      0.8291     0.8574     55.7136
19800     0.5850      0.8207     0.8574     56.8251
19900     0.3943      0.8819     0.8574     56.1013
20000     0.5103      0.8270     0.8574     56.2818
20100     0.4520      0.8650     0.8574     55.7294
20199     0.5456      0.8312     0.8574     55.9800
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     0.4124      0.8629     0.8521     8.9427
00100     0.3791      0.8797     0.8543     57.1201
00200     0.3988      0.8776     0.8543     55.7289
00300     0.4324      0.8755     0.8550     56.4079
00400     0.4123      0.8713     0.8550     55.9382
00500     0.4018      0.8797     0.8550     55.5439
00600     0.3942      0.8797     0.8557     56.9021
00700     0.4211      0.8629     0.8580     57.0059
00800     0.4582      0.8544     0.8580     56.2057
00900     0.3940      0.8692     0.8580     56.2735
01000     0.3947      0.8797     0.8580     55.4231
01100     0.3372      0.8966     0.8580     56.5018
01200     0.3872      0.8797     0.8580     55.4217
01300     0.3862      0.8629     0.8580     55.3478
01400     0.4058      0.8650     0.8580     56.7101
01500     0.3986      0.8882     0.8580     55.8785
01600     0.3922      0.8797     0.8600     55.8599
01700     0.4055      0.8861     0.8600     56.9354
01800     0.3752      0.8882     0.8600     55.6822
01900     0.3176      0.8861     0.8600     56.2511
02000     0.4286      0.8734     0.8600     56.2806
02100     0.3624      0.9051     0.8600     55.7127
02200     0.3520      0.9008     0.8604     56.6835
02300     0.3882      0.8924     0.8604     55.9659
02400     0.4227      0.8671     0.8604     56.6088
02500     0.3537      0.9051     0.8604     57.5114
02600     0.3738      0.8650     0.8623     55.8557
02700     0.4206      0.8776     0.8623     56.3729
02800     0.3745      0.8776     0.8623     56.3266
02900     0.4151      0.8734     0.8623     56.4542
03000     0.4143      0.8692     0.8623     56.7823
03100     0.4119      0.8776     0.8623     56.6406
03200     0.3773      0.8840     0.8623     56.2203
03300     0.3577      0.8924     0.8623     58.0437
03400     0.4245      0.8797     0.8623     57.2860
03500     0.4197      0.8523     0.8623     56.9990
03600     0.4356      0.8565     0.8623     55.4770
03700     0.4194      0.8418     0.8623     56.1896
03800     0.4489      0.8565     0.8623     56.3404
03900     0.3852      0.8776     0.8623     56.2695
04000     0.3420      0.9008     0.8623     56.3311
04100     0.4155      0.8608     0.8623     56.7571
04200     0.3763      0.8903     0.8623     56.6778
04300     0.3925      0.8671     0.8623     56.8524
04400     0.3833      0.8755     0.8623     55.8233
04500     0.3855      0.8734     0.8623     57.5201
04600     0.3969      0.8755     0.8623     56.2346
04700     0.3570      0.8903     0.8623     55.4848
04800     0.4327      0.8692     0.8623     56.2521
04900     0.4069      0.8671     0.8623     56.5024
05000     0.4232      0.8608     0.8623     55.8648
05100     0.3866      0.8692     0.8623     58.5996
05200     0.3494      0.9008     0.8623     57.1567
05300     0.3595      0.8861     0.8623     56.3242
05400     0.4181      0.8565     0.8623     57.4797
05500     0.4513      0.8629     0.8623     56.1983
05600     0.4462      0.8608     0.8623     57.0179
05700     0.3780      0.8797     0.8623     57.0254
05800     0.4062      0.8671     0.8623     56.2615
05900     0.3754      0.8713     0.8623     57.2892
06000     0.2986      0.9156     0.8623     56.2675
06100     0.3228      0.8966     0.8623     55.8755
06200     0.3508      0.8882     0.8623     56.5082
06300     0.4012      0.8713     0.8623     56.1552
06400     0.3830      0.8671     0.8623     56.1645
06500     0.3696      0.8861     0.8623     56.4028
06600     0.4224      0.8671     0.8623     56.2816
06700     0.4562      0.8502     0.8623     56.6264
06800     0.3861      0.8650     0.8623     56.0664
06900     0.3300      0.8987     0.8623     55.8581
07000     0.3654      0.8797     0.8623     56.0391
07100     0.3598      0.8924     0.8623     56.3752
07200     0.4174      0.8629     0.8623     55.6515
07300     0.3709      0.8713     0.8623     56.6294
07400     0.4100      0.8755     0.8623     56.0536
07500     0.4026      0.8713     0.8623     58.2950
07600     0.3902      0.8713     0.8623     56.6273
07700     0.3835      0.8755     0.8628     55.8976
07800     0.4507      0.8586     0.8628     56.5576
07900     0.4418      0.8671     0.8628     56.3438
08000     0.3717      0.8819     0.8628     56.4139
08100     0.3306      0.8987     0.8628     57.0513
08200     0.3614      0.8987     0.8628     56.5010
08300     0.3858      0.8734     0.8628     57.2019
08400     0.4603      0.8481     0.8628     55.9323
08500     0.3554      0.9030     0.8628     56.9408
08600     0.3253      0.8987     0.8628     56.2944
08700     0.3812      0.8650     0.8628     56.4971
08800     0.3405      0.9051     0.8628     56.7666
08900     0.3523      0.8819     0.8628     57.1016
09000     0.4122      0.8608     0.8628     58.6882
09100     0.3728      0.8713     0.8628     57.9088
09200     0.3565      0.8882     0.8628     57.6209
09300     0.4093      0.8819     0.8628     56.2622
09400     0.3715      0.8819     0.8628     58.0483
09500     0.4502      0.8608     0.8628     57.1998
09600     0.4081      0.8692     0.8628     58.4016
09700     0.4150      0.8734     0.8628     57.3645
09800     0.4507      0.8629     0.8628     56.0425
09900     0.3185      0.9219     0.8628     56.7693
Start testing:
Test Accuracy: 0.8468
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
