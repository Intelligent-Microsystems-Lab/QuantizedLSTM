Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=12, quant_actNM=12, quant_inp=12, quant_w=12, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
c9851e63-1505-4db9-b903-d5fa15e4e42a
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, 1.)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 82, in forward
    if len(x_range) > 1:
TypeError: object of type 'float' has no len()
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=12, quant_actNM=12, quant_inp=12, quant_w=12, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
34ea36fa-b559-40be-b90b-52d76bbf5df8
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, torch.tensor([1.]))
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 94, in forward
    x_scaled = x/x_range
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=12, quant_actNM=12, quant_inp=12, quant_w=12, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
63e8edd6-bb91-4ba5-b94b-8246c5a3a61b
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, torch.tensor([1.]).to(input.device))
TypeError: 'torch.device' object is not callable
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=12, quant_actNM=12, quant_inp=12, quant_w=12, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
18f5b5db-5de5-4f17-8b1c-2912df1ae8c1
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.8263      0.0696     0.0883     10.3848
00100     2.3663      0.2321     0.2612     55.3232
00200     1.8075      0.4135     0.4404     54.8278
00300     1.4537      0.5401     0.5414     55.1809
00400     1.3073      0.6224     0.6081     54.5878
00500     1.1768      0.6329     0.6612     54.2830
00600     1.0621      0.6730     0.6959     55.8607
00700     0.9453      0.7215     0.7175     54.5730
00800     0.9187      0.7300     0.7315     55.1341
00900     0.9785      0.6920     0.7454     54.8826
01000     0.8918      0.7426     0.7551     54.6990
01100     0.9195      0.7194     0.7559     55.5387
01200     0.8923      0.7532     0.7669     54.7281
01300     0.8005      0.7553     0.7676     55.2074
01400     0.8997      0.7300     0.7742     54.7867
01500     0.7676      0.7743     0.7742     54.8395
01600     0.8170      0.7489     0.7745     55.3766
01700     0.7546      0.7785     0.7764     55.3268
01800     0.7640      0.7700     0.7829     55.4658
01900     0.7582      0.7806     0.7853     55.7386
02000     0.7968      0.7700     0.7853     54.6619
02100     0.7445      0.7764     0.7853     55.0815
02200     0.6941      0.7869     0.7853     55.2128
02300     0.7003      0.7932     0.7936     55.7485
02400     0.6682      0.8038     0.7936     55.9590
02500     0.7514      0.7764     0.8007     55.1822
02600     0.7375      0.7911     0.8007     54.9631
02700     0.7175      0.7932     0.8007     55.6867
02800     0.7324      0.7700     0.8046     54.6896
02900     0.7453      0.7848     0.8100     55.3738
03000     0.7205      0.7954     0.8100     55.4899
03100     0.7104      0.7996     0.8100     54.7527
03200     0.7190      0.7848     0.8100     55.2024
03300     0.6051      0.8101     0.8116     54.5355
03400     0.7353      0.7869     0.8116     54.6460
03500     0.6869      0.7806     0.8116     55.9991
03600     0.7365      0.7722     0.8116     55.9725
03700     0.7153      0.7932     0.8121     54.6619
03800     0.6500      0.8038     0.8121     55.4757
03900     0.7563      0.7827     0.8121     54.9516
04000     0.6603      0.7975     0.8121     55.2033
04100     0.6388      0.8059     0.8121     54.4548
04200     0.6047      0.8270     0.8141     54.6622
04300     0.6469      0.8143     0.8141     55.1716
04400     0.6790      0.7996     0.8141     54.8724
04500     0.5910      0.8228     0.8141     54.6416
04600     0.6612      0.8207     0.8141     55.3998
04700     0.5735      0.8291     0.8141     54.4673
04800     0.6100      0.8207     0.8141     54.9545
04900     0.6073      0.8165     0.8141     54.6282
05000     0.6462      0.8038     0.8141     54.5370
05100     0.6581      0.8207     0.8152     56.4727
05200     0.6229      0.8122     0.8152     57.0118
05300     0.7045      0.8080     0.8152     55.2269
05400     0.6536      0.7996     0.8152     54.9434
05500     0.6393      0.8080     0.8172     54.0624
05600     0.6111      0.8143     0.8189     54.9292
05700     0.6015      0.8207     0.8189     55.3020
05800     0.6797      0.7996     0.8189     54.6859
05900     0.5762      0.8291     0.8306     55.2616
06000     0.7361      0.7679     0.8306     54.5404
06100     0.6234      0.8186     0.8306     54.7243
06200     0.6344      0.8101     0.8306     55.9746
06300     0.5294      0.8481     0.8306     54.5354
06400     0.6766      0.8059     0.8306     54.9920
06500     0.6514      0.7996     0.8306     54.8723
06600     0.6712      0.7869     0.8306     55.3993
06700     0.6546      0.8143     0.8306     55.3267
06800     0.6370      0.7954     0.8306     54.8355
06900     0.6258      0.8143     0.8306     54.3662
07000     0.6660      0.8143     0.8306     54.6922
07100     0.6030      0.8186     0.8306     55.3935
07200     0.5861      0.8376     0.8306     56.2926
07300     0.7169      0.7806     0.8306     54.7557
07400     0.6074      0.8270     0.8306     54.3724
07500     0.5469      0.8376     0.8306     55.3012
07600     0.6823      0.8059     0.8306     54.7584
07700     0.5164      0.8523     0.8306     54.7923
07800     0.5962      0.8207     0.8306     55.3035
07900     0.6778      0.7911     0.8306     54.6179
08000     0.6003      0.8439     0.8306     55.8298
08100     0.6567      0.8101     0.8321     56.4524
08200     0.5528      0.8354     0.8321     55.5626
08300     0.5434      0.8397     0.8321     55.3296
08400     0.6272      0.8122     0.8321     55.0410
08500     0.6130      0.8165     0.8321     55.4414
08600     0.5441      0.8418     0.8321     55.0636
08700     0.6114      0.8165     0.8321     54.6335
08800     0.6062      0.8080     0.8321     55.0660
08900     0.6443      0.8080     0.8321     54.9718
09000     0.5603      0.8376     0.8321     54.7817
09100     0.5587      0.8291     0.8321     54.7728
09200     0.6365      0.8143     0.8321     54.8431
09300     0.5835      0.8418     0.8321     54.4857
09400     0.5623      0.8354     0.8321     54.8281
09500     0.6221      0.8080     0.8321     54.2180
09600     0.5307      0.8333     0.8321     54.9251
09700     0.5832      0.8165     0.8321     54.4305
09800     0.5909      0.8333     0.8321     54.5301
09900     0.5613      0.8270     0.8321     55.0031
10000     0.5222      0.8460     0.8321     55.9813
10100     0.6467      0.7890     0.8321     54.7563
10200     0.5984      0.8270     0.8321     54.6146
10300     0.5551      0.8481     0.8321     55.5017
10400     0.5570      0.8397     0.8321     55.4537
10500     0.5177      0.8565     0.8321     55.1237
10600     0.5562      0.8418     0.8321     55.0514
10700     0.6037      0.8270     0.8321     55.6408
10800     0.5887      0.8333     0.8321     54.9672
10900     0.5773      0.8270     0.8321     55.0850
11000     0.4715      0.8439     0.8321     54.7908
11100     0.5150      0.8397     0.8321     54.6309
11200     0.4608      0.8565     0.8321     55.2359
11300     0.5928      0.8080     0.8321     54.3360
11400     0.5384      0.8439     0.8321     54.4461
11500     0.5324      0.8439     0.8321     56.2645
11600     0.6378      0.8228     0.8321     58.5000
11700     0.6118      0.7996     0.8321     57.4681
11800     0.6146      0.8080     0.8321     56.9536
11900     0.5870      0.8186     0.8321     55.8374
12000     0.4672      0.8629     0.8321     56.8141
12100     0.5539      0.8312     0.8321     57.2963
12200     0.5128      0.8333     0.8321     59.4597
12300     0.5730      0.8376     0.8321     58.0125
12400     0.5791      0.8080     0.8321     57.8785
12500     0.5805      0.8333     0.8321     56.4385
12600     0.5386      0.8502     0.8321     57.3161
12700     0.5780      0.8354     0.8321     56.5922
12800     0.5110      0.8460     0.8321     59.8073
12900     0.5393      0.8291     0.8321     57.1861
13000     0.5504      0.8354     0.8321     57.0441
13100     0.5390      0.8397     0.8321     57.0364
13200     0.5174      0.8397     0.8336     56.7495
13300     0.5773      0.8312     0.8336     56.6983
13400     0.5641      0.8354     0.8336     56.0821
13500     0.5520      0.8418     0.8357     55.7074
13600     0.5983      0.8038     0.8357     57.3872
13700     0.5180      0.8439     0.8357     55.1348
13800     0.5274      0.8354     0.8357     54.3856
13900     0.5383      0.8291     0.8357     55.1490
14000     0.5163      0.8629     0.8357     54.8551
14100     0.5465      0.8397     0.8357     54.6324
14200     0.5192      0.8481     0.8357     54.8089
14300     0.5260      0.8481     0.8357     55.3126
14400     0.5296      0.8439     0.8357     55.2064
14500     0.6028      0.8312     0.8357     54.4667
14600     0.5269      0.8565     0.8357     54.8542
14700     0.5488      0.8439     0.8357     55.3001
14800     0.6068      0.8312     0.8360     54.7538
14900     0.5012      0.8565     0.8360     54.5557
15000     0.4929      0.8629     0.8360     55.1193
15100     0.5766      0.8354     0.8360     55.0909
15200     0.5645      0.8354     0.8360     55.0749
15300     0.5501      0.8291     0.8360     54.3222
15400     0.5477      0.8249     0.8374     54.9230
15500     0.6182      0.8207     0.8374     56.5063
15600     0.5921      0.8354     0.8374     54.7223
15700     0.5364      0.8586     0.8374     54.2958
15800     0.6519      0.7848     0.8374     55.1820
15900     0.5313      0.8565     0.8385     54.6650
16000     0.5243      0.8523     0.8385     55.6329
16100     0.6526      0.7890     0.8385     54.7082
16200     0.5804      0.8312     0.8385     54.7824
16300     0.5810      0.8460     0.8385     55.3985
16400     0.4869      0.8418     0.8385     54.7117
16500     0.5386      0.8291     0.8385     55.2677
16600     0.5668      0.8122     0.8385     55.1078
16700     0.5066      0.8418     0.8385     54.6178
16800     0.5233      0.8333     0.8385     55.9348
16900     0.5859      0.8502     0.8385     55.2407
17000     0.5718      0.8101     0.8385     55.3967
17100     0.4975      0.8397     0.8385     56.5767
17200     0.6297      0.7932     0.8385     54.6275
17300     0.6173      0.8059     0.8385     54.2545
17400     0.5223      0.8354     0.8385     55.2797
17500     0.6279      0.8143     0.8385     54.8328
17600     0.5394      0.8376     0.8385     55.4665
17700     0.5447      0.8523     0.8385     54.6372
17800     0.5639      0.8291     0.8385     54.5108
17900     0.5696      0.8439     0.8385     55.6246
18000     0.5341      0.8439     0.8385     54.6993
18100     0.5217      0.8439     0.8385     54.7145
18200     0.5662      0.8481     0.8385     55.3446
18300     0.5615      0.8354     0.8385     54.9601
18400     0.5657      0.8544     0.8385     55.1195
18500     0.5406      0.8439     0.8385     54.3127
18600     0.6437      0.7911     0.8385     54.6021
18700     0.5641      0.8354     0.8385     54.9209
18800     0.4833      0.8481     0.8385     55.1619
18900     0.6331      0.7954     0.8385     55.1425
19000     0.5055      0.8502     0.8385     55.5392
19100     0.4418      0.8734     0.8385     54.4562
19200     0.4995      0.8439     0.8385     55.1610
19300     0.5974      0.8101     0.8385     54.6138
19400     0.5477      0.8439     0.8385     54.8505
19500     0.5148      0.8502     0.8385     54.9412
19600     0.4820      0.8565     0.8385     54.7498
19700     0.5273      0.8481     0.8386     55.1047
19800     0.6077      0.8122     0.8386     55.7415
19900     0.5771      0.8333     0.8386     56.6728
20000     0.6043      0.8249     0.8390     57.5415
20100     0.5334      0.8502     0.8390     56.1819
20199     0.5728      0.8291     0.8390     55.8213
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     0.5186      0.8312     0.8362     8.8606
00100     0.3533      0.9030     0.8440     54.7962
00200     0.4858      0.8544     0.8440     54.6761
00300     0.4893      0.8586     0.8440     54.7950
00400     0.3676      0.8882     0.8440     55.4023
00500     0.4107      0.8797     0.8440     55.1329
00600     0.4710      0.8671     0.8440     54.7978
00700     0.4616      0.8629     0.8440     54.6911
00800     0.4242      0.8608     0.8440     54.5412
00900     0.5072      0.8397     0.8440     55.0962
01000     0.3840      0.9072     0.8440     55.5905
01100     0.4834      0.8460     0.8440     55.1964
01200     0.5557      0.8270     0.8440     54.9294
01300     0.4712      0.8629     0.8440     54.3633
01400     0.5011      0.8565     0.8440     55.5079
01500     0.4715      0.8502     0.8440     54.7164
01600     0.4892      0.8713     0.8440     54.2086
01700     0.4872      0.8734     0.8440     55.7518
01800     0.3728      0.9051     0.8440     54.6257
01900     0.3799      0.9008     0.8440     55.3914
02000     0.5083      0.8523     0.8440     55.0193
02100     0.4522      0.8671     0.8440     55.1598
02200     0.4230      0.8840     0.8440     54.8456
02300     0.4190      0.8797     0.8440     54.3834
02400     0.4127      0.8924     0.8440     54.5871
02500     0.4633      0.8629     0.8451     55.1876
02600     0.4650      0.8629     0.8451     54.8113
02700     0.4316      0.8755     0.8455     55.4883
02800     0.4465      0.8608     0.8456     54.3236
02900     0.4452      0.8629     0.8458     55.0049
03000     0.4754      0.8608     0.8460     55.5664
03100     0.4764      0.8650     0.8460     54.6964
03200     0.4381      0.8755     0.8460     54.7691
03300     0.4483      0.8460     0.8460     55.2910
03400     0.4261      0.8692     0.8470     54.6530
03500     0.4683      0.8755     0.8470     55.3445
03600     0.4353      0.8776     0.8470     54.8084
03700     0.4792      0.8523     0.8479     54.2847
03800     0.4343      0.8629     0.8479     54.9605
03900     0.4067      0.8840     0.8479     54.5872
04000     0.4520      0.8840     0.8479     55.0690
04100     0.4516      0.8586     0.8479     54.8936
04200     0.4820      0.8439     0.8479     54.8349
04300     0.4314      0.8692     0.8479     55.2299
04400     0.4153      0.8861     0.8479     54.6304
04500     0.4560      0.8692     0.8479     55.3146
04600     0.4890      0.8629     0.8479     56.0456
04700     0.4962      0.8418     0.8479     55.4120
04800     0.3649      0.9072     0.8479     55.2541
04900     0.4932      0.8629     0.8479     55.4934
05000     0.4479      0.8692     0.8479     54.9552
05100     0.4415      0.8734     0.8479     55.4875
05200     0.4676      0.8629     0.8479     54.2157
05300     0.4510      0.8734     0.8479     54.4504
05400     0.4730      0.8671     0.8479     55.5570
05500     0.4104      0.8797     0.8479     55.2607
05600     0.4705      0.8586     0.8479     54.8095
05700     0.4962      0.8608     0.8479     55.2205
05800     0.4333      0.8755     0.8479     54.4122
05900     0.4296      0.8861     0.8500     55.9979
06000     0.4494      0.8755     0.8500     54.4244
06100     0.4504      0.8776     0.8500     55.1143
06200     0.4560      0.8692     0.8500     55.0893
06300     0.4818      0.8629     0.8500     54.7913
06400     0.4596      0.8629     0.8500     54.6053
06500     0.4578      0.8671     0.8500     54.8497
06600     0.4872      0.8608     0.8500     55.4244
06700     0.4876      0.8629     0.8500     55.5970
06800     0.4775      0.8586     0.8500     54.6243
06900     0.3521      0.9135     0.8500     55.3779
07000     0.4599      0.8523     0.8500     55.0456
07100     0.4354      0.8671     0.8500     54.7368
07200     0.4162      0.8924     0.8500     54.8766
07300     0.5128      0.8460     0.8500     55.2397
07400     0.4723      0.8523     0.8500     54.7796
07500     0.3865      0.8882     0.8500     54.9392
07600     0.4763      0.8544     0.8500     54.6617
07700     0.4249      0.8755     0.8500     55.1006
07800     0.4368      0.8755     0.8500     55.5854
07900     0.4686      0.8565     0.8500     55.5126
08000     0.4605      0.8629     0.8504     54.7409
08100     0.4176      0.8734     0.8504     55.8785
08200     0.4279      0.8713     0.8504     54.6999
08300     0.4002      0.8987     0.8504     55.4185
08400     0.4364      0.8608     0.8505     54.0233
08500     0.4298      0.8713     0.8505     54.5489
08600     0.4608      0.8629     0.8505     55.9418
08700     0.4264      0.8776     0.8505     54.6118
08800     0.4778      0.8629     0.8505     56.2058
08900     0.4227      0.8861     0.8505     55.2057
09000     0.4521      0.8776     0.8505     54.1795
09100     0.4402      0.8713     0.8505     55.0634
09200     0.4422      0.8713     0.8505     54.4347
09300     0.4792      0.8608     0.8505     55.0665
09400     0.4572      0.8629     0.8505     55.4296
09500     0.4644      0.8713     0.8505     54.9369
09600     0.4681      0.8671     0.8505     54.6967
09700     0.4758      0.8502     0.8505     55.5012
09800     0.4404      0.8861     0.8505     54.6469
09900     0.4515      0.8523     0.8505     54.7524
Start testing:
Test Accuracy: 0.8342
