Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 362, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.0, finetuning_epochs=0, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.0005,0.0001,0.00002', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,10000', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
d9959179-8d18-43ad-a6bb-e1e476bbc5ef
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.5350      0.0844     0.0801     11.6975
00100     2.5219      0.0696     0.0803     73.6604
00200     2.5139      0.0781     0.0805     73.6595
00300     2.5096      0.0802     0.0813     76.4437
00400     2.5072      0.0802     0.0813     74.5537
00500     2.5057      0.0865     0.0813     75.2663
00600     2.5047      0.0802     0.0813     70.3581
00700     2.5039      0.0781     0.0813     74.2567
00800     2.5034      0.0823     0.0813     74.7226
00900     2.5029      0.0970     0.0813     74.3248
01000     2.5025      0.0844     0.0813     74.8312
01100     2.5022      0.0738     0.0813     76.4252
01200     2.5020      0.0675     0.0813     73.0078
01300     2.5017      0.0802     0.0813     74.0855
01400     2.5015      0.0781     0.0813     75.4101
01500     2.5014      0.0907     0.0813     75.0653
01600     2.5012      0.0802     0.0813     75.4264
01700     2.5011      0.0675     0.0813     74.6048
01800     2.5010      0.0802     0.0813     74.3561
01900     2.5008      0.0654     0.0813     74.7836
02000     2.5007      0.0738     0.0813     74.9037
02100     2.5007      0.0928     0.0813     76.8381
02200     2.5006      0.0992     0.0813     73.6681
02300     2.5005      0.0738     0.0813     74.8543
02400     2.5004      0.0949     0.0813     74.8435
02500     2.5004      0.1034     0.0813     77.3784
02600     2.5003      0.0675     0.0813     75.7860
02700     2.5003      0.0886     0.0813     76.1090
02800     2.5002      0.0949     0.0813     74.0478
02900     2.5002      0.0907     0.0813     75.7517
03000     2.5001      0.0907     0.0813     75.2841
03100     2.5001      0.0738     0.0813     74.6280
03200     2.5000      0.0886     0.0813     74.5561
03300     2.5000      0.0992     0.0813     74.5354
03400     2.5000      0.0759     0.0813     74.7895
03500     2.4999      0.0802     0.0813     75.3331
03600     2.4999      0.0886     0.0813     75.2824
03700     2.4999      0.0717     0.0813     74.8116
03800     2.4999      0.0675     0.0813     75.9333
03900     2.4998      0.0949     0.0813     74.8409
04000     2.4998      0.0802     0.0813     75.9832
04100     2.4998      0.0886     0.0813     74.4773
04200     2.4998      0.0696     0.0813     77.6569
04300     2.4997      0.0907     0.0813     76.6852
04400     2.4997      0.0907     0.0813     73.7270
04500     2.4997      0.0886     0.0813     73.8930
04600     2.4997      0.0738     0.0813     74.5031
04700     2.4997      0.0928     0.0813     75.5010
04800     2.4996      0.0865     0.0813     74.7470
04900     2.4996      0.0738     0.0813     76.3348
05000     2.4996      0.0717     0.0813     75.8093
05100     2.4996      0.0865     0.0813     76.4718
05200     2.4996      0.0738     0.0813     73.7209
05300     2.4995      0.0717     0.0813     78.5079
05400     2.4995      0.0907     0.0813     74.5293
05500     2.4995      0.0738     0.0813     76.1912
05600     2.4995      0.0949     0.0813     76.8256
05700     2.4995      0.0907     0.0813     74.5950
05800     2.4994      0.0717     0.0813     76.0792
05900     2.4994      0.0992     0.0813     74.7066
06000     2.4994      0.0591     0.0813     76.1592
06100     2.4994      0.0992     0.0813     75.2392
06200     2.4994      0.0844     0.0813     76.4253
06300     2.4993      0.0696     0.0813     76.0765
06400     2.4993      0.0928     0.0813     77.1233
06500     2.4993      0.0886     0.0813     75.2206
06600     2.4993      0.0781     0.0813     77.0416
06700     2.4993      0.0759     0.0813     76.9352
06800     2.4992      0.1034     0.0813     75.2729
06900     2.4992      0.0949     0.0813     76.2509
07000     2.4992      0.0802     0.0813     75.2406
07100     2.4992      0.0928     0.0813     76.4902
07200     2.4992      0.0696     0.0813     77.9555
07300     2.4992      0.0970     0.0813     74.6641
07400     2.4991      0.0591     0.0813     75.6730
07500     2.4991      0.0781     0.0813     76.6421
07600     2.4991      0.0759     0.0813     74.2773
07700     2.4991      0.0633     0.0813     75.0124
07800     2.4991      0.0844     0.0813     74.8970
07900     2.4990      0.0675     0.0813     76.5252
08000     2.4990      0.0823     0.0813     76.1364
08100     2.4990      0.1013     0.0813     74.2650
08200     2.4990      0.0633     0.0813     75.7117
08300     2.4990      0.0738     0.0813     76.2259
08400     2.4989      0.0633     0.0813     75.3301
08500     2.4989      0.0738     0.0813     75.1620
08600     2.4989      0.0844     0.0813     75.4223
08700     2.4989      0.0928     0.0813     76.7850
08800     2.4989      0.0633     0.0813     74.1300
08900     2.4989      0.0612     0.0813     73.7055
09000     2.4988      0.0738     0.0813     74.2664
09100     2.4988      0.0633     0.0813     75.4000
09200     2.4988      0.0570     0.0813     76.2321
09300     2.4988      0.0844     0.0813     76.7264
09400     2.4988      0.0823     0.0813     76.5352
09500     2.4987      0.0738     0.0813     75.9481
09600     2.4987      0.0696     0.0813     75.3029
09700     2.4987      0.0781     0.0813     74.0429
09800     2.4987      0.0591     0.0813     74.1645
09900     2.4987      0.0759     0.0813     77.0985
10000     2.4986      0.0759     0.0813     75.1167
10100     2.4986      0.0781     0.0813     74.0596
10200     2.4986      0.0865     0.0813     74.9800
10300     2.4986      0.0696     0.0813     76.7729
10400     2.4986      0.0928     0.0813     74.5066
10500     2.4986      0.0970     0.0813     74.3502
10600     2.4986      0.0844     0.0813     74.4465
10700     2.4986      0.0949     0.0813     76.6741
10800     2.4986      0.0675     0.0813     75.9984
10900     2.4986      0.0696     0.0813     75.5782
11000     2.4986      0.0992     0.0813     74.5849
11100     2.4986      0.0781     0.0813     75.3821
11200     2.4986      0.0738     0.0813     76.0525
11300     2.4986      0.0738     0.0813     77.1954
11400     2.4986      0.0654     0.0813     75.2767
11500     2.4986      0.0633     0.0813     77.7786
11600     2.4986      0.0886     0.0813     74.5662
11700     2.4986      0.0844     0.0813     74.4522
11800     2.4986      0.0823     0.0813     74.6290
11900     2.4986      0.0781     0.0813     76.5246
12000     2.4986      0.0759     0.0813     77.5078
12100     2.4986      0.0633     0.0813     76.1062
12200     2.4986      0.0802     0.0813     76.5290
12300     2.4986      0.0907     0.0813     73.6168
12400     2.4986      0.0654     0.0813     73.5779
12500     2.4986      0.0549     0.0813     77.6734
12600     2.4986      0.0886     0.0813     75.4643
12700     2.4985      0.0738     0.0813     75.4558
12800     2.4985      0.0717     0.0813     75.9781
12900     2.4985      0.0844     0.0813     75.7684
13000     2.4985      0.0907     0.0813     75.7706
13100     2.4985      0.0802     0.0813     75.0198
13200     2.4985      0.0907     0.0813     76.2872
13300     2.4985      0.0949     0.0813     74.8191
13400     2.4985      0.0823     0.0813     74.7627
13500     2.4985      0.0696     0.0813     77.2055
13600     2.4985      0.0802     0.0813     76.6939
13700     2.4985      0.0865     0.0813     75.7407
13800     2.4985      0.0886     0.0813     75.4446
13900     2.4985      0.0738     0.0813     77.5874
14000     2.4985      0.0949     0.0813     75.5854
14100     2.4985      0.0865     0.0813     75.9948
14200     2.4985      0.0865     0.0813     74.6564
14300     2.4985      0.0696     0.0813     76.5886
14400     2.4985      0.1034     0.0813     74.9271
14500     2.4985      0.0717     0.0813     77.7949
14600     2.4985      0.0738     0.0813     75.5160
14700     2.4985      0.0886     0.0813     77.5322
14800     2.4985      0.0696     0.0813     76.5154
14900     2.4985      0.0823     0.0813     76.8232
15000     2.4985      0.0844     0.0813     75.9103
15100     2.4985      0.1055     0.0813     74.8269
15200     2.4985      0.0717     0.0813     76.4004
15300     2.4985      0.0696     0.0813     75.1694
15400     2.4985      0.0970     0.0813     75.5033
15500     2.4984      0.0759     0.0813     76.0639
15600     2.4984      0.0928     0.0813     75.1512
15700     2.4984      0.0696     0.0813     75.2297
15800     2.4984      0.0696     0.0813     75.4112
15900     2.4984      0.0759     0.0813     74.0091
16000     2.4984      0.1160     0.0813     76.7534
16100     2.4984      0.0844     0.0813     76.2198
16200     2.4984      0.0907     0.0813     74.0745
16300     2.4984      0.0823     0.0813     77.9553
16400     2.4984      0.0886     0.0813     76.2987
16500     2.4984      0.0654     0.0813     76.8935
16600     2.4984      0.1034     0.0813     75.0220
16700     2.4984      0.0970     0.0813     74.5163
16800     2.4984      0.0696     0.0813     76.1310
16900     2.4984      0.0886     0.0813     74.5053
17000     2.4984      0.0886     0.0813     76.2649
17100     2.4984      0.0886     0.0813     76.2314
17200     2.4984      0.0549     0.0813     74.5057
17300     2.4984      0.0506     0.0813     75.8698
17400     2.4984      0.0823     0.0813     76.6355
17500     2.4984      0.0738     0.0813     75.0415
17600     2.4984      0.0802     0.0813     79.1308
17700     2.4984      0.0928     0.0813     76.6489
17800     2.4984      0.0949     0.0813     76.4590
17900     2.4984      0.0654     0.0813     76.6482
18000     2.4984      0.0928     0.0813     75.7009
18100     2.4984      0.0759     0.0813     74.9573
18200     2.4984      0.0612     0.0813     74.9289
18300     2.4983      0.0802     0.0813     75.6493
18400     2.4983      0.0823     0.0813     77.1110
18500     2.4983      0.0865     0.0813     74.2314
18600     2.4983      0.0992     0.0813     74.0032
18700     2.4983      0.0717     0.0813     74.9425
18800     2.4983      0.0738     0.0813     75.6074
18900     2.4983      0.0464     0.0813     76.6146
19000     2.4983      0.0886     0.0813     75.8335
19100     2.4983      0.0527     0.0813     75.1585
19200     2.4983      0.0886     0.0813     74.8821
19300     2.4983      0.0928     0.0813     73.8153
19400     2.4983      0.0781     0.0813     75.3346
19500     2.4983      0.0865     0.0813     77.4456
19600     2.4983      0.0802     0.0813     76.1463
19700     2.4983      0.0865     0.0813     75.5911
19800     2.4983      0.0844     0.0813     74.8555
19900     2.4983      0.0654     0.0813     76.3759
20000     2.4983      0.0717     0.0813     75.9745
20100     2.4983      0.0886     0.0813     74.8467
20200     2.4983      0.0717     0.0813     75.4786
20300     2.4983      0.0802     0.0817     75.1855
20400     2.4983      0.0675     0.0817     75.8324
20500     2.4983      0.0907     0.0817     75.0114
20600     2.4983      0.0781     0.0817     75.1145
20700     2.4983      0.0612     0.0817     77.4114
20800     2.4983      0.0717     0.0817     75.6869
20900     2.4983      0.0717     0.0817     76.5776
21000     2.4983      0.0865     0.0817     75.3910
21100     2.4983      0.0675     0.0817     76.7226
21200     2.4983      0.0781     0.0817     76.3854
21300     2.4983      0.0633     0.0817     75.4725
21400     2.4983      0.0802     0.0817     77.0167
21500     2.4983      0.0759     0.0817     74.8134
21600     2.4983      0.0738     0.0817     77.2927
21700     2.4983      0.0970     0.0817     75.9471
21800     2.4983      0.0823     0.0817     75.5171
21900     2.4983      0.1076     0.0817     77.4257
22000     2.4983      0.0591     0.0817     76.1430
22100     2.4983      0.0759     0.0817     74.3355
22200     2.4983      0.0865     0.0817     76.5293
22300     2.4983      0.0738     0.0817     75.7325
22400     2.4983      0.0633     0.0817     75.1007
22500     2.4983      0.0781     0.0817     76.7568
22600     2.4983      0.0759     0.0817     76.0393
22700     2.4983      0.0781     0.0817     76.2881
22800     2.4983      0.0612     0.0817     75.2554
22900     2.4983      0.0802     0.0817     74.6749
23000     2.4983      0.1034     0.0817     76.5499
23100     2.4983      0.0570     0.0817     76.2834
23200     2.4983      0.0907     0.0817     75.3392
23300     2.4983      0.0717     0.0817     74.9887
23400     2.4983      0.0823     0.0817     75.8762
23500     2.4983      0.0781     0.0817     75.0085
23600     2.4983      0.0696     0.0817     74.3040
23700     2.4983      0.0675     0.0817     77.2710
23800     2.4983      0.0696     0.0817     75.4909
23900     2.4983      0.0823     0.0817     74.1177
24000     2.4983      0.0696     0.0817     75.5657
24100     2.4983      0.0907     0.0817     75.6545
24200     2.4983      0.0717     0.0817     75.2301
24300     2.4983      0.0570     0.0817     74.8112
24400     2.4983      0.0781     0.0817     76.2889
24500     2.4982      0.0781     0.0817     77.0902
24600     2.4982      0.0781     0.0817     76.0130
24700     2.4982      0.0844     0.0817     77.8238
24800     2.4982      0.0696     0.0817     77.1873
24900     2.4982      0.0738     0.0817     75.1230
25000     2.4982      0.0696     0.0817     75.9006
25100     2.4982      0.0717     0.0817     75.1840
25200     2.4982      0.0802     0.0817     75.5193
25300     2.4982      0.0675     0.0817     76.5680
25400     2.4982      0.0759     0.0817     75.8766
25500     2.4982      0.0802     0.0817     75.4418
25600     2.4982      0.0928     0.0817     75.7793
25700     2.4982      0.0696     0.0817     76.9381
25800     2.4982      0.0633     0.0817     76.2192
25900     2.4982      0.0591     0.0817     76.6560
26000     2.4982      0.1055     0.0817     74.8343
26100     2.4982      0.0844     0.0817     75.8883
26200     2.4982      0.0549     0.0817     76.5378
26300     2.4982      0.0738     0.0817     77.0561
26400     2.4982      0.1055     0.0817     77.2040
26500     2.4982      0.0738     0.0817     74.2009
26600     2.4982      0.0675     0.0817     76.6769
26700     2.4982      0.0823     0.0817     75.3207
26800     2.4982      0.0570     0.0817     75.9315
26900     2.4982      0.0865     0.0817     76.0994
27000     2.4982      0.0696     0.0817     75.3019
27100     2.4982      0.0949     0.0817     77.1326
27200     2.4982      0.0823     0.0817     76.7070
27300     2.4982      0.0823     0.0817     75.0044
27400     2.4982      0.0759     0.0817     77.4607
27500     2.4982      0.0802     0.0817     75.7524
27600     2.4982      0.0633     0.0817     76.1257
27700     2.4982      0.0759     0.0817     75.3792
27800     2.4982      0.0612     0.0817     77.1872
27900     2.4982      0.0696     0.0817     74.6870
28000     2.4982      0.0591     0.0817     78.2728
28100     2.4982      0.0949     0.0817     73.0473
28200     2.4982      0.1013     0.0817     72.0473
28300     2.4982      0.0802     0.0817     73.4267
28400     2.4982      0.0570     0.0817     71.9455
28500     2.4982      0.0865     0.0817     71.9899
28600     2.4982      0.0717     0.0817     72.8869
28700     2.4982      0.0675     0.0817     73.2631
28800     2.4982      0.0907     0.0817     73.6957
28900     2.4982      0.0928     0.0817     71.9169
29000     2.4982      0.0717     0.0817     72.8279
29100     2.4982      0.0823     0.0817     72.3140
29200     2.4982      0.0928     0.0817     72.6784
29300     2.4982      0.0633     0.0817     71.5529
29400     2.4982      0.0527     0.0817     72.5252
29500     2.4982      0.0802     0.0817     70.7820
29600     2.4982      0.0717     0.0817     72.1124
29700     2.4982      0.0738     0.0817     71.8637
29800     2.4982      0.0675     0.0817     71.2726
29900     2.4982      0.0759     0.0817     72.0130
29999     2.4982      0.0949     0.0817     71.1444
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Start testing:
Test Accuracy: 0.0789
