Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 362, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.0, finetuning_epochs=0, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.0005,0.0001,0.00002', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,10000', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
d9959179-8d18-43ad-a6bb-e1e476bbc5ef
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.5350      0.0844     0.0801     11.6975
00100     2.5219      0.0696     0.0803     73.6604
00200     2.5139      0.0781     0.0805     73.6595
00300     2.5096      0.0802     0.0813     76.4437
00400     2.5072      0.0802     0.0813     74.5537
00500     2.5057      0.0865     0.0813     75.2663
00600     2.5047      0.0802     0.0813     70.3581
00700     2.5039      0.0781     0.0813     74.2567
00800     2.5034      0.0823     0.0813     74.7226
00900     2.5029      0.0970     0.0813     74.3248
01000     2.5025      0.0844     0.0813     74.8312
01100     2.5022      0.0738     0.0813     76.4252
01200     2.5020      0.0675     0.0813     73.0078
01300     2.5017      0.0802     0.0813     74.0855
01400     2.5015      0.0781     0.0813     75.4101
01500     2.5014      0.0907     0.0813     75.0653
01600     2.5012      0.0802     0.0813     75.4264
01700     2.5011      0.0675     0.0813     74.6048
01800     2.5010      0.0802     0.0813     74.3561
01900     2.5008      0.0654     0.0813     74.7836
02000     2.5007      0.0738     0.0813     74.9037
02100     2.5007      0.0928     0.0813     76.8381
02200     2.5006      0.0992     0.0813     73.6681
02300     2.5005      0.0738     0.0813     74.8543
02400     2.5004      0.0949     0.0813     74.8435
02500     2.5004      0.1034     0.0813     77.3784
02600     2.5003      0.0675     0.0813     75.7860
02700     2.5003      0.0886     0.0813     76.1090
02800     2.5002      0.0949     0.0813     74.0478
02900     2.5002      0.0907     0.0813     75.7517
03000     2.5001      0.0907     0.0813     75.2841
03100     2.5001      0.0738     0.0813     74.6280
03200     2.5000      0.0886     0.0813     74.5561
03300     2.5000      0.0992     0.0813     74.5354
03400     2.5000      0.0759     0.0813     74.7895
03500     2.4999      0.0802     0.0813     75.3331
03600     2.4999      0.0886     0.0813     75.2824
03700     2.4999      0.0717     0.0813     74.8116
03800     2.4999      0.0675     0.0813     75.9333
03900     2.4998      0.0949     0.0813     74.8409
04000     2.4998      0.0802     0.0813     75.9832
04100     2.4998      0.0886     0.0813     74.4773
04200     2.4998      0.0696     0.0813     77.6569
04300     2.4997      0.0907     0.0813     76.6852
04400     2.4997      0.0907     0.0813     73.7270
04500     2.4997      0.0886     0.0813     73.8930
04600     2.4997      0.0738     0.0813     74.5031
04700     2.4997      0.0928     0.0813     75.5010
04800     2.4996      0.0865     0.0813     74.7470
04900     2.4996      0.0738     0.0813     76.3348
05000     2.4996      0.0717     0.0813     75.8093
05100     2.4996      0.0865     0.0813     76.4718
05200     2.4996      0.0738     0.0813     73.7209
05300     2.4995      0.0717     0.0813     78.5079
05400     2.4995      0.0907     0.0813     74.5293
05500     2.4995      0.0738     0.0813     76.1912
05600     2.4995      0.0949     0.0813     76.8256
05700     2.4995      0.0907     0.0813     74.5950
05800     2.4994      0.0717     0.0813     76.0792
05900     2.4994      0.0992     0.0813     74.7066
06000     2.4994      0.0591     0.0813     76.1592
06100     2.4994      0.0992     0.0813     75.2392
06200     2.4994      0.0844     0.0813     76.4253
06300     2.4993      0.0696     0.0813     76.0765
06400     2.4993      0.0928     0.0813     77.1233
06500     2.4993      0.0886     0.0813     75.2206
06600     2.4993      0.0781     0.0813     77.0416
06700     2.4993      0.0759     0.0813     76.9352
06800     2.4992      0.1034     0.0813     75.2729
06900     2.4992      0.0949     0.0813     76.2509
07000     2.4992      0.0802     0.0813     75.2406
07100     2.4992      0.0928     0.0813     76.4902
07200     2.4992      0.0696     0.0813     77.9555
07300     2.4992      0.0970     0.0813     74.6641
07400     2.4991      0.0591     0.0813     75.6730
07500     2.4991      0.0781     0.0813     76.6421
07600     2.4991      0.0759     0.0813     74.2773
07700     2.4991      0.0633     0.0813     75.0124
07800     2.4991      0.0844     0.0813     74.8970
07900     2.4990      0.0675     0.0813     76.5252
08000     2.4990      0.0823     0.0813     76.1364
08100     2.4990      0.1013     0.0813     74.2650
08200     2.4990      0.0633     0.0813     75.7117
08300     2.4990      0.0738     0.0813     76.2259
08400     2.4989      0.0633     0.0813     75.3301
08500     2.4989      0.0738     0.0813     75.1620
08600     2.4989      0.0844     0.0813     75.4223
08700     2.4989      0.0928     0.0813     76.7850
08800     2.4989      0.0633     0.0813     74.1300
08900     2.4989      0.0612     0.0813     73.7055
09000     2.4988      0.0738     0.0813     74.2664
09100     2.4988      0.0633     0.0813     75.4000
09200     2.4988      0.0570     0.0813     76.2321
09300     2.4988      0.0844     0.0813     76.7264
09400     2.4988      0.0823     0.0813     76.5352
09500     2.4987      0.0738     0.0813     75.9481
09600     2.4987      0.0696     0.0813     75.3029
09700     2.4987      0.0781     0.0813     74.0429
09800     2.4987      0.0591     0.0813     74.1645
09900     2.4987      0.0759     0.0813     77.0985
10000     2.4986      0.0759     0.0813     75.1167
10100     2.4986      0.0781     0.0813     74.0596
10200     2.4986      0.0865     0.0813     74.9800
10300     2.4986      0.0696     0.0813     76.7729
10400     2.4986      0.0928     0.0813     74.5066
10500     2.4986      0.0970     0.0813     74.3502
10600     2.4986      0.0844     0.0813     74.4465
10700     2.4986      0.0949     0.0813     76.6741
10800     2.4986      0.0675     0.0813     75.9984
10900     2.4986      0.0696     0.0813     75.5782
11000     2.4986      0.0992     0.0813     74.5849
11100     2.4986      0.0781     0.0813     75.3821
11200     2.4986      0.0738     0.0813     76.0525
11300     2.4986      0.0738     0.0813     77.1954
11400     2.4986      0.0654     0.0813     75.2767
11500     2.4986      0.0633     0.0813     77.7786
11600     2.4986      0.0886     0.0813     74.5662
11700     2.4986      0.0844     0.0813     74.4522
11800     2.4986      0.0823     0.0813     74.6290
11900     2.4986      0.0781     0.0813     76.5246
12000     2.4986      0.0759     0.0813     77.5078
12100     2.4986      0.0633     0.0813     76.1062
12200     2.4986      0.0802     0.0813     76.5290
12300     2.4986      0.0907     0.0813     73.6168
12400     2.4986      0.0654     0.0813     73.5779
12500     2.4986      0.0549     0.0813     77.6734
12600     2.4986      0.0886     0.0813     75.4643
12700     2.4985      0.0738     0.0813     75.4558
12800     2.4985      0.0717     0.0813     75.9781
12900     2.4985      0.0844     0.0813     75.7684
13000     2.4985      0.0907     0.0813     75.7706
13100     2.4985      0.0802     0.0813     75.0198
13200     2.4985      0.0907     0.0813     76.2872
13300     2.4985      0.0949     0.0813     74.8191
13400     2.4985      0.0823     0.0813     74.7627
13500     2.4985      0.0696     0.0813     77.2055
13600     2.4985      0.0802     0.0813     76.6939
13700     2.4985      0.0865     0.0813     75.7407
13800     2.4985      0.0886     0.0813     75.4446
13900     2.4985      0.0738     0.0813     77.5874
14000     2.4985      0.0949     0.0813     75.5854
14100     2.4985      0.0865     0.0813     75.9948
14200     2.4985      0.0865     0.0813     74.6564
14300     2.4985      0.0696     0.0813     76.5886
14400     2.4985      0.1034     0.0813     74.9271
14500     2.4985      0.0717     0.0813     77.7949
14600     2.4985      0.0738     0.0813     75.5160
14700     2.4985      0.0886     0.0813     77.5322
14800     2.4985      0.0696     0.0813     76.5154
14900     2.4985      0.0823     0.0813     76.8232
15000     2.4985      0.0844     0.0813     75.9103
15100     2.4985      0.1055     0.0813     74.8269
15200     2.4985      0.0717     0.0813     76.4004
15300     2.4985      0.0696     0.0813     75.1694
15400     2.4985      0.0970     0.0813     75.5033
15500     2.4984      0.0759     0.0813     76.0639
15600     2.4984      0.0928     0.0813     75.1512
15700     2.4984      0.0696     0.0813     75.2297
15800     2.4984      0.0696     0.0813     75.4112
15900     2.4984      0.0759     0.0813     74.0091
16000     2.4984      0.1160     0.0813     76.7534
16100     2.4984      0.0844     0.0813     76.2198
16200     2.4984      0.0907     0.0813     74.0745
16300     2.4984      0.0823     0.0813     77.9553
16400     2.4984      0.0886     0.0813     76.2987
16500     2.4984      0.0654     0.0813     76.8935
16600     2.4984      0.1034     0.0813     75.0220
16700     2.4984      0.0970     0.0813     74.5163
16800     2.4984      0.0696     0.0813     76.1310
16900     2.4984      0.0886     0.0813     74.5053
17000     2.4984      0.0886     0.0813     76.2649
17100     2.4984      0.0886     0.0813     76.2314
17200     2.4984      0.0549     0.0813     74.5057
17300     2.4984      0.0506     0.0813     75.8698
17400     2.4984      0.0823     0.0813     76.6355
17500     2.4984      0.0738     0.0813     75.0415
17600     2.4984      0.0802     0.0813     79.1308
17700     2.4984      0.0928     0.0813     76.6489
17800     2.4984      0.0949     0.0813     76.4590
17900     2.4984      0.0654     0.0813     76.6482
18000     2.4984      0.0928     0.0813     75.7009
18100     2.4984      0.0759     0.0813     74.9573
18200     2.4984      0.0612     0.0813     74.9289
18300     2.4983      0.0802     0.0813     75.6493
18400     2.4983      0.0823     0.0813     77.1110
18500     2.4983      0.0865     0.0813     74.2314
18600     2.4983      0.0992     0.0813     74.0032
18700     2.4983      0.0717     0.0813     74.9425
18800     2.4983      0.0738     0.0813     75.6074
18900     2.4983      0.0464     0.0813     76.6146
19000     2.4983      0.0886     0.0813     75.8335
19100     2.4983      0.0527     0.0813     75.1585
19200     2.4983      0.0886     0.0813     74.8821
19300     2.4983      0.0928     0.0813     73.8153
19400     2.4983      0.0781     0.0813     75.3346
19500     2.4983      0.0865     0.0813     77.4456
19600     2.4983      0.0802     0.0813     76.1463
19700     2.4983      0.0865     0.0813     75.5911
19800     2.4983      0.0844     0.0813     74.8555
19900     2.4983      0.0654     0.0813     76.3759
20000     2.4983      0.0717     0.0813     75.9745
20100     2.4983      0.0886     0.0813     74.8467
20200     2.4983      0.0717     0.0813     75.4786
20300     2.4983      0.0802     0.0817     75.1855
20400     2.4983      0.0675     0.0817     75.8324
20500     2.4983      0.0907     0.0817     75.0114
20600     2.4983      0.0781     0.0817     75.1145
20700     2.4983      0.0612     0.0817     77.4114
20800     2.4983      0.0717     0.0817     75.6869
20900     2.4983      0.0717     0.0817     76.5776
21000     2.4983      0.0865     0.0817     75.3910
21100     2.4983      0.0675     0.0817     76.7226
21200     2.4983      0.0781     0.0817     76.3854
21300     2.4983      0.0633     0.0817     75.4725
21400     2.4983      0.0802     0.0817     77.0167
21500     2.4983      0.0759     0.0817     74.8134
21600     2.4983      0.0738     0.0817     77.2927
21700     2.4983      0.0970     0.0817     75.9471
21800     2.4983      0.0823     0.0817     75.5171
21900     2.4983      0.1076     0.0817     77.4257
22000     2.4983      0.0591     0.0817     76.1430
22100     2.4983      0.0759     0.0817     74.3355
22200     2.4983      0.0865     0.0817     76.5293
22300     2.4983      0.0738     0.0817     75.7325
22400     2.4983      0.0633     0.0817     75.1007
22500     2.4983      0.0781     0.0817     76.7568
22600     2.4983      0.0759     0.0817     76.0393
22700     2.4983      0.0781     0.0817     76.2881
22800     2.4983      0.0612     0.0817     75.2554
22900     2.4983      0.0802     0.0817     74.6749
23000     2.4983      0.1034     0.0817     76.5499
23100     2.4983      0.0570     0.0817     76.2834
23200     2.4983      0.0907     0.0817     75.3392
23300     2.4983      0.0717     0.0817     74.9887
23400     2.4983      0.0823     0.0817     75.8762
23500     2.4983      0.0781     0.0817     75.0085
23600     2.4983      0.0696     0.0817     74.3040
23700     2.4983      0.0675     0.0817     77.2710
23800     2.4983      0.0696     0.0817     75.4909
23900     2.4983      0.0823     0.0817     74.1177
24000     2.4983      0.0696     0.0817     75.5657
24100     2.4983      0.0907     0.0817     75.6545
24200     2.4983      0.0717     0.0817     75.2301
24300     2.4983      0.0570     0.0817     74.8112
24400     2.4983      0.0781     0.0817     76.2889
24500     2.4982      0.0781     0.0817     77.0902
24600     2.4982      0.0781     0.0817     76.0130
24700     2.4982      0.0844     0.0817     77.8238
24800     2.4982      0.0696     0.0817     77.1873
24900     2.4982      0.0738     0.0817     75.1230
25000     2.4982      0.0696     0.0817     75.9006
25100     2.4982      0.0717     0.0817     75.1840
25200     2.4982      0.0802     0.0817     75.5193
25300     2.4982      0.0675     0.0817     76.5680
25400     2.4982      0.0759     0.0817     75.8766
25500     2.4982      0.0802     0.0817     75.4418
25600     2.4982      0.0928     0.0817     75.7793
25700     2.4982      0.0696     0.0817     76.9381
25800     2.4982      0.0633     0.0817     76.2192
25900     2.4982      0.0591     0.0817     76.6560
26000     2.4982      0.1055     0.0817     74.8343
26100     2.4982      0.0844     0.0817     75.8883
26200     2.4982      0.0549     0.0817     76.5378
26300     2.4982      0.0738     0.0817     77.0561
26400     2.4982      0.1055     0.0817     77.2040
26500     2.4982      0.0738     0.0817     74.2009
26600     2.4982      0.0675     0.0817     76.6769
26700     2.4982      0.0823     0.0817     75.3207
26800     2.4982      0.0570     0.0817     75.9315
26900     2.4982      0.0865     0.0817     76.0994
27000     2.4982      0.0696     0.0817     75.3019
27100     2.4982      0.0949     0.0817     77.1326
27200     2.4982      0.0823     0.0817     76.7070
27300     2.4982      0.0823     0.0817     75.0044
27400     2.4982      0.0759     0.0817     77.4607
27500     2.4982      0.0802     0.0817     75.7524
27600     2.4982      0.0633     0.0817     76.1257
27700     2.4982      0.0759     0.0817     75.3792
27800     2.4982      0.0612     0.0817     77.1872
27900     2.4982      0.0696     0.0817     74.6870
28000     2.4982      0.0591     0.0817     78.2728
28100     2.4982      0.0949     0.0817     73.0473
28200     2.4982      0.1013     0.0817     72.0473
28300     2.4982      0.0802     0.0817     73.4267
28400     2.4982      0.0570     0.0817     71.9455
28500     2.4982      0.0865     0.0817     71.9899
28600     2.4982      0.0717     0.0817     72.8869
28700     2.4982      0.0675     0.0817     73.2631
28800     2.4982      0.0907     0.0817     73.6957
28900     2.4982      0.0928     0.0817     71.9169
29000     2.4982      0.0717     0.0817     72.8279
29100     2.4982      0.0823     0.0817     72.3140
29200     2.4982      0.0928     0.0817     72.6784
29300     2.4982      0.0633     0.0817     71.5529
29400     2.4982      0.0527     0.0817     72.5252
29500     2.4982      0.0802     0.0817     70.7820
29600     2.4982      0.0717     0.0817     72.1124
29700     2.4982      0.0738     0.0817     71.8637
29800     2.4982      0.0675     0.0817     71.2726
29900     2.4982      0.0759     0.0817     72.0130
29999     2.4982      0.0949     0.0817     71.1444
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Start testing:
Test Accuracy: 0.0789
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=3, quant_actNM=3, quant_inp=3, quant_w=3, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
8e6636cf-1d4b-4456-9410-ecfcfa841505
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.5350      0.0717     0.0807     15.1754
00100     2.5068      0.0570     0.0810     77.1983
00200     2.5033      0.0865     0.0810     76.9954
00300     2.5020      0.0823     0.0810     75.5333
00400     2.5013      0.0591     0.0810     76.1306
00500     2.5008      0.0907     0.0810     75.9626
00600     2.5005      0.0886     0.0810     75.1146
00700     2.5003      0.0970     0.0810     76.5211
00800     2.5001      0.0738     0.0810     76.7092
00900     2.5000      0.0717     0.0810     76.3027
01000     2.4999      0.0928     0.0810     75.3688
01100     2.4998      0.0759     0.0810     77.1348
01200     2.4997      0.0738     0.0810     78.3202
01300     2.4996      0.0907     0.0810     76.7956
01400     2.4995      0.0654     0.0810     76.4811
01500     2.4994      0.0781     0.0810     75.5549
01600     2.4993      0.0696     0.0810     76.8048
01700     2.4993      0.0949     0.0810     76.6354
01800     2.4992      0.0844     0.0810     76.2258
01900     2.4991      0.0738     0.0810     77.3625
02000     2.4990      0.0675     0.0810     77.6264
02100     2.4990      0.0970     0.0810     75.6799
02200     2.4989      0.0844     0.0810     75.9712
02300     2.4988      0.0844     0.0810     76.8430
02400     2.4987      0.0823     0.0810     76.2518
02500     2.4987      0.1097     0.0810     76.3195
02600     2.4986      0.0738     0.0810     76.4064
02700     2.4985      0.0970     0.0810     75.9594
02800     2.4984      0.0928     0.0810     75.5012
02900     2.4984      0.0865     0.0810     75.4317
03000     2.4983      0.0654     0.0810     76.3604
03100     2.4982      0.0759     0.0810     77.2106
03200     2.4982      0.0949     0.0810     75.1852
03300     2.4981      0.1076     0.0810     77.6469
03400     2.4980      0.0570     0.0810     75.5589
03500     2.4979      0.0633     0.0810     77.3278
03600     2.4979      0.0886     0.0810     75.8366
03700     2.4978      0.0823     0.0810     76.2581
03800     2.4977      0.0759     0.0810     77.7900
03900     2.4977      0.0759     0.0810     75.4555
04000     2.4976      0.0527     0.0810     78.1519
04100     2.4975      0.0654     0.0810     76.7177
04200     2.4975      0.0865     0.0810     75.3468
04300     2.4974      0.0802     0.0810     77.5620
04400     2.4973      0.0844     0.0810     75.6833
04500     2.4972      0.0802     0.0810     76.5751
04600     2.4972      0.0802     0.0810     76.5116
04700     2.4971      0.0527     0.0814     76.9539
04800     2.4970      0.0633     0.0814     75.8055
04900     2.4970      0.0992     0.0814     77.3822
05000     2.4969      0.1055     0.0814     76.5486
05100     2.4968      0.0781     0.0814     77.1966
05200     2.4968      0.0802     0.0814     77.1971
05300     2.4967      0.0823     0.0814     75.4141
05400     2.4966      0.0527     0.0814     77.2452
05500     2.4966      0.0738     0.0814     75.4877
05600     2.4965      0.0970     0.0814     76.3747
05700     2.4964      0.0612     0.0814     76.9275
05800     2.4964      0.0823     0.0814     75.9133
05900     2.4963      0.0675     0.0814     77.2596
06000     2.4962      0.0928     0.0814     76.0246
06100     2.4962      0.0759     0.0814     75.2439
06200     2.4961      0.0992     0.0814     77.5758
06300     2.4960      0.1076     0.0814     76.7967
06400     2.4960      0.0802     0.0814     76.3422
06500     2.4959      0.0654     0.0814     76.9393
06600     2.4958      0.0844     0.0814     76.5347
06700     2.4958      0.1076     0.0814     76.8125
06800     2.4957      0.0738     0.0814     76.3319
06900     2.4956      0.0886     0.0814     77.3087
07000     2.4956      0.0570     0.0814     78.9795
07100     2.4955      0.0992     0.0814     75.6894
07200     2.4954      0.0907     0.0814     76.3167
07300     2.4954      0.0886     0.0814     75.7412
07400     2.4953      0.0949     0.0814     77.2433
07500     2.4952      0.0759     0.0814     76.3452
07600     2.4952      0.0738     0.0814     76.9396
07700     2.4951      0.0886     0.0814     75.9506
07800     2.4950      0.0865     0.0814     77.4281
07900     2.4950      0.0781     0.0814     77.4587
08000     2.4949      0.0886     0.0814     77.0631
08100     2.4949      0.0823     0.0814     77.1998
08200     2.4948      0.0675     0.0814     76.2122
08300     2.4947      0.0865     0.0814     76.0186
08400     2.4947      0.0802     0.0814     76.1149
08500     2.4946      0.0717     0.0814     76.2978
08600     2.4945      0.0781     0.0814     76.4456
08700     2.4945      0.0717     0.0814     76.7149
08800     2.4944      0.0781     0.0814     76.0639
08900     2.4943      0.0802     0.0814     75.9374
09000     2.4943      0.1034     0.0814     75.5512
09100     2.4942      0.0738     0.0814     79.9101
09200     2.4942      0.0865     0.0814     76.3568
09300     2.4941      0.0823     0.0814     76.2463
09400     2.4940      0.0759     0.0814     76.8843
09500     2.4940      0.1160     0.0814     77.2466
09600     2.4939      0.0759     0.0814     76.4677
09700     2.4938      0.1076     0.0814     76.8906
09800     2.4938      0.0907     0.0814     77.5766
09900     2.4937      0.0654     0.0814     77.4231
10000     2.4936      0.0717     0.0814     75.6762
10100     2.4936      0.0949     0.0814     75.9512
10200     2.4936      0.0781     0.0814     78.1818
10300     2.4936      0.0823     0.0814     76.7435
10400     2.4936      0.0970     0.0814     75.3384
10500     2.4936      0.0675     0.0814     76.7363
10600     2.4936      0.0633     0.0814     78.1783
10700     2.4935      0.0823     0.0814     74.9435
10800     2.4935      0.0675     0.0814     76.7841
10900     2.4935      0.0717     0.0814     75.9497
11000     2.4935      0.0907     0.0814     77.2830
11100     2.4935      0.0802     0.0814     75.8256
11200     2.4935      0.0759     0.0814     77.3789
11300     2.4934      0.0612     0.0814     77.7872
11400     2.4934      0.0928     0.0814     78.6677
11500     2.4934      0.0654     0.0814     76.6994
11600     2.4934      0.1013     0.0814     78.3100
11700     2.4934      0.0844     0.0814     77.1040
11800     2.4934      0.0886     0.0814     77.3147
11900     2.4933      0.0823     0.0814     75.8667
12000     2.4933      0.0928     0.0814     76.8931
12100     2.4933      0.0802     0.0814     77.5712
12200     2.4933      0.0738     0.0814     78.2139
12300     2.4933      0.0844     0.0814     76.5945
12400     2.4933      0.0759     0.0814     77.8701
12500     2.4933      0.0823     0.0814     77.8066
12600     2.4932      0.0612     0.0814     77.7464
12700     2.4932      0.0865     0.0814     77.4610
12800     2.4932      0.0886     0.0814     78.6328
12900     2.4932      0.0949     0.0814     76.7912
13000     2.4932      0.0844     0.0814     77.7192
13100     2.4932      0.0992     0.0814     77.9753
13200     2.4931      0.0907     0.0814     75.6733
13300     2.4931      0.0738     0.0814     78.8951
13400     2.4931      0.0865     0.0814     78.2895
13500     2.4931      0.0612     0.0814     78.5539
13600     2.4931      0.1013     0.0814     76.5814
13700     2.4931      0.0781     0.0814     76.8069
13800     2.4930      0.0844     0.0814     76.9001
13900     2.4930      0.1013     0.0814     77.7753
14000     2.4930      0.0970     0.0814     76.5849
14100     2.4930      0.0949     0.0814     76.1630
14200     2.4930      0.0823     0.0814     75.5607
14300     2.4930      0.0612     0.0814     76.0383
14400     2.4930      0.0696     0.0814     77.7665
14500     2.4929      0.0970     0.0814     75.5623
14600     2.4929      0.0992     0.0814     76.6677
14700     2.4929      0.0823     0.0814     77.7463
14800     2.4929      0.0907     0.0814     75.7384
14900     2.4929      0.0865     0.0814     76.1254
15000     2.4929      0.0949     0.0814     77.6721
15100     2.4928      0.1097     0.0814     77.1624
15200     2.4928      0.0886     0.0814     76.6961
15300     2.4928      0.0759     0.0814     75.2689
15400     2.4928      0.0907     0.0814     76.4683
15500     2.4928      0.1097     0.0814     76.3546
15600     2.4928      0.0844     0.0814     76.9512
15700     2.4928      0.0865     0.0814     76.6718
15800     2.4927      0.0886     0.0814     76.4170
15900     2.4927      0.0506     0.0814     76.2003
16000     2.4927      0.0802     0.0814     76.9981
16100     2.4927      0.0823     0.0814     76.6817
16200     2.4927      0.0949     0.0814     76.9619
16300     2.4927      0.0781     0.0814     78.5686
16400     2.4926      0.0781     0.0814     77.5718
16500     2.4926      0.0970     0.0814     77.2918
16600     2.4926      0.1055     0.0814     77.7294
16700     2.4926      0.0717     0.0814     75.7953
16800     2.4926      0.0570     0.0814     78.8193
16900     2.4926      0.0907     0.0814     75.7738
17000     2.4925      0.0907     0.0814     77.5876
17100     2.4925      0.0696     0.0814     77.0187
17200     2.4925      0.0781     0.0814     77.3807
17300     2.4925      0.0865     0.0814     76.8922
17400     2.4925      0.0654     0.0814     77.5723
17500     2.4925      0.0823     0.0814     78.1464
17600     2.4925      0.0802     0.0814     77.2255
17700     2.4924      0.0907     0.0814     75.7482
17800     2.4924      0.0865     0.0814     77.5652
17900     2.4924      0.0886     0.0814     79.2200
18000     2.4924      0.0717     0.0814     77.5785
18100     2.4924      0.0633     0.0814     77.0074
18200     2.4924      0.0823     0.0814     76.0891
18300     2.4923      0.0654     0.0814     77.3005
18400     2.4923      0.1013     0.0814     77.0225
18500     2.4923      0.0886     0.0814     76.6499
18600     2.4923      0.0738     0.0814     78.1929
18700     2.4923      0.0759     0.0814     78.5058
18800     2.4923      0.0675     0.0814     76.4542
18900     2.4922      0.0949     0.0814     78.5767
19000     2.4922      0.0781     0.0814     75.9946
19100     2.4922      0.0865     0.0814     76.8516
19200     2.4922      0.0759     0.0814     75.8497
19300     2.4922      0.0759     0.0814     77.8592
19400     2.4922      0.0865     0.0814     78.2977
19500     2.4922      0.0970     0.0814     76.8297
19600     2.4921      0.0907     0.0814     78.2313
19700     2.4921      0.0949     0.0814     78.2010
19800     2.4921      0.0865     0.0814     77.1101
19900     2.4921      0.0717     0.0814     76.2044
20000     2.4921      0.0802     0.0814     79.4193
20100     2.4921      0.0802     0.0814     77.0818
20199     2.4921      0.0485     0.0814     76.1245
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.4921      0.0949     0.0805     10.8270
00100     2.4921      0.0907     0.0805     76.3176
00200     2.4921      0.0675     0.0805     75.6985
00300     2.4921      0.0781     0.0807     75.0944
00400     2.4921      0.0591     0.0807     77.0315
00500     2.4921      0.0865     0.0807     77.6860
00600     2.4921      0.0633     0.0807     77.4304
00700     2.4921      0.0992     0.0807     75.0663
00800     2.4921      0.0802     0.0807     77.2018
00900     2.4920      0.1076     0.0809     77.3692
01000     2.4920      0.0802     0.0809     75.8387
01100     2.4920      0.0738     0.0809     76.8094
01200     2.4920      0.0802     0.0809     76.1808
01300     2.4920      0.0781     0.0809     75.0785
01400     2.4920      0.1034     0.0809     75.3626
01500     2.4920      0.0907     0.0809     75.1811
01600     2.4920      0.0738     0.0814     76.2475
01700     2.4920      0.0928     0.0814     75.5136
01800     2.4920      0.0928     0.0814     76.1455
01900     2.4920      0.0696     0.0814     75.1189
02000     2.4920      0.0759     0.0814     75.1943
02100     2.4920      0.0696     0.0814     74.4054
02200     2.4920      0.0696     0.0814     75.2686
02300     2.4920      0.0844     0.0814     76.5303
02400     2.4920      0.0696     0.0814     76.9517
02500     2.4920      0.0886     0.0814     76.3635
02600     2.4920      0.0886     0.0814     76.1878
02700     2.4920      0.0886     0.0814     76.4984
02800     2.4920      0.0781     0.0814     77.1800
02900     2.4920      0.0823     0.0814     76.3683
03000     2.4920      0.0696     0.0814     76.1517
03100     2.4920      0.0759     0.0814     76.5719
03200     2.4920      0.0823     0.0814     76.3483
03300     2.4920      0.0717     0.0814     77.3211
03400     2.4920      0.0738     0.0814     78.1123
03500     2.4920      0.0886     0.0814     78.8157
03600     2.4920      0.0696     0.0814     77.0615
03700     2.4920      0.0717     0.0814     76.6123
03800     2.4920      0.0886     0.0814     78.5627
03900     2.4920      0.0591     0.0814     75.4592
04000     2.4920      0.0549     0.0814     78.1537
04100     2.4920      0.0759     0.0814     77.8509
04200     2.4920      0.0675     0.0814     75.6374
04300     2.4920      0.0675     0.0814     76.6118
04400     2.4920      0.0781     0.0814     76.8367
04500     2.4920      0.0844     0.0814     76.6223
04600     2.4920      0.0591     0.0814     77.1023
04700     2.4920      0.0802     0.0814     77.1789
04800     2.4920      0.0844     0.0814     75.9405
04900     2.4920      0.0886     0.0814     76.3053
05000     2.4920      0.0928     0.0814     75.4329
05100     2.4919      0.0675     0.0814     77.3132
05200     2.4919      0.0992     0.0814     76.3586
05300     2.4919      0.0928     0.0814     76.5894
05400     2.4919      0.0865     0.0814     77.3365
05500     2.4919      0.0759     0.0814     77.9803
05600     2.4919      0.0970     0.0814     75.9352
05700     2.4919      0.1013     0.0814     78.8839
05800     2.4919      0.0865     0.0814     77.0961
05900     2.4919      0.0759     0.0814     77.7848
06000     2.4919      0.0738     0.0814     76.8966
06100     2.4919      0.0949     0.0814     76.7342
06200     2.4919      0.0928     0.0814     76.8523
06300     2.4919      0.0865     0.0814     76.9189
06400     2.4919      0.0865     0.0814     77.0316
06500     2.4919      0.1076     0.0814     78.6251
06600     2.4919      0.0907     0.0814     75.4353
06700     2.4919      0.0738     0.0814     76.9675
06800     2.4919      0.0907     0.0814     76.2263
06900     2.4919      0.0907     0.0814     78.0226
07000     2.4919      0.0759     0.0814     77.8204
07100     2.4919      0.0527     0.0814     77.5187
07200     2.4919      0.0949     0.0814     77.9161
07300     2.4919      0.0907     0.0814     77.4410
07400     2.4919      0.0612     0.0814     76.4009
07500     2.4919      0.0865     0.0814     78.2890
07600     2.4919      0.0738     0.0814     77.6090
07700     2.4919      0.0738     0.0814     77.8038
07800     2.4919      0.0781     0.0814     77.7777
07900     2.4919      0.0802     0.0814     75.6801
08000     2.4919      0.0675     0.0814     77.7892
08100     2.4919      0.0865     0.0814     75.6993
08200     2.4919      0.0612     0.0814     80.0956
08300     2.4919      0.0865     0.0814     79.3524
08400     2.4919      0.0738     0.0814     76.1756
08500     2.4919      0.0992     0.0814     75.7447
08600     2.4919      0.0844     0.0814     76.2284
08700     2.4919      0.1076     0.0814     75.6199
08800     2.4919      0.0506     0.0814     75.3593
08900     2.4919      0.0781     0.0814     74.8872
09000     2.4919      0.0696     0.0814     75.1618
09100     2.4919      0.0886     0.0814     76.2366
09200     2.4919      0.0717     0.0814     75.4733
09300     2.4918      0.0654     0.0814     74.8939
09400     2.4918      0.0696     0.0814     75.8962
09500     2.4918      0.0865     0.0814     75.4349
09600     2.4918      0.0696     0.0814     75.5946
09700     2.4918      0.0717     0.0814     76.1648
09800     2.4918      0.0907     0.0814     77.2450
09900     2.4918      0.0738     0.0814     76.7791
Start testing:
Test Accuracy: 0.0789
