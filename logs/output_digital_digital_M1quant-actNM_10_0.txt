Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=10, quant_actNM=10, quant_inp=10, quant_w=10, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
a417615b-3df0-4d5a-9005-ed938770f76d
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 373, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, 1.)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 82, in forward
    if len(x_range) > 1:
TypeError: object of type 'float' has no len()
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=10, quant_actNM=10, quant_inp=10, quant_w=10, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
ef90d1df-05cd-43ae-b100-392bf1367ab1
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 373, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, torch.tensor([1.]).device(input.device))
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 94, in forward
    x_scaled = x/x_range
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=10, quant_actNM=10, quant_inp=10, quant_w=10, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
d1af7e38-9f69-4679-95e8-f7bf94d8b6fe
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     3.5461      0.0949     0.0903     10.5159
00100     2.2292      0.2405     0.2300     57.6026
00200     1.7675      0.4219     0.4345     56.6101
00300     1.5517      0.4831     0.5199     57.3307
00400     1.3246      0.6013     0.5875     57.4543
00500     1.2560      0.6118     0.6238     57.4282
00600     1.0494      0.6857     0.6662     57.6087
00700     1.1530      0.6435     0.6863     57.2122
00800     1.0770      0.6371     0.6949     57.6113
00900     0.9968      0.6646     0.7004     57.3047
01000     1.0553      0.6688     0.7270     58.1211
01100     0.8704      0.7131     0.7354     58.2713
01200     0.9149      0.7194     0.7499     57.8327
01300     0.9220      0.7004     0.7593     58.5290
01400     0.8098      0.7468     0.7593     59.4095
01500     0.8376      0.7152     0.7691     58.5494
01600     0.8687      0.7194     0.7691     58.7766
01700     0.8222      0.7236     0.7749     59.3388
01800     0.8064      0.7468     0.7749     59.0895
01900     0.8184      0.7321     0.7749     57.9676
02000     0.7469      0.7785     0.7784     58.6036
02100     0.7937      0.7426     0.7788     57.9142
02200     0.7926      0.7215     0.7822     58.3985
02300     0.7241      0.7637     0.7822     58.1709
02400     0.7293      0.7553     0.7876     58.6651
02500     0.7041      0.7637     0.7876     59.2821
02600     0.7744      0.7363     0.7876     58.5904
02700     0.6716      0.7827     0.7876     58.7862
02800     0.6641      0.8101     0.7963     57.7464
02900     0.7405      0.7722     0.7990     58.7514
03000     0.6922      0.7848     0.7990     59.9444
03100     0.7056      0.7658     0.7995     58.9484
03200     0.7441      0.7511     0.7997     58.6248
03300     0.6387      0.7890     0.7997     58.5292
03400     0.7614      0.7405     0.7997     59.1247
03500     0.7001      0.7827     0.7997     59.0496
03600     0.6187      0.7975     0.8058     58.0483
03700     0.6330      0.7827     0.8058     58.0775
03800     0.7450      0.7553     0.8150     59.7033
03900     0.5963      0.7975     0.8150     58.0945
04000     0.6671      0.7911     0.8150     58.8606
04100     0.5594      0.8228     0.8150     58.6487
04200     0.6831      0.7722     0.8150     58.9514
04300     0.6994      0.7574     0.8150     58.9611
04400     0.6105      0.8101     0.8150     59.5871
04500     0.6823      0.7764     0.8150     59.9055
04600     0.6206      0.8143     0.8150     59.0329
04700     0.5994      0.8038     0.8150     58.4118
04800     0.6641      0.7890     0.8150     59.4572
04900     0.6529      0.7954     0.8209     58.8728
05000     0.6698      0.7785     0.8209     58.5097
05100     0.6761      0.7848     0.8209     59.7953
05200     0.5539      0.8312     0.8209     58.0335
05300     0.6424      0.7932     0.8209     58.4935
05400     0.6382      0.7911     0.8209     58.9478
05500     0.6598      0.7954     0.8209     59.5367
05600     0.5319      0.8165     0.8209     59.3850
05700     0.7103      0.7743     0.8209     59.0743
05800     0.6407      0.7954     0.8209     59.3977
05900     0.5525      0.8333     0.8209     60.2156
06000     0.5359      0.8291     0.8231     59.2407
06100     0.6154      0.7954     0.8231     58.9620
06200     0.6283      0.7996     0.8270     59.8449
06300     0.5185      0.8333     0.8270     58.9222
06400     0.5931      0.8143     0.8270     58.7384
06500     0.6124      0.7975     0.8270     57.6382
06600     0.6419      0.7827     0.8270     58.1443
06700     0.5975      0.8080     0.8270     58.6993
06800     0.5076      0.8270     0.8270     57.2126
06900     0.5635      0.8207     0.8270     58.2043
07000     0.5823      0.8122     0.8270     59.3485
07100     0.5320      0.8165     0.8270     57.5050
07200     0.6110      0.8017     0.8270     58.7454
07300     0.6388      0.7996     0.8270     59.0511
07400     0.5646      0.8312     0.8270     59.9730
07500     0.7084      0.7869     0.8270     58.4345
07600     0.6591      0.7996     0.8270     57.7279
07700     0.6144      0.7975     0.8274     58.8062
07800     0.5732      0.8207     0.8274     58.0866
07900     0.6534      0.7722     0.8274     58.8104
08000     0.5777      0.8122     0.8295     59.2165
08100     0.6030      0.8038     0.8295     58.8348
08200     0.6379      0.7890     0.8295     57.0663
08300     0.5795      0.8312     0.8295     59.1432
08400     0.5576      0.8228     0.8295     57.8690
08500     0.6099      0.8165     0.8295     57.9560
08600     0.5556      0.8143     0.8295     58.6867
08700     0.5717      0.8291     0.8295     57.4935
08800     0.6306      0.7975     0.8295     59.1022
08900     0.5227      0.8165     0.8295     59.5219
09000     0.6155      0.8038     0.8295     58.7913
09100     0.5644      0.8143     0.8295     58.0498
09200     0.6194      0.8101     0.8339     58.4434
09300     0.5635      0.8249     0.8361     59.7173
09400     0.4880      0.8460     0.8361     59.2581
09500     0.6084      0.8228     0.8361     58.7441
09600     0.6101      0.8101     0.8361     59.1478
09700     0.6344      0.7890     0.8361     58.8410
09800     0.5231      0.8249     0.8361     58.6522
09900     0.6039      0.8080     0.8361     58.7732
10000     0.5380      0.8312     0.8361     58.2546
10100     0.5001      0.8397     0.8361     59.2130
10200     0.5580      0.8207     0.8365     58.8955
10300     0.5308      0.8312     0.8365     58.8283
10400     0.5410      0.8165     0.8365     59.1299
10500     0.6074      0.7996     0.8365     58.5243
10600     0.5706      0.8101     0.8393     59.0321
10700     0.5900      0.7848     0.8393     59.3479
10800     0.4955      0.8460     0.8393     58.8028
10900     0.6308      0.7932     0.8393     59.3961
11000     0.5707      0.8291     0.8405     58.5822
11100     0.5696      0.8080     0.8405     58.8569
11200     0.4930      0.8439     0.8405     60.0186
11300     0.4677      0.8418     0.8405     58.4610
11400     0.5247      0.8270     0.8405     58.0490
11500     0.4485      0.8544     0.8414     59.2746
11600     0.5375      0.8207     0.8414     58.3585
11700     0.5306      0.8249     0.8414     59.3946
11800     0.5266      0.8228     0.8417     58.5003
11900     0.5993      0.7996     0.8417     58.6858
12000     0.5025      0.8397     0.8446     59.4359
12100     0.5820      0.8080     0.8451     58.3725
12200     0.6073      0.8186     0.8451     57.9371
12300     0.5459      0.7975     0.8451     59.9378
12400     0.5354      0.8228     0.8451     59.4321
12500     0.5085      0.8460     0.8451     59.9038
12600     0.4572      0.8586     0.8470     58.9236
12700     0.4779      0.8629     0.8470     58.2000
12800     0.4977      0.8481     0.8470     58.9517
12900     0.5435      0.8228     0.8470     57.6559
13000     0.6267      0.7848     0.8470     57.7991
13100     0.4228      0.8882     0.8470     58.1785
13200     0.4826      0.8502     0.8470     57.8671
13300     0.4730      0.8565     0.8470     57.8180
13400     0.4997      0.8460     0.8470     57.9353
13500     0.4974      0.8418     0.8470     58.9313
13600     0.4742      0.8544     0.8470     60.0335
13700     0.4819      0.8333     0.8470     57.5279
13800     0.4887      0.8439     0.8470     58.8417
13900     0.5051      0.8586     0.8470     58.6525
14000     0.4746      0.8544     0.8470     57.4240
14100     0.4868      0.8460     0.8470     58.1194
14200     0.5234      0.8249     0.8470     58.7781
14300     0.4882      0.8460     0.8470     57.4699
14400     0.5034      0.8249     0.8470     58.6946
14500     0.5219      0.8291     0.8470     58.4297
14600     0.4317      0.8755     0.8470     59.8751
14700     0.5140      0.8544     0.8470     58.8473
14800     0.5166      0.8439     0.8470     59.5720
14900     0.5102      0.8354     0.8470     59.3654
15000     0.5802      0.8186     0.8470     58.0298
15100     0.5371      0.8291     0.8470     58.6747
15200     0.5461      0.8291     0.8470     59.5545
15300     0.5375      0.8186     0.8470     59.5551
15400     0.4521      0.8629     0.8470     57.9593
15500     0.5599      0.8122     0.8470     60.1793
15600     0.4874      0.8586     0.8470     58.4047
15700     0.5653      0.8207     0.8470     62.2873
15800     0.5133      0.8207     0.8470     59.2611
15900     0.5600      0.8207     0.8470     58.1252
16000     0.5248      0.8312     0.8470     59.3142
16100     0.5101      0.8376     0.8470     59.2365
16200     0.5045      0.8523     0.8470     59.8585
16300     0.6280      0.8038     0.8470     59.3184
16400     0.5008      0.8544     0.8470     58.2053
16500     0.6696      0.7848     0.8470     59.5865
16600     0.5093      0.8502     0.8470     58.2257
16700     0.4992      0.8397     0.8470     59.3114
16800     0.5242      0.8249     0.8470     59.2277
16900     0.5873      0.7932     0.8470     56.6200
17000     0.4614      0.8671     0.8470     58.5002
17100     0.5849      0.8228     0.8470     59.4046
17200     0.4931      0.8418     0.8482     59.4379
17300     0.5239      0.8186     0.8482     58.3266
17400     0.4904      0.8418     0.8482     58.5332
17500     0.6702      0.7827     0.8482     56.9168
17600     0.4840      0.8333     0.8482     58.7551
17700     0.4791      0.8439     0.8498     57.9875
17800     0.5203      0.8228     0.8498     58.0493
17900     0.5623      0.8080     0.8498     58.0496
18000     0.4509      0.8713     0.8498     58.7412
18100     0.5364      0.8270     0.8498     60.0445
18200     0.5748      0.7911     0.8498     58.9198
18300     0.4563      0.8608     0.8498     59.9019
18400     0.5429      0.8143     0.8498     61.2234
18500     0.5754      0.8101     0.8512     60.4493
18600     0.5382      0.8376     0.8512     60.7996
18700     0.5342      0.8354     0.8512     60.5108
18800     0.4811      0.8523     0.8512     58.7116
18900     0.4695      0.8354     0.8512     59.9884
19000     0.5645      0.8354     0.8512     60.3311
19100     0.5210      0.8397     0.8512     59.8084
19200     0.5143      0.8418     0.8512     59.7735
19300     0.5460      0.8122     0.8512     60.2846
19400     0.5020      0.8376     0.8512     60.4300
19500     0.5039      0.8397     0.8512     60.2242
19600     0.4840      0.8439     0.8512     59.5373
19700     0.4501      0.8650     0.8512     59.7294
19800     0.5213      0.8333     0.8512     57.7729
19900     0.5443      0.8397     0.8512     58.1241
20000     0.4824      0.8523     0.8512     59.1083
20100     0.5150      0.8354     0.8512     58.4998
20199     0.4524      0.8544     0.8512     57.6320
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     0.4483      0.8544     0.8452     8.9363
00100     0.4305      0.8692     0.8517     57.4112
00200     0.4884      0.8481     0.8517     57.9006
00300     0.4787      0.8460     0.8517     58.6674
00400     0.4128      0.8544     0.8517     59.1689
00500     0.3908      0.8776     0.8532     57.3302
00600     0.4327      0.8629     0.8540     58.1361
00700     0.4383      0.8502     0.8540     58.5315
00800     0.4906      0.8586     0.8541     58.4760
00900     0.5029      0.8354     0.8554     57.5566
01000     0.4393      0.8713     0.8554     57.1241
01100     0.3819      0.8882     0.8554     58.7955
01200     0.4087      0.8776     0.8554     58.6392
01300     0.4727      0.8376     0.8554     57.5795
01400     0.4308      0.8523     0.8554     58.4626
01500     0.5001      0.8228     0.8559     58.5392
01600     0.4659      0.8439     0.8559     57.7770
01700     0.3893      0.8966     0.8569     57.3063
01800     0.4425      0.8586     0.8569     59.1806
01900     0.4906      0.8460     0.8577     58.0252
02000     0.3952      0.8734     0.8577     57.3312
02100     0.4126      0.8819     0.8577     57.7139
02200     0.4166      0.8755     0.8577     58.2653
02300     0.4327      0.8523     0.8577     58.1904
02400     0.4552      0.8502     0.8577     58.8823
02500     0.3744      0.8671     0.8577     57.6256
02600     0.4317      0.8502     0.8577     58.1305
02700     0.4184      0.8502     0.8577     59.0220
02800     0.4775      0.8523     0.8577     58.1902
02900     0.4734      0.8565     0.8577     57.5748
03000     0.4422      0.8608     0.8577     58.6770
03100     0.4082      0.8713     0.8577     58.3099
03200     0.4133      0.8544     0.8577     57.8592
03300     0.4468      0.8544     0.8586     58.9586
03400     0.4408      0.8586     0.8586     59.2797
03500     0.4338      0.8502     0.8586     58.6045
03600     0.4125      0.8650     0.8586     57.5004
03700     0.4620      0.8481     0.8586     58.7622
03800     0.4001      0.8840     0.8586     59.5126
03900     0.4545      0.8629     0.8586     58.6440
04000     0.4233      0.8692     0.8586     58.7217
04100     0.4743      0.8312     0.8586     59.2938
04200     0.5041      0.8333     0.8586     58.1323
04300     0.3830      0.8671     0.8586     59.1250
04400     0.4191      0.8797     0.8586     59.2488
04500     0.4162      0.8819     0.8586     59.3537
04600     0.4901      0.8291     0.8586     58.8542
04700     0.4547      0.8376     0.8586     58.9433
04800     0.4456      0.8755     0.8586     59.2186
04900     0.4585      0.8418     0.8586     58.4107
05000     0.4200      0.8819     0.8586     58.5099
05100     0.4115      0.8523     0.8586     60.2468
05200     0.4409      0.8671     0.8586     59.4880
05300     0.4375      0.8713     0.8586     59.3170
05400     0.3943      0.8924     0.8586     59.9558
05500     0.4756      0.8523     0.8586     59.0105
05600     0.4585      0.8629     0.8586     58.6824
05700     0.3782      0.8755     0.8586     59.3745
05800     0.4545      0.8629     0.8586     58.3568
05900     0.4315      0.8608     0.8586     58.9799
06000     0.3558      0.8966     0.8586     59.9690
06100     0.3981      0.8882     0.8586     58.4148
06200     0.4885      0.8333     0.8586     59.1894
06300     0.4184      0.8692     0.8586     57.4926
06400     0.4374      0.8629     0.8586     58.2970
06500     0.5042      0.8418     0.8586     59.1839
06600     0.3436      0.8987     0.8586     59.2823
06700     0.4701      0.8523     0.8586     59.2955
06800     0.4208      0.8882     0.8586     58.1563
06900     0.4242      0.8671     0.8586     58.9664
07000     0.4522      0.8755     0.8586     58.8078
07100     0.4066      0.8797     0.8586     59.1410
07200     0.3883      0.8882     0.8586     58.4656
07300     0.4253      0.8586     0.8586     59.2348
07400     0.4638      0.8481     0.8586     59.0061
07500     0.4842      0.8312     0.8586     58.3616
07600     0.4307      0.8629     0.8586     58.7427
07700     0.4015      0.8692     0.8586     58.9553
07800     0.4122      0.8502     0.8586     57.7842
07900     0.3902      0.8819     0.8586     58.2787
08000     0.3799      0.8924     0.8586     58.0991
08100     0.3929      0.8755     0.8586     58.4641
08200     0.4196      0.8671     0.8586     58.5638
08300     0.4181      0.8713     0.8586     58.8081
08400     0.4374      0.8608     0.8586     57.8166
08500     0.4219      0.8882     0.8586     58.5853
08600     0.3982      0.8629     0.8586     59.0370
08700     0.4922      0.8481     0.8586     57.6225
08800     0.4362      0.8586     0.8586     59.2721
08900     0.4312      0.8565     0.8586     58.2292
09000     0.4045      0.8671     0.8586     61.2884
09100     0.3827      0.8734     0.8586     59.2864
09200     0.4657      0.8460     0.8586     58.9047
09300     0.4472      0.8565     0.8586     59.7472
09400     0.4477      0.8544     0.8615     58.2783
09500     0.4027      0.8819     0.8615     59.6682
09600     0.3879      0.8713     0.8615     58.6941
09700     0.4109      0.8650     0.8615     57.6987
09800     0.4371      0.8755     0.8615     58.5195
09900     0.4553      0.8629     0.8615     57.7900
Start testing:
Test Accuracy: 0.8374
