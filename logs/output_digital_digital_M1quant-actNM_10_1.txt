Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=10, quant_actNM=10, quant_inp=10, quant_w=10, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
abcd8d35-9d99-4af5-a373-ccf86280697b
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 373, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, 1.)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 82, in forward
    if len(x_range) > 1:
TypeError: object of type 'float' has no len()
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=10, quant_actNM=10, quant_inp=10, quant_w=10, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
8f26dcb8-12a0-4123-ac5e-36b6184159c5
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 373, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, torch.tensor([1.]).device(input.device))
TypeError: 'torch.device' object is not callable
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=10, quant_actNM=10, quant_inp=10, quant_w=10, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
47eedc77-d933-49c2-9385-226f2c1ac15b
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     3.4604      0.1013     0.1351     9.7061
00100     2.1676      0.2574     0.3174     53.5161
00200     1.6412      0.4831     0.4949     53.6405
00300     1.3601      0.5696     0.5807     54.1649
00400     1.2275      0.6203     0.6392     53.9774
00500     1.0645      0.6519     0.6755     53.5252
00600     1.0609      0.6582     0.6951     53.9566
00700     0.9289      0.7278     0.7139     53.4295
00800     0.8969      0.7194     0.7312     54.1964
00900     1.0203      0.6561     0.7470     53.7839
01000     0.8828      0.7321     0.7586     53.9122
01100     0.8390      0.7236     0.7662     54.6509
01200     0.8311      0.7595     0.7710     54.9093
01300     0.8794      0.7110     0.7710     55.2071
01400     0.7653      0.7553     0.7804     54.8486
01500     0.6992      0.7932     0.7887     54.8547
01600     0.7021      0.7848     0.7902     55.7485
01700     0.7743      0.7553     0.7948     55.2698
01800     0.6974      0.7806     0.7948     54.7513
01900     0.6747      0.7932     0.8046     54.8827
02000     0.7154      0.7785     0.8046     54.3683
02100     0.7341      0.7447     0.8046     54.8691
02200     0.7302      0.7827     0.8058     55.0303
02300     0.7224      0.7785     0.8127     54.9088
02400     0.6972      0.7932     0.8148     55.4879
02500     0.6099      0.8059     0.8148     54.9134
02600     0.5876      0.8038     0.8148     54.8583
02700     0.6436      0.8143     0.8199     54.6354
02800     0.5611      0.8312     0.8199     55.0880
02900     0.7129      0.7658     0.8199     54.4826
03000     0.6770      0.7848     0.8199     54.4446
03100     0.7105      0.7722     0.8223     54.2533
03200     0.6268      0.7932     0.8223     55.0975
03300     0.7049      0.7785     0.8223     54.6105
03400     0.6509      0.8122     0.8231     54.9310
03500     0.6432      0.7806     0.8291     56.4760
03600     0.5653      0.8122     0.8291     54.3052
03700     0.6481      0.7911     0.8291     53.9501
03800     0.6784      0.7722     0.8291     55.1204
03900     0.5161      0.8586     0.8291     54.8377
04000     0.5931      0.8249     0.8291     55.5891
04100     0.5770      0.8354     0.8291     54.9621
04200     0.6477      0.7890     0.8291     53.9190
04300     0.5557      0.8228     0.8291     55.0318
04400     0.5066      0.8376     0.8310     54.5176
04500     0.5826      0.8143     0.8310     54.6418
04600     0.5170      0.8565     0.8334     55.4958
04700     0.5681      0.8080     0.8334     55.0972
04800     0.5830      0.8080     0.8334     55.8364
04900     0.5985      0.8038     0.8334     54.8371
05000     0.6156      0.7996     0.8334     54.9463
05100     0.5202      0.8186     0.8365     55.3368
05200     0.5438      0.8165     0.8365     54.5456
05300     0.5099      0.8291     0.8365     54.4664
05400     0.5570      0.8207     0.8365     54.5687
05500     0.5141      0.8439     0.8365     54.9697
05600     0.4942      0.8481     0.8365     55.1452
05700     0.5799      0.8312     0.8365     54.9747
05800     0.4977      0.8460     0.8365     54.2702
05900     0.5159      0.8397     0.8369     55.0047
06000     0.4917      0.8502     0.8369     54.9622
06100     0.5486      0.8186     0.8369     54.3814
06200     0.6008      0.8143     0.8399     55.1953
06300     0.4878      0.8439     0.8399     55.1221
06400     0.6116      0.8186     0.8399     54.7705
06500     0.5618      0.8249     0.8399     54.4840
06600     0.5358      0.8439     0.8415     54.8660
06700     0.5982      0.7996     0.8415     55.0288
06800     0.5372      0.8228     0.8438     55.1415
06900     0.5760      0.8270     0.8438     54.9506
07000     0.4804      0.8291     0.8438     55.5661
07100     0.4288      0.8565     0.8438     54.8692
07200     0.5503      0.8228     0.8438     55.3673
07300     0.5453      0.8333     0.8438     54.1268
07400     0.4748      0.8608     0.8438     54.5690
07500     0.6181      0.8080     0.8438     55.3996
07600     0.5316      0.8101     0.8441     54.8884
07700     0.5385      0.8186     0.8441     55.1572
07800     0.5301      0.8186     0.8441     55.0631
07900     0.4069      0.8586     0.8441     54.9957
08000     0.5428      0.8207     0.8441     54.9568
08100     0.4875      0.8481     0.8441     54.7737
08200     0.4713      0.8460     0.8441     54.7581
08300     0.5601      0.8439     0.8441     55.2004
08400     0.5240      0.8333     0.8444     54.5171
08500     0.6084      0.8038     0.8444     54.9145
08600     0.4958      0.8397     0.8444     54.6389
08700     0.4820      0.8565     0.8444     54.3678
08800     0.5141      0.8523     0.8444     55.2091
08900     0.5092      0.8397     0.8444     54.2497
09000     0.5403      0.8397     0.8496     54.9148
09100     0.4580      0.8397     0.8496     55.2710
09200     0.5061      0.8249     0.8515     54.9442
09300     0.5319      0.8291     0.8515     54.5201
09400     0.5562      0.8143     0.8515     54.0669
09500     0.5589      0.8291     0.8515     55.4866
09600     0.5164      0.8291     0.8515     54.7030
09700     0.4726      0.8312     0.8520     54.6298
09800     0.4543      0.8439     0.8520     54.6984
09900     0.4671      0.8523     0.8520     55.7976
10000     0.5833      0.8122     0.8520     54.9772
10100     0.5261      0.8333     0.8520     55.0472
10200     0.4684      0.8586     0.8520     54.9980
10300     0.5530      0.8143     0.8520     54.8577
10400     0.4857      0.8502     0.8520     54.9586
10500     0.4661      0.8629     0.8520     54.4216
10600     0.4993      0.8439     0.8520     54.1978
10700     0.4291      0.8629     0.8520     55.1006
10800     0.5060      0.8502     0.8520     54.4518
10900     0.4711      0.8439     0.8520     54.9058
11000     0.5031      0.8397     0.8520     54.8124
11100     0.5280      0.8228     0.8520     55.2092
11200     0.5223      0.8312     0.8520     55.0890
11300     0.5356      0.8418     0.8520     55.2057
11400     0.4224      0.8692     0.8574     54.8979
11500     0.4885      0.8397     0.8574     55.2761
11600     0.4551      0.8565     0.8574     54.8695
11700     0.4580      0.8629     0.8574     55.1907
11800     0.4735      0.8565     0.8574     54.4505
11900     0.4620      0.8544     0.8574     54.8954
12000     0.5020      0.8270     0.8574     55.5705
12100     0.4955      0.8397     0.8574     54.3052
12200     0.4405      0.8629     0.8574     55.0989
12300     0.5080      0.8291     0.8574     55.1243
12400     0.5069      0.8523     0.8589     54.6820
12500     0.4760      0.8544     0.8589     56.0482
12600     0.4797      0.8523     0.8589     54.3860
12700     0.5238      0.8143     0.8589     54.1049
12800     0.4552      0.8650     0.8589     54.8280
12900     0.4179      0.8544     0.8589     54.0992
13000     0.4655      0.8481     0.8589     54.0848
13100     0.4495      0.8608     0.8589     54.5664
13200     0.4125      0.8734     0.8589     53.7181
13300     0.4466      0.8713     0.8589     54.1883
13400     0.4381      0.8692     0.8589     53.7298
13500     0.4986      0.8376     0.8589     53.8165
13600     0.4879      0.8397     0.8589     54.2835
13700     0.4653      0.8502     0.8589     53.7877
13800     0.4795      0.8523     0.8589     53.5859
13900     0.4757      0.8439     0.8589     55.1519
14000     0.5444      0.8186     0.8589     54.9062
14100     0.4316      0.8629     0.8589     55.5343
14200     0.3870      0.8861     0.8589     55.0859
14300     0.5306      0.8165     0.8589     55.2430
14400     0.4496      0.8671     0.8589     54.7903
14500     0.3858      0.8945     0.8589     55.0622
14600     0.4691      0.8650     0.8589     54.6622
14700     0.5110      0.8376     0.8589     55.8789
14800     0.3974      0.8650     0.8589     54.8961
14900     0.4939      0.8312     0.8589     55.2463
15000     0.4473      0.8460     0.8589     55.1219
15100     0.5354      0.8207     0.8589     55.1061
15200     0.5363      0.8460     0.8589     55.9362
15300     0.5216      0.8312     0.8589     55.2638
15400     0.4969      0.8776     0.8589     55.4657
15500     0.4578      0.8418     0.8589     54.7189
15600     0.4566      0.8502     0.8589     54.7740
15700     0.4554      0.8481     0.8589     55.2639
15800     0.4548      0.8565     0.8589     55.2453
15900     0.5452      0.8291     0.8589     55.3428
16000     0.4109      0.8713     0.8589     55.3273
16100     0.3833      0.8755     0.8589     54.5953
16200     0.4508      0.8397     0.8589     55.7868
16300     0.5107      0.8418     0.8589     55.0125
16400     0.4362      0.8629     0.8589     54.6434
16500     0.4699      0.8460     0.8603     54.7698
16600     0.4084      0.8608     0.8603     54.4026
16700     0.4200      0.8608     0.8603     54.7360
16800     0.4297      0.8608     0.8603     55.0999
16900     0.4376      0.8565     0.8603     55.0967
17000     0.5092      0.8502     0.8603     56.3711
17100     0.4564      0.8481     0.8603     55.4512
17200     0.4543      0.8523     0.8603     55.6118
17300     0.5172      0.8481     0.8603     56.3385
17400     0.4771      0.8481     0.8603     54.8166
17500     0.4738      0.8544     0.8603     56.3834
17600     0.4402      0.8502     0.8603     56.4029
17700     0.3854      0.8924     0.8603     55.9160
17800     0.3737      0.8734     0.8603     55.4159
17900     0.4851      0.8418     0.8603     54.7437
18000     0.4212      0.8671     0.8603     55.1595
18100     0.4143      0.8629     0.8603     55.7096
18200     0.4552      0.8460     0.8603     54.7802
18300     0.4610      0.8608     0.8603     54.8229
18400     0.4356      0.8629     0.8603     54.8967
18500     0.4696      0.8565     0.8603     55.4261
18600     0.4771      0.8439     0.8603     55.3168
18700     0.4326      0.8650     0.8603     55.4234
18800     0.4920      0.8502     0.8603     56.6178
18900     0.4475      0.8481     0.8603     54.9069
19000     0.4331      0.8671     0.8603     54.9080
19100     0.4259      0.8713     0.8603     54.7494
19200     0.4552      0.8439     0.8603     55.4553
19300     0.4887      0.8460     0.8603     54.7464
19400     0.4328      0.8629     0.8603     55.9337
19500     0.4855      0.8502     0.8603     55.3727
19600     0.4711      0.8502     0.8603     55.6315
19700     0.4314      0.8776     0.8603     56.0110
19800     0.5102      0.8397     0.8603     55.9906
19900     0.4376      0.8565     0.8603     56.0367
20000     0.4283      0.8586     0.8603     55.3444
20100     0.4068      0.8734     0.8603     55.8069
20199     0.4478      0.8776     0.8603     54.6190
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     0.3630      0.8924     0.8585     9.2798
00100     0.4142      0.8565     0.8585     54.9448
00200     0.4441      0.8797     0.8607     55.0831
00300     0.3536      0.9093     0.8607     56.1386
00400     0.4161      0.8608     0.8607     54.8945
00500     0.3700      0.8903     0.8608     55.5891
00600     0.4373      0.8481     0.8608     54.4357
00700     0.3666      0.8734     0.8608     55.1532
00800     0.3397      0.9051     0.8608     55.2945
00900     0.3504      0.8903     0.8608     54.8027
01000     0.3848      0.8776     0.8616     55.0337
01100     0.3664      0.8882     0.8616     55.1167
01200     0.3783      0.9114     0.8616     54.8557
01300     0.4113      0.8650     0.8620     55.6007
01400     0.3565      0.8797     0.8620     55.0529
01500     0.3701      0.8861     0.8620     54.9260
01600     0.3944      0.8819     0.8627     55.3691
01700     0.4503      0.8629     0.8639     54.9924
01800     0.3650      0.9051     0.8639     55.6145
01900     0.4275      0.8755     0.8639     55.3052
02000     0.3442      0.8903     0.8639     54.5788
02100     0.3794      0.8840     0.8667     54.8371
02200     0.3835      0.8924     0.8668     54.5024
02300     0.3969      0.8903     0.8668     55.5859
02400     0.3800      0.8987     0.8668     55.8042
02500     0.3615      0.8924     0.8668     54.6019
02600     0.3975      0.8734     0.8668     54.7996
02700     0.4759      0.8481     0.8668     56.4953
02800     0.4138      0.8692     0.8668     55.5076
02900     0.3731      0.8840     0.8668     55.2610
03000     0.3581      0.8861     0.8668     54.6258
03100     0.4387      0.8439     0.8668     54.5551
03200     0.4475      0.8608     0.8668     55.2788
03300     0.3914      0.8692     0.8668     54.4462
03400     0.3811      0.8755     0.8668     55.1925
03500     0.3452      0.9030     0.8668     55.2320
03600     0.4210      0.8882     0.8668     54.6190
03700     0.3940      0.8671     0.8668     56.6493
03800     0.3755      0.8755     0.8668     55.5051
03900     0.4137      0.8861     0.8668     55.3783
04000     0.3947      0.8755     0.8668     55.8290
04100     0.3237      0.9114     0.8668     54.8524
04200     0.3880      0.8692     0.8668     54.6043
04300     0.3502      0.8861     0.8668     55.6059
04400     0.3790      0.8861     0.8668     54.7617
04500     0.3337      0.9072     0.8668     55.1761
04600     0.4093      0.8671     0.8668     54.6667
04700     0.4089      0.8671     0.8668     54.9268
04800     0.3381      0.9008     0.8668     55.3316
04900     0.4086      0.8755     0.8668     54.7316
05000     0.3994      0.8797     0.8668     54.9004
05100     0.3554      0.9072     0.8668     55.9582
05200     0.3575      0.8882     0.8668     54.0068
05300     0.3732      0.8924     0.8668     55.5647
05400     0.4233      0.8629     0.8668     55.0640
05500     0.4329      0.8755     0.8668     54.7961
05600     0.3786      0.8713     0.8668     55.0737
05700     0.3714      0.8882     0.8668     54.7190
05800     0.3403      0.8903     0.8668     54.5772
05900     0.4127      0.8565     0.8668     55.0426
06000     0.3565      0.8819     0.8668     54.7719
06100     0.3778      0.8755     0.8668     55.5631
06200     0.3506      0.8924     0.8668     54.9588
06300     0.3679      0.8924     0.8668     55.0947
06400     0.3653      0.8797     0.8668     55.4452
06500     0.3555      0.8882     0.8668     54.7932
06600     0.3520      0.8840     0.8668     55.1805
06700     0.3669      0.8966     0.8668     55.5098
06800     0.4451      0.8713     0.8668     54.7175
06900     0.3535      0.8671     0.8668     55.5820
07000     0.3102      0.8840     0.8668     54.7001
07100     0.4510      0.8565     0.8668     54.6132
07200     0.4737      0.8312     0.8668     55.0635
07300     0.4359      0.8629     0.8668     54.6045
07400     0.4186      0.8797     0.8668     55.1403
07500     0.3449      0.8966     0.8668     56.6922
07600     0.3609      0.8903     0.8668     55.1807
07700     0.3900      0.8776     0.8668     54.7872
07800     0.3556      0.8861     0.8668     54.7038
07900     0.3503      0.8882     0.8668     55.4008
08000     0.4438      0.8755     0.8668     55.5431
08100     0.4087      0.8755     0.8668     56.8021
08200     0.3647      0.8797     0.8668     56.3906
08300     0.4097      0.8650     0.8668     56.6275
08400     0.3771      0.8819     0.8668     56.2506
08500     0.4529      0.8523     0.8668     58.4968
08600     0.4206      0.8861     0.8668     55.2984
08700     0.3824      0.8755     0.8668     60.0372
08800     0.3769      0.8797     0.8668     61.8890
08900     0.3698      0.8797     0.8668     57.1552
09000     0.3575      0.8861     0.8668     56.5467
09100     0.3828      0.8861     0.8668     56.1251
09200     0.4037      0.8650     0.8668     55.0655
09300     0.4015      0.8671     0.8668     55.8073
09400     0.4368      0.8650     0.8668     54.9741
09500     0.4028      0.8840     0.8668     55.2306
09600     0.3762      0.8882     0.8668     55.4127
09700     0.3117      0.9051     0.8668     54.5023
09800     0.4089      0.8629     0.8668     54.6717
09900     0.3632      0.8797     0.8668     55.3694
Start testing:
Test Accuracy: 0.8437
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
