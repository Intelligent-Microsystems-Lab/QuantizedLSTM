Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 49, in __init__
    torch.nn.init.uniform_(self.cell.weight_ih, a = -limit2, b = limit2)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 49, in __init__
    torch.nn.init.uniform_(self.cell.weight_ih, a = -limit2, b = limit2)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 49, in __init__
    torch.nn.init.uniform_(self.cell.weight_ih, a = -limit2, b = limit2)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 49, in __init__
    torch.nn.init.uniform_(self.cell.weight_ih, a = -limit2, b = limit2)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 362, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.0, finetuning_epochs=0, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.0005,0.0001,0.00002', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,10000', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
b0c9d4bf-1382-473f-a808-55d378431c8d
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.5526      0.0844     0.0801     13.4217
00100     2.5429      0.0696     0.0803     83.5810
00200     2.5399      0.0781     0.1051     81.2607
00300     2.7522      0.1456     0.1051     80.8369
00400     2.6241      0.1561     0.2010     77.7455
00500     3.2717      0.2236     0.2459     77.8328
00600     3.0120      0.2363     0.2620     77.5695
00700     2.9829      0.2764     0.2905     79.4060
00800     3.0084      0.3101     0.2966     80.1125
00900     2.7607      0.2954     0.3336     81.6501
01000     2.8385      0.3122     0.3336     78.4942
01100     2.5301      0.3523     0.3415     80.7014
01200     2.7814      0.2806     0.3415     78.0413
01300     2.9191      0.3143     0.3415     77.8451
01400     2.7069      0.3397     0.3415     78.5759
01500     2.5207      0.3249     0.3634     77.8822
01600     2.5424      0.3481     0.3741     78.5967
01700     2.4449      0.3418     0.3741     78.1156
01800     2.5547      0.3523     0.3741     78.5329
01900     2.6751      0.3376     0.3741     78.2902
02000     3.1029      0.3270     0.3796     78.0168
02100     3.0525      0.3671     0.3975     78.1910
02200     2.8147      0.4072     0.4130     77.6798
02300     2.8898      0.3755     0.4329     79.0064
02400     2.6448      0.4177     0.4385     79.0322
02500     2.7602      0.4051     0.4422     78.1916
02600     2.6996      0.4346     0.4770     78.6410
02700     2.9936      0.3755     0.4770     78.5109
02800     2.5659      0.4451     0.4855     77.9447
02900     2.6095      0.4051     0.4855     77.8372
03000     2.4248      0.4262     0.4855     77.5991
03100     2.3997      0.4578     0.4916     77.9269
03200     2.4709      0.4430     0.4916     78.0504
03300     2.4155      0.4620     0.4916     78.0391
03400     2.0661      0.4557     0.4916     77.9338
03500     1.8899      0.4958     0.5497     78.0049
03600     2.1032      0.5169     0.5497     77.7941
03700     2.4890      0.4937     0.5497     77.2002
03800     2.3054      0.4684     0.5610     78.8887
03900     1.9629      0.5359     0.5685     77.5280
04000     1.9266      0.5063     0.5685     78.2185
04100     1.8505      0.5190     0.5685     77.6624
04200     2.0636      0.4895     0.5741     77.8855
04300     2.2840      0.5316     0.5877     77.9397
04400     1.9889      0.5359     0.5877     77.7924
04500     2.0261      0.5063     0.5877     77.8193
04600     2.1828      0.4895     0.5877     78.2715
04700     1.8124      0.5190     0.5877     77.4468
04800     1.8123      0.5211     0.5877     78.2254
04900     1.9606      0.4852     0.5877     78.0257
05000     1.9410      0.5190     0.5877     77.7637
05100     2.7935      0.4283     0.5877     77.4540
05200     1.8986      0.5485     0.5916     78.0569
05300     1.7785      0.5738     0.5916     78.5207
05400     1.8667      0.5084     0.5988     78.6175
05500     1.8005      0.5759     0.6151     78.3900
05600     2.0020      0.5316     0.6151     78.0697
05700     2.2602      0.4810     0.6151     78.0358
05800     1.8367      0.5675     0.6151     77.8157
05900     1.6548      0.5717     0.6151     77.8491
06000     1.7689      0.5295     0.6151     78.1277
06100     2.0353      0.5549     0.6222     78.0038
06200     1.9078      0.5295     0.6222     78.3682
06300     1.7986      0.5633     0.6222     77.9070
06400     1.8645      0.5886     0.6222     77.9560
06500     1.6059      0.5992     0.6222     78.2582
06600     2.0351      0.5295     0.6251     78.4220
06700     1.8217      0.5316     0.6251     78.8066
06800     1.6448      0.5992     0.6258     78.5188
06900     1.7568      0.5696     0.6436     77.9027
07000     1.5231      0.6013     0.6436     78.0448
07100     1.9839      0.5380     0.6436     78.0129
07200     1.6709      0.5865     0.6436     78.5806
07300     1.9567      0.5759     0.6436     78.0503
07400     1.8090      0.5570     0.6436     78.0562
07500     1.8936      0.5464     0.6436     78.5844
07600     1.8346      0.5464     0.6436     77.2816
07700     1.8525      0.5148     0.6436     77.6538
07800     1.8975      0.5970     0.6436     77.9360
07900     1.9206      0.5338     0.6436     77.5515
08000     1.5897      0.5316     0.6469     77.8830
08100     1.5868      0.5802     0.6469     77.5995
08200     1.8047      0.5654     0.6522     78.0027
08300     1.6729      0.5422     0.6522     77.5199
08400     1.6223      0.5802     0.6522     77.2818
08500     1.6557      0.5717     0.6522     77.6849
08600     1.6737      0.6034     0.6522     78.3196
08700     1.5084      0.5907     0.6522     78.0249
08800     1.6592      0.6076     0.6522     78.4898
08900     1.7955      0.5549     0.6522     77.6508
09000     1.7786      0.5464     0.6522     78.2218
09100     1.6095      0.6118     0.6579     78.5374
09200     1.6702      0.5759     0.6579     77.4504
09300     1.7533      0.5612     0.6579     78.2182
09400     1.7801      0.5359     0.6579     78.2132
09500     1.6353      0.5970     0.6579     77.4107
09600     1.9705      0.5654     0.6579     77.9799
09700     1.9302      0.5907     0.6579     78.5505
09800     1.6690      0.5253     0.6579     77.7729
09900     1.7644      0.5654     0.6579     77.8797
10000     1.4515      0.6266     0.6699     77.9265
10100     1.8215      0.5675     0.6699     77.6325
10200     1.4979      0.6371     0.6699     78.0235
10300     1.7560      0.5781     0.6699     77.8712
10400     1.8226      0.5928     0.6699     78.0539
10500     1.5317      0.6392     0.6699     77.9085
10600     1.6658      0.6139     0.6699     78.0484
10700     1.9150      0.5907     0.6699     78.2806
10800     1.7358      0.5865     0.6699     78.2599
10900     1.6163      0.5928     0.6699     77.8783
11000     1.6616      0.5886     0.6699     78.7782
11100     1.8183      0.5907     0.6699     78.1769
11200     1.7257      0.5633     0.6699     78.0221
11300     1.8814      0.5823     0.6699     77.7307
11400     1.5708      0.6329     0.6699     78.1462
11500     1.5972      0.6245     0.6699     78.9058
11600     1.6148      0.5970     0.6699     77.5722
11700     1.7110      0.6034     0.6699     77.7558
11800     1.7975      0.5717     0.6699     77.6782
11900     1.6004      0.6181     0.6699     77.2334
12000     1.8697      0.5696     0.6699     78.5470
12100     1.5900      0.5907     0.6699     77.6490
12200     1.7710      0.5970     0.6795     77.6889
12300     1.6031      0.6266     0.6795     78.2851
12400     1.4958      0.6181     0.6795     77.9421
12500     1.7424      0.5970     0.6795     78.2185
12600     1.5452      0.6287     0.6795     77.6735
12700     1.3481      0.6308     0.6795     78.1179
12800     1.6990      0.5970     0.6795     78.2180
12900     1.5502      0.6224     0.6795     77.3507
13000     1.7951      0.5949     0.6795     77.8150
13100     1.5613      0.6055     0.6795     78.3864
13200     1.7363      0.6118     0.6795     78.0058
13300     1.7493      0.6076     0.6795     78.3628
13400     1.6564      0.6160     0.6795     78.5294
13500     1.6191      0.6414     0.6795     77.3506
13600     1.7473      0.6139     0.6795     78.4743
13700     1.6029      0.6097     0.6795     78.1080
13800     1.5109      0.6013     0.6795     78.1808
13900     1.4228      0.6308     0.6795     78.8570
14000     1.5541      0.6139     0.6795     77.6585
14100     1.6192      0.6181     0.6795     77.7127
14200     1.6819      0.6097     0.6795     78.2257
14300     1.5170      0.6266     0.6795     77.7839
14400     1.8889      0.5781     0.6795     77.9984
14500     1.5777      0.6160     0.6795     78.2494
14600     1.5284      0.6245     0.6795     78.5423
14700     1.8351      0.6013     0.6795     77.8745
14800     1.5609      0.5886     0.6829     77.6971
14900     1.5964      0.5865     0.6829     77.4756
15000     1.5087      0.5970     0.6829     78.2652
15100     1.9005      0.5759     0.6829     77.4070
15200     1.6669      0.6034     0.6829     78.1906
15300     1.7013      0.5612     0.6829     77.2868
15400     1.8081      0.5654     0.6829     77.4095
15500     1.6139      0.5970     0.6829     77.8469
15600     1.4811      0.6350     0.6829     78.1890
15700     1.7227      0.6097     0.6829     78.0998
15800     1.4987      0.6034     0.6829     78.3184
15900     1.5893      0.6582     0.6829     77.6489
16000     1.5587      0.6266     0.6829     78.7634
16100     1.7413      0.5844     0.6829     77.7534
16200     1.6776      0.5970     0.6829     78.0765
16300     1.6492      0.6624     0.6829     77.8944
16400     1.6619      0.6097     0.6829     77.7393
16500     1.6116      0.6013     0.6829     78.3399
16600     1.5608      0.6139     0.6829     79.4670
16700     1.5245      0.6076     0.6829     77.3970
16800     1.9576      0.5759     0.6829     77.1707
16900     1.5171      0.5949     0.6829     77.7704
17000     1.5144      0.6034     0.6829     78.5571
17100     1.7323      0.5992     0.6829     77.7334
17200     1.7020      0.5802     0.6829     77.8379
17300     1.9736      0.5928     0.6829     78.7679
17400     1.5919      0.6245     0.6829     77.8410
17500     1.6722      0.6076     0.6829     77.8902
17600     1.4835      0.6414     0.6829     78.0432
17700     1.5571      0.6266     0.6829     77.5582
17800     1.6075      0.6118     0.6829     78.3315
17900     1.5815      0.6118     0.6829     77.7141
18000     1.5580      0.6139     0.6829     77.5615
18100     1.8543      0.5970     0.6829     78.0212
18200     1.6231      0.6266     0.6829     78.2773
18300     1.7080      0.5823     0.6829     77.8752
18400     1.5585      0.6118     0.6850     77.7992
18500     1.8013      0.5781     0.6850     78.0963
18600     1.6665      0.6097     0.6850     79.2702
18700     1.8374      0.6097     0.6850     78.4823
18800     1.7619      0.5970     0.6934     77.8294
18900     1.7909      0.5823     0.6934     77.8024
19000     1.6727      0.5970     0.6934     78.5202
19100     1.6784      0.6055     0.6934     78.5940
19200     1.6592      0.6013     0.6934     79.7704
19300     1.6235      0.6097     0.6934     78.2098
19400     1.8243      0.5886     0.6934     78.5848
19500     1.5427      0.6055     0.6934     78.4936
19600     1.6178      0.6392     0.6934     78.1365
19700     1.7235      0.6498     0.6934     77.2327
19800     1.7261      0.5907     0.6934     78.4676
19900     1.4983      0.5970     0.6934     77.7550
20000     1.4608      0.5823     0.6934     77.2850
20100     1.5658      0.6097     0.6934     77.4164
20200     1.3999      0.6477     0.6937     77.7796
20300     1.6016      0.5970     0.7003     79.0949
20400     1.5389      0.6266     0.7059     79.0686
20500     1.4608      0.6371     0.7059     77.6495
20600     1.6588      0.6266     0.7059     78.7245
20700     1.6988      0.6097     0.7059     77.6768
20800     1.6030      0.6097     0.7059     78.6690
20900     1.3940      0.6329     0.7059     76.8956
21000     1.5526      0.6498     0.7059     77.6194
21100     1.4808      0.6160     0.7059     78.2289
21200     1.5152      0.6350     0.7059     77.4808
21300     1.6704      0.6160     0.7059     77.4730
21400     1.5962      0.6118     0.7059     78.4558
21500     1.6303      0.5907     0.7059     77.5074
21600     1.5861      0.6456     0.7059     77.5749
21700     1.4669      0.6266     0.7059     77.4346
21800     1.3410      0.6371     0.7059     78.2383
21900     1.5980      0.6139     0.7059     78.6633
22000     1.6551      0.5949     0.7059     78.2625
22100     1.5385      0.6266     0.7060     78.0449
22200     1.6984      0.5907     0.7060     78.4030
22300     1.4876      0.6160     0.7060     77.8511
22400     1.5955      0.6287     0.7060     78.4447
22500     1.7262      0.6055     0.7060     78.1136
22600     1.6397      0.6266     0.7060     78.1822
22700     1.7275      0.6350     0.7060     78.2311
22800     1.5445      0.6245     0.7060     77.5946
22900     1.4422      0.6371     0.7060     78.2625
23000     1.5822      0.6266     0.7060     77.9297
23100     1.6652      0.6181     0.7060     77.2456
23200     1.6022      0.6392     0.7060     78.4098
23300     1.3772      0.6540     0.7060     77.2606
23400     1.5524      0.5865     0.7060     78.1366
23500     1.5592      0.6287     0.7060     78.4639
23600     1.6655      0.6160     0.7060     77.4680
23700     1.6254      0.6308     0.7060     78.5171
23800     1.6505      0.5865     0.7060     78.0667
23900     1.4378      0.6392     0.7060     77.7050
24000     1.5271      0.6097     0.7060     78.2065
24100     1.6777      0.6139     0.7060     78.2406
24200     1.4602      0.6266     0.7060     77.8665
24300     1.3264      0.6793     0.7060     78.3686
24400     1.5292      0.5970     0.7060     77.6241
24500     1.6149      0.6076     0.7060     77.6729
24600     1.4621      0.6561     0.7060     78.2667
24700     1.4306      0.6266     0.7060     77.8446
24800     1.8729      0.5886     0.7060     77.8204
24900     1.6414      0.5464     0.7060     77.3114
25000     1.5480      0.6245     0.7060     78.3605
25100     1.6985      0.5949     0.7060     78.1562
25200     1.4196      0.6392     0.7060     77.5634
25300     1.3879      0.6350     0.7060     77.6783
25400     1.5233      0.6266     0.7060     78.6090
25500     1.4516      0.6160     0.7060     78.4563
25600     1.4579      0.6287     0.7060     78.4122
25700     1.4378      0.6582     0.7060     77.7612
25800     1.5718      0.6308     0.7060     77.5846
25900     1.3991      0.6371     0.7060     78.8403
26000     1.4648      0.6414     0.7060     77.6679
26100     1.5049      0.6097     0.7060     77.5089
26200     1.3587      0.6034     0.7060     77.7155
26300     1.6716      0.6245     0.7060     77.5845
26400     1.7834      0.6076     0.7060     77.8657
26500     1.6689      0.6076     0.7060     78.6229
26600     1.7154      0.5970     0.7060     77.1752
26700     1.7381      0.5865     0.7060     75.5596
26800     1.5159      0.6371     0.7060     77.1160
26900     1.4727      0.6224     0.7060     77.5419
27000     1.7347      0.5696     0.7060     79.0414
27100     1.3629      0.6435     0.7060     79.3216
27200     1.4565      0.6350     0.7060     79.3950
27300     1.4991      0.6435     0.7060     77.8652
27400     1.7759      0.5949     0.7060     77.9149
27500     1.6265      0.6160     0.7060     78.2354
27600     1.4512      0.6392     0.7060     77.4685
27700     1.5297      0.5970     0.7060     77.9648
27800     1.6465      0.5823     0.7060     78.4978
27900     1.5564      0.6034     0.7060     77.5239
28000     1.7298      0.5970     0.7060     77.5792
28100     1.6789      0.6371     0.7060     78.0171
28200     1.4798      0.6181     0.7060     78.3924
28300     1.7018      0.5928     0.7060     78.4333
28400     1.4286      0.6414     0.7060     78.5373
28500     1.4124      0.6203     0.7060     77.6959
28600     1.3984      0.5907     0.7060     77.9585
28700     1.5510      0.6076     0.7060     77.8324
28800     1.5068      0.6181     0.7060     78.7748
28900     1.7800      0.5928     0.7060     78.3821
29000     1.4084      0.6013     0.7060     78.0820
29100     1.2926      0.6561     0.7060     78.1748
29200     1.4559      0.6266     0.7060     77.5216
29300     1.4923      0.6055     0.7060     78.0344
29400     1.3000      0.6477     0.7060     78.7091
29500     1.3981      0.6224     0.7060     78.1404
29600     1.5810      0.6477     0.7060     78.4563
29700     1.6101      0.6055     0.7060     77.9075
29800     1.5541      0.6392     0.7060     78.3757
29900     1.4133      0.6435     0.7060     78.1176
29999     1.3713      0.6498     0.7060     77.4260
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Start testing:
Test Accuracy: 0.6887
