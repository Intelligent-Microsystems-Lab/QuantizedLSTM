Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 49, in __init__
    torch.nn.init.uniform_(self.cell.weight_ih, a = -limit2, b = limit2)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 49, in __init__
    torch.nn.init.uniform_(self.cell.weight_ih, a = -limit2, b = limit2)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 49, in __init__
    torch.nn.init.uniform_(self.cell.weight_ih, a = -limit2, b = limit2)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=193012823, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 49, in __init__
    torch.nn.init.uniform_(self.cell.weight_ih, a = -limit2, b = limit2)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 362, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.0, finetuning_epochs=0, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.0005,0.0001,0.00002', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,10000', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
b0c9d4bf-1382-473f-a808-55d378431c8d
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.5526      0.0844     0.0801     13.4217
00100     2.5429      0.0696     0.0803     83.5810
00200     2.5399      0.0781     0.1051     81.2607
00300     2.7522      0.1456     0.1051     80.8369
00400     2.6241      0.1561     0.2010     77.7455
00500     3.2717      0.2236     0.2459     77.8328
00600     3.0120      0.2363     0.2620     77.5695
00700     2.9829      0.2764     0.2905     79.4060
00800     3.0084      0.3101     0.2966     80.1125
00900     2.7607      0.2954     0.3336     81.6501
01000     2.8385      0.3122     0.3336     78.4942
01100     2.5301      0.3523     0.3415     80.7014
01200     2.7814      0.2806     0.3415     78.0413
01300     2.9191      0.3143     0.3415     77.8451
01400     2.7069      0.3397     0.3415     78.5759
01500     2.5207      0.3249     0.3634     77.8822
01600     2.5424      0.3481     0.3741     78.5967
01700     2.4449      0.3418     0.3741     78.1156
01800     2.5547      0.3523     0.3741     78.5329
01900     2.6751      0.3376     0.3741     78.2902
02000     3.1029      0.3270     0.3796     78.0168
02100     3.0525      0.3671     0.3975     78.1910
02200     2.8147      0.4072     0.4130     77.6798
02300     2.8898      0.3755     0.4329     79.0064
02400     2.6448      0.4177     0.4385     79.0322
02500     2.7602      0.4051     0.4422     78.1916
02600     2.6996      0.4346     0.4770     78.6410
02700     2.9936      0.3755     0.4770     78.5109
02800     2.5659      0.4451     0.4855     77.9447
02900     2.6095      0.4051     0.4855     77.8372
03000     2.4248      0.4262     0.4855     77.5991
03100     2.3997      0.4578     0.4916     77.9269
03200     2.4709      0.4430     0.4916     78.0504
03300     2.4155      0.4620     0.4916     78.0391
03400     2.0661      0.4557     0.4916     77.9338
03500     1.8899      0.4958     0.5497     78.0049
03600     2.1032      0.5169     0.5497     77.7941
03700     2.4890      0.4937     0.5497     77.2002
03800     2.3054      0.4684     0.5610     78.8887
03900     1.9629      0.5359     0.5685     77.5280
04000     1.9266      0.5063     0.5685     78.2185
04100     1.8505      0.5190     0.5685     77.6624
04200     2.0636      0.4895     0.5741     77.8855
04300     2.2840      0.5316     0.5877     77.9397
04400     1.9889      0.5359     0.5877     77.7924
04500     2.0261      0.5063     0.5877     77.8193
04600     2.1828      0.4895     0.5877     78.2715
04700     1.8124      0.5190     0.5877     77.4468
04800     1.8123      0.5211     0.5877     78.2254
04900     1.9606      0.4852     0.5877     78.0257
05000     1.9410      0.5190     0.5877     77.7637
05100     2.7935      0.4283     0.5877     77.4540
05200     1.8986      0.5485     0.5916     78.0569
05300     1.7785      0.5738     0.5916     78.5207
05400     1.8667      0.5084     0.5988     78.6175
05500     1.8005      0.5759     0.6151     78.3900
05600     2.0020      0.5316     0.6151     78.0697
05700     2.2602      0.4810     0.6151     78.0358
05800     1.8367      0.5675     0.6151     77.8157
05900     1.6548      0.5717     0.6151     77.8491
06000     1.7689      0.5295     0.6151     78.1277
06100     2.0353      0.5549     0.6222     78.0038
06200     1.9078      0.5295     0.6222     78.3682
06300     1.7986      0.5633     0.6222     77.9070
06400     1.8645      0.5886     0.6222     77.9560
06500     1.6059      0.5992     0.6222     78.2582
06600     2.0351      0.5295     0.6251     78.4220
06700     1.8217      0.5316     0.6251     78.8066
06800     1.6448      0.5992     0.6258     78.5188
06900     1.7568      0.5696     0.6436     77.9027
07000     1.5231      0.6013     0.6436     78.0448
07100     1.9839      0.5380     0.6436     78.0129
07200     1.6709      0.5865     0.6436     78.5806
07300     1.9567      0.5759     0.6436     78.0503
07400     1.8090      0.5570     0.6436     78.0562
07500     1.8936      0.5464     0.6436     78.5844
07600     1.8346      0.5464     0.6436     77.2816
07700     1.8525      0.5148     0.6436     77.6538
07800     1.8975      0.5970     0.6436     77.9360
07900     1.9206      0.5338     0.6436     77.5515
08000     1.5897      0.5316     0.6469     77.8830
08100     1.5868      0.5802     0.6469     77.5995
08200     1.8047      0.5654     0.6522     78.0027
08300     1.6729      0.5422     0.6522     77.5199
08400     1.6223      0.5802     0.6522     77.2818
08500     1.6557      0.5717     0.6522     77.6849
08600     1.6737      0.6034     0.6522     78.3196
08700     1.5084      0.5907     0.6522     78.0249
08800     1.6592      0.6076     0.6522     78.4898
08900     1.7955      0.5549     0.6522     77.6508
09000     1.7786      0.5464     0.6522     78.2218
09100     1.6095      0.6118     0.6579     78.5374
09200     1.6702      0.5759     0.6579     77.4504
09300     1.7533      0.5612     0.6579     78.2182
09400     1.7801      0.5359     0.6579     78.2132
09500     1.6353      0.5970     0.6579     77.4107
09600     1.9705      0.5654     0.6579     77.9799
09700     1.9302      0.5907     0.6579     78.5505
09800     1.6690      0.5253     0.6579     77.7729
09900     1.7644      0.5654     0.6579     77.8797
10000     1.4515      0.6266     0.6699     77.9265
10100     1.8215      0.5675     0.6699     77.6325
10200     1.4979      0.6371     0.6699     78.0235
10300     1.7560      0.5781     0.6699     77.8712
10400     1.8226      0.5928     0.6699     78.0539
10500     1.5317      0.6392     0.6699     77.9085
10600     1.6658      0.6139     0.6699     78.0484
10700     1.9150      0.5907     0.6699     78.2806
10800     1.7358      0.5865     0.6699     78.2599
10900     1.6163      0.5928     0.6699     77.8783
11000     1.6616      0.5886     0.6699     78.7782
11100     1.8183      0.5907     0.6699     78.1769
11200     1.7257      0.5633     0.6699     78.0221
11300     1.8814      0.5823     0.6699     77.7307
11400     1.5708      0.6329     0.6699     78.1462
11500     1.5972      0.6245     0.6699     78.9058
11600     1.6148      0.5970     0.6699     77.5722
11700     1.7110      0.6034     0.6699     77.7558
11800     1.7975      0.5717     0.6699     77.6782
11900     1.6004      0.6181     0.6699     77.2334
12000     1.8697      0.5696     0.6699     78.5470
12100     1.5900      0.5907     0.6699     77.6490
12200     1.7710      0.5970     0.6795     77.6889
12300     1.6031      0.6266     0.6795     78.2851
12400     1.4958      0.6181     0.6795     77.9421
12500     1.7424      0.5970     0.6795     78.2185
12600     1.5452      0.6287     0.6795     77.6735
12700     1.3481      0.6308     0.6795     78.1179
12800     1.6990      0.5970     0.6795     78.2180
12900     1.5502      0.6224     0.6795     77.3507
13000     1.7951      0.5949     0.6795     77.8150
13100     1.5613      0.6055     0.6795     78.3864
13200     1.7363      0.6118     0.6795     78.0058
13300     1.7493      0.6076     0.6795     78.3628
13400     1.6564      0.6160     0.6795     78.5294
13500     1.6191      0.6414     0.6795     77.3506
13600     1.7473      0.6139     0.6795     78.4743
13700     1.6029      0.6097     0.6795     78.1080
13800     1.5109      0.6013     0.6795     78.1808
13900     1.4228      0.6308     0.6795     78.8570
14000     1.5541      0.6139     0.6795     77.6585
14100     1.6192      0.6181     0.6795     77.7127
14200     1.6819      0.6097     0.6795     78.2257
14300     1.5170      0.6266     0.6795     77.7839
14400     1.8889      0.5781     0.6795     77.9984
14500     1.5777      0.6160     0.6795     78.2494
14600     1.5284      0.6245     0.6795     78.5423
14700     1.8351      0.6013     0.6795     77.8745
14800     1.5609      0.5886     0.6829     77.6971
14900     1.5964      0.5865     0.6829     77.4756
15000     1.5087      0.5970     0.6829     78.2652
15100     1.9005      0.5759     0.6829     77.4070
15200     1.6669      0.6034     0.6829     78.1906
15300     1.7013      0.5612     0.6829     77.2868
15400     1.8081      0.5654     0.6829     77.4095
15500     1.6139      0.5970     0.6829     77.8469
15600     1.4811      0.6350     0.6829     78.1890
15700     1.7227      0.6097     0.6829     78.0998
15800     1.4987      0.6034     0.6829     78.3184
15900     1.5893      0.6582     0.6829     77.6489
16000     1.5587      0.6266     0.6829     78.7634
16100     1.7413      0.5844     0.6829     77.7534
16200     1.6776      0.5970     0.6829     78.0765
16300     1.6492      0.6624     0.6829     77.8944
16400     1.6619      0.6097     0.6829     77.7393
16500     1.6116      0.6013     0.6829     78.3399
16600     1.5608      0.6139     0.6829     79.4670
16700     1.5245      0.6076     0.6829     77.3970
16800     1.9576      0.5759     0.6829     77.1707
16900     1.5171      0.5949     0.6829     77.7704
17000     1.5144      0.6034     0.6829     78.5571
17100     1.7323      0.5992     0.6829     77.7334
17200     1.7020      0.5802     0.6829     77.8379
17300     1.9736      0.5928     0.6829     78.7679
17400     1.5919      0.6245     0.6829     77.8410
17500     1.6722      0.6076     0.6829     77.8902
17600     1.4835      0.6414     0.6829     78.0432
17700     1.5571      0.6266     0.6829     77.5582
17800     1.6075      0.6118     0.6829     78.3315
17900     1.5815      0.6118     0.6829     77.7141
18000     1.5580      0.6139     0.6829     77.5615
18100     1.8543      0.5970     0.6829     78.0212
18200     1.6231      0.6266     0.6829     78.2773
18300     1.7080      0.5823     0.6829     77.8752
18400     1.5585      0.6118     0.6850     77.7992
18500     1.8013      0.5781     0.6850     78.0963
18600     1.6665      0.6097     0.6850     79.2702
18700     1.8374      0.6097     0.6850     78.4823
18800     1.7619      0.5970     0.6934     77.8294
18900     1.7909      0.5823     0.6934     77.8024
19000     1.6727      0.5970     0.6934     78.5202
19100     1.6784      0.6055     0.6934     78.5940
19200     1.6592      0.6013     0.6934     79.7704
19300     1.6235      0.6097     0.6934     78.2098
19400     1.8243      0.5886     0.6934     78.5848
19500     1.5427      0.6055     0.6934     78.4936
19600     1.6178      0.6392     0.6934     78.1365
19700     1.7235      0.6498     0.6934     77.2327
19800     1.7261      0.5907     0.6934     78.4676
19900     1.4983      0.5970     0.6934     77.7550
20000     1.4608      0.5823     0.6934     77.2850
20100     1.5658      0.6097     0.6934     77.4164
20200     1.3999      0.6477     0.6937     77.7796
20300     1.6016      0.5970     0.7003     79.0949
20400     1.5389      0.6266     0.7059     79.0686
20500     1.4608      0.6371     0.7059     77.6495
20600     1.6588      0.6266     0.7059     78.7245
20700     1.6988      0.6097     0.7059     77.6768
20800     1.6030      0.6097     0.7059     78.6690
20900     1.3940      0.6329     0.7059     76.8956
21000     1.5526      0.6498     0.7059     77.6194
21100     1.4808      0.6160     0.7059     78.2289
21200     1.5152      0.6350     0.7059     77.4808
21300     1.6704      0.6160     0.7059     77.4730
21400     1.5962      0.6118     0.7059     78.4558
21500     1.6303      0.5907     0.7059     77.5074
21600     1.5861      0.6456     0.7059     77.5749
21700     1.4669      0.6266     0.7059     77.4346
21800     1.3410      0.6371     0.7059     78.2383
21900     1.5980      0.6139     0.7059     78.6633
22000     1.6551      0.5949     0.7059     78.2625
22100     1.5385      0.6266     0.7060     78.0449
22200     1.6984      0.5907     0.7060     78.4030
22300     1.4876      0.6160     0.7060     77.8511
22400     1.5955      0.6287     0.7060     78.4447
22500     1.7262      0.6055     0.7060     78.1136
22600     1.6397      0.6266     0.7060     78.1822
22700     1.7275      0.6350     0.7060     78.2311
22800     1.5445      0.6245     0.7060     77.5946
22900     1.4422      0.6371     0.7060     78.2625
23000     1.5822      0.6266     0.7060     77.9297
23100     1.6652      0.6181     0.7060     77.2456
23200     1.6022      0.6392     0.7060     78.4098
23300     1.3772      0.6540     0.7060     77.2606
23400     1.5524      0.5865     0.7060     78.1366
23500     1.5592      0.6287     0.7060     78.4639
23600     1.6655      0.6160     0.7060     77.4680
23700     1.6254      0.6308     0.7060     78.5171
23800     1.6505      0.5865     0.7060     78.0667
23900     1.4378      0.6392     0.7060     77.7050
24000     1.5271      0.6097     0.7060     78.2065
24100     1.6777      0.6139     0.7060     78.2406
24200     1.4602      0.6266     0.7060     77.8665
24300     1.3264      0.6793     0.7060     78.3686
24400     1.5292      0.5970     0.7060     77.6241
24500     1.6149      0.6076     0.7060     77.6729
24600     1.4621      0.6561     0.7060     78.2667
24700     1.4306      0.6266     0.7060     77.8446
24800     1.8729      0.5886     0.7060     77.8204
24900     1.6414      0.5464     0.7060     77.3114
25000     1.5480      0.6245     0.7060     78.3605
25100     1.6985      0.5949     0.7060     78.1562
25200     1.4196      0.6392     0.7060     77.5634
25300     1.3879      0.6350     0.7060     77.6783
25400     1.5233      0.6266     0.7060     78.6090
25500     1.4516      0.6160     0.7060     78.4563
25600     1.4579      0.6287     0.7060     78.4122
25700     1.4378      0.6582     0.7060     77.7612
25800     1.5718      0.6308     0.7060     77.5846
25900     1.3991      0.6371     0.7060     78.8403
26000     1.4648      0.6414     0.7060     77.6679
26100     1.5049      0.6097     0.7060     77.5089
26200     1.3587      0.6034     0.7060     77.7155
26300     1.6716      0.6245     0.7060     77.5845
26400     1.7834      0.6076     0.7060     77.8657
26500     1.6689      0.6076     0.7060     78.6229
26600     1.7154      0.5970     0.7060     77.1752
26700     1.7381      0.5865     0.7060     75.5596
26800     1.5159      0.6371     0.7060     77.1160
26900     1.4727      0.6224     0.7060     77.5419
27000     1.7347      0.5696     0.7060     79.0414
27100     1.3629      0.6435     0.7060     79.3216
27200     1.4565      0.6350     0.7060     79.3950
27300     1.4991      0.6435     0.7060     77.8652
27400     1.7759      0.5949     0.7060     77.9149
27500     1.6265      0.6160     0.7060     78.2354
27600     1.4512      0.6392     0.7060     77.4685
27700     1.5297      0.5970     0.7060     77.9648
27800     1.6465      0.5823     0.7060     78.4978
27900     1.5564      0.6034     0.7060     77.5239
28000     1.7298      0.5970     0.7060     77.5792
28100     1.6789      0.6371     0.7060     78.0171
28200     1.4798      0.6181     0.7060     78.3924
28300     1.7018      0.5928     0.7060     78.4333
28400     1.4286      0.6414     0.7060     78.5373
28500     1.4124      0.6203     0.7060     77.6959
28600     1.3984      0.5907     0.7060     77.9585
28700     1.5510      0.6076     0.7060     77.8324
28800     1.5068      0.6181     0.7060     78.7748
28900     1.7800      0.5928     0.7060     78.3821
29000     1.4084      0.6013     0.7060     78.0820
29100     1.2926      0.6561     0.7060     78.1748
29200     1.4559      0.6266     0.7060     77.5216
29300     1.4923      0.6055     0.7060     78.0344
29400     1.3000      0.6477     0.7060     78.7091
29500     1.3981      0.6224     0.7060     78.1404
29600     1.5810      0.6477     0.7060     78.4563
29700     1.6101      0.6055     0.7060     77.9075
29800     1.5541      0.6392     0.7060     78.3757
29900     1.4133      0.6435     0.7060     78.1176
29999     1.3713      0.6498     0.7060     77.4260
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Start testing:
Test Accuracy: 0.6887
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=4, quant_actNM=4, quant_inp=4, quant_w=4, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
18adbbd5-3245-4faa-8c45-a9fa8541f7e2
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.5526      0.0717     0.0819     12.6565
00100     2.9131      0.1688     0.0819     72.3973
00200     3.9206      0.1983     0.0820     73.5466
00300     3.5155      0.3017     0.0950     76.0021
00400     3.6249      0.3333     0.0950     74.5124
00500     3.6534      0.2827     0.0950     74.5072
00600     3.4776      0.3038     0.0967     73.9091
00700     3.2675      0.3291     0.1136     73.6835
00800     3.1218      0.3861     0.1136     75.7987
00900     3.0372      0.3776     0.1137     76.1458
01000     2.8028      0.4093     0.1137     73.2188
01100     2.7825      0.4008     0.1137     75.0858
01200     2.5782      0.4494     0.1137     74.5019
01300     2.6274      0.4262     0.1199     74.2346
01400     2.2444      0.4768     0.1199     73.7981
01500     2.1773      0.5105     0.1337     73.2614
01600     2.2191      0.4684     0.1337     76.0196
01700     2.1124      0.5211     0.1438     75.8698
01800     2.2757      0.4958     0.1515     75.5899
01900     2.2644      0.5021     0.1515     75.1035
02000     2.0592      0.5190     0.1653     74.9122
02100     1.7938      0.5865     0.1653     74.9856
02200     2.2530      0.5232     0.1653     75.7761
02300     1.8570      0.5865     0.1700     76.8690
02400     1.9602      0.5654     0.1700     75.1490
02500     1.8738      0.5823     0.1700     76.0590
02600     1.7740      0.5549     0.1772     77.1545
02700     2.0939      0.5506     0.1777     77.7282
02800     1.9618      0.5570     0.1777     75.3782
02900     1.7021      0.5844     0.1777     76.8380
03000     1.6411      0.5844     0.1777     76.0551
03100     1.7055      0.6013     0.1777     75.1480
03200     1.9100      0.5717     0.1777     76.5142
03300     1.4454      0.6371     0.1777     73.8060
03400     2.1384      0.5612     0.1777     77.4957
03500     1.5933      0.5907     0.1871     77.3757
03600     1.9520      0.5633     0.1951     76.0102
03700     1.7598      0.5865     0.1951     76.8561
03800     1.9919      0.5654     0.1951     74.3891
03900     1.6665      0.6245     0.1951     75.4484
04000     1.5941      0.6414     0.1996     74.1382
04100     1.8473      0.5823     0.1996     71.5582
04200     2.0452      0.5886     0.1996     72.6782
04300     1.7080      0.6181     0.2048     73.0209
04400     1.4660      0.6139     0.2048     72.6458
04500     1.8775      0.6013     0.2048     73.8014
04600     1.5238      0.6034     0.2048     72.9144
04700     1.5385      0.6350     0.2090     75.8137
04800     1.6661      0.6139     0.2090     72.6355
04900     1.6025      0.5844     0.2090     72.9124
05000     1.4311      0.6477     0.2140     73.3515
05100     1.4315      0.6646     0.2333     71.1703
05200     1.4184      0.6435     0.2357     73.2657
05300     1.4280      0.6350     0.2357     71.3406
05400     1.5750      0.5844     0.2357     72.8575
05500     1.3374      0.6498     0.2357     73.6561
05600     1.5397      0.6456     0.2420     72.0945
05700     1.3782      0.6371     0.2420     72.5564
05800     1.4192      0.6308     0.2420     74.1156
05900     1.3896      0.6751     0.2420     77.4493
06000     1.4933      0.6266     0.2420     76.0852
06100     1.6250      0.6245     0.2420     76.4195
06200     1.4777      0.6730     0.2424     76.3625
06300     1.4850      0.6498     0.2508     76.3964
06400     1.4717      0.6667     0.2508     76.7965
06500     1.6985      0.5886     0.2508     76.6802
06600     1.4841      0.6498     0.2561     75.6237
06700     1.2680      0.6814     0.2767     75.6243
06800     1.5974      0.6498     0.2767     71.5881
06900     1.4148      0.6371     0.2767     71.2429
07000     1.4239      0.6371     0.2915     73.1180
07100     1.3383      0.6730     0.2915     74.1714
07200     1.5288      0.6814     0.2915     73.9471
07300     1.3822      0.6983     0.2915     74.0314
07400     1.7320      0.6329     0.2915     74.0675
07500     1.4230      0.6392     0.2915     74.5791
07600     1.3280      0.6329     0.2915     76.5098
07700     1.3905      0.6371     0.2915     72.9778
07800     1.2612      0.6667     0.2915     73.6580
07900     1.4723      0.6456     0.2965     74.2082
08000     1.4301      0.6519     0.2965     74.3159
08100     1.4886      0.6498     0.2965     73.6949
08200     1.5690      0.6519     0.2965     74.8282
08300     1.4081      0.6646     0.2965     74.5003
08400     1.5139      0.6709     0.2965     74.4581
08500     1.6086      0.6435     0.2965     77.5426
08600     1.2463      0.6730     0.2965     76.7078
08700     1.5269      0.6498     0.2965     76.1163
08800     1.6592      0.6371     0.2965     76.8614
08900     1.5955      0.6519     0.2965     76.6253
09000     1.5619      0.6603     0.2965     78.8574
09100     1.4518      0.6983     0.2965     73.3225
09200     1.7086      0.6097     0.2965     75.3573
09300     1.2972      0.6751     0.2965     75.6399
09400     1.4662      0.6414     0.3022     72.7407
09500     1.4362      0.6793     0.3025     74.5529
09600     1.3970      0.6646     0.3025     76.4834
09700     1.1974      0.6899     0.3025     73.0774
09800     1.4427      0.6477     0.3025     76.8040
09900     1.3835      0.6498     0.3025     75.3131
10000     1.5171      0.6730     0.3025     74.7919
10100     1.3513      0.6624     0.3031     72.5757
10200     1.3499      0.6603     0.3054     75.5894
10300     1.4343      0.6540     0.3054     74.8787
10400     1.6690      0.6287     0.3054     74.5497
10500     1.5279      0.6477     0.3054     75.1504
10600     1.3617      0.6709     0.3054     74.8176
10700     1.4497      0.6793     0.3054     74.6352
10800     1.4943      0.6519     0.3054     73.9301
10900     1.3295      0.6899     0.3054     75.6427
11000     1.4114      0.6561     0.3054     75.4423
11100     1.3599      0.6624     0.3054     75.2126
11200     1.2795      0.7025     0.3054     73.9173
11300     1.5037      0.6793     0.3054     74.5946
11400     1.5663      0.6835     0.3054     74.0235
11500     1.4370      0.6350     0.3054     75.1647
11600     1.4361      0.6540     0.3054     75.1886
11700     1.2937      0.6814     0.3054     75.1878
11800     1.2071      0.6941     0.3054     75.3107
11900     1.5798      0.6519     0.3054     75.8782
12000     1.3965      0.6582     0.3054     75.3347
12100     1.3085      0.6709     0.3054     74.5553
12200     1.4198      0.6688     0.3054     75.7053
12300     1.5174      0.6941     0.3054     74.5994
12400     1.4584      0.6983     0.3054     74.5376
12500     1.5908      0.6477     0.3054     75.0385
12600     1.2924      0.6941     0.3054     75.3095
12700     1.3444      0.6624     0.3054     75.0396
12800     1.3691      0.6646     0.3054     76.6558
12900     1.4872      0.6561     0.3054     74.4548
13000     1.4190      0.6730     0.3054     74.9115
13100     1.4562      0.6793     0.3054     75.0908
13200     1.3331      0.7110     0.3054     75.7158
13300     1.2815      0.7025     0.3054     74.8778
13400     1.2974      0.6751     0.3054     73.8686
13500     1.2864      0.7068     0.3054     75.4695
13600     1.2136      0.7173     0.3054     76.0470
13700     1.3076      0.6814     0.3054     75.5935
13800     1.4357      0.6878     0.3054     74.3178
13900     1.2217      0.7089     0.3123     72.6414
14000     1.3081      0.6772     0.3123     74.1240
14100     1.2164      0.7278     0.3123     73.2156
14200     1.3564      0.6435     0.3123     74.9938
14300     1.4350      0.6371     0.3123     75.3712
14400     1.2707      0.6857     0.3123     74.5465
14500     1.4123      0.6603     0.3123     75.4824
14600     1.3417      0.6793     0.3123     74.3354
14700     1.2860      0.7173     0.3123     74.0203
14800     1.2181      0.7342     0.3123     73.8151
14900     1.2648      0.6814     0.3123     73.4555
15000     1.2693      0.6920     0.3123     75.4150
15100     1.2877      0.7236     0.3123     75.5084
15200     1.1842      0.6751     0.3123     76.1568
15300     1.4174      0.6561     0.3123     74.7454
15400     1.3664      0.6983     0.3123     74.0042
15500     1.4100      0.6814     0.3123     76.5344
15600     1.3095      0.7004     0.3123     75.3089
15700     1.3476      0.7046     0.3123     73.0443
15800     1.2802      0.6878     0.3123     72.1563
15900     1.3462      0.6498     0.3153     75.5917
16000     1.3130      0.7025     0.3153     74.8094
16100     1.2750      0.6857     0.3153     73.3978
16200     1.0790      0.7194     0.3153     72.9333
16300     1.1184      0.7152     0.3153     74.1784
16400     1.2800      0.6624     0.3153     73.5365
16500     1.3881      0.7004     0.3153     73.7355
16600     1.2945      0.7089     0.3153     75.1475
16700     1.4736      0.6456     0.3153     74.3280
16800     1.3718      0.6962     0.3153     75.0501
16900     1.4530      0.6772     0.3189     74.5405
17000     1.5945      0.6751     0.3189     73.8925
17100     1.3801      0.6835     0.3189     75.9496
17200     1.2029      0.6793     0.3189     74.0175
17300     1.5264      0.6899     0.3189     73.7556
17400     1.2299      0.6751     0.3189     74.5567
17500     1.3571      0.6920     0.3189     74.6887
17600     1.2588      0.6983     0.3189     74.8817
17700     1.4990      0.6603     0.3189     74.2722
17800     1.5796      0.6646     0.3228     76.2431
17900     1.5081      0.6835     0.3228     76.8756
18000     1.3449      0.6561     0.3291     74.1413
18100     1.2436      0.6751     0.3291     75.0000
18200     1.4751      0.6793     0.3291     74.4844
18300     1.1205      0.6962     0.3291     74.3681
18400     1.2972      0.6582     0.3291     75.1200
18500     1.2799      0.6835     0.3291     73.3154
18600     1.2184      0.6582     0.3291     75.3518
18700     1.3154      0.6772     0.3291     76.1402
18800     1.3968      0.6561     0.3291     73.2174
18900     1.3046      0.6835     0.3291     74.2095
19000     1.2709      0.6582     0.3291     73.4865
19100     1.1219      0.7194     0.3335     74.1880
19200     1.3467      0.6667     0.3335     75.0608
19300     1.1109      0.7068     0.3335     76.2733
19400     1.5847      0.6308     0.3412     74.5885
19500     1.4361      0.6899     0.3412     75.6118
19600     1.3007      0.6772     0.3412     75.2384
19700     1.2756      0.6814     0.3412     74.0039
19800     1.5118      0.6688     0.3412     74.9250
19900     1.3260      0.6814     0.3412     73.0846
20000     1.4172      0.6709     0.3412     75.6003
20100     1.2915      0.6730     0.3412     73.9017
20199     1.2025      0.6920     0.3412     72.9004
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     3.7175      0.2743     0.3203     10.0727
00100     2.7346      0.3460     0.3972     74.8911
00200     2.3985      0.4409     0.4761     76.1193
00300     2.7835      0.4304     0.4902     76.9141
00400     2.7420      0.4241     0.5315     76.8230
00500     2.3096      0.4599     0.5350     74.9748
00600     2.1551      0.4873     0.5615     75.2743
00700     2.2289      0.5190     0.5615     74.8345
00800     2.4033      0.4578     0.5753     74.7335
00900     2.3467      0.5338     0.5753     75.8522
01000     2.2233      0.4916     0.5754     73.3071
01100     2.4587      0.5105     0.5754     74.0655
01200     2.3094      0.4705     0.5973     72.4656
01300     2.4467      0.5359     0.5973     72.7288
01400     2.1279      0.4768     0.6040     73.8853
01500     2.5388      0.5084     0.6040     74.0424
01600     2.1434      0.5105     0.6040     75.0352
01700     2.2359      0.5570     0.6040     74.0438
01800     2.1369      0.5633     0.6138     73.1400
01900     2.1611      0.5295     0.6138     73.6860
02000     2.0070      0.5591     0.6138     73.8738
02100     2.1697      0.4937     0.6138     73.7060
02200     2.2795      0.5232     0.6138     73.1243
02300     2.4261      0.4916     0.6254     74.8063
02400     2.1468      0.5527     0.6254     76.1414
02500     1.8638      0.5907     0.6254     73.4905
02600     2.2759      0.5190     0.6277     73.4655
02700     2.4165      0.5316     0.6277     73.8368
02800     1.9251      0.5907     0.6277     74.2132
02900     2.2656      0.5612     0.6277     74.2682
03000     2.0894      0.5549     0.6277     75.6527
03100     2.2406      0.5823     0.6321     77.2521
03200     2.3355      0.5380     0.6359     76.5822
03300     2.1416      0.5549     0.6416     75.5044
03400     2.0720      0.5949     0.6416     74.0195
03500     1.8512      0.6139     0.6416     75.8893
03600     2.0851      0.5738     0.6416     73.1257
03700     1.8113      0.5886     0.6416     76.1926
03800     1.9656      0.5612     0.6416     74.0436
03900     1.9242      0.5633     0.6416     76.7976
04000     2.0940      0.5970     0.6416     73.8937
04100     1.7842      0.6139     0.6416     74.0995
04200     1.8436      0.5781     0.6416     75.6563
04300     2.0282      0.5654     0.6416     76.4773
04400     2.2699      0.5591     0.6416     75.3652
04500     1.9547      0.6266     0.6519     74.2550
04600     2.1209      0.5591     0.6519     74.6017
04700     2.1516      0.5506     0.6519     75.6314
04800     1.9988      0.5485     0.6673     75.1959
04900     1.5523      0.6835     0.6673     75.7997
05000     2.0081      0.5886     0.6673     73.8564
05100     2.2692      0.5675     0.6673     74.6305
05200     1.7929      0.6097     0.6673     73.3642
05300     1.6346      0.6181     0.6673     74.3447
05400     1.9896      0.5633     0.6673     74.7593
05500     2.1127      0.5717     0.6673     74.9818
05600     1.7193      0.6245     0.6673     74.6139
05700     2.0954      0.5570     0.6673     73.5429
05800     1.7749      0.6055     0.6673     75.6292
05900     2.0005      0.4662     0.6673     74.8257
06000     2.1382      0.5253     0.6673     72.7472
06100     1.8825      0.5148     0.6673     73.4543
06200     2.0758      0.5949     0.6673     72.7208
06300     1.9845      0.4747     0.6673     74.4711
06400     1.8388      0.5633     0.6673     73.2781
06500     1.6914      0.5865     0.6673     76.0235
06600     1.7807      0.5675     0.6673     76.1227
06700     1.8418      0.5612     0.6673     75.9858
06800     1.9647      0.5675     0.6678     75.0356
06900     1.8587      0.5907     0.6678     74.8742
07000     1.8803      0.6266     0.6678     73.4420
07100     2.0307      0.5464     0.6758     73.8085
07200     1.9774      0.5675     0.6758     76.7374
07300     1.6466      0.6582     0.6758     75.8682
07400     1.6612      0.6245     0.6758     73.4540
07500     2.1302      0.5865     0.6758     74.5269
07600     1.9855      0.6181     0.6758     75.0948
07700     1.6790      0.6034     0.6790     75.0168
07800     1.7226      0.5696     0.6790     74.2681
07900     1.9247      0.5970     0.6790     73.5618
08000     1.8370      0.6034     0.6790     71.6905
08100     1.8261      0.5549     0.6790     72.6969
08200     1.9388      0.5591     0.6790     73.5742
08300     1.7308      0.6245     0.6790     73.5827
08400     1.7947      0.5865     0.6790     72.9698
08500     1.9824      0.5949     0.6790     74.4105
08600     1.8336      0.6371     0.6810     72.9466
08700     1.9071      0.6456     0.6810     75.6646
08800     1.9560      0.5992     0.6810     72.8131
08900     1.8675      0.5865     0.6810     74.0256
09000     1.7650      0.6203     0.6810     74.5745
09100     1.9929      0.6013     0.6810     74.0835
09200     2.0572      0.6034     0.6810     72.9197
09300     2.1153      0.5675     0.6860     76.1657
09400     2.0220      0.5485     0.6886     74.6903
09500     1.8870      0.6245     0.6886     73.9827
09600     1.8122      0.6266     0.6886     72.6816
09700     1.9446      0.5970     0.6886     75.3170
09800     1.6868      0.5802     0.6902     73.9711
09900     1.6491      0.6477     0.6902     77.8003
Start testing:
Test Accuracy: 0.7484
