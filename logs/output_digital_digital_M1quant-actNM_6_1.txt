Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=6, quant_actNM=6, quant_inp=6, quant_w=6, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
7a821df3-cd9e-4c34-a7b5-a6303b8422d7
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 373, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, 1.)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 82, in forward
    if len(x_range) > 1:
TypeError: object of type 'float' has no len()
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=6, quant_actNM=6, quant_inp=6, quant_w=6, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
3c3cad7a-2c55-4ea5-b22f-631a57703937
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 373, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, torch.tensor([1.]).device(input.device))
TypeError: 'torch.device' object is not callable
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=6, quant_actNM=6, quant_inp=6, quant_w=6, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
37e74369-1fd6-4e08-ab71-bc7cdea9fd51
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     3.6261      0.1034     0.1417     9.9530
00100     2.5231      0.1118     0.1417     67.0034
00200     1.9967      0.2616     0.3147     69.3793
00300     1.7208      0.3713     0.3981     68.9067
00400     1.6472      0.4114     0.4535     67.8842
00500     1.4716      0.4789     0.4934     68.1440
00600     1.4883      0.4895     0.5479     68.4495
00700     1.3576      0.5042     0.5479     68.7493
00800     1.3279      0.5401     0.5741     68.8367
00900     1.3376      0.5338     0.6022     68.8382
01000     1.1540      0.5928     0.6283     69.5499
01100     1.1643      0.5949     0.6477     68.4849
01200     1.1969      0.6498     0.6479     69.7603
01300     1.2278      0.5654     0.6546     69.4368
01400     1.0679      0.6034     0.6618     68.9839
01500     1.0388      0.6540     0.6839     69.1431
01600     1.0053      0.6561     0.6839     69.9142
01700     1.1051      0.6308     0.6927     68.6065
01800     0.9488      0.6772     0.6927     68.5514
01900     0.9299      0.7236     0.7069     68.7073
02000     1.0075      0.6350     0.7069     69.0394
02100     1.0039      0.6392     0.7069     70.5125
02200     0.9607      0.6371     0.7069     69.8443
02300     0.9780      0.6603     0.7069     69.2340
02400     0.9860      0.6519     0.7274     69.8206
02500     0.8941      0.6920     0.7274     69.9924
02600     0.7864      0.6941     0.7274     69.7948
02700     0.9576      0.7152     0.7364     70.6248
02800     0.8330      0.7152     0.7364     67.9848
02900     1.0147      0.6835     0.7364     68.3815
03000     0.9606      0.6751     0.7364     68.9919
03100     0.9575      0.7046     0.7435     67.9074
03200     0.9503      0.6920     0.7461     68.2949
03300     0.9361      0.6857     0.7461     68.1097
03400     0.8863      0.7152     0.7483     68.9199
03500     0.9711      0.6561     0.7496     68.2986
03600     0.8809      0.7278     0.7537     68.1382
03700     0.8286      0.7405     0.7537     68.2226
03800     0.9079      0.6920     0.7537     68.9306
03900     0.8203      0.7300     0.7537     68.8549
04000     0.8626      0.6983     0.7537     67.9222
04100     0.8263      0.7257     0.7537     68.7224
04200     0.8537      0.7089     0.7555     68.0790
04300     0.8243      0.7447     0.7581     67.7427
04400     0.7886      0.7257     0.7586     68.0334
04500     0.7619      0.7511     0.7648     68.3642
04600     0.7570      0.7722     0.7648     68.6365
04700     0.7887      0.7152     0.7648     68.5015
04800     0.8169      0.7342     0.7648     68.0825
04900     0.8557      0.7257     0.7741     67.9597
05000     0.8414      0.7110     0.7741     67.8919
05100     0.8021      0.7068     0.7741     68.0969
05200     0.8271      0.7215     0.7792     68.3081
05300     0.7398      0.7426     0.7792     67.9586
05400     0.7215      0.7595     0.7792     69.1875
05500     0.7652      0.7173     0.7792     68.1540
05600     0.7475      0.7363     0.7792     68.1080
05700     0.7706      0.7278     0.7792     68.4746
05800     0.7749      0.7595     0.7792     67.9636
05900     0.7539      0.7574     0.7792     68.3880
06000     0.7551      0.7489     0.7792     68.4018
06100     0.8315      0.7236     0.7792     68.5773
06200     0.8006      0.7321     0.7792     68.6123
06300     0.7780      0.7489     0.7792     68.3915
06400     0.7681      0.7489     0.7792     69.4423
06500     0.8206      0.7257     0.7792     68.3442
06600     0.7068      0.7764     0.7810     68.9891
06700     0.7891      0.7511     0.7810     68.6259
06800     0.8073      0.7025     0.7810     68.2358
06900     0.7333      0.7658     0.7810     67.7383
07000     0.6874      0.7574     0.7810     68.7391
07100     0.7053      0.7658     0.7810     67.8154
07200     0.8078      0.7236     0.7810     68.6400
07300     0.7886      0.7679     0.7810     68.3185
07400     0.7582      0.7489     0.7810     68.5855
07500     0.8468      0.7426     0.7810     68.1662
07600     0.8330      0.7300     0.7810     68.2682
07700     0.6956      0.7658     0.7853     67.9544
07800     0.7029      0.7722     0.7853     68.4121
07900     0.7237      0.7743     0.7853     68.6376
08000     0.8087      0.7236     0.7853     69.0362
08100     0.7784      0.7447     0.7853     68.9818
08200     0.6789      0.7637     0.7853     68.0711
08300     0.8072      0.7426     0.7853     68.5898
08400     0.7158      0.7700     0.7901     68.4000
08500     0.7500      0.7278     0.7917     68.3039
08600     0.8184      0.7300     0.7917     69.0493
08700     0.6796      0.7616     0.7917     68.9551
08800     0.7857      0.7743     0.7917     68.6894
08900     0.6666      0.7827     0.7917     68.4744
09000     0.7578      0.7532     0.7917     67.7813
09100     0.6965      0.7848     0.7917     68.8266
09200     0.7308      0.7616     0.7917     67.8787
09300     0.7209      0.7785     0.7917     68.0923
09400     0.7776      0.7700     0.7917     68.1209
09500     0.6726      0.7426     0.7917     67.9178
09600     0.7497      0.7363     0.7917     68.6031
09700     0.7341      0.7426     0.7917     67.7348
09800     0.5714      0.8038     0.7925     68.7971
09900     0.7274      0.7384     0.7925     69.2561
10000     0.7874      0.7363     0.7925     68.8359
10100     0.7648      0.7215     0.7977     69.2793
10200     0.6823      0.7785     0.7977     69.4256
10300     0.6861      0.7637     0.7999     68.1650
10400     0.7537      0.7553     0.7999     68.2967
10500     0.7093      0.7848     0.7999     68.6804
10600     0.6998      0.7468     0.7999     67.9390
10700     0.7487      0.7595     0.8016     68.7223
10800     0.7469      0.7743     0.8016     68.2536
10900     0.7320      0.7700     0.8016     68.1436
11000     0.8121      0.7152     0.8028     68.6472
11100     0.7405      0.7700     0.8028     68.8236
11200     0.7116      0.7722     0.8028     68.7180
11300     0.7079      0.7722     0.8028     68.3389
11400     0.5686      0.8101     0.8028     68.0841
11500     0.8125      0.7194     0.8028     68.9242
11600     0.6165      0.7700     0.8028     68.8136
11700     0.6539      0.7827     0.8028     68.6388
11800     0.8172      0.7321     0.8028     68.9652
11900     0.6975      0.7468     0.8028     68.6886
12000     0.7987      0.7426     0.8028     68.1384
12100     0.7013      0.7890     0.8028     68.6768
12200     0.7194      0.7658     0.8028     68.4165
12300     0.7634      0.7110     0.8028     68.3634
12400     0.6714      0.7658     0.8028     67.8955
12500     0.6735      0.7574     0.8028     68.2709
12600     0.6748      0.7848     0.8028     68.9611
12700     0.8197      0.7342     0.8028     67.7596
12800     0.6689      0.7806     0.8028     68.5118
12900     0.6446      0.7890     0.8028     67.9016
13000     0.6760      0.7869     0.8028     68.6932
13100     0.6935      0.7764     0.8028     68.6356
13200     0.6090      0.7954     0.8028     68.2774
13300     0.6885      0.7764     0.8066     69.1255
13400     0.6637      0.7848     0.8066     68.6084
13500     0.7984      0.7447     0.8066     68.2267
13600     0.7911      0.7236     0.8066     68.8607
13700     0.6721      0.7700     0.8066     68.5862
13800     0.7716      0.7785     0.8066     68.4457
13900     0.7812      0.7468     0.8066     68.6121
14000     0.8238      0.7278     0.8066     68.1156
14100     0.6906      0.7679     0.8066     68.0283
14200     0.6477      0.8017     0.8066     69.5212
14300     0.7713      0.7300     0.8066     68.3190
14400     0.7168      0.7827     0.8066     68.6299
14500     0.6186      0.7996     0.8066     68.7835
14600     0.7867      0.7405     0.8066     68.4169
14700     0.7589      0.7405     0.8066     69.0257
14800     0.7442      0.7574     0.8066     67.9011
14900     0.7774      0.7447     0.8066     67.7929
15000     0.7386      0.7215     0.8066     68.8687
15100     0.7389      0.7489     0.8066     68.2552
15200     0.7184      0.7532     0.8066     68.8262
15300     0.7251      0.7785     0.8066     68.7252
15400     0.7396      0.7743     0.8066     68.3338
15500     0.8275      0.7068     0.8066     69.0410
15600     0.7168      0.7616     0.8066     68.5480
15700     0.7006      0.7532     0.8066     68.4292
15800     0.7111      0.7574     0.8066     69.1344
15900     0.8021      0.7426     0.8066     68.2609
16000     0.6552      0.7932     0.8066     68.7797
16100     0.6455      0.8165     0.8066     68.3563
16200     0.6581      0.7700     0.8066     68.3669
16300     0.8395      0.7194     0.8066     68.8669
16400     0.6808      0.7553     0.8066     69.4452
16500     0.7381      0.7553     0.8066     68.7173
16600     0.6378      0.7975     0.8066     68.5176
16700     0.6551      0.7743     0.8066     67.9757
16800     0.5770      0.7911     0.8066     68.4860
16900     0.7018      0.7890     0.8066     68.8243
17000     0.7312      0.7384     0.8066     68.4952
17100     0.6933      0.7700     0.8066     68.3004
17200     0.6896      0.7679     0.8098     84.3875
17300     0.6695      0.7722     0.8098     68.0048
17400     0.6957      0.7511     0.8098     68.6671
17500     0.7977      0.7426     0.8098     67.5084
17600     0.7016      0.7426     0.8098     68.5614
17700     0.6882      0.7616     0.8098     68.3051
17800     0.6308      0.7996     0.8098     67.9523
17900     0.7228      0.7700     0.8098     69.2546
18000     0.6241      0.8017     0.8098     68.5568
18100     0.6096      0.7869     0.8098     68.1982
18200     0.6667      0.7679     0.8098     68.7392
18300     0.6623      0.7890     0.8098     68.0752
18400     0.6098      0.7890     0.8098     69.1657
18500     0.7180      0.7785     0.8098     68.6424
18600     0.7323      0.7532     0.8098     68.5924
18700     0.7376      0.7447     0.8098     69.4262
18800     0.6930      0.7447     0.8098     68.9811
18900     0.6939      0.7363     0.8098     68.0821
19000     0.6812      0.7679     0.8098     68.6887
19100     0.6040      0.7954     0.8098     68.5744
19200     0.7835      0.7511     0.8098     68.7876
19300     0.7215      0.7743     0.8098     68.1527
19400     0.7662      0.7574     0.8098     68.8959
19500     0.7435      0.7532     0.8098     68.6151
19600     0.7309      0.7616     0.8098     68.5872
19700     0.6618      0.7911     0.8098     68.5218
19800     0.7157      0.7785     0.8098     68.6904
19900     0.6764      0.7827     0.8098     68.7582
20000     0.7082      0.7637     0.8098     68.5666
20100     0.6948      0.7700     0.8098     68.2956
20199     0.6676      0.7764     0.8098     67.9906
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     0.6216      0.7911     0.7983     9.6463
00100     0.6765      0.7722     0.8000     68.4035
00200     0.6388      0.7911     0.8003     69.9953
00300     0.6889      0.7658     0.8003     68.8876
00400     0.6599      0.7806     0.8035     68.6117
00500     0.5518      0.8397     0.8095     67.9284
00600     0.5820      0.7827     0.8095     68.0470
00700     0.6011      0.7996     0.8095     67.9452
00800     0.5809      0.8249     0.8095     67.6211
00900     0.5096      0.8418     0.8095     68.2294
01000     0.5603      0.8059     0.8095     67.7319
01100     0.6381      0.8059     0.8095     68.5970
01200     0.6023      0.7954     0.8095     67.8606
01300     0.5982      0.7806     0.8095     68.1057
01400     0.5803      0.8101     0.8103     68.2131
01500     0.6360      0.7848     0.8103     67.8754
01600     0.6152      0.7722     0.8103     67.5561
01700     0.6371      0.7785     0.8103     68.7980
01800     0.6406      0.7911     0.8103     68.1337
01900     0.6769      0.7848     0.8103     67.9917
02000     0.5216      0.8228     0.8103     68.1013
02100     0.6052      0.7954     0.8103     69.3443
02200     0.5761      0.8017     0.8103     68.2497
02300     0.6442      0.7785     0.8103     67.6189
02400     0.6058      0.8038     0.8103     68.6285
02500     0.6760      0.7658     0.8103     67.4989
02600     0.7264      0.7468     0.8103     68.4160
02700     0.6602      0.7869     0.8104     67.6674
02800     0.6335      0.8038     0.8104     67.4177
02900     0.6491      0.7911     0.8104     68.3840
03000     0.5418      0.8038     0.8104     68.8063
03100     0.6599      0.7890     0.8104     68.2575
03200     0.6734      0.7806     0.8104     67.8243
03300     0.6493      0.7996     0.8104     68.7347
03400     0.5718      0.8101     0.8104     68.6132
03500     0.6228      0.7954     0.8104     68.4269
03600     0.7020      0.7511     0.8104     67.9315
03700     0.6363      0.8017     0.8104     68.9898
03800     0.6025      0.7869     0.8104     68.8389
03900     0.5769      0.8165     0.8104     68.1451
04000     0.6402      0.8101     0.8180     68.0064
04100     0.4953      0.8207     0.8180     67.9645
04200     0.5563      0.7996     0.8180     67.9611
04300     0.5764      0.8228     0.8180     68.5227
04400     0.6218      0.8038     0.8180     68.1887
04500     0.6417      0.7932     0.8180     68.2741
04600     0.6600      0.7890     0.8180     68.4100
04700     0.6532      0.7932     0.8180     68.0798
04800     0.5833      0.8122     0.8180     68.1594
04900     0.5876      0.7954     0.8180     68.1078
05000     0.6149      0.8207     0.8180     67.4452
05100     0.5719      0.8143     0.8180     68.0049
05200     0.5868      0.8059     0.8180     67.5500
05300     0.5653      0.8122     0.8180     68.0796
05400     0.6167      0.8080     0.8180     68.1897
05500     0.6952      0.7785     0.8180     68.6224
05600     0.5790      0.7932     0.8180     68.6994
05700     0.6479      0.7890     0.8180     68.2038
05800     0.6044      0.8059     0.8180     67.9070
05900     0.6627      0.7848     0.8180     68.0847
06000     0.5849      0.7975     0.8180     68.6867
06100     0.5943      0.8017     0.8180     67.7586
06200     0.5050      0.8312     0.8180     68.2789
06300     0.5820      0.7954     0.8180     68.1454
06400     0.6035      0.7890     0.8180     68.7486
06500     0.5548      0.8101     0.8180     68.0781
06600     0.6032      0.8291     0.8180     69.4871
06700     0.5764      0.8143     0.8180     67.9984
06800     0.6366      0.7848     0.8180     68.3164
06900     0.5481      0.8122     0.8180     68.0397
07000     0.5485      0.8270     0.8180     68.5518
07100     0.6523      0.7954     0.8180     68.4567
07200     0.6183      0.7932     0.8180     69.3067
07300     0.6500      0.7869     0.8180     68.2751
07400     0.6535      0.7764     0.8180     68.7325
07500     0.6409      0.7785     0.8180     68.2335
07600     0.5868      0.7954     0.8180     68.4497
07700     0.5982      0.8186     0.8180     68.1530
07800     0.5343      0.7954     0.8180     69.0851
07900     0.6156      0.7785     0.8180     68.0714
08000     0.6167      0.7890     0.8180     68.1930
08100     0.6476      0.7827     0.8180     68.6327
08200     0.5696      0.8333     0.8180     68.3943
08300     0.6476      0.7595     0.8180     68.5458
08400     0.5994      0.7869     0.8180     68.2985
08500     0.6427      0.7954     0.8180     67.2003
08600     0.6587      0.7827     0.8180     68.9327
08700     0.6276      0.7637     0.8180     68.3868
08800     0.6254      0.7743     0.8180     67.9754
08900     0.6571      0.7954     0.8180     67.8643
09000     0.6220      0.7954     0.8180     68.5023
09100     0.6396      0.7658     0.8180     68.6897
09200     0.6227      0.7827     0.8180     68.0083
09300     0.6197      0.7975     0.8180     68.2946
09400     0.7079      0.7764     0.8180     68.5859
09500     0.6296      0.8017     0.8180     67.9984
09600     0.5657      0.8017     0.8180     68.8220
09700     0.5604      0.8017     0.8180     68.5473
09800     0.6272      0.8143     0.8180     67.8144
09900     0.6046      0.8101     0.8180     69.1246
Start testing:
Test Accuracy: 0.7981
