Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=10, quant_actNM=10, quant_inp=10, quant_w=10, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
77e5f246-7bb9-45dc-8755-410f0fc7ab16
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, 1.)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 82, in forward
    if len(x_range) > 1:
TypeError: object of type 'float' has no len()
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=10, quant_actNM=10, quant_inp=10, quant_w=10, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
0e994098-1b8c-4cd5-a78a-73a5bc504654
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 147, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 209, in forward
    part1 = CustomMM_bmm.apply(quant_pass(pact_a_bmm(input.repeat(self.n_blocks, 1, 1), self.a1), self.ib, self.a1), self.weight_ih, self.bias_ih, self.noise_level, self.wb, self.bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 145, in forward
    wq = quant_pass(weight, wb, torch.tensor([1.]))
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 94, in forward
    x_scaled = x/x_range
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=10, quant_actNM=10, quant_inp=10, quant_w=10, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
bb492ad9-39d5-412d-915c-05f3c3795fb0
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.9664      0.0844     0.0749     9.9952
00100     2.0681      0.3481     0.3757     53.9039
00200     1.6639      0.4789     0.5077     54.1845
00300     1.3963      0.5654     0.5813     54.7749
00400     1.2405      0.5844     0.6276     54.5755
00500     1.1226      0.6814     0.6613     54.1181
00600     1.0511      0.6857     0.6893     54.6522
00700     1.0062      0.6983     0.7124     54.2568
00800     0.9593      0.7068     0.7153     55.0317
00900     0.9148      0.7194     0.7409     54.1292
01000     0.8568      0.7363     0.7478     54.7533
01100     0.8425      0.7384     0.7544     53.9544
01200     0.8743      0.7321     0.7551     53.7737
01300     0.9024      0.7257     0.7647     54.4089
01400     0.7722      0.7595     0.7697     54.3478
01500     0.7738      0.7553     0.7697     53.8462
01600     0.7541      0.7511     0.7794     54.9674
01700     0.7832      0.7405     0.7817     54.3064
01800     0.7482      0.7679     0.7817     54.0823
01900     0.7030      0.7954     0.7817     54.8230
02000     0.7096      0.7954     0.7834     54.7696
02100     0.7661      0.7764     0.7959     53.8911
02200     0.7354      0.7722     0.7959     54.2828
02300     0.6600      0.8017     0.7996     54.2850
02400     0.7106      0.7827     0.7996     55.1178
02500     0.7365      0.7827     0.7997     54.5270
02600     0.7032      0.7911     0.8075     55.7676
02700     0.6544      0.7890     0.8075     55.4856
02800     0.7114      0.7743     0.8075     55.0113
02900     0.6708      0.7996     0.8079     54.5518
03000     0.6423      0.7975     0.8079     55.5202
03100     0.7403      0.7785     0.8079     53.8561
03200     0.6585      0.8059     0.8079     54.4804
03300     0.6794      0.7869     0.8129     54.1680
03400     0.7267      0.7890     0.8129     53.6252
03500     0.7802      0.7785     0.8129     55.3906
03600     0.6163      0.8207     0.8129     54.1034
03700     0.6469      0.8143     0.8129     54.1360
03800     0.5906      0.8080     0.8129     54.7612
03900     0.6807      0.8165     0.8157     53.4008
04000     0.6856      0.7975     0.8157     54.0055
04100     0.6737      0.7932     0.8213     54.2278
04200     0.6431      0.7890     0.8213     54.2915
04300     0.6126      0.8143     0.8213     55.1528
04400     0.6493      0.8080     0.8213     53.8253
04500     0.6501      0.8186     0.8213     53.3619
04600     0.6909      0.7890     0.8213     56.9891
04700     0.6735      0.7806     0.8213     56.7482
04800     0.7190      0.7890     0.8213     56.8179
04900     0.6499      0.7996     0.8213     56.9574
05000     0.6759      0.8186     0.8213     57.7778
05100     0.5802      0.8376     0.8213     58.1600
05200     0.6988      0.7890     0.8213     56.5643
05300     0.5997      0.8270     0.8253     57.6284
05400     0.6111      0.8122     0.8253     57.7059
05500     0.5885      0.8312     0.8257     56.4725
05600     0.6429      0.8143     0.8257     57.2886
05700     0.6593      0.8080     0.8257     56.5380
05800     0.6474      0.7806     0.8257     57.0150
05900     0.5267      0.8249     0.8257     56.3211
06000     0.6308      0.8059     0.8286     55.4775
06100     0.5682      0.8312     0.8286     55.3330
06200     0.6055      0.8207     0.8329     54.4325
06300     0.6218      0.8122     0.8329     53.7582
06400     0.7229      0.7827     0.8329     54.1961
06500     0.6281      0.8017     0.8329     53.5367
06600     0.7028      0.7911     0.8329     54.0213
06700     0.6830      0.7954     0.8329     54.2173
06800     0.6405      0.8059     0.8329     54.5612
06900     0.5852      0.8333     0.8329     54.0188
07000     0.5423      0.8460     0.8329     54.0422
07100     0.6067      0.8207     0.8339     53.8559
07200     0.6270      0.8122     0.8363     54.6950
07300     0.5549      0.8312     0.8363     54.5931
07400     0.5965      0.8101     0.8363     53.5935
07500     0.5983      0.8059     0.8363     54.1063
07600     0.6144      0.7996     0.8363     53.6248
07700     0.5815      0.8249     0.8366     54.0390
07800     0.5422      0.8544     0.8366     54.5216
07900     0.5493      0.8333     0.8366     54.1437
08000     0.5426      0.8439     0.8366     54.6625
08100     0.6335      0.7975     0.8366     54.0746
08200     0.5743      0.7932     0.8366     54.9484
08300     0.6069      0.8228     0.8406     54.6315
08400     0.5791      0.8249     0.8406     53.7725
08500     0.5873      0.8333     0.8406     54.2430
08600     0.6237      0.8249     0.8406     54.2170
08700     0.6163      0.8143     0.8406     53.6138
08800     0.5636      0.8207     0.8406     53.8691
08900     0.5397      0.8418     0.8406     54.1144
09000     0.5697      0.8270     0.8406     54.3783
09100     0.5613      0.8354     0.8406     55.2406
09200     0.5967      0.8017     0.8406     54.1329
09300     0.6090      0.8080     0.8406     54.0060
09400     0.6110      0.8122     0.8406     54.3130
09500     0.5682      0.8122     0.8406     53.4704
09600     0.6074      0.8101     0.8406     55.7068
09700     0.6628      0.7975     0.8419     53.8411
09800     0.5404      0.8312     0.8419     53.9063
09900     0.5817      0.8333     0.8419     55.1493
10000     0.6740      0.8038     0.8419     54.2944
10100     0.5085      0.8354     0.8419     54.0910
10200     0.5799      0.8270     0.8419     53.7988
10300     0.5352      0.8523     0.8449     54.4924
10400     0.5334      0.8502     0.8483     55.3113
10500     0.5450      0.8376     0.8483     55.4269
10600     0.5198      0.8502     0.8483     54.7612
10700     0.5685      0.8397     0.8483     54.5251
10800     0.6172      0.8228     0.8483     53.7709
10900     0.5190      0.8523     0.8483     54.3623
11000     0.5314      0.8502     0.8483     54.6292
11100     0.5194      0.8544     0.8483     54.0202
11200     0.4948      0.8608     0.8483     55.0323
11300     0.5222      0.8481     0.8483     53.4787
11400     0.5113      0.8523     0.8483     52.1950
11500     0.5772      0.8270     0.8483     54.0798
11600     0.5954      0.8165     0.8483     54.4321
11700     0.6006      0.8249     0.8483     54.0929
11800     0.5946      0.8122     0.8483     54.5745
11900     0.5670      0.8312     0.8499     54.4142
12000     0.5112      0.8376     0.8499     54.2147
12100     0.5764      0.8270     0.8499     53.9666
12200     0.5516      0.8376     0.8499     54.4904
12300     0.5427      0.8291     0.8499     54.4575
12400     0.5827      0.8249     0.8499     54.3397
12500     0.5136      0.8439     0.8499     53.9271
12600     0.5739      0.8291     0.8499     54.5499
12700     0.5501      0.8312     0.8509     53.8352
12800     0.5789      0.8376     0.8509     53.8960
12900     0.4383      0.8608     0.8509     53.3672
13000     0.5092      0.8460     0.8509     53.7621
13100     0.4645      0.8629     0.8509     54.2074
13200     0.5226      0.8418     0.8509     54.3879
13300     0.5136      0.8523     0.8509     54.1074
13400     0.6121      0.8122     0.8509     54.0936
13500     0.5835      0.8291     0.8509     53.4744
13600     0.4700      0.8819     0.8509     54.8861
13700     0.5245      0.8291     0.8509     53.7590
13800     0.5570      0.8376     0.8509     54.1042
13900     0.5328      0.8333     0.8509     54.1362
14000     0.5139      0.8608     0.8514     53.9912
14100     0.5631      0.8291     0.8514     53.8537
14200     0.4785      0.8608     0.8514     54.3437
14300     0.4974      0.8523     0.8514     54.1391
14400     0.5296      0.8439     0.8514     54.2089
14500     0.5400      0.8418     0.8514     53.8056
14600     0.6063      0.8122     0.8514     54.3698
14700     0.5107      0.8460     0.8514     54.0941
14800     0.5066      0.8460     0.8514     54.0566
14900     0.5726      0.8354     0.8514     53.6484
15000     0.4955      0.8481     0.8544     54.6335
15100     0.5792      0.8186     0.8544     53.9098
15200     0.5001      0.8460     0.8544     54.8060
15300     0.5021      0.8502     0.8544     54.2614
15400     0.5773      0.8249     0.8544     54.0207
15500     0.4867      0.8523     0.8544     54.4327
15600     0.5289      0.8376     0.8544     54.8372
15700     0.6030      0.8249     0.8544     53.5816
15800     0.5388      0.8460     0.8544     54.3486
15900     0.5408      0.8418     0.8544     53.6999
16000     0.5240      0.8460     0.8544     54.2585
16100     0.6011      0.8270     0.8544     53.7463
16200     0.5612      0.8376     0.8544     54.2249
16300     0.4908      0.8523     0.8544     54.8047
16400     0.5330      0.8650     0.8544     54.5353
16500     0.4284      0.8692     0.8544     54.4969
16600     0.4729      0.8523     0.8544     54.6483
16700     0.4202      0.8797     0.8544     54.4433
16800     0.5226      0.8312     0.8544     54.8644
16900     0.5223      0.8629     0.8544     53.6141
17000     0.5272      0.8376     0.8544     54.1412
17100     0.5907      0.8354     0.8544     54.8534
17200     0.5456      0.8312     0.8544     53.9990
17300     0.4965      0.8629     0.8544     53.9665
17400     0.5780      0.8207     0.8544     55.0114
17500     0.4900      0.8629     0.8544     54.2636
17600     0.5316      0.8439     0.8544     54.7586
17700     0.5436      0.8354     0.8544     53.6782
17800     0.5193      0.8481     0.8544     53.8159
17900     0.5388      0.8397     0.8544     54.9015
18000     0.5536      0.8397     0.8544     54.2659
18100     0.5550      0.8122     0.8544     54.4954
18200     0.5942      0.8101     0.8544     55.3822
18300     0.5141      0.8460     0.8544     54.6305
18400     0.4932      0.8376     0.8544     54.5217
18500     0.5830      0.8122     0.8544     54.5254
18600     0.5916      0.8249     0.8544     54.0521
18700     0.5796      0.8291     0.8544     54.8566
18800     0.4505      0.8586     0.8544     54.0883
18900     0.5663      0.8312     0.8544     53.9220
19000     0.5495      0.8397     0.8544     54.6411
19100     0.5552      0.8376     0.8544     53.8832
19200     0.5309      0.8397     0.8544     54.7773
19300     0.4955      0.8333     0.8544     53.7353
19400     0.5820      0.8312     0.8544     53.8238
19500     0.5297      0.8481     0.8544     54.4951
19600     0.5547      0.8397     0.8544     54.4055
19700     0.5392      0.8312     0.8544     54.6453
19800     0.5140      0.8354     0.8544     54.7556
19900     0.4892      0.8523     0.8544     53.9928
20000     0.4807      0.8481     0.8552     54.5264
20100     0.5546      0.8460     0.8552     53.9450
20199     0.4912      0.8523     0.8552     54.0306
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     0.4210      0.8586     0.8527     8.8488
00100     0.4154      0.8776     0.8578     54.1449
00200     0.4572      0.8608     0.8578     54.8408
00300     0.4420      0.8713     0.8597     54.2695
00400     0.4543      0.8692     0.8597     53.4299
00500     0.4887      0.8502     0.8600     53.9993
00600     0.4750      0.8586     0.8600     54.5358
00700     0.4071      0.8713     0.8603     55.1045
00800     0.4134      0.8882     0.8603     53.6567
00900     0.4782      0.8502     0.8608     54.2952
01000     0.4814      0.8376     0.8608     54.1886
01100     0.4621      0.8544     0.8608     54.6469
01200     0.5032      0.8439     0.8608     54.3194
01300     0.5222      0.8460     0.8608     53.8661
01400     0.4721      0.8755     0.8608     54.5594
01500     0.4136      0.8840     0.8608     54.8791
01600     0.4648      0.8755     0.8608     53.7788
01700     0.4539      0.8565     0.8608     54.1023
01800     0.4213      0.8776     0.8608     53.7197
01900     0.4008      0.8882     0.8608     54.5165
02000     0.4166      0.8776     0.8608     53.9095
02100     0.4618      0.8608     0.8608     54.3048
02200     0.4271      0.8861     0.8608     54.3050
02300     0.4748      0.8608     0.8608     54.0912
02400     0.4067      0.8713     0.8608     55.1465
02500     0.4247      0.8692     0.8608     54.0462
02600     0.4281      0.8776     0.8608     53.9560
02700     0.4025      0.8861     0.8608     53.9929
02800     0.4683      0.8671     0.8608     53.6055
02900     0.3921      0.8840     0.8608     53.6935
03000     0.4497      0.8565     0.8608     54.5653
03100     0.4793      0.8608     0.8608     54.0414
03200     0.4202      0.8734     0.8608     53.6222
03300     0.3946      0.8840     0.8608     53.7914
03400     0.4646      0.8523     0.8608     53.7294
03500     0.4615      0.8671     0.8608     54.2595
03600     0.4297      0.8819     0.8608     54.2535
03700     0.4658      0.8565     0.8608     54.3105
03800     0.4319      0.8650     0.8608     54.5851
03900     0.5284      0.8228     0.8608     54.1602
04000     0.4382      0.8713     0.8608     54.0282
04100     0.4219      0.8903     0.8608     54.7986
04200     0.4851      0.8565     0.8608     54.0718
04300     0.5058      0.8333     0.8608     54.6533
04400     0.4529      0.8671     0.8608     54.3019
04500     0.4516      0.8840     0.8608     53.9432
04600     0.4171      0.8861     0.8608     54.0369
04700     0.4627      0.8608     0.8608     53.6954
04800     0.4068      0.8755     0.8608     54.5962
04900     0.4190      0.8629     0.8608     54.3703
05000     0.5194      0.8418     0.8608     54.1140
05100     0.4568      0.8608     0.8608     54.6920
05200     0.3677      0.8924     0.8608     54.3308
05300     0.4720      0.8523     0.8608     53.9149
05400     0.4770      0.8586     0.8608     55.3049
05500     0.4714      0.8692     0.8608     54.5062
05600     0.4297      0.8629     0.8608     54.0444
05700     0.3994      0.8692     0.8608     54.4282
05800     0.4372      0.8713     0.8608     54.0490
05900     0.4735      0.8502     0.8608     54.8982
06000     0.5050      0.8354     0.8608     54.7908
06100     0.4124      0.8755     0.8608     54.4828
06200     0.3995      0.8819     0.8608     55.2347
06300     0.3858      0.8861     0.8608     54.5968
06400     0.4530      0.8713     0.8608     54.3327
06500     0.4279      0.8755     0.8608     54.3549
06600     0.4932      0.8502     0.8608     54.7099
06700     0.4373      0.8840     0.8608     54.6533
06800     0.4402      0.8692     0.8608     53.8933
06900     0.4836      0.8418     0.8608     53.9345
07000     0.4107      0.8819     0.8616     55.0680
07100     0.4602      0.8671     0.8616     53.9138
07200     0.4437      0.8629     0.8616     53.6928
07300     0.4293      0.8692     0.8616     54.2770
07400     0.3904      0.8776     0.8616     54.3623
07500     0.5606      0.8270     0.8616     55.0516
07600     0.4834      0.8650     0.8616     53.9640
07700     0.4447      0.8692     0.8616     54.0554
07800     0.4044      0.9008     0.8616     54.1230
07900     0.4178      0.8861     0.8616     53.1739
08000     0.4643      0.8608     0.8616     53.9812
08100     0.3696      0.9093     0.8616     54.3627
08200     0.4275      0.8692     0.8616     54.9453
08300     0.4867      0.8586     0.8616     53.6881
08400     0.4306      0.8755     0.8616     53.2160
08500     0.4443      0.8734     0.8616     53.2068
08600     0.5509      0.8333     0.8616     53.6685
08700     0.4569      0.8565     0.8616     53.4555
08800     0.4870      0.8502     0.8616     53.0567
08900     0.4813      0.8502     0.8616     53.8105
09000     0.4398      0.8692     0.8616     53.3701
09100     0.5024      0.8291     0.8616     53.9684
09200     0.4612      0.8586     0.8616     53.1510
09300     0.3993      0.8903     0.8616     53.0906
09400     0.4216      0.8819     0.8616     53.4223
09500     0.4348      0.8734     0.8616     53.1446
09600     0.3930      0.9051     0.8616     53.2431
09700     0.4383      0.8755     0.8616     53.5104
09800     0.4885      0.8565     0.8616     53.1709
09900     0.3556      0.8966     0.8616     53.6271
Start testing:
Test Accuracy: 0.8411
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=10, quant_actNM=10, quant_inp=10, quant_w=10, random_seed=8627169, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
2f235c39-e3ae-4d55-8a5e-23400020a4c4
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.9664      0.0844     0.0749     10.5296
00100     2.0681      0.3481     0.3757     75.5489
00200     1.6639      0.4789     0.5077     72.5451
00300     1.3963      0.5654     0.5813     73.2835
00400     1.2405      0.5844     0.6276     73.7240
00500     1.1226      0.6814     0.6613     73.4190
00600     1.0511      0.6857     0.6893     73.8144
00700     1.0062      0.6983     0.7124     73.0822
00800     0.9593      0.7068     0.7153     73.8784
00900     0.9148      0.7194     0.7409     72.4092
01000     0.8568      0.7363     0.7478     75.2995
01100     0.8425      0.7384     0.7544     74.4176
01200     0.8743      0.7321     0.7551     75.6827
01300     0.9024      0.7257     0.7647     72.8041
01400     0.7722      0.7595     0.7697     73.7271
01500     0.7738      0.7553     0.7697     74.9215
01600     0.7541      0.7511     0.7794     73.6747
01700     0.7832      0.7405     0.7817     73.9917
01800     0.7482      0.7679     0.7817     72.7988
01900     0.7030      0.7954     0.7817     73.8531
02000     0.7096      0.7954     0.7834     73.4320
02100     0.7661      0.7764     0.7959     74.1227
02200     0.7354      0.7722     0.7959     73.2059
02300     0.6600      0.8017     0.7996     74.0524
02400     0.7106      0.7827     0.7996     74.0748
02500     0.7365      0.7827     0.7997     73.1150
02600     0.7032      0.7911     0.8075     73.4265
02700     0.6544      0.7890     0.8075     73.9900
02800     0.7114      0.7743     0.8075     73.2409
02900     0.6708      0.7996     0.8079     72.9334
03000     0.6423      0.7975     0.8079     73.5451
03100     0.7403      0.7785     0.8079     74.2735
03200     0.6585      0.8059     0.8079     74.4660
03300     0.6794      0.7869     0.8129     73.4856
03400     0.7267      0.7890     0.8129     73.1417
03500     0.7802      0.7785     0.8129     73.1362
03600     0.6163      0.8207     0.8129     73.8109
03700     0.6469      0.8143     0.8129     73.1798
03800     0.5906      0.8080     0.8129     72.0768
03900     0.6807      0.8165     0.8157     71.9760
04000     0.6856      0.7975     0.8157     72.4363
04100     0.6737      0.7932     0.8213     72.2677
04200     0.6431      0.7890     0.8213     75.8865
04300     0.6126      0.8143     0.8213     72.7343
04400     0.6493      0.8080     0.8213     72.0679
04500     0.6501      0.8186     0.8213     73.2330
04600     0.6909      0.7890     0.8213     74.1953
04700     0.6735      0.7806     0.8213     74.2376
04800     0.7190      0.7890     0.8213     73.9490
04900     0.6499      0.7996     0.8213     73.5390
05000     0.6759      0.8186     0.8213     74.0582
05100     0.5802      0.8376     0.8213     73.3406
05200     0.6988      0.7890     0.8213     73.8517
05300     0.5997      0.8270     0.8253     73.0121
05400     0.6111      0.8122     0.8253     73.3676
05500     0.5885      0.8312     0.8257     73.2798
05600     0.6429      0.8143     0.8257     73.0986
05700     0.6593      0.8080     0.8257     73.7236
05800     0.6474      0.7806     0.8257     73.3086
05900     0.5267      0.8249     0.8257     73.1226
06000     0.6308      0.8059     0.8286     72.0767
06100     0.5682      0.8312     0.8286     72.8756
06200     0.6055      0.8207     0.8329     76.3028
06300     0.6218      0.8122     0.8329     73.5682
06400     0.7229      0.7827     0.8329     73.3346
06500     0.6281      0.8017     0.8329     75.0514
06600     0.7028      0.7911     0.8329     73.3613
06700     0.6830      0.7954     0.8329     74.6647
06800     0.6405      0.8059     0.8329     72.7662
06900     0.5852      0.8333     0.8329     72.3137
07000     0.5423      0.8460     0.8329     74.0307
07100     0.6067      0.8207     0.8339     72.1645
07200     0.6270      0.8122     0.8363     72.5649
07300     0.5549      0.8312     0.8363     73.2589
07400     0.5965      0.8101     0.8363     74.0917
07500     0.5983      0.8059     0.8363     75.7123
07600     0.6144      0.7996     0.8363     72.8730
07700     0.5815      0.8249     0.8366     72.9654
07800     0.5422      0.8544     0.8366     72.1667
07900     0.5493      0.8333     0.8366     72.0677
08000     0.5426      0.8439     0.8366     71.7519
08100     0.6335      0.7975     0.8366     74.5554
08200     0.5743      0.7932     0.8366     74.9139
08300     0.6069      0.8228     0.8406     73.3217
08400     0.5791      0.8249     0.8406     74.8194
08500     0.5873      0.8333     0.8406     73.0106
08600     0.6237      0.8249     0.8406     74.2469
08700     0.6163      0.8143     0.8406     73.9396
08800     0.5636      0.8207     0.8406     74.4168
08900     0.5397      0.8418     0.8406     73.0291
09000     0.5697      0.8270     0.8406     73.8511
09100     0.5613      0.8354     0.8406     72.9949
09200     0.5967      0.8017     0.8406     76.1110
