Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=512, canonical_testing=False, cs=0.5, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, hidden=111, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=0.1, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=6, quant_actNM=8, quant_inp=4, quant_w=None, random_seed=8627169, rows_bias=3, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=640, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
a11a6a59-4f00-4fb4-9774-c031b9acf25a
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 161, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 228, in forward
    activated_input = quant_pass(pact_a_bmm(input_gate_out * activation_out, self.a8), self.abNM, self.a8)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 89, in forward
    x01q =  torch.round(x01 * step_d ) / step_d
RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 10.76 GiB total capacity; 8.54 GiB already allocated; 3.12 MiB free; 9.80 GiB reserved in total by PyTorch)
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=512, canonical_testing=False, cs=0.5, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, hidden=111, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=0.1, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=6, quant_actNM=8, quant_inp=4, quant_w=None, random_seed=8627169, rows_bias=3, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=640, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
b4791f30-be03-4f8f-8302-0ff4090ca200
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 161, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 228, in forward
    activated_input = quant_pass(pact_a_bmm(input_gate_out * activation_out, self.a8), self.abNM, self.a8)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 89, in forward
    x01q =  torch.round(x01 * step_d ) / step_d
RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 10.76 GiB total capacity; 8.54 GiB already allocated; 3.12 MiB free; 9.80 GiB reserved in total by PyTorch)
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=512, canonical_testing=False, cs=0.5, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, hidden=111, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=0.1, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=6, quant_actNM=8, quant_inp=4, quant_w=None, random_seed=8627169, rows_bias=3, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=640, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
26590f71-6db7-4862-aa4f-3347be6a1ed5
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.6103      0.0898     0.0710     11.0229
00100     1.5336      0.5039     0.5303     54.4239
00200     1.1344      0.6523     0.6868     53.9899
00300     1.0373      0.6797     0.7163     54.9358
00400     0.9600      0.7168     0.7350     54.3837
00500     0.8218      0.7480     0.7675     55.2858
00600     0.8510      0.7168     0.7948     54.8219
00700     0.7852      0.7539     0.7948     54.3860
00800     0.8134      0.7383     0.7948     54.8403
00900     0.8245      0.7246     0.7948     55.0654
01000     0.8206      0.7324     0.7948     55.5089
01100     0.7067      0.7773     0.7948     54.2794
01200     0.7549      0.7637     0.8263     53.2628
01300     0.6816      0.8027     0.8263     53.5964
01400     0.6266      0.8047     0.8263     54.8759
01500     0.7111      0.7773     0.8263     54.5962
01600     0.6090      0.8203     0.8263     57.3846
01700     0.7784      0.7422     0.8263     56.0604
01800     0.7465      0.7422     0.8283     55.8161
01900     0.6788      0.7949     0.8283     55.3474
02000     0.7635      0.7539     0.8283     54.2408
02100     0.6876      0.7852     0.8283     55.0870
02200     0.6745      0.8164     0.8466     54.5679
02300     0.6710      0.7969     0.8466     54.7423
02400     0.7086      0.7754     0.8466     55.4720
02500     0.6780      0.7891     0.8466     55.2566
02600     0.6661      0.8047     0.8466     55.9007
02700     0.5961      0.8262     0.8466     54.4765
02800     0.7492      0.7793     0.8466     54.4681
02900     0.6508      0.7969     0.8466     55.7657
03000     0.6458      0.7832     0.8466     54.6534
03100     0.5751      0.8184     0.8466     54.2744
03200     0.6516      0.7891     0.8466     55.1005
03300     0.6629      0.7832     0.8466     54.3829
03400     0.6096      0.8125     0.8466     54.9047
03500     0.5503      0.8281     0.8466     54.7257
03600     0.6356      0.8066     0.8466     54.9507
03700     0.6705      0.7773     0.8466     56.5719
03800     0.5876      0.8086     0.8466     55.8594
03900     0.6172      0.8262     0.8466     57.2170
04000     0.5637      0.8281     0.8466     55.7402
04100     0.6180      0.8008     0.8466     54.7207
04200     0.5583      0.8438     0.8466     56.0003
04300     0.6182      0.8242     0.8466     54.8186
04400     0.6268      0.8066     0.8466     54.5456
04500     0.6544      0.7891     0.8466     54.8274
04600     0.5640      0.8359     0.8466     53.3565
04700     0.5750      0.8281     0.8466     56.4897
04800     0.5474      0.8438     0.8466     54.9549
04900     0.5187      0.8574     0.8466     54.7721
05000     0.5563      0.8223     0.8466     55.3658
05100     0.6090      0.8242     0.8466     53.8296
05200     0.5449      0.8438     0.8586     54.4250
05300     0.5844      0.8379     0.8674     56.1182
05400     0.6650      0.7871     0.8674     54.1047
05500     0.6820      0.7969     0.8674     55.2240
05600     0.5031      0.8418     0.8674     54.5555
05700     0.5524      0.8379     0.8674     54.3332
05800     0.6430      0.7734     0.8674     54.9665
05900     0.5474      0.8438     0.8674     54.3648
06000     0.5273      0.8496     0.8674     54.4085
06100     0.4880      0.8555     0.8674     54.3027
06200     0.5055      0.8496     0.8709     54.8090
06300     0.5743      0.8164     0.8709     54.7503
06400     0.5525      0.8359     0.8709     54.5897
06500     0.5943      0.8242     0.8709     54.8962
06600     0.5456      0.8301     0.8709     55.2636
06700     0.5381      0.8438     0.8709     54.7924
06800     0.6019      0.8105     0.8709     54.3477
06900     0.5363      0.8320     0.8709     55.2125
07000     0.4841      0.8477     0.8709     54.2526
07100     0.5429      0.8359     0.8709     55.4478
07200     0.4990      0.8555     0.8709     54.5446
07300     0.5289      0.8340     0.8709     54.7626
07400     0.5644      0.8281     0.8709     54.8290
07500     0.6239      0.7988     0.8709     54.6498
07600     0.5775      0.8301     0.8709     55.1974
07700     0.5591      0.8438     0.8709     55.0599
07800     0.5704      0.8281     0.8709     54.9269
07900     0.5268      0.8398     0.8709     56.0470
08000     0.5734      0.8203     0.8709     54.8741
08100     0.4519      0.8652     0.8709     55.0172
08200     0.6058      0.8242     0.8709     55.7471
08300     0.4995      0.8262     0.8709     55.0471
08400     0.5115      0.8535     0.8709     54.9051
08500     0.5257      0.8281     0.8709     56.1028
08600     0.6085      0.8105     0.8709     54.4652
08700     0.4880      0.8438     0.8709     55.2069
08800     0.4764      0.8672     0.8709     54.7697
08900     0.4868      0.8613     0.8709     54.6416
09000     0.5068      0.8496     0.8709     55.8886
09100     0.6289      0.8164     0.8709     55.4963
09200     0.5125      0.8359     0.8709     54.4290
09300     0.4985      0.8438     0.8709     55.0524
09400     0.4886      0.8398     0.8709     54.7235
09500     0.4945      0.8633     0.8709     55.2126
09600     0.5058      0.8555     0.8709     55.3986
09700     0.5134      0.8379     0.8709     54.4304
09800     0.5065      0.8477     0.8709     54.9842
09900     0.5315      0.8398     0.8709     54.3695
10000     0.5045      0.8398     0.8709     54.4403
10100     0.4705      0.8555     0.8709     54.4298
10200     0.5132      0.8340     0.8709     54.6692
10300     0.5172      0.8340     0.8709     55.4053
10400     0.4588      0.8477     0.8709     54.7488
10500     0.4854      0.8535     0.8709     54.3476
10600     0.4504      0.8789     0.8709     54.9353
10700     0.3987      0.8867     0.8709     54.8722
10800     0.5103      0.8594     0.8709     54.2524
10900     0.4365      0.8750     0.8709     56.0081
11000     0.5093      0.8281     0.8709     54.1341
11100     0.4368      0.8730     0.8709     55.0228
11200     0.4216      0.8770     0.8709     54.1832
11300     0.4986      0.8477     0.8709     54.3828
11400     0.4771      0.8555     0.8709     56.1437
11500     0.5051      0.8496     0.8709     56.8889
11600     0.4778      0.8711     0.8709     57.6515
11700     0.4304      0.8770     0.8709     57.8638
11800     0.5158      0.8496     0.8709     56.5866
11900     0.5115      0.8438     0.8709     58.0307
12000     0.4501      0.8848     0.8709     57.1882
12100     0.4066      0.8750     0.8709     57.2169
12200     0.4635      0.8691     0.8709     57.7603
12300     0.4220      0.8926     0.8709     57.2234
12400     0.3686      0.9043     0.8709     57.3043
12500     0.4504      0.8750     0.8709     57.9781
12600     0.3970      0.8906     0.8709     57.8541
12700     0.5146      0.8555     0.8709     60.4431
12800     0.4380      0.8691     0.8709     58.7866
12900     0.4392      0.8789     0.8709     58.0545
13000     0.3929      0.8828     0.8709     60.9156
13100     0.5087      0.8418     0.8709     60.2626
13200     0.4575      0.8770     0.8709     58.9072
13300     0.4190      0.8828     0.8709     58.6973
13400     0.4833      0.8379     0.8709     55.3290
13500     0.4514      0.8770     0.8709     55.0993
13600     0.4397      0.8711     0.8709     53.9271
13700     0.4621      0.8730     0.8709     54.3679
13800     0.4962      0.8535     0.8709     56.0277
13900     0.4641      0.8711     0.8709     54.6893
14000     0.4215      0.8789     0.8709     55.1905
14100     0.4617      0.8633     0.8709     55.2846
14200     0.4951      0.8516     0.8709     55.3411
14300     0.4689      0.8418     0.8709     54.1174
14400     0.4412      0.8828     0.8709     53.7634
14500     0.4166      0.8691     0.8709     55.7936
14600     0.4537      0.8691     0.8709     54.9849
14700     0.4564      0.8496     0.8709     54.1594
14800     0.4141      0.8789     0.8709     54.5902
14900     0.4559      0.8516     0.8709     54.5497
15000     0.4990      0.8496     0.8709     54.2823
15100     0.4615      0.8672     0.8709     55.4320
15200     0.5175      0.8438     0.8709     54.1023
15300     0.4363      0.8848     0.8709     55.4453
15400     0.4193      0.8848     0.8709     54.1473
15500     0.4503      0.8594     0.8709     54.1409
15600     0.5073      0.8477     0.8709     54.5282
15700     0.4601      0.8555     0.8709     54.8696
15800     0.4483      0.8594     0.8709     54.5013
15900     0.4582      0.8750     0.8709     55.6912
16000     0.4406      0.8672     0.8709     53.9530
16100     0.4757      0.8457     0.8709     54.3128
16200     0.3974      0.8926     0.8709     55.7764
16300     0.4513      0.8652     0.8709     53.2851
16400     0.4468      0.8711     0.8709     54.1293
16500     0.3924      0.8770     0.8709     54.8701
16600     0.4087      0.8965     0.8709     54.3298
16700     0.4006      0.8984     0.8709     55.5347
16800     0.4585      0.8711     0.8709     54.0956
16900     0.3969      0.8789     0.8709     54.5840
17000     0.4783      0.8477     0.8709     54.0710
17100     0.4093      0.8867     0.8709     53.4387
17200     0.3642      0.8945     0.8709     53.6236
17300     0.5056      0.8574     0.8709     54.7665
17400     0.4916      0.8535     0.8709     54.0017
17500     0.3878      0.8809     0.8709     55.0059
17600     0.4264      0.8750     0.8709     53.8495
17700     0.4097      0.8828     0.8709     54.3235
17800     0.4445      0.8672     0.8709     54.0050
17900     0.4146      0.8789     0.8709     56.1950
18000     0.3937      0.8867     0.8709     55.3530
18100     0.3773      0.8945     0.8709     55.1033
18200     0.5140      0.8398     0.8709     56.5662
18300     0.4495      0.8711     0.8709     55.2464
18400     0.4325      0.8613     0.8709     54.9905
18500     0.4771      0.8594     0.8709     54.4767
18600     0.4410      0.8711     0.8709     54.3599
18700     0.4673      0.8652     0.8709     55.8517
18800     0.4066      0.8809     0.8709     54.7111
18900     0.3836      0.8906     0.8709     54.8417
19000     0.4616      0.8594     0.8709     54.4219
19100     0.4314      0.8691     0.8709     56.0478
19200     0.3773      0.8887     0.8709     53.6721
19300     0.4427      0.8711     0.8709     54.4625
19400     0.4640      0.8594     0.8709     55.8609
19500     0.4612      0.8652     0.8709     56.1804
19600     0.4471      0.8711     0.8709     55.3188
19700     0.3842      0.8789     0.8709     54.0418
19800     0.3902      0.8867     0.8709     54.5183
19900     0.4289      0.8809     0.8709     54.6886
20000     0.4016      0.8887     0.8709     54.4764
20100     0.4194      0.8789     0.8709     54.4517
20199     0.3999      0.8730     0.8709     53.6534
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     0.7789      0.7754     0.7937     8.7177
00100     0.4860      0.8516     0.8800     53.6266
00200     0.4536      0.8613     0.8800     55.5220
00300     0.4060      0.8691     0.8914     54.6518
00400     0.4626      0.8613     0.8914     53.2771
00500     0.5545      0.8164     0.8914     55.1466
00600     0.4284      0.8887     0.8914     53.7420
00700     0.4698      0.8711     0.8914     54.3684
00800     0.4742      0.8672     0.8914     54.0051
00900     0.4243      0.8750     0.8965     55.6976
01000     0.5387      0.8398     0.8965     55.7702
01100     0.4597      0.8672     0.8965     54.4682
01200     0.4505      0.8711     0.8965     54.9356
01300     0.3904      0.8965     0.8965     53.7698
01400     0.4537      0.8809     0.8965     54.7942
01500     0.4874      0.8398     0.8965     54.6381
01600     0.4178      0.8789     0.8965     53.8840
01700     0.4146      0.8633     0.8965     54.3971
01800     0.4515      0.8691     0.8965     54.6825
01900     0.4626      0.8594     0.8965     53.7264
02000     0.4430      0.8711     0.8965     55.0388
02100     0.4337      0.8770     0.8965     53.5990
02200     0.4179      0.8809     0.8965     53.9775
02300     0.3955      0.8965     0.8965     56.2674
02400     0.4864      0.8574     0.8965     53.5166
02500     0.4365      0.8730     0.8965     53.3880
02600     0.3802      0.8887     0.8965     54.2310
02700     0.4359      0.8730     0.8965     54.7060
02800     0.4894      0.8555     0.8965     54.7195
02900     0.4144      0.8711     0.8965     53.9681
03000     0.4317      0.8672     0.8965     54.4264
03100     0.4807      0.8516     0.8965     54.3508
03200     0.3926      0.8984     0.8965     54.0420
03300     0.4218      0.8809     0.8965     54.0406
03400     0.4851      0.8477     0.8965     54.3262
03500     0.4461      0.8477     0.8965     54.1901
03600     0.4277      0.8770     0.8965     54.8385
03700     0.4283      0.8770     0.8965     54.7815
03800     0.4545      0.8672     0.8965     53.7182
03900     0.4641      0.8652     0.8965     54.4714
04000     0.4703      0.8652     0.8965     53.8334
04100     0.4133      0.8730     0.8965     55.6070
04200     0.4281      0.8809     0.8965     53.5167
04300     0.4690      0.8574     0.8965     53.8164
04400     0.4028      0.8828     0.8965     55.2072
04500     0.4634      0.8516     0.8965     53.5991
04600     0.3848      0.8945     0.8965     54.4580
04700     0.4325      0.8809     0.8965     54.4337
04800     0.4292      0.8672     0.8965     58.5504
04900     0.4021      0.8809     0.8965     55.9042
05000     0.4616      0.8555     0.9018     55.2363
05100     0.4555      0.8633     0.9018     55.7731
05200     0.4967      0.8438     0.9018     55.9112
05300     0.3769      0.8926     0.9018     55.4323
05400     0.4065      0.8926     0.9018     55.1153
05500     0.4786      0.8730     0.9018     53.7802
05600     0.3787      0.9102     0.9018     54.5105
05700     0.5167      0.8320     0.9018     54.4667
05800     0.3522      0.9043     0.9018     54.4391
05900     0.4957      0.8477     0.9018     53.9724
06000     0.5005      0.8477     0.9018     53.5349
06100     0.4207      0.8789     0.9018     55.4830
06200     0.4017      0.8848     0.9018     54.4746
06300     0.3916      0.8906     0.9018     54.3778
06400     0.3906      0.8867     0.9018     54.4299
06500     0.4124      0.8828     0.9018     55.1722
06600     0.5064      0.8359     0.9018     54.6025
06700     0.3721      0.8984     0.9018     54.9787
06800     0.4109      0.8730     0.9018     54.1477
06900     0.4388      0.8828     0.9018     53.5745
07000     0.3928      0.8867     0.9018     54.4153
07100     0.4225      0.8711     0.9018     55.6107
07200     0.4304      0.8711     0.9018     54.2546
07300     0.4027      0.8789     0.9018     54.6678
07400     0.4472      0.8711     0.9018     54.6695
07500     0.3618      0.9043     0.9018     54.8124
07600     0.4860      0.8535     0.9018     54.1324
07700     0.4158      0.8848     0.9018     55.9532
07800     0.3951      0.8906     0.9018     54.8962
07900     0.5076      0.8340     0.9018     54.6775
08000     0.4117      0.8809     0.9018     54.7615
08100     0.3868      0.8906     0.9018     55.4757
08200     0.4546      0.8633     0.9018     55.4231
08300     0.4474      0.8613     0.9018     54.6966
08400     0.5123      0.8516     0.9018     54.5900
08500     0.4404      0.8770     0.9018     55.0742
08600     0.3903      0.8867     0.9018     54.4508
08700     0.4166      0.8809     0.9018     53.8435
08800     0.4508      0.8730     0.9018     55.7327
08900     0.4145      0.8848     0.9018     55.6967
09000     0.3974      0.8867     0.9018     55.9173
09100     0.3463      0.9004     0.9018     56.6716
09200     0.4069      0.8809     0.9018     56.0046
09300     0.4151      0.8828     0.9018     54.8454
09400     0.4017      0.8887     0.9018     54.4131
09500     0.3946      0.8770     0.9018     54.2439
09600     0.4356      0.8672     0.9018     56.2987
09700     0.4669      0.8535     0.9018     54.3349
09800     0.4126      0.8809     0.9018     53.8084
09900     0.4077      0.8887     0.9018     55.2947
Start testing:
Test Accuracy: 0.8812
