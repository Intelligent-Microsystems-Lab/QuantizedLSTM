Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=235899598, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=235899598, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 362, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.0, finetuning_epochs=0, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.0005,0.0001,0.00002', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=235899598, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,10000', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
97278542-0a97-4eb8-9458-1c54125aae38
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.5282      0.0633     0.0807     11.7209
00100     2.5208      0.0612     0.0811     58.1870
00200     2.5150      0.0696     0.0811     57.1250
00300     2.5107      0.0759     0.0811     57.7495
00400     2.5076      0.0886     0.0811     58.1238
00500     2.5056      0.0970     0.0811     58.8290
00600     2.5042      0.1076     0.0811     57.9977
00700     2.5032      0.0781     0.0811     57.3184
00800     2.5026      0.0844     0.0811     59.4794
00900     2.5021      0.0802     0.0811     58.7186
01000     2.5017      0.0907     0.0811     59.0397
01100     2.5014      0.0992     0.0811     58.8315
01200     2.5012      0.0654     0.0811     57.9809
01300     2.5010      0.1013     0.0811     58.8159
01400     2.5009      0.1013     0.0812     58.6335
01500     2.5008      0.1076     0.0812     58.1596
01600     2.5007      0.0506     0.0812     59.4142
01700     2.5006      0.0464     0.0812     60.0506
01800     2.5005      0.1055     0.0812     57.9926
01900     2.5005      0.0823     0.0812     58.5656
02000     2.5004      0.0823     0.0812     58.0510
02100     2.5003      0.0844     0.0812     58.5159
02200     2.5003      0.0907     0.0812     59.2639
02300     2.5003      0.0717     0.0812     59.0862
02400     2.5002      0.0717     0.0814     58.4783
02500     2.5002      0.0802     0.0814     58.8669
02600     2.5001      0.0633     0.0814     57.1821
02700     2.5001      0.0738     0.0814     59.0724
02800     2.5001      0.0970     0.0814     58.4886
02900     2.5001      0.0844     0.0814     58.3343
03000     2.5000      0.0717     0.0814     59.2884
03100     2.5000      0.0865     0.0814     59.2290
03200     2.5000      0.0823     0.0814     59.0030
03300     2.5000      0.0823     0.0814     58.1834
03400     2.4999      0.0759     0.0814     57.6000
03500     2.4999      0.0907     0.0814     58.7682
03600     2.4999      0.0865     0.0814     57.7158
03700     2.4999      0.0717     0.0814     58.6681
03800     2.4998      0.0970     0.0814     58.4936
03900     2.4998      0.0506     0.0814     58.2377
04000     2.4998      0.0802     0.0814     58.7341
04100     2.4998      0.0738     0.0814     58.0735
04200     2.4998      0.0992     0.0814     59.1041
04300     2.4997      0.0823     0.0814     58.4014
04400     2.4997      0.0823     0.0814     58.7745
04500     2.4997      0.0464     0.0814     58.8264
04600     2.4997      0.0781     0.0814     59.2410
04700     2.4997      0.0928     0.0814     58.5699
04800     2.4996      0.0886     0.0814     58.5246
04900     2.4996      0.0612     0.0814     57.1929
05000     2.4996      0.0844     0.0814     58.0319
05100     2.4996      0.0823     0.0814     58.7220
05200     2.4996      0.1055     0.0814     58.5138
05300     2.4995      0.0717     0.0814     58.1903
05400     2.4995      0.0717     0.0814     58.7835
05500     2.4995      0.0738     0.0814     59.7879
05600     2.4995      0.0738     0.0814     59.0883
05700     2.4995      0.0654     0.0814     57.4175
05800     2.4994      0.0759     0.0814     58.3553
05900     2.4994      0.0759     0.0814     58.8384
06000     2.4994      0.0633     0.0814     58.2458
06100     2.4994      0.0865     0.0814     58.5549
06200     2.4994      0.0717     0.0814     57.6897
06300     2.4993      0.0928     0.0814     58.1866
06400     2.4993      0.0738     0.0814     59.0922
06500     2.4993      0.0675     0.0814     58.0290
06600     2.4993      0.0675     0.0814     58.3075
06700     2.4993      0.0907     0.0814     58.1468
06800     2.4992      0.0759     0.0814     57.5778
06900     2.4992      0.0844     0.0814     57.9012
07000     2.4992      0.0886     0.0814     58.0399
07100     2.4992      0.0781     0.0814     58.1331
07200     2.4992      0.0886     0.0814     60.3247
07300     2.4992      0.0781     0.0814     59.3657
07400     2.4991      0.0781     0.0814     58.4754
07500     2.4991      0.0422     0.0814     58.8537
07600     2.4991      0.0865     0.0814     58.4102
07700     2.4991      0.0949     0.0814     59.9145
07800     2.4991      0.0949     0.0814     59.6168
07900     2.4990      0.0675     0.0814     58.7451
08000     2.4990      0.0781     0.0814     58.7862
08100     2.4990      0.0675     0.0814     59.2471
08200     2.4990      0.0654     0.0814     58.6629
08300     2.4990      0.0949     0.0814     59.0862
08400     2.4989      0.0633     0.0814     58.8630
08500     2.4989      0.0886     0.0814     60.3489
08600     2.4989      0.0738     0.0814     59.8327
08700     2.4989      0.0696     0.0814     59.0447
08800     2.4989      0.0844     0.0814     57.7194
08900     2.4989      0.0865     0.0814     57.6949
09000     2.4988      0.0738     0.0814     58.7753
09100     2.4988      0.0823     0.0814     58.8017
09200     2.4988      0.0738     0.0814     57.8160
09300     2.4988      0.0781     0.0814     58.3093
09400     2.4988      0.0802     0.0814     58.8297
09500     2.4987      0.0654     0.0814     57.5752
09600     2.4987      0.0802     0.0814     58.4752
09700     2.4987      0.0823     0.0814     57.5473
09800     2.4987      0.0865     0.0814     57.8073
09900     2.4987      0.0844     0.0814     59.1365
10000     2.4986      0.0886     0.0814     57.9249
10100     2.4986      0.0717     0.0814     58.3156
10200     2.4986      0.0992     0.0814     58.6419
10300     2.4986      0.0759     0.0814     58.6402
10400     2.4986      0.0886     0.0814     58.8629
10500     2.4986      0.0570     0.0814     58.3240
10600     2.4986      0.0675     0.0814     58.1921
10700     2.4986      0.0738     0.0814     59.2753
10800     2.4986      0.0844     0.0814     59.3185
10900     2.4986      0.0970     0.0814     58.4714
11000     2.4986      0.0759     0.0814     58.9182
11100     2.4986      0.0696     0.0814     59.1379
11200     2.4986      0.0970     0.0814     58.8490
11300     2.4986      0.0907     0.0814     57.8963
11400     2.4986      0.0823     0.0814     60.0913
11500     2.4986      0.0802     0.0814     59.9873
11600     2.4986      0.0970     0.0814     57.3666
11700     2.4986      0.0823     0.0814     58.0022
11800     2.4986      0.0738     0.0814     58.9334
11900     2.4986      0.0802     0.0814     57.7836
12000     2.4986      0.0886     0.0814     57.3303
12100     2.4986      0.1118     0.0814     56.8803
12200     2.4986      0.0654     0.0814     57.3376
12300     2.4986      0.0823     0.0814     58.3383
12400     2.4986      0.1118     0.0814     56.9194
12500     2.4986      0.0759     0.0814     57.4193
12600     2.4986      0.0781     0.0814     58.4070
12700     2.4985      0.0802     0.0814     57.6007
12800     2.4985      0.0865     0.0814     57.7548
12900     2.4985      0.0570     0.0814     59.1203
13000     2.4985      0.0759     0.0814     57.9952
13100     2.4985      0.0844     0.0814     57.7362
13200     2.4985      0.0549     0.0814     58.0096
13300     2.4985      0.0865     0.0814     58.3966
13400     2.4985      0.0781     0.0814     60.1041
13500     2.4985      0.0696     0.0814     59.0174
13600     2.4985      0.0823     0.0814     58.1910
13700     2.4985      0.0717     0.0814     57.2266
13800     2.4985      0.0675     0.0814     59.4644
13900     2.4985      0.0612     0.0814     57.9178
14000     2.4985      0.1076     0.0814     57.4541
14100     2.4985      0.0865     0.0814     57.3365
14200     2.4985      0.0633     0.0814     58.7031
14300     2.4985      0.0865     0.0814     57.9458
14400     2.4985      0.0865     0.0814     58.2320
14500     2.4985      0.0549     0.0814     57.8024
14600     2.4985      0.0907     0.0814     56.8066
14700     2.4985      0.0928     0.0814     57.0076
14800     2.4985      0.0759     0.0814     58.0748
14900     2.4985      0.0970     0.0814     57.9299
15000     2.4985      0.0759     0.0814     60.1181
15100     2.4985      0.1160     0.0814     58.5782
15200     2.4985      0.0865     0.0814     58.7809
15300     2.4985      0.0928     0.0814     57.7999
15400     2.4985      0.0549     0.0814     57.8006
15500     2.4984      0.0781     0.0814     58.7452
15600     2.4984      0.0823     0.0814     58.4283
15700     2.4984      0.0570     0.0814     57.9278
15800     2.4984      0.0823     0.0814     59.4007
15900     2.4984      0.0844     0.0814     58.8795
16000     2.4984      0.0759     0.0814     58.5617
16100     2.4984      0.0654     0.0814     58.3338
16200     2.4984      0.0633     0.0814     58.2793
16300     2.4984      0.0844     0.0814     58.1146
16400     2.4984      0.0823     0.0814     57.9406
16500     2.4984      0.0717     0.0814     58.7841
16600     2.4984      0.0928     0.0814     57.9709
16700     2.4984      0.0696     0.0814     57.9340
16800     2.4984      0.0865     0.0814     58.1887
16900     2.4984      0.0844     0.0814     58.3499
17000     2.4984      0.0612     0.0814     59.1840
17100     2.4984      0.0907     0.0814     59.5549
17200     2.4984      0.0675     0.0814     58.8499
17300     2.4984      0.0886     0.0814     58.4406
17400     2.4984      0.0591     0.0814     58.9056
17500     2.4984      0.0759     0.0814     58.4871
17600     2.4984      0.0907     0.0814     58.5939
17700     2.4984      0.0759     0.0814     58.1025
17800     2.4984      0.0527     0.0814     57.7872
17900     2.4984      0.0865     0.0814     59.8274
18000     2.4984      0.0696     0.0814     58.4758
18100     2.4984      0.0844     0.0814     58.9995
18200     2.4984      0.0612     0.0814     58.2658
18300     2.4983      0.0949     0.0814     57.9830
18400     2.4983      0.0823     0.0814     59.2980
18500     2.4983      0.0886     0.0814     58.5992
18600     2.4983      0.0886     0.0814     57.8785
18700     2.4983      0.0802     0.0814     59.4126
18800     2.4983      0.0675     0.0814     58.6072
18900     2.4983      0.0886     0.0814     58.3882
19000     2.4983      0.0738     0.0814     58.7850
19100     2.4983      0.0738     0.0814     58.1918
19200     2.4983      0.0781     0.0814     58.4036
19300     2.4983      0.0865     0.0814     57.6883
19400     2.4983      0.0949     0.0814     59.3742
19500     2.4983      0.0886     0.0814     57.4787
19600     2.4983      0.0696     0.0814     58.2299
19700     2.4983      0.0823     0.0814     57.1143
19800     2.4983      0.0992     0.0814     57.6160
19900     2.4983      0.0696     0.0814     57.2715
20000     2.4983      0.0633     0.0814     59.7032
20100     2.4983      0.0717     0.0814     57.8676
20200     2.4983      0.1224     0.0814     58.1496
20300     2.4983      0.0570     0.0814     60.1701
20400     2.4983      0.0886     0.0814     58.2789
20500     2.4983      0.0949     0.0814     57.3553
20600     2.4983      0.0928     0.0814     58.5293
20700     2.4983      0.0928     0.0814     57.8421
20800     2.4983      0.0591     0.0814     58.1460
20900     2.4983      0.0781     0.0814     58.2790
21000     2.4983      0.0675     0.0814     57.8428
21100     2.4983      0.0591     0.0814     58.5993
21200     2.4983      0.0675     0.0814     58.2253
21300     2.4983      0.0823     0.0814     58.2011
21400     2.4983      0.0907     0.0814     60.2593
21500     2.4983      0.0949     0.0814     57.6054
21600     2.4983      0.0738     0.0814     58.1606
21700     2.4983      0.1076     0.0814     58.3758
21800     2.4983      0.0970     0.0814     57.7600
21900     2.4983      0.0675     0.0814     58.4893
22000     2.4983      0.0844     0.0814     58.0021
22100     2.4983      0.0675     0.0814     58.0247
22200     2.4983      0.0844     0.0814     58.3759
22300     2.4983      0.0844     0.0814     58.0918
22400     2.4983      0.0823     0.0814     58.1479
22500     2.4983      0.0802     0.0814     57.3019
22600     2.4983      0.0970     0.0814     56.6573
22700     2.4983      0.0696     0.0814     59.4356
22800     2.4983      0.0675     0.0814     57.7270
22900     2.4983      0.0865     0.0814     58.9945
23000     2.4983      0.0802     0.0814     59.0643
23100     2.4983      0.0506     0.0814     58.5781
23200     2.4983      0.0886     0.0814     59.3245
23300     2.4983      0.0717     0.0814     59.4971
23400     2.4983      0.0865     0.0814     57.9996
23500     2.4983      0.0823     0.0814     58.9254
23600     2.4983      0.0823     0.0814     57.6644
23700     2.4983      0.0738     0.0814     59.3578
23800     2.4983      0.0844     0.0814     57.7928
23900     2.4983      0.0823     0.0814     57.5694
24000     2.4983      0.1013     0.0814     57.7869
24100     2.4983      0.0844     0.0814     57.5324
24200     2.4983      0.1013     0.0814     58.7936
24300     2.4983      0.1055     0.0814     59.1303
24400     2.4983      0.0949     0.0814     58.7996
24500     2.4982      0.0675     0.0814     57.9714
24600     2.4982      0.0759     0.0814     59.1032
24700     2.4982      0.0781     0.0814     59.0171
24800     2.4982      0.0865     0.0814     59.6610
24900     2.4982      0.0844     0.0814     58.5958
25000     2.4982      0.0675     0.0814     57.9068
25100     2.4982      0.0781     0.0814     58.6489
25200     2.4982      0.0717     0.0814     57.5016
25300     2.4982      0.0886     0.0814     58.3646
25400     2.4982      0.0591     0.0814     58.8598
25500     2.4982      0.0865     0.0814     58.3226
25600     2.4982      0.0781     0.0814     58.4852
25700     2.4982      0.0717     0.0814     57.5857
25800     2.4982      0.0992     0.0814     58.2465
25900     2.4982      0.0570     0.0814     59.3335
26000     2.4982      0.0549     0.0814     58.1375
26100     2.4982      0.0907     0.0814     58.5369
26200     2.4982      0.0823     0.0814     58.3175
26300     2.4982      0.0928     0.0814     58.6977
26400     2.4982      0.0675     0.0814     58.5415
26500     2.4982      0.0717     0.0814     59.7149
26600     2.4982      0.0844     0.0814     58.6871
26700     2.4982      0.0781     0.0814     59.3023
26800     2.4982      0.0949     0.0814     58.1242
26900     2.4982      0.0781     0.0814     58.2756
27000     2.4982      0.0823     0.0814     58.7998
27100     2.4982      0.0759     0.0814     57.7122
27200     2.4982      0.0928     0.0814     56.7982
27300     2.4982      0.0781     0.0814     54.7765
27400     2.4982      0.0633     0.0814     56.2594
27500     2.4982      0.0781     0.0814     58.4745
27600     2.4982      0.1055     0.0814     57.2949
27700     2.4982      0.0949     0.0814     57.0267
27800     2.4982      0.0654     0.0814     57.3028
27900     2.4982      0.0696     0.0814     57.4069
28000     2.4982      0.0907     0.0814     57.0691
28100     2.4982      0.0865     0.0814     57.0057
28200     2.4982      0.0717     0.0814     57.8654
28300     2.4982      0.0844     0.0814     57.9179
28400     2.4982      0.0802     0.0814     58.4923
28500     2.4982      0.0675     0.0814     58.1403
28600     2.4982      0.0928     0.0814     59.2358
28700     2.4982      0.0675     0.0814     64.9786
28800     2.4982      0.0781     0.0814     62.0152
28900     2.4982      0.0844     0.0814     63.3074
29000     2.4982      0.1076     0.0814     59.0915
29100     2.4982      0.0738     0.0814     60.2004
29200     2.4982      0.0759     0.0814     59.1053
29300     2.4982      0.0591     0.0814     60.1761
29400     2.4982      0.0844     0.0814     59.5137
29500     2.4982      0.0823     0.0814     57.9556
29600     2.4982      0.0844     0.0814     59.4293
29700     2.4982      0.0992     0.0814     58.5842
29800     2.4982      0.0781     0.0814     58.4937
29900     2.4982      0.0886     0.0814     58.8962
29999     2.4982      0.0865     0.0814     57.7991
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Start testing:
Test Accuracy: 0.0789
