Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=14, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=235899598, rows_bias=100, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=235899598, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 361, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=235899598, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
Traceback (most recent call last):
  File "KWS_LSTM.py", line 112, in <module>
    model = KWS_LSTM_cs(input_dim = args.n_mfcc, hidden_dim = args.hidden, output_dim = len(args.word_list), device = device, wb = args.quant_w, abMVM = args.quant_actMVM, abNM = args.quant_actNM, ib = args.quant_inp, noise_level = 0, drop_p = args.drop_p, n_msb = args.n_msb, pact_a = args.pact_a, bias_r = args.rows_bias)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 362, in __init__
    self.lstmBlocks = LSTMLayer(LSTMCellQ_bmm, self.drop_p, self.input_dim, self.hidden_dim, self.wb, self.ib, self.abMVM, self.abNM, self.noise_level, self.n_msb, pact_a, bias_r)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 48, in __init__
    torch.nn.init.uniform_(self.cell.weight_hh, a = -limit1, b = limit1)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 125, in uniform_
    return _no_grad_uniform_(tensor, a, b)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/init.py", line 16, in _no_grad_uniform_
    return tensor.uniform_(a, b)
TypeError: uniform_(): argument 'from' (position 1) must be float, not Tensor
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.0, finetuning_epochs=0, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.0005,0.0001,0.00002', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.0, noise_injectionT=0.0, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=235899598, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,10000', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
97278542-0a97-4eb8-9458-1c54125aae38
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.5282      0.0633     0.0807     11.7209
00100     2.5208      0.0612     0.0811     58.1870
00200     2.5150      0.0696     0.0811     57.1250
00300     2.5107      0.0759     0.0811     57.7495
00400     2.5076      0.0886     0.0811     58.1238
00500     2.5056      0.0970     0.0811     58.8290
00600     2.5042      0.1076     0.0811     57.9977
00700     2.5032      0.0781     0.0811     57.3184
00800     2.5026      0.0844     0.0811     59.4794
00900     2.5021      0.0802     0.0811     58.7186
01000     2.5017      0.0907     0.0811     59.0397
01100     2.5014      0.0992     0.0811     58.8315
01200     2.5012      0.0654     0.0811     57.9809
01300     2.5010      0.1013     0.0811     58.8159
01400     2.5009      0.1013     0.0812     58.6335
01500     2.5008      0.1076     0.0812     58.1596
01600     2.5007      0.0506     0.0812     59.4142
01700     2.5006      0.0464     0.0812     60.0506
01800     2.5005      0.1055     0.0812     57.9926
01900     2.5005      0.0823     0.0812     58.5656
02000     2.5004      0.0823     0.0812     58.0510
02100     2.5003      0.0844     0.0812     58.5159
02200     2.5003      0.0907     0.0812     59.2639
02300     2.5003      0.0717     0.0812     59.0862
02400     2.5002      0.0717     0.0814     58.4783
02500     2.5002      0.0802     0.0814     58.8669
02600     2.5001      0.0633     0.0814     57.1821
02700     2.5001      0.0738     0.0814     59.0724
02800     2.5001      0.0970     0.0814     58.4886
02900     2.5001      0.0844     0.0814     58.3343
03000     2.5000      0.0717     0.0814     59.2884
03100     2.5000      0.0865     0.0814     59.2290
03200     2.5000      0.0823     0.0814     59.0030
03300     2.5000      0.0823     0.0814     58.1834
03400     2.4999      0.0759     0.0814     57.6000
03500     2.4999      0.0907     0.0814     58.7682
03600     2.4999      0.0865     0.0814     57.7158
03700     2.4999      0.0717     0.0814     58.6681
03800     2.4998      0.0970     0.0814     58.4936
03900     2.4998      0.0506     0.0814     58.2377
04000     2.4998      0.0802     0.0814     58.7341
04100     2.4998      0.0738     0.0814     58.0735
04200     2.4998      0.0992     0.0814     59.1041
04300     2.4997      0.0823     0.0814     58.4014
04400     2.4997      0.0823     0.0814     58.7745
04500     2.4997      0.0464     0.0814     58.8264
04600     2.4997      0.0781     0.0814     59.2410
04700     2.4997      0.0928     0.0814     58.5699
04800     2.4996      0.0886     0.0814     58.5246
04900     2.4996      0.0612     0.0814     57.1929
05000     2.4996      0.0844     0.0814     58.0319
05100     2.4996      0.0823     0.0814     58.7220
05200     2.4996      0.1055     0.0814     58.5138
05300     2.4995      0.0717     0.0814     58.1903
05400     2.4995      0.0717     0.0814     58.7835
05500     2.4995      0.0738     0.0814     59.7879
05600     2.4995      0.0738     0.0814     59.0883
05700     2.4995      0.0654     0.0814     57.4175
05800     2.4994      0.0759     0.0814     58.3553
05900     2.4994      0.0759     0.0814     58.8384
06000     2.4994      0.0633     0.0814     58.2458
06100     2.4994      0.0865     0.0814     58.5549
06200     2.4994      0.0717     0.0814     57.6897
06300     2.4993      0.0928     0.0814     58.1866
06400     2.4993      0.0738     0.0814     59.0922
06500     2.4993      0.0675     0.0814     58.0290
06600     2.4993      0.0675     0.0814     58.3075
06700     2.4993      0.0907     0.0814     58.1468
06800     2.4992      0.0759     0.0814     57.5778
06900     2.4992      0.0844     0.0814     57.9012
07000     2.4992      0.0886     0.0814     58.0399
07100     2.4992      0.0781     0.0814     58.1331
07200     2.4992      0.0886     0.0814     60.3247
07300     2.4992      0.0781     0.0814     59.3657
07400     2.4991      0.0781     0.0814     58.4754
07500     2.4991      0.0422     0.0814     58.8537
07600     2.4991      0.0865     0.0814     58.4102
07700     2.4991      0.0949     0.0814     59.9145
07800     2.4991      0.0949     0.0814     59.6168
07900     2.4990      0.0675     0.0814     58.7451
08000     2.4990      0.0781     0.0814     58.7862
08100     2.4990      0.0675     0.0814     59.2471
08200     2.4990      0.0654     0.0814     58.6629
08300     2.4990      0.0949     0.0814     59.0862
08400     2.4989      0.0633     0.0814     58.8630
08500     2.4989      0.0886     0.0814     60.3489
08600     2.4989      0.0738     0.0814     59.8327
08700     2.4989      0.0696     0.0814     59.0447
08800     2.4989      0.0844     0.0814     57.7194
08900     2.4989      0.0865     0.0814     57.6949
09000     2.4988      0.0738     0.0814     58.7753
09100     2.4988      0.0823     0.0814     58.8017
09200     2.4988      0.0738     0.0814     57.8160
09300     2.4988      0.0781     0.0814     58.3093
09400     2.4988      0.0802     0.0814     58.8297
09500     2.4987      0.0654     0.0814     57.5752
09600     2.4987      0.0802     0.0814     58.4752
09700     2.4987      0.0823     0.0814     57.5473
09800     2.4987      0.0865     0.0814     57.8073
09900     2.4987      0.0844     0.0814     59.1365
10000     2.4986      0.0886     0.0814     57.9249
10100     2.4986      0.0717     0.0814     58.3156
10200     2.4986      0.0992     0.0814     58.6419
10300     2.4986      0.0759     0.0814     58.6402
10400     2.4986      0.0886     0.0814     58.8629
10500     2.4986      0.0570     0.0814     58.3240
10600     2.4986      0.0675     0.0814     58.1921
10700     2.4986      0.0738     0.0814     59.2753
10800     2.4986      0.0844     0.0814     59.3185
10900     2.4986      0.0970     0.0814     58.4714
11000     2.4986      0.0759     0.0814     58.9182
11100     2.4986      0.0696     0.0814     59.1379
11200     2.4986      0.0970     0.0814     58.8490
11300     2.4986      0.0907     0.0814     57.8963
11400     2.4986      0.0823     0.0814     60.0913
11500     2.4986      0.0802     0.0814     59.9873
11600     2.4986      0.0970     0.0814     57.3666
11700     2.4986      0.0823     0.0814     58.0022
11800     2.4986      0.0738     0.0814     58.9334
11900     2.4986      0.0802     0.0814     57.7836
12000     2.4986      0.0886     0.0814     57.3303
12100     2.4986      0.1118     0.0814     56.8803
12200     2.4986      0.0654     0.0814     57.3376
12300     2.4986      0.0823     0.0814     58.3383
12400     2.4986      0.1118     0.0814     56.9194
12500     2.4986      0.0759     0.0814     57.4193
12600     2.4986      0.0781     0.0814     58.4070
12700     2.4985      0.0802     0.0814     57.6007
12800     2.4985      0.0865     0.0814     57.7548
12900     2.4985      0.0570     0.0814     59.1203
13000     2.4985      0.0759     0.0814     57.9952
13100     2.4985      0.0844     0.0814     57.7362
13200     2.4985      0.0549     0.0814     58.0096
13300     2.4985      0.0865     0.0814     58.3966
13400     2.4985      0.0781     0.0814     60.1041
13500     2.4985      0.0696     0.0814     59.0174
13600     2.4985      0.0823     0.0814     58.1910
13700     2.4985      0.0717     0.0814     57.2266
13800     2.4985      0.0675     0.0814     59.4644
13900     2.4985      0.0612     0.0814     57.9178
14000     2.4985      0.1076     0.0814     57.4541
14100     2.4985      0.0865     0.0814     57.3365
14200     2.4985      0.0633     0.0814     58.7031
14300     2.4985      0.0865     0.0814     57.9458
14400     2.4985      0.0865     0.0814     58.2320
14500     2.4985      0.0549     0.0814     57.8024
14600     2.4985      0.0907     0.0814     56.8066
14700     2.4985      0.0928     0.0814     57.0076
14800     2.4985      0.0759     0.0814     58.0748
14900     2.4985      0.0970     0.0814     57.9299
15000     2.4985      0.0759     0.0814     60.1181
15100     2.4985      0.1160     0.0814     58.5782
15200     2.4985      0.0865     0.0814     58.7809
15300     2.4985      0.0928     0.0814     57.7999
15400     2.4985      0.0549     0.0814     57.8006
15500     2.4984      0.0781     0.0814     58.7452
15600     2.4984      0.0823     0.0814     58.4283
15700     2.4984      0.0570     0.0814     57.9278
15800     2.4984      0.0823     0.0814     59.4007
15900     2.4984      0.0844     0.0814     58.8795
16000     2.4984      0.0759     0.0814     58.5617
16100     2.4984      0.0654     0.0814     58.3338
16200     2.4984      0.0633     0.0814     58.2793
16300     2.4984      0.0844     0.0814     58.1146
16400     2.4984      0.0823     0.0814     57.9406
16500     2.4984      0.0717     0.0814     58.7841
16600     2.4984      0.0928     0.0814     57.9709
16700     2.4984      0.0696     0.0814     57.9340
16800     2.4984      0.0865     0.0814     58.1887
16900     2.4984      0.0844     0.0814     58.3499
17000     2.4984      0.0612     0.0814     59.1840
17100     2.4984      0.0907     0.0814     59.5549
17200     2.4984      0.0675     0.0814     58.8499
17300     2.4984      0.0886     0.0814     58.4406
17400     2.4984      0.0591     0.0814     58.9056
17500     2.4984      0.0759     0.0814     58.4871
17600     2.4984      0.0907     0.0814     58.5939
17700     2.4984      0.0759     0.0814     58.1025
17800     2.4984      0.0527     0.0814     57.7872
17900     2.4984      0.0865     0.0814     59.8274
18000     2.4984      0.0696     0.0814     58.4758
18100     2.4984      0.0844     0.0814     58.9995
18200     2.4984      0.0612     0.0814     58.2658
18300     2.4983      0.0949     0.0814     57.9830
18400     2.4983      0.0823     0.0814     59.2980
18500     2.4983      0.0886     0.0814     58.5992
18600     2.4983      0.0886     0.0814     57.8785
18700     2.4983      0.0802     0.0814     59.4126
18800     2.4983      0.0675     0.0814     58.6072
18900     2.4983      0.0886     0.0814     58.3882
19000     2.4983      0.0738     0.0814     58.7850
19100     2.4983      0.0738     0.0814     58.1918
19200     2.4983      0.0781     0.0814     58.4036
19300     2.4983      0.0865     0.0814     57.6883
19400     2.4983      0.0949     0.0814     59.3742
19500     2.4983      0.0886     0.0814     57.4787
19600     2.4983      0.0696     0.0814     58.2299
19700     2.4983      0.0823     0.0814     57.1143
19800     2.4983      0.0992     0.0814     57.6160
19900     2.4983      0.0696     0.0814     57.2715
20000     2.4983      0.0633     0.0814     59.7032
20100     2.4983      0.0717     0.0814     57.8676
20200     2.4983      0.1224     0.0814     58.1496
20300     2.4983      0.0570     0.0814     60.1701
20400     2.4983      0.0886     0.0814     58.2789
20500     2.4983      0.0949     0.0814     57.3553
20600     2.4983      0.0928     0.0814     58.5293
20700     2.4983      0.0928     0.0814     57.8421
20800     2.4983      0.0591     0.0814     58.1460
20900     2.4983      0.0781     0.0814     58.2790
21000     2.4983      0.0675     0.0814     57.8428
21100     2.4983      0.0591     0.0814     58.5993
21200     2.4983      0.0675     0.0814     58.2253
21300     2.4983      0.0823     0.0814     58.2011
21400     2.4983      0.0907     0.0814     60.2593
21500     2.4983      0.0949     0.0814     57.6054
21600     2.4983      0.0738     0.0814     58.1606
21700     2.4983      0.1076     0.0814     58.3758
21800     2.4983      0.0970     0.0814     57.7600
21900     2.4983      0.0675     0.0814     58.4893
22000     2.4983      0.0844     0.0814     58.0021
22100     2.4983      0.0675     0.0814     58.0247
22200     2.4983      0.0844     0.0814     58.3759
22300     2.4983      0.0844     0.0814     58.0918
22400     2.4983      0.0823     0.0814     58.1479
22500     2.4983      0.0802     0.0814     57.3019
22600     2.4983      0.0970     0.0814     56.6573
22700     2.4983      0.0696     0.0814     59.4356
22800     2.4983      0.0675     0.0814     57.7270
22900     2.4983      0.0865     0.0814     58.9945
23000     2.4983      0.0802     0.0814     59.0643
23100     2.4983      0.0506     0.0814     58.5781
23200     2.4983      0.0886     0.0814     59.3245
23300     2.4983      0.0717     0.0814     59.4971
23400     2.4983      0.0865     0.0814     57.9996
23500     2.4983      0.0823     0.0814     58.9254
23600     2.4983      0.0823     0.0814     57.6644
23700     2.4983      0.0738     0.0814     59.3578
23800     2.4983      0.0844     0.0814     57.7928
23900     2.4983      0.0823     0.0814     57.5694
24000     2.4983      0.1013     0.0814     57.7869
24100     2.4983      0.0844     0.0814     57.5324
24200     2.4983      0.1013     0.0814     58.7936
24300     2.4983      0.1055     0.0814     59.1303
24400     2.4983      0.0949     0.0814     58.7996
24500     2.4982      0.0675     0.0814     57.9714
24600     2.4982      0.0759     0.0814     59.1032
24700     2.4982      0.0781     0.0814     59.0171
24800     2.4982      0.0865     0.0814     59.6610
24900     2.4982      0.0844     0.0814     58.5958
25000     2.4982      0.0675     0.0814     57.9068
25100     2.4982      0.0781     0.0814     58.6489
25200     2.4982      0.0717     0.0814     57.5016
25300     2.4982      0.0886     0.0814     58.3646
25400     2.4982      0.0591     0.0814     58.8598
25500     2.4982      0.0865     0.0814     58.3226
25600     2.4982      0.0781     0.0814     58.4852
25700     2.4982      0.0717     0.0814     57.5857
25800     2.4982      0.0992     0.0814     58.2465
25900     2.4982      0.0570     0.0814     59.3335
26000     2.4982      0.0549     0.0814     58.1375
26100     2.4982      0.0907     0.0814     58.5369
26200     2.4982      0.0823     0.0814     58.3175
26300     2.4982      0.0928     0.0814     58.6977
26400     2.4982      0.0675     0.0814     58.5415
26500     2.4982      0.0717     0.0814     59.7149
26600     2.4982      0.0844     0.0814     58.6871
26700     2.4982      0.0781     0.0814     59.3023
26800     2.4982      0.0949     0.0814     58.1242
26900     2.4982      0.0781     0.0814     58.2756
27000     2.4982      0.0823     0.0814     58.7998
27100     2.4982      0.0759     0.0814     57.7122
27200     2.4982      0.0928     0.0814     56.7982
27300     2.4982      0.0781     0.0814     54.7765
27400     2.4982      0.0633     0.0814     56.2594
27500     2.4982      0.0781     0.0814     58.4745
27600     2.4982      0.1055     0.0814     57.2949
27700     2.4982      0.0949     0.0814     57.0267
27800     2.4982      0.0654     0.0814     57.3028
27900     2.4982      0.0696     0.0814     57.4069
28000     2.4982      0.0907     0.0814     57.0691
28100     2.4982      0.0865     0.0814     57.0057
28200     2.4982      0.0717     0.0814     57.8654
28300     2.4982      0.0844     0.0814     57.9179
28400     2.4982      0.0802     0.0814     58.4923
28500     2.4982      0.0675     0.0814     58.1403
28600     2.4982      0.0928     0.0814     59.2358
28700     2.4982      0.0675     0.0814     64.9786
28800     2.4982      0.0781     0.0814     62.0152
28900     2.4982      0.0844     0.0814     63.3074
29000     2.4982      0.1076     0.0814     59.0915
29100     2.4982      0.0738     0.0814     60.2004
29200     2.4982      0.0759     0.0814     59.1053
29300     2.4982      0.0591     0.0814     60.1761
29400     2.4982      0.0844     0.0814     59.5137
29500     2.4982      0.0823     0.0814     57.9556
29600     2.4982      0.0844     0.0814     59.4293
29700     2.4982      0.0992     0.0814     58.5842
29800     2.4982      0.0781     0.0814     58.4937
29900     2.4982      0.0886     0.0814     58.8962
29999     2.4982      0.0865     0.0814     57.7991
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Start testing:
Test Accuracy: 0.0789
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=474, canonical_testing=False, cs=0.1, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, gain_blocks=2, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=1.0, method=1, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=2, quant_actNM=2, quant_inp=2, quant_w=2, random_seed=235899598, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=641, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
f0c65827-687f-4714-aa79-55263ad7928d
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.5282      0.0633     0.0806     12.7230
00100     2.5074      0.0928     0.0807     58.4410
00200     2.5025      0.0802     0.0808     57.5694
00300     2.5012      0.0781     0.0808     57.2286
00400     2.5007      0.0928     0.0808     57.1751
00500     2.5004      0.0675     0.0808     57.0708
00600     2.5002      0.0717     0.0808     57.3721
00700     2.5001      0.0570     0.0808     56.9406
00800     2.5000      0.0844     0.0808     57.5218
00900     2.4999      0.0802     0.0808     57.6906
01000     2.4998      0.0949     0.0808     57.5733
01100     2.4997      0.0738     0.0810     57.1925
01200     2.4997      0.0865     0.0810     56.8511
01300     2.4996      0.0696     0.0810     58.0175
01400     2.4995      0.0823     0.0810     57.1986
01500     2.4994      0.0717     0.0810     58.2767
01600     2.4993      0.0865     0.0810     57.7128
01700     2.4993      0.1034     0.0810     58.0868
01800     2.4992      0.0865     0.0810     57.3192
01900     2.4991      0.0506     0.0810     58.2514
02000     2.4990      0.0696     0.0810     58.2752
02100     2.4990      0.0738     0.0810     57.1930
02200     2.4989      0.0907     0.0812     59.1203
02300     2.4988      0.1160     0.0812     57.9729
02400     2.4987      0.0844     0.0812     58.0622
02500     2.4987      0.0570     0.0812     58.3969
02600     2.4986      0.0738     0.0812     58.1621
02700     2.4985      0.0591     0.0812     59.9378
02800     2.4984      0.0717     0.0812     57.8535
02900     2.4984      0.0781     0.0814     59.1156
03000     2.4983      0.0717     0.0814     58.0746
03100     2.4982      0.0928     0.0814     58.4674
03200     2.4982      0.0844     0.0814     58.5636
03300     2.4981      0.0717     0.0814     58.6314
03400     2.4980      0.1076     0.0814     57.4951
03500     2.4979      0.0696     0.0814     58.3316
03600     2.4979      0.0654     0.0814     58.7681
03700     2.4978      0.0865     0.0814     57.6497
03800     2.4977      0.0654     0.0814     57.3947
03900     2.4977      0.0844     0.0814     58.2658
04000     2.4976      0.0844     0.0814     59.4732
04100     2.4975      0.0781     0.0814     57.0924
04200     2.4975      0.0759     0.0814     57.2793
04300     2.4974      0.0612     0.0814     57.7544
04400     2.4973      0.0886     0.0814     56.5713
04500     2.4972      0.0633     0.0814     57.3242
04600     2.4972      0.0823     0.0814     58.0953
04700     2.4971      0.0907     0.0814     57.4460
04800     2.4970      0.0802     0.0814     59.3243
04900     2.4970      0.0907     0.0814     57.5482
05000     2.4969      0.0633     0.0814     57.8590
05100     2.4968      0.0844     0.0814     57.9287
05200     2.4968      0.0844     0.0814     57.6755
05300     2.4967      0.0823     0.0814     58.6140
05400     2.4966      0.0717     0.0814     57.8816
05500     2.4966      0.0675     0.0814     57.3916
05600     2.4965      0.0802     0.0814     58.7090
05700     2.4964      0.0802     0.0814     57.5524
05800     2.4964      0.1076     0.0814     59.2019
05900     2.4963      0.0696     0.0814     58.7750
06000     2.4962      0.0696     0.0814     57.8729
06100     2.4962      0.0738     0.0814     57.8904
06200     2.4961      0.0591     0.0814     57.6612
06300     2.4960      0.0696     0.0814     57.5192
06400     2.4960      0.0844     0.0814     57.9939
06500     2.4959      0.0781     0.0814     56.9396
06600     2.4958      0.0591     0.0814     57.5169
06700     2.4958      0.0907     0.0814     57.6799
06800     2.4957      0.0738     0.0814     58.0259
06900     2.4956      0.0886     0.0814     59.4001
07000     2.4956      0.0591     0.0814     58.5325
07100     2.4955      0.0759     0.0814     57.7364
07200     2.4954      0.0781     0.0814     59.5834
07300     2.4954      0.0823     0.0814     57.3083
07400     2.4953      0.0759     0.0814     57.8037
07500     2.4952      0.0549     0.0814     58.5422
07600     2.4952      0.0823     0.0814     59.8685
07700     2.4951      0.0823     0.0814     57.7948
07800     2.4950      0.0865     0.0814     58.8695
07900     2.4950      0.0949     0.0814     57.2943
08000     2.4949      0.0759     0.0814     58.2275
08100     2.4949      0.1055     0.0814     56.9991
08200     2.4948      0.0675     0.0814     57.2606
08300     2.4947      0.0907     0.0814     57.8569
08400     2.4947      0.0781     0.0814     58.1603
08500     2.4946      0.0781     0.0814     56.5028
08600     2.4945      0.0886     0.0814     59.0539
08700     2.4945      0.0738     0.0814     58.5263
08800     2.4944      0.0865     0.0814     59.6509
08900     2.4943      0.0759     0.0814     58.0241
09000     2.4943      0.0612     0.0814     57.8923
09100     2.4942      0.0717     0.0814     59.8753
09200     2.4942      0.0759     0.0814     57.3764
09300     2.4941      0.0759     0.0814     58.5308
09400     2.4940      0.0591     0.0814     58.9512
09500     2.4940      0.0949     0.0814     57.6925
09600     2.4939      0.0928     0.0814     57.6109
09700     2.4938      0.0949     0.0814     58.3815
09800     2.4938      0.0675     0.0814     56.8179
09900     2.4937      0.0886     0.0814     59.2083
10000     2.4936      0.0781     0.0814     58.9825
10100     2.4936      0.0970     0.0814     58.9696
10200     2.4936      0.0886     0.0814     59.9864
10300     2.4936      0.0907     0.0814     57.3490
10400     2.4936      0.0949     0.0814     58.5046
10500     2.4936      0.0675     0.0814     59.0205
10600     2.4936      0.0844     0.0814     58.6757
10700     2.4935      0.1034     0.0814     57.6135
10800     2.4935      0.0781     0.0814     59.0354
10900     2.4935      0.0633     0.0814     58.4308
11000     2.4935      0.0527     0.0814     57.4944
11100     2.4935      0.0759     0.0814     58.4832
11200     2.4935      0.0928     0.0814     58.8120
11300     2.4934      0.0823     0.0814     57.9588
11400     2.4934      0.0949     0.0814     57.4373
11500     2.4934      0.0865     0.0814     58.3563
11600     2.4934      0.0696     0.0814     58.0518
11700     2.4934      0.0823     0.0814     57.4844
11800     2.4934      0.0823     0.0814     59.0354
11900     2.4933      0.0654     0.0814     57.1511
12000     2.4933      0.0844     0.0814     58.3948
12100     2.4933      0.0633     0.0814     58.1965
12200     2.4933      0.0928     0.0814     57.3711
12300     2.4933      0.0696     0.0814     58.5597
12400     2.4933      0.0823     0.0814     57.0362
12500     2.4933      0.0802     0.0814     57.2117
12600     2.4932      0.0781     0.0814     58.0626
12700     2.4932      0.0886     0.0814     58.1062
12800     2.4932      0.0970     0.0814     59.2791
12900     2.4932      0.0802     0.0814     59.1750
13000     2.4932      0.0844     0.0814     57.5989
13100     2.4932      0.0759     0.0814     59.4412
13200     2.4931      0.0570     0.0814     58.0402
13300     2.4931      0.0759     0.0814     58.5730
13400     2.4931      0.0844     0.0814     57.5082
13500     2.4931      0.1013     0.0814     58.6528
13600     2.4931      0.0738     0.0814     58.4340
13700     2.4931      0.0886     0.0814     58.9416
13800     2.4930      0.0844     0.0814     58.6961
13900     2.4930      0.0696     0.0814     58.0611
14000     2.4930      0.1034     0.0814     57.5161
14100     2.4930      0.0992     0.0814     58.3803
14200     2.4930      0.0823     0.0814     58.2636
14300     2.4930      0.0865     0.0814     59.2632
14400     2.4930      0.0823     0.0814     58.5157
14500     2.4929      0.0570     0.0814     57.8836
14600     2.4929      0.0675     0.0814     57.5161
14700     2.4929      0.0570     0.0814     59.1224
14800     2.4929      0.0949     0.0814     59.4476
14900     2.4929      0.0696     0.0814     58.0772
15000     2.4929      0.0823     0.0814     57.5872
15100     2.4928      0.0781     0.0814     58.1755
15200     2.4928      0.0907     0.0814     59.0031
15300     2.4928      0.0949     0.0814     57.8741
15400     2.4928      0.0823     0.0814     58.4755
15500     2.4928      0.1034     0.0814     58.4853
15600     2.4928      0.0886     0.0814     58.6490
15700     2.4928      0.0865     0.0814     57.5502
15800     2.4927      0.0928     0.0814     58.2438
15900     2.4927      0.0928     0.0814     59.0562
16000     2.4927      0.0802     0.0814     58.0653
16100     2.4927      0.0844     0.0814     60.6353
16200     2.4927      0.0823     0.0814     57.5614
16300     2.4927      0.0738     0.0814     58.5960
16400     2.4926      0.0781     0.0814     57.8644
16500     2.4926      0.0759     0.0814     58.9836
16600     2.4926      0.0696     0.0814     58.7401
16700     2.4926      0.0970     0.0814     58.4868
16800     2.4926      0.0781     0.0814     59.2864
16900     2.4926      0.0823     0.0814     58.5294
17000     2.4925      0.1013     0.0814     58.2732
17100     2.4925      0.0781     0.0814     59.1188
17200     2.4925      0.0802     0.0814     59.4287
17300     2.4925      0.0802     0.0814     57.6895
17400     2.4925      0.0844     0.0814     59.4765
17500     2.4925      0.0823     0.0814     57.7908
17600     2.4925      0.0907     0.0814     58.1320
17700     2.4924      0.0759     0.0814     57.3869
17800     2.4924      0.0886     0.0814     57.5540
17900     2.4924      0.0844     0.0814     57.8501
18000     2.4924      0.0886     0.0814     59.2122
18100     2.4924      0.0654     0.0814     57.8668
18200     2.4924      0.0570     0.0816     58.2617
18300     2.4923      0.0907     0.0816     57.8954
18400     2.4923      0.0759     0.0816     58.0146
18500     2.4923      0.0781     0.0816     57.8405
18600     2.4923      0.0781     0.0816     57.3822
18700     2.4923      0.0759     0.0816     57.6657
18800     2.4923      0.0696     0.0816     57.7706
18900     2.4922      0.0717     0.0816     58.0890
19000     2.4922      0.0865     0.0816     59.0440
19100     2.4922      0.1013     0.0816     59.3583
19200     2.4922      0.0717     0.0816     58.5060
19300     2.4922      0.0907     0.0816     58.5598
19400     2.4922      0.0759     0.0816     58.8500
19500     2.4922      0.1013     0.0816     58.8320
19600     2.4921      0.0717     0.0816     59.2792
19700     2.4921      0.0949     0.0816     58.2248
19800     2.4921      0.0738     0.0816     59.2209
19900     2.4921      0.0949     0.0816     59.0995
20000     2.4921      0.0781     0.0816     59.2159
20100     2.4921      0.0949     0.0816     58.0976
20199     2.4921      0.0759     0.0816     57.3287
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.4921      0.0759     0.0802     9.9189
00100     2.4921      0.0865     0.0803     57.0983
00200     2.4921      0.0802     0.0809     56.6647
00300     2.4921      0.0738     0.0809     56.2339
00400     2.4921      0.0886     0.0809     57.0238
00500     2.4921      0.0759     0.0809     58.3548
00600     2.4921      0.0844     0.0812     56.9515
00700     2.4921      0.0823     0.0812     57.5110
00800     2.4921      0.0823     0.0812     57.0624
00900     2.4920      0.0738     0.0812     56.6080
01000     2.4920      0.0654     0.0812     57.0262
01100     2.4920      0.0802     0.0812     57.8445
01200     2.4920      0.0907     0.0812     56.1775
01300     2.4920      0.0781     0.0812     57.3699
01400     2.4920      0.0738     0.0812     57.7460
01500     2.4920      0.0992     0.0812     56.8609
01600     2.4920      0.0654     0.0812     57.3214
01700     2.4920      0.0738     0.0812     57.5843
01800     2.4920      0.0717     0.0812     58.8815
01900     2.4920      0.0781     0.0812     58.1998
02000     2.4920      0.0802     0.0812     57.6578
02100     2.4920      0.0970     0.0812     56.8913
02200     2.4920      0.0696     0.0812     56.5907
02300     2.4920      0.0970     0.0812     56.1047
02400     2.4920      0.0759     0.0812     57.2486
02500     2.4920      0.0844     0.0812     59.0753
02600     2.4920      0.0823     0.0812     57.0208
02700     2.4920      0.0738     0.0812     56.8351
02800     2.4920      0.0992     0.0812     57.2371
02900     2.4920      0.1055     0.0812     57.1919
03000     2.4920      0.0802     0.0812     57.3318
03100     2.4920      0.0717     0.0812     56.5903
03200     2.4920      0.0844     0.0812     57.7374
03300     2.4920      0.0633     0.0812     57.8444
03400     2.4920      0.0654     0.0812     56.2493
03500     2.4920      0.0865     0.0812     57.6422
03600     2.4920      0.0886     0.0812     57.1218
03700     2.4920      0.0759     0.0812     57.4089
03800     2.4920      0.0781     0.0812     58.7620
03900     2.4920      0.0675     0.0812     56.7486
04000     2.4920      0.0844     0.0812     56.6030
04100     2.4920      0.0802     0.0812     57.8934
04200     2.4920      0.0781     0.0812     57.6692
04300     2.4920      0.0802     0.0812     57.9534
04400     2.4920      0.0696     0.0812     56.5033
04500     2.4920      0.0696     0.0812     57.0804
04600     2.4920      0.1076     0.0812     58.9878
04700     2.4920      0.0928     0.0812     55.8788
04800     2.4920      0.0928     0.0812     56.6675
04900     2.4920      0.0570     0.0812     57.1521
05000     2.4920      0.0949     0.0816     58.0445
05100     2.4919      0.0612     0.0816     57.2577
05200     2.4919      0.0759     0.0816     57.9836
05300     2.4919      0.0759     0.0816     56.9082
05400     2.4919      0.0823     0.0816     58.1117
05500     2.4919      0.0886     0.0816     58.9405
05600     2.4919      0.0992     0.0816     56.9789
05700     2.4919      0.0717     0.0816     60.2227
05800     2.4919      0.0949     0.0816     59.2696
05900     2.4919      0.0992     0.0816     58.3669
06000     2.4919      0.0823     0.0816     56.5939
06100     2.4919      0.0992     0.0816     56.9715
06200     2.4919      0.0865     0.0816     57.5695
06300     2.4919      0.0759     0.0816     57.3775
06400     2.4919      0.1097     0.0816     56.9932
06500     2.4919      0.0759     0.0816     57.8421
06600     2.4919      0.0633     0.0816     56.4799
06700     2.4919      0.1013     0.0816     56.7412
06800     2.4919      0.0633     0.0816     59.0822
06900     2.4919      0.0759     0.0816     56.5608
07000     2.4919      0.0696     0.0816     57.5091
07100     2.4919      0.0738     0.0816     57.4158
07200     2.4919      0.0738     0.0816     56.6394
07300     2.4919      0.0802     0.0816     58.0718
07400     2.4919      0.1013     0.0816     56.6548
07500     2.4919      0.0802     0.0816     57.9092
07600     2.4919      0.0717     0.0816     57.5192
07700     2.4919      0.0781     0.0816     57.4171
07800     2.4919      0.0865     0.0816     57.9231
07900     2.4919      0.0907     0.0816     56.6140
08000     2.4919      0.0781     0.0816     59.1230
08100     2.4919      0.0781     0.0816     58.2297
08200     2.4919      0.0717     0.0816     58.0426
08300     2.4919      0.0759     0.0816     57.6864
08400     2.4919      0.0928     0.0816     57.7188
08500     2.4919      0.0675     0.0816     57.0900
08600     2.4919      0.0591     0.0816     61.1276
08700     2.4919      0.0865     0.0816     58.6686
08800     2.4919      0.0738     0.0816     57.5971
08900     2.4919      0.1034     0.0816     57.5674
09000     2.4919      0.0675     0.0816     56.9834
09100     2.4919      0.0970     0.0816     58.1752
09200     2.4919      0.0696     0.0816     57.2184
09300     2.4918      0.0738     0.0816     57.1600
09400     2.4918      0.0696     0.0816     58.6537
09500     2.4918      0.1013     0.0816     58.2220
09600     2.4918      0.0949     0.0816     57.9249
09700     2.4918      0.0612     0.0816     58.0315
09800     2.4918      0.0738     0.0816     56.6430
09900     2.4918      0.0738     0.0816     56.7059
Start testing:
Test Accuracy: 0.0789
