Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=512, canonical_testing=False, cs=0.5, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=0.1, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=6, quant_actNM=8, quant_inp=4, quant_w=None, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=640, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
d45714cc-64bf-400d-bf40-646c78005037
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 161, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 212, in forward
    gates = quant_pass(pact_a_bmm( quant_pass(pact_a_bmm(part1, self.a12), self.abMVM, self.a12) + quant_pass(pact_a_bmm(part2, self.a13), self.abMVM, self.a13), self.a14), self.abNM, self.a14)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 89, in forward
    x01q =  torch.round(x01 * step_d ) / step_d
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.76 GiB total capacity; 8.45 GiB already allocated; 9.12 MiB free; 9.79 GiB reserved in total by PyTorch)
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=512, canonical_testing=False, cs=0.5, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=0.1, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=6, quant_actNM=8, quant_inp=4, quant_w=None, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=640, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
7413ef2d-32c0-4c37-a919-ebcafb458ed6
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 161, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 212, in forward
    gates = quant_pass(pact_a_bmm( quant_pass(pact_a_bmm(part1, self.a12), self.abMVM, self.a12) + quant_pass(pact_a_bmm(part2, self.a13), self.abMVM, self.a13), self.a14), self.abNM, self.a14)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 89, in forward
    x01q =  torch.round(x01 * step_d ) / step_d
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.76 GiB total capacity; 8.45 GiB already allocated; 9.12 MiB free; 9.79 GiB reserved in total by PyTorch)
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=512, canonical_testing=False, cs=0.5, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=0.1, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=6, quant_actNM=8, quant_inp=4, quant_w=None, random_seed=193012823, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=640, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
e10a91ef-559d-4018-b8f9-320b1b583ed1
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.6134      0.1035     0.1420     10.8401
00100     1.6368      0.4258     0.5145     68.4923
00200     1.1894      0.6230     0.6707     67.9730
00300     1.0576      0.6719     0.7121     69.2126
00400     0.9603      0.7188     0.7395     69.5524
00500     0.9106      0.7344     0.7798     69.0019
00600     0.9068      0.7305     0.7849     70.1264
00700     0.8205      0.7695     0.7849     68.2620
00800     0.7595      0.7422     0.8166     69.5727
00900     0.7416      0.7754     0.8166     68.5843
01000     0.8714      0.7285     0.8166     68.6767
01100     0.7640      0.7598     0.8166     69.3238
01200     0.7439      0.7520     0.8225     69.0839
01300     0.7720      0.7871     0.8225     69.3775
01400     0.7118      0.7793     0.8225     68.9177
01500     0.6686      0.8047     0.8296     68.8031
01600     0.8527      0.7344     0.8296     70.5699
01700     0.7057      0.7832     0.8296     71.1325
01800     0.6382      0.8008     0.8311     70.5841
01900     0.6251      0.8164     0.8311     70.8593
02000     0.7135      0.7871     0.8311     69.9000
02100     0.7157      0.7852     0.8311     70.3685
02200     0.6719      0.8223     0.8311     70.7200
02300     0.7145      0.7754     0.8311     69.3686
02400     0.7111      0.7812     0.8311     69.7536
02500     0.6068      0.8125     0.8311     68.8702
02600     0.6178      0.8066     0.8311     69.6917
02700     0.7387      0.7891     0.8311     69.1923
02800     0.6576      0.7988     0.8311     68.7157
02900     0.6942      0.7891     0.8311     69.5142
03000     0.6457      0.8164     0.8311     69.0564
03100     0.5895      0.8203     0.8311     69.2302
03200     0.6633      0.7969     0.8311     69.4135
03300     0.6306      0.8008     0.8311     69.0067
03400     0.6837      0.7988     0.8311     69.0513
03500     0.6057      0.8105     0.8318     68.3500
03600     0.6299      0.7910     0.8318     69.5047
03700     0.6721      0.8066     0.8318     69.4169
03800     0.6827      0.7891     0.8318     69.5453
03900     0.5675      0.8359     0.8318     69.8888
04000     0.6130      0.8242     0.8318     69.5455
04100     0.6322      0.8066     0.8318     68.9298
04200     0.6471      0.7969     0.8318     70.0742
04300     0.5722      0.8242     0.8318     68.7991
04400     0.6168      0.8086     0.8318     69.1985
04500     0.6241      0.8066     0.8324     69.4776
04600     0.6309      0.8125     0.8324     69.6308
04700     0.5913      0.8105     0.8324     69.2459
04800     0.6020      0.8242     0.8324     69.7074
04900     0.6696      0.7949     0.8470     69.0510
05000     0.5869      0.8281     0.8470     69.4815
05100     0.6090      0.8086     0.8470     69.2562
05200     0.5654      0.8477     0.8470     69.9523
05300     0.5644      0.8398     0.8470     69.5715
05400     0.5640      0.8203     0.8470     69.2193
05500     0.5973      0.8223     0.8470     70.0131
05600     0.5590      0.8340     0.8470     69.1585
05700     0.6400      0.8086     0.8470     69.0981
05800     0.6366      0.8125     0.8470     68.6643
05900     0.6832      0.7871     0.8470     69.6770
06000     0.6380      0.8125     0.8470     69.9412
06100     0.5041      0.8535     0.8470     69.3736
06200     0.6302      0.8125     0.8470     69.5626
06300     0.5338      0.8438     0.8470     69.4122
06400     0.5228      0.8301     0.8514     68.7769
06500     0.5818      0.8203     0.8514     69.4380
06600     0.5077      0.8633     0.8514     69.2951
06700     0.4958      0.8438     0.8514     69.6970
06800     0.5712      0.8418     0.8514     69.7949
06900     0.5261      0.8477     0.8514     69.0113
07000     0.5514      0.8262     0.8514     69.1822
07100     0.5653      0.8105     0.8525     69.4105
07200     0.6321      0.8047     0.8525     69.3738
07300     0.5480      0.8457     0.8525     69.4229
07400     0.5318      0.8477     0.8525     69.2154
07500     0.6074      0.8125     0.8525     69.0316
07600     0.5214      0.8535     0.8525     69.8071
07700     0.5082      0.8477     0.8525     69.1548
07800     0.5300      0.8359     0.8525     69.3873
07900     0.5610      0.8301     0.8525     69.0499
08000     0.5540      0.8477     0.8525     69.0689
08100     0.4775      0.8457     0.8525     70.0433
08200     0.5933      0.8203     0.8525     69.3142
08300     0.4504      0.8711     0.8525     69.2937
08400     0.5382      0.8223     0.8525     69.3241
08500     0.5972      0.8203     0.8582     69.0719
08600     0.5814      0.8164     0.8582     69.7886
08700     0.5554      0.8242     0.8582     69.6140
08800     0.5588      0.8281     0.8598     69.4334
08900     0.5061      0.8398     0.8598     69.5841
09000     0.5605      0.8223     0.8598     68.8650
09100     0.5437      0.8418     0.8598     69.4431
09200     0.4769      0.8535     0.8598     68.8715
09300     0.5663      0.8301     0.8598     68.6658
09400     0.4827      0.8633     0.8598     69.2771
09500     0.4199      0.8770     0.8598     69.0729
09600     0.4949      0.8574     0.8598     69.2698
09700     0.4622      0.8691     0.8598     69.6292
09800     0.5519      0.8320     0.8598     69.2284
09900     0.4659      0.8613     0.8598     69.8392
10000     0.5655      0.8359     0.8598     69.6163
10100     0.4125      0.8770     0.8598     69.2091
10200     0.4672      0.8633     0.8598     69.9857
10300     0.5387      0.8477     0.8598     69.2775
10400     0.5472      0.8301     0.8598     69.5804
10500     0.4410      0.8594     0.8598     68.9169
10600     0.4333      0.8887     0.8598     69.1987
10700     0.5003      0.8496     0.8598     69.5672
10800     0.4493      0.8789     0.8598     69.2314
10900     0.4411      0.8633     0.8598     68.6915
11000     0.4586      0.8594     0.8598     69.1611
11100     0.4869      0.8535     0.8598     69.5321
11200     0.4679      0.8613     0.8598     69.7600
11300     0.4155      0.8789     0.8598     69.3485
11400     0.4914      0.8496     0.8598     69.1841
11500     0.5030      0.8457     0.8598     69.1007
11600     0.5760      0.8125     0.8598     69.4516
11700     0.4225      0.8867     0.8598     69.6278
11800     0.4070      0.8730     0.8598     68.9267
11900     0.4675      0.8770     0.8598     68.9640
12000     0.5046      0.8477     0.8598     70.0493
12100     0.4898      0.8516     0.8598     69.3120
12200     0.4187      0.8730     0.8598     68.6409
12300     0.4383      0.8652     0.8598     69.7178
12400     0.3867      0.8887     0.8598     69.5262
12500     0.4939      0.8535     0.8598     69.8252
12600     0.4380      0.8535     0.8598     69.3431
12700     0.4282      0.8809     0.8598     69.2939
12800     0.4603      0.8633     0.8598     69.4971
12900     0.5231      0.8516     0.8598     69.2551
13000     0.4304      0.8711     0.8598     69.6288
13100     0.4619      0.8555     0.8598     69.6972
13200     0.4195      0.8750     0.8598     69.0211
13300     0.5663      0.8652     0.8598     69.6799
13400     0.4753      0.8652     0.8598     69.2445
13500     0.3987      0.8828     0.8598     68.7697
13600     0.4317      0.8691     0.8598     70.0443
13700     0.4304      0.8828     0.8598     69.8685
13800     0.5079      0.8418     0.8598     69.6078
13900     0.4885      0.8574     0.8598     68.4584
14000     0.4074      0.8984     0.8598     68.1806
14100     0.4669      0.8691     0.8598     69.4056
14200     0.4981      0.8457     0.8598     68.5860
14300     0.4475      0.8770     0.8598     67.9901
14400     0.4832      0.8555     0.8598     69.0733
14500     0.4102      0.8828     0.8598     68.6008
14600     0.4282      0.8750     0.8598     68.6972
14700     0.4569      0.8750     0.8598     68.8664
14800     0.3971      0.8887     0.8598     69.6873
14900     0.5038      0.8535     0.8598     68.7816
15000     0.5429      0.8418     0.8598     69.2559
15100     0.4749      0.8652     0.8598     68.8735
15200     0.4689      0.8633     0.8598     69.4750
15300     0.4721      0.8633     0.8598     68.8960
15400     0.4471      0.8652     0.8598     69.1120
15500     0.4806      0.8477     0.8598     69.3055
15600     0.4176      0.8750     0.8598     69.1609
15700     0.4217      0.8770     0.8598     69.7984
15800     0.4155      0.8613     0.8598     69.3159
15900     0.4320      0.8633     0.8598     68.7360
16000     0.4450      0.8574     0.8598     69.3568
16100     0.4543      0.8594     0.8598     69.1169
16200     0.4345      0.8809     0.8598     68.9052
16300     0.4154      0.8906     0.8598     68.7083
16400     0.4006      0.8789     0.8598     69.4266
16500     0.4071      0.8867     0.8598     69.6509
16600     0.4643      0.8691     0.8598     69.3508
16700     0.3882      0.8789     0.8598     68.9644
16800     0.4687      0.8691     0.8598     69.1527
16900     0.4133      0.8828     0.8598     69.0367
17000     0.3833      0.8945     0.8598     68.7641
17100     0.4196      0.8691     0.8598     68.6510
17200     0.4267      0.8672     0.8598     68.7824
17300     0.4057      0.8945     0.8598     68.8197
17400     0.3647      0.8984     0.8598     69.4729
17500     0.4392      0.8750     0.8598     68.9299
17600     0.4583      0.8633     0.8598     69.5849
17700     0.4164      0.8711     0.8598     69.4416
17800     0.4185      0.8867     0.8598     68.8508
17900     0.4393      0.8711     0.8598     69.2163
18000     0.5076      0.8496     0.8598     69.3254
18100     0.5256      0.8379     0.8598     69.3976
18200     0.4367      0.8809     0.8598     69.0423
18300     0.4813      0.8438     0.8598     68.8112
18400     0.4566      0.8652     0.8598     70.0830
18500     0.4744      0.8672     0.8598     68.7085
18600     0.4294      0.8770     0.8598     69.3585
18700     0.4494      0.8672     0.8598     69.8880
18800     0.4232      0.8594     0.8598     68.9141
18900     0.4422      0.8672     0.8598     69.0467
19000     0.5025      0.8633     0.8598     69.1942
19100     0.4187      0.8867     0.8598     69.2197
19200     0.3765      0.8887     0.8598     69.4298
19300     0.4282      0.8672     0.8598     69.2699
19400     0.4413      0.8711     0.8598     69.0903
19500     0.5104      0.8457     0.8598     69.1137
19600     0.4344      0.8789     0.8598     69.4464
19700     0.4327      0.8809     0.8598     69.5043
19800     0.4044      0.8887     0.8598     69.2961
19900     0.4318      0.8672     0.8598     68.5243
20000     0.4198      0.8789     0.8598     69.3819
20100     0.3990      0.8965     0.8598     69.4885
20199     0.4499      0.8672     0.8598     68.1358
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     0.6105      0.8066     0.7640     9.8986
00100     0.4331      0.8672     0.8843     68.6403
00200     0.4552      0.8574     0.8843     69.5662
00300     0.5077      0.8379     0.8889     69.0270
00400     0.4536      0.8711     0.8889     68.8830
00500     0.4240      0.8984     0.8889     68.8336
00600     0.4092      0.8789     0.8889     68.7789
00700     0.4585      0.8594     0.8889     68.3512
00800     0.4236      0.8730     0.8889     69.1351
00900     0.4580      0.8750     0.8889     68.3969
01000     0.4143      0.8809     0.8889     68.9949
01100     0.5102      0.8496     0.8889     68.9491
01200     0.4474      0.8633     0.8889     68.2703
01300     0.4060      0.8809     0.8889     68.3340
01400     0.4441      0.8574     0.8889     68.9630
01500     0.4752      0.8652     0.8889     68.9949
01600     0.4276      0.8672     0.8889     69.0339
01700     0.3900      0.8848     0.8889     69.0745
01800     0.3793      0.8945     0.8918     68.9031
01900     0.4077      0.8770     0.8918     68.9432
02000     0.4687      0.8750     0.8918     69.0797
02100     0.4693      0.8496     0.8918     69.7165
02200     0.5164      0.8340     0.8918     68.4503
02300     0.5099      0.8379     0.8918     68.8717
02400     0.4215      0.8750     0.8918     69.5421
02500     0.4329      0.8770     0.8918     68.6864
02600     0.3635      0.9004     0.8918     70.0875
02700     0.4482      0.8691     0.8918     68.9564
02800     0.4402      0.8750     0.8918     68.5029
02900     0.4335      0.8789     0.8918     68.7679
03000     0.4541      0.8613     0.8918     68.3795
03100     0.3869      0.8887     0.8918     68.6772
03200     0.4989      0.8555     0.8918     69.3875
03300     0.3367      0.9062     0.8918     68.8417
03400     0.4737      0.8594     0.8918     68.7051
03500     0.4359      0.8691     0.8918     68.2925
03600     0.4248      0.8828     0.8918     69.0256
03700     0.4776      0.8613     0.8918     69.6230
03800     0.4472      0.8672     0.8918     68.3013
03900     0.4878      0.8555     0.8918     69.2326
04000     0.4062      0.8750     0.8918     68.8993
04100     0.4494      0.8613     0.8918     68.7810
04200     0.4348      0.8633     0.8918     69.3148
04300     0.4318      0.8770     0.8918     69.1620
04400     0.4142      0.8906     0.8918     68.7067
04500     0.4955      0.8398     0.8918     69.7514
04600     0.3948      0.8809     0.8918     69.0464
04700     0.4580      0.8750     0.8918     69.1962
04800     0.3908      0.8887     0.8918     68.7216
04900     0.4098      0.8789     0.8918     68.9118
05000     0.4534      0.8711     0.8918     69.7130
05100     0.4171      0.8770     0.8918     68.7796
05200     0.4696      0.8535     0.8918     68.8211
05300     0.4055      0.8828     0.8920     68.4611
05400     0.4908      0.8340     0.8920     68.6951
05500     0.4142      0.8828     0.8920     69.8234
05600     0.4638      0.8594     0.8920     69.1931
05700     0.4204      0.8887     0.8920     68.6815
05800     0.4262      0.8594     0.8920     69.6540
05900     0.4135      0.8809     0.8920     69.4092
06000     0.3936      0.8828     0.8920     69.5834
06100     0.4037      0.8770     0.8920     69.0546
06200     0.5028      0.8418     0.8920     69.1016
06300     0.3968      0.8848     0.8920     68.8821
06400     0.4603      0.8848     0.8920     68.2441
06500     0.4683      0.8652     0.8920     68.3228
06600     0.4913      0.8633     0.8920     68.9616
06700     0.4521      0.8691     0.8920     68.9873
06800     0.4513      0.8730     0.8920     69.9218
06900     0.4951      0.8574     0.8920     68.9480
07000     0.4112      0.8887     0.8920     68.2102
07100     0.3869      0.8750     0.8920     69.7392
07200     0.4879      0.8555     0.8920     69.5310
07300     0.3720      0.8828     0.8920     68.9798
07400     0.4004      0.8750     0.8920     68.9442
07500     0.4537      0.8633     0.8934     68.9494
07600     0.4204      0.8887     0.8934     69.2881
07700     0.4512      0.8594     0.8934     68.1924
07800     0.4487      0.8652     0.8934     68.9305
07900     0.4858      0.8613     0.8934     68.6248
08000     0.4650      0.8633     0.8934     69.4916
08100     0.4229      0.8691     0.8934     69.4437
08200     0.4496      0.8652     0.8934     69.3643
08300     0.4390      0.8691     0.8934     68.8300
08400     0.4339      0.8750     0.8934     70.0161
08500     0.4767      0.8457     0.8934     69.5921
08600     0.4393      0.8730     0.8934     69.8536
08700     0.4348      0.8711     0.8934     68.4335
08800     0.4442      0.8750     0.8934     69.0501
08900     0.4900      0.8555     0.8934     69.2377
09000     0.3937      0.8926     0.8934     68.7967
09100     0.4072      0.8730     0.8934     69.2827
09200     0.3998      0.8848     0.8955     69.3886
09300     0.4389      0.8770     0.8955     69.1184
09400     0.4200      0.8672     0.8955     69.1434
09500     0.4614      0.8672     0.8955     68.8535
09600     0.3953      0.8984     0.8955     68.5334
09700     0.5007      0.8555     0.8955     69.2052
09800     0.4509      0.8516     0.8955     68.6807
09900     0.4628      0.8594     0.8955     69.1851
Start testing:
Test Accuracy: 0.8804
