Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=512, canonical_testing=False, cs=0.5, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, hidden=109, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=0.1, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=6, quant_actNM=8, quant_inp=4, quant_w=None, random_seed=8627169, rows_bias=5, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=640, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
cceb6544-89b0-454b-9c93-9981a7be711b
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 161, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 212, in forward
    gates = quant_pass(pact_a_bmm( quant_pass(pact_a_bmm(part1, self.a12), self.abMVM, self.a12) + quant_pass(pact_a_bmm(part2, self.a13), self.abMVM, self.a13), self.a14), self.abNM, self.a14)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 89, in forward
    x01q =  torch.round(x01 * step_d ) / step_d
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.76 GiB total capacity; 8.42 GiB already allocated; 17.12 MiB free; 9.78 GiB reserved in total by PyTorch)
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=512, canonical_testing=False, cs=0.5, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, hidden=109, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=0.1, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=6, quant_actNM=8, quant_inp=4, quant_w=None, random_seed=8627169, rows_bias=5, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=640, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
38993776-c4f3-420d-801c-b314fc5aa99f
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 161, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 212, in forward
    gates = quant_pass(pact_a_bmm( quant_pass(pact_a_bmm(part1, self.a12), self.abMVM, self.a12) + quant_pass(pact_a_bmm(part2, self.a13), self.abMVM, self.a13), self.a14), self.abNM, self.a14)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 89, in forward
    x01q =  torch.round(x01 * step_d ) / step_d
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.76 GiB total capacity; 8.42 GiB already allocated; 17.12 MiB free; 9.78 GiB reserved in total by PyTorch)
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=512, canonical_testing=False, cs=0.5, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, hidden=109, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=0.1, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=6, quant_actNM=8, quant_inp=4, quant_w=None, random_seed=8627169, rows_bias=5, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=640, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
ac143ba9-80b4-4da2-a450-521e98813e7e
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.7170      0.0918     0.0902     11.7180
00100     1.6402      0.4258     0.4800     55.2240
00200     1.2184      0.6094     0.6570     55.1014
00300     1.0423      0.6758     0.7083     52.0335
00400     0.9917      0.6953     0.7278     52.2226
00500     0.9897      0.7129     0.7570     52.9169
00600     0.8750      0.7090     0.7760     52.4959
00700     0.8454      0.7266     0.7941     51.9793
00800     0.8358      0.7441     0.7941     51.9923
00900     0.8264      0.7500     0.7941     52.5732
01000     0.9033      0.7246     0.8063     52.5866
01100     0.6271      0.8281     0.8063     52.0817
01200     0.6976      0.7734     0.8063     51.4006
01300     0.7343      0.7480     0.8063     53.5174
01400     0.7566      0.7656     0.8147     51.8959
01500     0.7729      0.7656     0.8179     51.5535
01600     0.6929      0.7910     0.8179     53.3657
01700     0.7237      0.7656     0.8292     51.9098
01800     0.7005      0.7852     0.8292     51.4216
01900     0.6967      0.7852     0.8292     51.3997
02000     0.7044      0.7793     0.8292     50.8416
02100     0.6615      0.8008     0.8292     51.3664
02200     0.6371      0.7949     0.8292     51.7443
02300     0.7615      0.7715     0.8292     52.3493
02400     0.7061      0.7949     0.8390     51.8025
02500     0.7940      0.7520     0.8390     51.5772
02600     0.7080      0.7734     0.8390     52.3959
02700     0.6329      0.8242     0.8440     51.9911
02800     0.6025      0.8359     0.8440     50.9379
02900     0.6495      0.7969     0.8440     51.5826
03000     0.6247      0.8066     0.8440     51.4565
03100     0.6324      0.7969     0.8440     51.0549
03200     0.6319      0.7988     0.8440     52.4921
03300     0.5964      0.8262     0.8440     50.8794
03400     0.6497      0.7949     0.8440     53.7050
03500     0.6318      0.8008     0.8440     52.0305
03600     0.6393      0.8008     0.8440     51.6718
03700     0.6168      0.8145     0.8440     52.5650
03800     0.6705      0.7930     0.8440     51.5725
03900     0.6127      0.8164     0.8440     52.2727
04000     0.6107      0.8223     0.8440     51.7565
04100     0.5967      0.8262     0.8440     52.8397
04200     0.6883      0.7793     0.8440     52.8548
04300     0.5889      0.8398     0.8440     52.1299
04400     0.6893      0.7773     0.8440     52.9794
04500     0.6983      0.7930     0.8440     51.5161
04600     0.6171      0.8164     0.8440     53.0118
04700     0.5965      0.8164     0.8521     52.6578
04800     0.6402      0.8223     0.8521     50.9936
04900     0.5453      0.8281     0.8521     52.1568
05000     0.5679      0.8379     0.8521     52.1259
05100     0.6308      0.7988     0.8521     52.4491
05200     0.6602      0.8027     0.8521     53.9060
05300     0.5928      0.8145     0.8521     51.1402
05400     0.5921      0.8281     0.8521     51.7689
05500     0.6300      0.8086     0.8521     52.0858
05600     0.5826      0.8301     0.8596     51.7288
05700     0.5123      0.8477     0.8596     51.0656
05800     0.6426      0.8066     0.8596     52.1872
05900     0.5688      0.8262     0.8596     55.0346
06000     0.5721      0.8242     0.8596     52.5907
06100     0.5594      0.8457     0.8596     52.8943
06200     0.5438      0.8574     0.8596     51.5146
06300     0.6212      0.8008     0.8596     52.3576
06400     0.6190      0.7988     0.8596     51.2942
06500     0.5221      0.8379     0.8596     52.1741
06600     0.5241      0.8359     0.8596     54.3306
06700     0.5862      0.8223     0.8596     54.2896
06800     0.5651      0.8320     0.8596     52.7573
06900     0.5096      0.8477     0.8596     52.5621
07000     0.4866      0.8652     0.8596     51.7661
07100     0.5851      0.8105     0.8596     51.5464
07200     0.6768      0.7988     0.8596     52.7878
07300     0.4964      0.8574     0.8596     51.4432
07400     0.5501      0.8359     0.8596     52.0519
07500     0.5721      0.8125     0.8596     51.0814
07600     0.5159      0.8477     0.8596     51.4744
07700     0.5951      0.8027     0.8596     51.0678
07800     0.4395      0.8711     0.8596     53.4666
07900     0.5459      0.8184     0.8596     53.2447
08000     0.5376      0.8398     0.8596     52.4897
08100     0.5082      0.8477     0.8596     52.4925
08200     0.5403      0.8359     0.8596     51.2985
08300     0.4947      0.8398     0.8596     52.2920
08400     0.5979      0.8086     0.8596     53.5274
08500     0.5198      0.8516     0.8596     52.0742
08600     0.5312      0.8496     0.8646     52.6249
08700     0.4706      0.8652     0.8646     52.4625
08800     0.4885      0.8594     0.8646     52.3695
08900     0.5391      0.8457     0.8646     53.4839
09000     0.5519      0.8320     0.8646     52.1879
09100     0.5205      0.8594     0.8646     52.3423
09200     0.5597      0.8066     0.8646     53.8105
09300     0.5188      0.8633     0.8646     51.2323
09400     0.5353      0.8418     0.8646     52.7125
09500     0.4984      0.8418     0.8646     53.7757
09600     0.5082      0.8516     0.8646     52.8658
09700     0.5077      0.8574     0.8646     51.4831
09800     0.5161      0.8477     0.8646     52.4831
09900     0.5646      0.8125     0.8646     54.6351
10000     0.4969      0.8438     0.8646     51.1186
10100     0.5001      0.8379     0.8646     51.7340
10200     0.4973      0.8652     0.8646     51.8912
10300     0.5111      0.8633     0.8646     54.4455
10400     0.5034      0.8516     0.8646     51.3279
10500     0.4998      0.8457     0.8646     52.0537
10600     0.5486      0.8379     0.8646     52.4110
10700     0.4293      0.8730     0.8646     54.9048
10800     0.4983      0.8418     0.8646     52.9999
10900     0.4422      0.8613     0.8668     55.5016
11000     0.4635      0.8672     0.8668     54.3609
11100     0.4519      0.8672     0.8668     52.9218
11200     0.5893      0.8105     0.8668     53.4119
11300     0.4749      0.8594     0.8668     53.9434
11400     0.5286      0.8496     0.8668     53.5462
11500     0.5085      0.8398     0.8668     53.7359
11600     0.4422      0.8574     0.8668     54.0142
11700     0.4783      0.8496     0.8668     55.8024
11800     0.4630      0.8438     0.8668     54.0573
11900     0.3818      0.9004     0.8668     53.4019
12000     0.5317      0.8574     0.8668     54.4134
12100     0.4616      0.8633     0.8668     54.0860
12200     0.4877      0.8516     0.8668     54.6854
12300     0.4278      0.8770     0.8668     55.6026
12400     0.4619      0.8691     0.8668     54.4128
12500     0.4773      0.8691     0.8668     53.0661
12600     0.4230      0.8906     0.8668     55.4847
12700     0.5001      0.8516     0.8668     54.9082
12800     0.5263      0.8438     0.8668     53.7348
12900     0.4362      0.8828     0.8668     53.5194
13000     0.4624      0.8613     0.8668     54.7776
13100     0.4882      0.8691     0.8668     54.5501
13200     0.4267      0.8809     0.8668     54.2415
13300     0.4240      0.8848     0.8668     53.8034
13400     0.5512      0.8418     0.8668     54.2150
13500     0.4154      0.8965     0.8668     54.2051
13600     0.4745      0.8496     0.8668     53.4308
13700     0.4246      0.8770     0.8668     52.6353
13800     0.4998      0.8516     0.8668     53.3604
13900     0.4253      0.8711     0.8668     53.7055
14000     0.4183      0.8730     0.8668     54.3917
14100     0.4314      0.8770     0.8668     53.1876
14200     0.4626      0.8633     0.8668     54.2622
14300     0.3889      0.8945     0.8668     54.6800
14400     0.3914      0.8848     0.8668     55.4100
14500     0.5046      0.8262     0.8668     54.8116
14600     0.5156      0.8457     0.8668     55.0905
14700     0.4009      0.8848     0.8668     53.3987
14800     0.4901      0.8555     0.8668     53.4783
14900     0.4227      0.8789     0.8668     55.5679
15000     0.3914      0.8809     0.8668     55.0480
15100     0.4699      0.8652     0.8668     55.2126
15200     0.4458      0.8730     0.8668     54.5819
15300     0.4807      0.8496     0.8668     53.4043
15400     0.4367      0.8613     0.8668     53.9476
15500     0.4603      0.8691     0.8668     55.6456
15600     0.4444      0.8711     0.8668     55.2907
15700     0.4904      0.8555     0.8668     53.9246
15800     0.4574      0.8691     0.8668     54.2809
15900     0.4199      0.8789     0.8668     53.7063
16000     0.4598      0.8672     0.8668     53.7691
16100     0.5037      0.8477     0.8668     53.2661
16200     0.4597      0.8691     0.8668     55.8584
16300     0.4517      0.8594     0.8668     53.5793
16400     0.3488      0.9043     0.8668     54.3911
16500     0.4332      0.8887     0.8668     53.0999
16600     0.4360      0.8770     0.8668     53.8365
16700     0.4822      0.8535     0.8668     54.8435
16800     0.5352      0.8203     0.8668     54.6843
16900     0.4077      0.8906     0.8668     56.0284
17000     0.5097      0.8281     0.8668     56.4732
17100     0.4478      0.8711     0.8668     54.8201
17200     0.4195      0.8770     0.8668     53.1743
17300     0.4976      0.8496     0.8668     54.1210
17400     0.4820      0.8691     0.8668     54.1059
17500     0.3816      0.8906     0.8668     54.2188
17600     0.4507      0.8613     0.8668     55.9330
17700     0.4156      0.8848     0.8668     54.4734
17800     0.5073      0.8535     0.8668     54.2540
17900     0.3844      0.8828     0.8668     54.5946
18000     0.4837      0.8418     0.8668     55.2687
18100     0.4522      0.8477     0.8668     53.9495
18200     0.4014      0.8848     0.8668     54.3267
18300     0.4232      0.8867     0.8668     53.1865
18400     0.3609      0.9062     0.8668     54.2799
18500     0.4774      0.8652     0.8668     54.4824
18600     0.3936      0.8828     0.8668     55.3687
18700     0.5169      0.8340     0.8668     53.5032
18800     0.4377      0.8633     0.8668     53.5907
18900     0.4465      0.8730     0.8668     53.4386
19000     0.4281      0.8848     0.8668     54.7202
19100     0.4033      0.8848     0.8732     53.4076
19200     0.4249      0.8867     0.8732     53.9943
19300     0.3853      0.8867     0.8732     55.2881
19400     0.5310      0.8359     0.8732     55.0778
19500     0.4203      0.8848     0.8732     53.8324
19600     0.4042      0.8848     0.8732     53.4074
19700     0.4179      0.8789     0.8732     54.0470
19800     0.4247      0.8711     0.8732     55.5938
19900     0.3741      0.8848     0.8732     53.8445
20000     0.4206      0.8711     0.8732     54.8497
20100     0.4464      0.8711     0.8732     53.4607
20199     0.4280      0.8691     0.8732     54.5463
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     0.7297      0.7793     0.7897     8.6539
00100     0.5168      0.8340     0.8748     53.6149
00200     0.5166      0.8516     0.8748     53.3263
00300     0.4783      0.8633     0.8839     54.6667
00400     0.3996      0.8750     0.8839     54.1891
00500     0.4816      0.8457     0.8932     54.2754
00600     0.4256      0.8652     0.8932     56.6752
00700     0.3891      0.8926     0.8932     54.5148
00800     0.4755      0.8555     0.8932     53.8494
00900     0.4541      0.8516     0.8932     55.5408
01000     0.3995      0.8887     0.8932     55.6405
01100     0.5321      0.8418     0.8932     53.1260
01200     0.4529      0.8652     0.8932     53.8147
01300     0.4377      0.8809     0.8932     54.3760
01400     0.3664      0.8906     0.8932     56.8081
01500     0.4489      0.8711     0.8932     58.2018
01600     0.3948      0.8789     0.8932     54.9735
01700     0.4679      0.8516     0.8932     54.1456
01800     0.4758      0.8652     0.8943     54.8819
01900     0.4337      0.8926     0.8943     55.4036
02000     0.4183      0.8750     0.8943     54.5049
02100     0.4270      0.8711     0.8943     54.2675
02200     0.4427      0.8652     0.8943     54.1223
02300     0.4047      0.8867     0.8943     54.8932
02400     0.4418      0.8691     0.8943     55.3021
02500     0.5047      0.8516     0.8943     53.7546
02600     0.3998      0.8867     0.8943     54.4017
02700     0.4148      0.8867     0.8943     54.0879
02800     0.3823      0.8906     0.8943     53.1001
02900     0.3877      0.8848     0.8943     55.9796
03000     0.4086      0.8945     0.8943     55.6193
03100     0.4897      0.8516     0.8943     55.4358
03200     0.4274      0.8789     0.8943     55.7779
03300     0.3883      0.8906     0.8943     53.8851
03400     0.4060      0.8750     0.8943     53.6843
03500     0.3769      0.8945     0.8943     55.0964
03600     0.3962      0.8926     0.8943     55.3944
03700     0.4480      0.8574     0.8943     54.6710
03800     0.4250      0.8730     0.8943     53.4775
03900     0.4379      0.8691     0.8943     53.9995
04000     0.3748      0.8926     0.8943     54.1322
04100     0.4849      0.8672     0.8943     53.4035
04200     0.4835      0.8730     0.8943     54.0712
04300     0.4644      0.8477     0.8943     52.9679
04400     0.4646      0.8535     0.8943     55.0666
04500     0.5375      0.8359     0.8943     55.4713
04600     0.4374      0.8730     0.8943     54.3851
04700     0.4286      0.8711     0.8943     55.9864
04800     0.4767      0.8555     0.8943     54.9658
04900     0.5222      0.8320     0.8943     54.7038
05000     0.4537      0.8555     0.8943     54.1056
05100     0.4414      0.8613     0.8943     56.4263
05200     0.4253      0.8652     0.8943     53.9835
05300     0.4962      0.8496     0.8943     54.7966
05400     0.5119      0.8516     0.8943     55.1478
05500     0.4374      0.8672     0.8943     54.9627
05600     0.4646      0.8672     0.8943     54.7363
05700     0.3451      0.8906     0.8943     55.3496
05800     0.4044      0.8672     0.8943     54.6539
05900     0.4235      0.8691     0.8943     55.3859
06000     0.3736      0.8984     0.8943     54.3996
06100     0.3915      0.9043     0.8943     55.0411
06200     0.3942      0.8691     0.8943     55.2503
06300     0.4739      0.8633     0.8943     56.4584
06400     0.4494      0.8770     0.8943     54.3176
06500     0.4661      0.8711     0.8943     54.6949
06600     0.4531      0.8711     0.8943     55.7704
06700     0.4182      0.8789     0.8943     54.4699
06800     0.4717      0.8496     0.8943     56.4127
06900     0.4194      0.8730     0.8943     54.1007
07000     0.4170      0.8750     0.8943     54.5348
07100     0.4225      0.8789     0.8943     54.9733
07200     0.4757      0.8633     0.8943     54.9073
07300     0.4207      0.8750     0.8943     54.8961
07400     0.4574      0.8613     0.8943     55.4971
07500     0.4480      0.8711     0.8943     55.3012
07600     0.5018      0.8477     0.8943     55.6827
07700     0.4651      0.8594     0.8943     54.7137
07800     0.4233      0.8711     0.8943     54.5907
07900     0.3953      0.8906     0.8943     55.8332
08000     0.4398      0.8770     0.8943     54.4892
08100     0.4311      0.8770     0.8943     54.7712
08200     0.4210      0.8770     0.8943     54.6605
08300     0.4211      0.8730     0.8943     55.2749
08400     0.4345      0.8750     0.8943     56.3838
08500     0.4072      0.8809     0.8943     55.0828
08600     0.3931      0.8867     0.8943     55.7695
08700     0.4602      0.8711     0.8943     55.6776
08800     0.4451      0.8574     0.8943     57.0425
08900     0.4614      0.8633     0.8943     55.6790
09000     0.5017      0.8594     0.8943     54.6611
09100     0.4189      0.8711     0.8943     54.0214
09200     0.4676      0.8574     0.8943     54.3422
09300     0.4140      0.8770     0.8943     54.8497
09400     0.4402      0.8652     0.8943     55.2952
09500     0.4945      0.8555     0.8943     56.3111
09600     0.4114      0.8848     0.8943     54.5370
09700     0.4748      0.8535     0.8943     54.6150
09800     0.4029      0.8867     0.8943     55.3084
09900     0.4429      0.8770     0.8943     55.1583
Start testing:
Test Accuracy: 0.8814
