Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=512, canonical_testing=False, cs=0.5, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, hidden=112, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=0.1, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=6, quant_actNM=8, quant_inp=4, quant_w=None, random_seed=8627169, rows_bias=2, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=640, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
79b24e0d-1bbf-47c0-a3c8-742205a65ad5
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 161, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 228, in forward
    activated_input = quant_pass(pact_a_bmm(input_gate_out * activation_out, self.a8), self.abNM, self.a8)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 124, in pact_a_bmm
    return torch.sign(x) * .5 * (torch.abs(x) - torch.abs(torch.abs(x) - a) + a)
RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 10.76 GiB total capacity; 8.62 GiB already allocated; 3.12 MiB free; 9.80 GiB reserved in total by PyTorch)
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=512, canonical_testing=False, cs=0.5, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, hidden=112, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=0.1, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=6, quant_actNM=8, quant_inp=4, quant_w=None, random_seed=8627169, rows_bias=2, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=640, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
e1ffaa53-73d4-4aa4-af02-6f3c1ecdab0e
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 161, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 228, in forward
    activated_input = quant_pass(pact_a_bmm(input_gate_out * activation_out, self.a8), self.abNM, self.a8)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 124, in pact_a_bmm
    return torch.sign(x) * .5 * (torch.abs(x) - torch.abs(torch.abs(x) - a) + a)
RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 10.76 GiB total capacity; 8.62 GiB already allocated; 3.12 MiB free; 9.80 GiB reserved in total by PyTorch)
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=512, canonical_testing=False, cs=0.5, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, hidden=112, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=0.1, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=6, quant_actNM=8, quant_inp=4, quant_w=None, random_seed=8627169, rows_bias=2, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=640, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
4b128cb8-9351-4780-bffe-03b0e74442bc
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.6587      0.0605     0.0823     11.0270
00100     1.4831      0.5156     0.5359     55.2660
00200     1.1682      0.6270     0.6711     54.9369
00300     1.0701      0.6602     0.6711     55.5705
00400     0.9748      0.7168     0.7364     55.4754
00500     0.9830      0.6934     0.7948     55.4683
00600     0.7385      0.7695     0.7948     53.1934
00700     0.7937      0.7578     0.7948     53.4368
00800     0.8627      0.7168     0.7970     53.4178
00900     0.8302      0.7402     0.7970     53.7133
01000     0.7486      0.7754     0.7970     53.8697
01100     0.6875      0.7910     0.7970     54.0865
01200     0.7526      0.7793     0.7970     53.1066
01300     0.7317      0.7734     0.7970     54.1945
01400     0.7472      0.7773     0.8154     53.5940
01500     0.7143      0.7715     0.8154     53.4746
01600     0.7699      0.7773     0.8154     53.9047
01700     0.7310      0.7695     0.8292     53.3090
01800     0.6277      0.8105     0.8292     53.8360
01900     0.6469      0.8223     0.8292     54.6590
02000     0.6631      0.7930     0.8292     53.0497
02100     0.6459      0.8145     0.8292     54.1596
02200     0.6640      0.7910     0.8292     52.0569
02300     0.6299      0.8086     0.8292     52.9030
02400     0.6661      0.7891     0.8395     53.9051
02500     0.6857      0.7832     0.8395     51.9119
02600     0.6592      0.8008     0.8539     52.9080
02700     0.6926      0.7969     0.8539     53.8698
02800     0.6692      0.8184     0.8539     52.4197
02900     0.6078      0.8203     0.8539     54.4631
03000     0.6368      0.8223     0.8539     53.5478
03100     0.6068      0.8086     0.8539     54.9527
03200     0.6543      0.8086     0.8539     53.5016
03300     0.5549      0.8340     0.8539     54.7803
03400     0.5809      0.8008     0.8542     54.3541
03500     0.6012      0.8320     0.8542     52.6691
03600     0.6747      0.7988     0.8542     52.3329
03700     0.6720      0.8105     0.8542     53.5457
03800     0.5919      0.8184     0.8542     53.1888
03900     0.5242      0.8359     0.8542     53.2949
04000     0.6003      0.8047     0.8542     53.8891
04100     0.5742      0.8262     0.8542     53.5198
04200     0.5969      0.7988     0.8542     52.9917
04300     0.5076      0.8633     0.8542     52.4719
04400     0.5601      0.8320     0.8542     54.5622
04500     0.6174      0.8164     0.8542     53.6281
04600     0.5394      0.8359     0.8542     53.1872
04700     0.5659      0.8203     0.8542     53.3327
04800     0.6025      0.8164     0.8658     53.3240
04900     0.6565      0.7988     0.8658     52.6373
05000     0.6897      0.7793     0.8658     53.5135
05100     0.5744      0.8418     0.8658     51.7300
05200     0.5914      0.8086     0.8658     54.3911
05300     0.5074      0.8398     0.8658     53.2825
05400     0.5658      0.8223     0.8658     53.4763
05500     0.6243      0.8184     0.8658     54.2748
05600     0.6575      0.8047     0.8658     53.6485
05700     0.5575      0.8184     0.8658     52.5378
05800     0.5976      0.8105     0.8658     53.5650
05900     0.5749      0.8047     0.8658     53.1200
06000     0.5458      0.8496     0.8658     52.9279
06100     0.5305      0.8262     0.8658     52.9141
06200     0.5935      0.8262     0.8658     52.8802
06300     0.5114      0.8438     0.8658     53.2711
06400     0.5347      0.8184     0.8658     54.2172
06500     0.5910      0.8184     0.8658     54.1289
06600     0.5583      0.8359     0.8658     54.3483
06700     0.5052      0.8535     0.8658     53.6237
06800     0.5057      0.8457     0.8658     54.2811
06900     0.4961      0.8574     0.8658     54.6805
07000     0.5567      0.8359     0.8658     55.3436
07100     0.5043      0.8574     0.8658     54.9856
07200     0.6057      0.8184     0.8658     53.9768
07300     0.5339      0.8555     0.8658     52.1264
07400     0.5895      0.8125     0.8658     53.5574
07500     0.4875      0.8516     0.8658     54.6312
07600     0.5445      0.8438     0.8658     54.2179
07700     0.6040      0.7910     0.8658     53.5764
07800     0.5781      0.8281     0.8658     54.1043
07900     0.4687      0.8750     0.8658     52.5639
08000     0.5138      0.8340     0.8658     53.6133
08100     0.5692      0.8281     0.8658     54.2518
08200     0.5331      0.8359     0.8658     54.6528
08300     0.4784      0.8535     0.8658     52.7977
08400     0.5581      0.8262     0.8658     54.1791
08500     0.5354      0.8418     0.8658     53.3682
08600     0.4921      0.8574     0.8658     53.8663
08700     0.4596      0.8672     0.8658     54.6252
08800     0.6250      0.8184     0.8658     55.4952
08900     0.5358      0.8242     0.8658     53.4928
09000     0.5426      0.8457     0.8683     54.7976
09100     0.5086      0.8281     0.8683     54.1913
09200     0.5543      0.8379     0.8683     52.5220
09300     0.5099      0.8672     0.8683     53.8532
09400     0.5327      0.8574     0.8683     54.4076
09500     0.5027      0.8535     0.8683     53.3716
09600     0.5106      0.8633     0.8683     52.4007
09700     0.5129      0.8516     0.8683     53.8803
09800     0.5492      0.8516     0.8683     53.4183
09900     0.5876      0.8164     0.8683     53.0448
10000     0.4570      0.8594     0.8683     53.6398
10100     0.4761      0.8574     0.8683     53.1014
10200     0.5011      0.8633     0.8683     52.3560
10300     0.5152      0.8340     0.8683     53.9834
10400     0.4468      0.8867     0.8683     53.6879
10500     0.4604      0.8496     0.8683     52.9442
10600     0.4784      0.8672     0.8683     53.4812
10700     0.4177      0.8730     0.8683     55.8311
10800     0.3831      0.8867     0.8683     55.4538
10900     0.4520      0.8750     0.8683     54.1529
11000     0.4304      0.8770     0.8683     53.5492
11100     0.4418      0.8555     0.8683     53.6098
11200     0.4449      0.8770     0.8683     53.1229
11300     0.4358      0.8828     0.8683     53.3636
11400     0.5052      0.8457     0.8683     53.7217
11500     0.4717      0.8555     0.8683     53.3844
11600     0.4289      0.8789     0.8683     53.2983
11700     0.3454      0.9082     0.8683     52.7546
11800     0.4525      0.8594     0.8683     52.8834
11900     0.4387      0.8691     0.8683     53.2270
12000     0.4496      0.8828     0.8683     53.2799
12100     0.4978      0.8555     0.8683     52.7619
12200     0.4682      0.8594     0.8683     53.3648
12300     0.4087      0.8750     0.8683     52.7509
12400     0.4181      0.8633     0.8683     53.4824
12500     0.4472      0.8652     0.8683     52.9771
12600     0.4683      0.8594     0.8683     52.5615
12700     0.4544      0.8789     0.8683     52.9405
12800     0.3832      0.8906     0.8683     53.1141
12900     0.4667      0.8613     0.8683     53.3524
13000     0.4175      0.8770     0.8683     53.6505
13100     0.4975      0.8516     0.8683     53.2458
13200     0.4847      0.8633     0.8683     53.5247
13300     0.3918      0.8672     0.8683     53.3986
13400     0.4711      0.8516     0.8683     53.0367
13500     0.4133      0.8867     0.8683     53.7148
13600     0.4599      0.8809     0.8683     53.4755
13700     0.4596      0.8633     0.8683     53.6041
13800     0.5133      0.8496     0.8683     53.7707
13900     0.4439      0.8633     0.8683     53.0569
14000     0.4544      0.8438     0.8683     53.2404
14100     0.3499      0.9121     0.8683     53.0618
14200     0.4113      0.8750     0.8683     53.3002
14300     0.5095      0.8477     0.8683     53.1592
14400     0.4651      0.8496     0.8683     52.8837
14500     0.4234      0.8652     0.8683     53.2554
14600     0.4816      0.8633     0.8683     53.0945
14700     0.3894      0.8965     0.8683     52.7030
14800     0.3843      0.8906     0.8683     53.1945
14900     0.4429      0.8711     0.8683     53.0281
15000     0.4610      0.8496     0.8683     52.8683
15100     0.4683      0.8613     0.8683     53.8554
15200     0.4175      0.8770     0.8683     52.5412
15300     0.4480      0.8672     0.8683     52.9508
15400     0.4198      0.8730     0.8683     53.1667
15500     0.4587      0.8652     0.8683     52.6971
15600     0.4055      0.8711     0.8683     52.8263
15700     0.4140      0.8867     0.8683     52.3999
15800     0.3773      0.9043     0.8683     52.7619
15900     0.3907      0.8906     0.8683     52.7285
16000     0.4300      0.8652     0.8683     53.1519
16100     0.4297      0.8789     0.8683     52.7704
16200     0.4603      0.8516     0.8683     53.2064
16300     0.4077      0.8770     0.8683     52.6771
16400     0.4655      0.8613     0.8683     53.1777
16500     0.3807      0.8789     0.8683     53.4311
16600     0.4068      0.8828     0.8683     52.8991
16700     0.4319      0.8652     0.8683     54.1321
16800     0.4882      0.8535     0.8683     52.8063
16900     0.4429      0.8711     0.8683     52.7424
17000     0.4280      0.8711     0.8683     53.2707
17100     0.4487      0.8613     0.8683     52.5899
17200     0.3703      0.8906     0.8683     53.7630
17300     0.4536      0.8633     0.8683     52.8948
17400     0.5010      0.8438     0.8683     52.9213
17500     0.4117      0.8809     0.8683     53.1342
17600     0.4571      0.8672     0.8683     52.9030
17700     0.4597      0.8789     0.8683     53.1579
17800     0.4297      0.8730     0.8683     52.9682
17900     0.4852      0.8555     0.8683     53.1308
18000     0.4049      0.8887     0.8683     53.9680
18100     0.4240      0.8848     0.8683     53.2227
18200     0.4079      0.8906     0.8683     52.3996
18300     0.4247      0.8867     0.8683     52.9542
18400     0.4100      0.8770     0.8683     52.7162
18500     0.4405      0.8750     0.8683     53.0962
18600     0.4750      0.8770     0.8683     53.2621
18700     0.4122      0.8789     0.8683     52.7421
18800     0.3897      0.8789     0.8683     53.7140
18900     0.4267      0.8691     0.8683     52.5265
19000     0.4793      0.8672     0.8683     52.7354
19100     0.4025      0.8926     0.8683     53.3358
19200     0.5108      0.8516     0.8683     52.7433
19300     0.4297      0.8535     0.8683     53.3285
19400     0.3959      0.8926     0.8683     53.3411
19500     0.5016      0.8516     0.8683     52.6678
19600     0.4158      0.8770     0.8683     53.4465
19700     0.3888      0.9062     0.8683     53.7874
19800     0.4881      0.8438     0.8683     53.1639
19900     0.4329      0.8730     0.8683     53.3830
20000     0.3871      0.8770     0.8683     52.7961
20100     0.4617      0.8574     0.8683     52.7227
20199     0.3839      0.8867     0.8683     52.9293
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     0.8279      0.7441     0.7743     8.5047
00100     0.4074      0.8691     0.8871     52.8056
00200     0.4100      0.8809     0.8871     52.8489
00300     0.4845      0.8457     0.8871     52.2285
00400     0.4550      0.8633     0.8871     53.2755
00500     0.4569      0.8672     0.8871     52.9296
00600     0.3553      0.9004     0.8921     52.7655
00700     0.4618      0.8633     0.8921     52.7951
00800     0.4568      0.8594     0.8921     52.4975
00900     0.4289      0.8652     0.8921     52.7304
01000     0.5015      0.8574     0.8921     53.1833
01100     0.4318      0.8711     0.8921     52.6419
01200     0.4462      0.8555     0.8921     53.3648
01300     0.4433      0.8477     0.8921     52.4672
01400     0.4579      0.8750     0.8921     52.3166
01500     0.4555      0.8672     0.8921     52.9132
01600     0.4376      0.8594     0.8921     52.6379
01700     0.3660      0.8984     0.8921     52.9588
01800     0.3765      0.8809     0.8921     52.6172
01900     0.4631      0.8633     0.8921     52.6793
02000     0.4534      0.8535     0.8921     53.3657
02100     0.4172      0.8867     0.8921     52.5368
02200     0.4143      0.8809     0.8921     53.1537
02300     0.4215      0.8770     0.8921     53.5193
02400     0.3876      0.8906     0.8934     52.8502
02500     0.4393      0.8809     0.8934     52.7449
02600     0.5207      0.8516     0.8934     52.3812
02700     0.4334      0.8672     0.8934     52.8309
02800     0.4208      0.8828     0.8934     53.0525
02900     0.4026      0.8848     0.8934     52.7688
03000     0.4685      0.8613     0.8934     53.7052
03100     0.3883      0.8809     0.8934     52.4342
03200     0.3675      0.8965     0.8934     52.4868
03300     0.3968      0.8652     0.8934     52.7652
03400     0.4090      0.8770     0.8934     52.5626
03500     0.4800      0.8633     0.8934     52.7199
03600     0.4851      0.8516     0.8934     52.9716
03700     0.4820      0.8457     0.8934     52.4674
03800     0.3719      0.8965     0.8934     52.7957
03900     0.3697      0.8848     0.8934     52.8497
04000     0.4258      0.8633     0.8934     52.7868
04100     0.3100      0.9160     0.8965     53.3082
04200     0.4504      0.8672     0.8965     52.3058
04300     0.4262      0.8711     0.8965     53.0792
04400     0.4931      0.8438     0.8965     52.9150
04500     0.4380      0.8672     0.8965     52.7359
04600     0.4364      0.8770     0.8965     53.4740
04700     0.4337      0.8711     0.8965     52.9582
04800     0.3835      0.8848     0.8965     52.5031
04900     0.3882      0.8906     0.8965     52.6157
05000     0.3782      0.8984     0.8965     52.2937
05100     0.3992      0.8750     0.8965     52.9681
05200     0.4973      0.8359     0.8965     52.6436
05300     0.3474      0.9043     0.8965     52.8836
05400     0.4392      0.8809     0.8965     53.2281
05500     0.3998      0.8828     0.8965     52.6851
05600     0.4011      0.8848     0.8965     52.9380
05700     0.3815      0.8906     0.8965     52.8320
05800     0.4379      0.8633     0.8965     53.9010
05900     0.4770      0.8477     0.8965     54.6779
06000     0.4340      0.8809     0.8965     54.1833
06100     0.4195      0.8789     0.8965     54.4543
06200     0.4094      0.8789     0.8965     54.7309
06300     0.4704      0.8652     0.8965     54.0488
06400     0.4680      0.8555     0.8965     54.6477
06500     0.4356      0.8691     0.8965     54.1446
06600     0.3802      0.9004     0.8965     54.0845
06700     0.4450      0.8770     0.8965     54.8255
06800     0.4673      0.8574     0.8965     53.8739
06900     0.4205      0.8711     0.8965     54.5978
07000     0.3983      0.8965     0.8965     54.2270
07100     0.3998      0.8848     0.8965     54.3244
07200     0.4119      0.8828     0.8965     55.1841
07300     0.4319      0.8750     0.8965     54.6494
07400     0.4180      0.8945     0.8965     54.9975
07500     0.4102      0.8691     0.8965     54.8734
07600     0.3849      0.8926     0.8965     54.3907
07700     0.4180      0.8613     0.8965     55.3479
07800     0.4369      0.8770     0.8965     54.8866
07900     0.4115      0.8809     0.8965     54.6820
08000     0.4388      0.8789     0.8965     54.7642
08100     0.4213      0.8711     0.8965     54.0787
08200     0.4370      0.8770     0.8965     54.7984
08300     0.4447      0.8711     0.8965     54.0456
08400     0.4331      0.8828     0.8965     54.7115
08500     0.3983      0.8828     0.8965     54.7949
08600     0.4773      0.8535     0.8965     54.1966
08700     0.4564      0.8594     0.8965     54.3879
08800     0.4569      0.8574     0.8965     54.8044
08900     0.3755      0.8828     0.8965     54.1662
09000     0.3818      0.8926     0.8965     55.0244
09100     0.4666      0.8691     0.8965     54.0915
09200     0.3902      0.8984     0.8965     54.3313
09300     0.4024      0.8730     0.8965     55.7021
09400     0.3938      0.8809     0.8965     53.9099
09500     0.4404      0.8789     0.8965     54.1765
09600     0.4457      0.8633     0.8965     54.6728
09700     0.3843      0.8789     0.8965     53.9341
09800     0.3661      0.8867     0.8965     55.2701
09900     0.4348      0.8711     0.8965     54.4906
Start testing:
Test Accuracy: 0.8863
