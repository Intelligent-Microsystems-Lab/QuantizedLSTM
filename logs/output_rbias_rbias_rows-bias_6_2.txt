Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=512, canonical_testing=False, cs=0.5, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=0.1, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=6, quant_actNM=8, quant_inp=4, quant_w=None, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=640, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
c035c6fe-f5aa-46cd-ab3f-5d280f1a8779
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 161, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 212, in forward
    gates = quant_pass(pact_a_bmm( quant_pass(pact_a_bmm(part1, self.a12), self.abMVM, self.a12) + quant_pass(pact_a_bmm(part2, self.a13), self.abMVM, self.a13), self.a14), self.abNM, self.a14)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 89, in forward
    x01q =  torch.round(x01 * step_d ) / step_d
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.76 GiB total capacity; 8.45 GiB already allocated; 9.12 MiB free; 9.79 GiB reserved in total by PyTorch)
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=512, canonical_testing=False, cs=0.5, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=0.1, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=6, quant_actNM=8, quant_inp=4, quant_w=None, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=640, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
a7fb09ef-6b36-49ad-8968-d334c281ad6e
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
Traceback (most recent call last):
  File "KWS_LSTM.py", line 161, in <module>
    output = model(x_data)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 289, in forward
    lstm_out, _ = self.lstmBlocks(inputs, self.hidden_state)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 65, in forward
    out, state = self.cell(inputs[i], state, w_mask)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 212, in forward
    gates = quant_pass(pact_a_bmm( quant_pass(pact_a_bmm(part1, self.a12), self.abMVM, self.a12) + quant_pass(pact_a_bmm(part2, self.a13), self.abMVM, self.a13), self.a14), self.abNM, self.a14)
  File "/afs/crc.nd.edu/user/c/cschaef6/QuantizedLSTM/model.py", line 89, in forward
    x01q =  torch.round(x01 * step_d ) / step_d
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.76 GiB total capacity; 8.45 GiB already allocated; 9.12 MiB free; 9.79 GiB reserved in total by PyTorch)
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Namespace(background_frequency=0.8, background_volume=0.1, batch_size=512, canonical_testing=False, cs=0.5, dataloader_num_workers=8, dataset_path_test='data.nosync/speech_commands_test_set_v0.02', dataset_path_train='data.nosync/speech_commands_v0.02', drop_p=0.125, finetuning_epochs=10000, hidden=108, hop_length=320, l2=0.01, learning_rate='0.002,0.0005,0.00008', max_w=0.1, method=0, n_mfcc=40, n_msb=4, noise_injectionI=0.1, noise_injectionT=0.16, pact_a=True, quant_actMVM=6, quant_actNM=8, quant_inp=4, quant_w=None, random_seed=8627169, rows_bias=6, sample_rate=16000, silence_percentage=0.1, testing_percentage=10, time_shift_ms=100.0, training_steps='10000,10000,200', unknown_percentage=0.1, validation_percentage=10, win_length=640, word_list=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence'])
ff94ca8c-79da-4f87-ae7b-5f06160482b6
Start training with DropConnect:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     2.6493      0.0879     0.0689     10.6612
00100     1.5948      0.4844     0.5163     54.9747
00200     1.1891      0.6230     0.6591     54.5422
00300     1.0559      0.6719     0.7204     54.9509
00400     0.9867      0.6934     0.7439     52.9352
00500     0.9418      0.7188     0.7863     53.3315
00600     0.8329      0.7324     0.7863     53.7708
00700     0.8540      0.7285     0.8143     53.5809
00800     0.9165      0.7012     0.8143     54.4594
00900     0.7459      0.7559     0.8143     53.8464
01000     0.8469      0.7344     0.8143     53.8951
01100     0.6759      0.7891     0.8258     54.8773
01200     0.7116      0.7930     0.8258     53.8584
01300     0.6460      0.8047     0.8258     54.4334
01400     0.7403      0.7812     0.8410     54.0963
01500     0.6910      0.7812     0.8410     53.7137
01600     0.7252      0.7754     0.8410     54.4645
01700     0.6012      0.8203     0.8410     53.5378
01800     0.7259      0.7637     0.8410     53.3977
01900     0.7640      0.7773     0.8410     54.4253
02000     0.6252      0.8086     0.8410     54.2418
02100     0.6677      0.7832     0.8410     54.8482
02200     0.7028      0.7930     0.8410     53.7982
02300     0.6636      0.7871     0.8410     54.1625
02400     0.6595      0.8125     0.8410     55.0368
02500     0.6279      0.8262     0.8410     54.4879
02600     0.6180      0.8242     0.8410     54.8023
02700     0.6562      0.8047     0.8410     53.7837
02800     0.6079      0.8145     0.8410     54.0047
02900     0.5385      0.8340     0.8410     54.1704
03000     0.6800      0.7949     0.8410     54.0393
03100     0.6517      0.7969     0.8410     54.1295
03200     0.5835      0.8086     0.8410     54.1430
03300     0.5591      0.8379     0.8410     53.7860
03400     0.6141      0.8125     0.8410     54.0222
03500     0.5438      0.8340     0.8410     53.7076
03600     0.5864      0.8281     0.8410     53.9682
03700     0.6641      0.7988     0.8410     54.1023
03800     0.6198      0.8027     0.8410     53.7525
03900     0.6150      0.8105     0.8410     54.3367
04000     0.6181      0.8086     0.8410     53.4203
04100     0.5543      0.8164     0.8410     53.5534
04200     0.5069      0.8496     0.8410     54.5250
04300     0.5743      0.8340     0.8433     53.5665
04400     0.6216      0.8047     0.8433     53.8939
04500     0.6059      0.7871     0.8528     53.8049
04600     0.5588      0.8359     0.8528     54.0366
04700     0.6518      0.7891     0.8528     54.5702
04800     0.5678      0.8457     0.8528     53.7054
04900     0.5306      0.8535     0.8528     54.0860
05000     0.6256      0.8164     0.8528     54.0215
05100     0.6024      0.8281     0.8528     53.4527
05200     0.6191      0.8027     0.8528     54.3680
05300     0.5657      0.8281     0.8528     53.7049
05400     0.5824      0.8145     0.8528     53.6869
05500     0.5690      0.8184     0.8528     54.1013
05600     0.5993      0.8027     0.8528     53.3874
05700     0.5517      0.8418     0.8528     53.0873
05800     0.5622      0.8359     0.8538     54.1033
05900     0.5653      0.8359     0.8538     53.1140
06000     0.5553      0.8457     0.8538     54.4003
06100     0.6115      0.8164     0.8538     54.0046
06200     0.5619      0.8457     0.8538     53.2541
06300     0.5822      0.8242     0.8538     54.6635
06400     0.6326      0.8125     0.8538     53.4655
06500     0.6454      0.8184     0.8538     54.2769
06600     0.4995      0.8477     0.8538     53.7738
06700     0.5696      0.8223     0.8538     53.4515
06800     0.5204      0.8457     0.8598     54.0775
06900     0.5837      0.8262     0.8598     53.7024
07000     0.5264      0.8477     0.8598     53.8779
07100     0.5535      0.8418     0.8598     54.3812
07200     0.5154      0.8398     0.8618     54.1578
07300     0.5580      0.8418     0.8618     53.8068
07400     0.5705      0.8223     0.8618     53.4155
07500     0.5543      0.8340     0.8618     53.7808
07600     0.6461      0.7969     0.8618     54.3600
07700     0.5127      0.8516     0.8618     53.8960
07800     0.5539      0.8242     0.8618     54.3408
07900     0.6134      0.8027     0.8618     53.8282
08000     0.4898      0.8457     0.8618     53.6621
08100     0.5454      0.8359     0.8618     54.3653
08200     0.5910      0.8105     0.8618     53.7581
08300     0.5502      0.8281     0.8618     54.2262
08400     0.4782      0.8594     0.8618     54.2783
08500     0.4916      0.8379     0.8618     53.9447
08600     0.5537      0.8438     0.8618     54.3916
08700     0.5437      0.8242     0.8618     53.5420
08800     0.5254      0.8340     0.8618     53.6935
08900     0.6199      0.8145     0.8618     53.8041
09000     0.6288      0.7969     0.8618     53.5847
09100     0.5380      0.8555     0.8618     53.7452
09200     0.5397      0.8477     0.8618     54.1820
09300     0.5187      0.8398     0.8618     53.6869
09400     0.4551      0.8711     0.8618     54.8822
09500     0.5106      0.8418     0.8618     53.6762
09600     0.5425      0.8184     0.8618     53.9088
09700     0.4631      0.8398     0.8618     54.4280
09800     0.5046      0.8438     0.8618     53.3277
09900     0.6042      0.8105     0.8618     53.9941
10000     0.5536      0.8359     0.8618     54.6296
10100     0.5308      0.8457     0.8618     53.6486
10200     0.5456      0.8418     0.8618     54.4804
10300     0.5285      0.8418     0.8618     54.3340
10400     0.4235      0.8613     0.8618     56.3379
10500     0.4526      0.8652     0.8618     57.0430
10600     0.4692      0.8652     0.8618     56.5108
10700     0.4313      0.8691     0.8618     56.5398
10800     0.4256      0.8828     0.8618     56.9352
10900     0.5312      0.8359     0.8618     56.2348
11000     0.4214      0.8848     0.8618     56.9137
11100     0.4247      0.8867     0.8618     56.4998
11200     0.4910      0.8555     0.8618     56.7513
11300     0.4230      0.8691     0.8618     57.3716
11400     0.4572      0.8574     0.8618     56.1390
11500     0.4929      0.8496     0.8618     56.3596
11600     0.4758      0.8613     0.8618     57.2418
11700     0.4832      0.8457     0.8618     56.7388
11800     0.3891      0.9004     0.8618     57.3256
11900     0.4622      0.8672     0.8618     57.0342
12000     0.4987      0.8301     0.8618     56.6033
12100     0.4662      0.8633     0.8618     56.6613
12200     0.4939      0.8320     0.8638     56.4105
12300     0.4409      0.8691     0.8638     57.2348
12400     0.4890      0.8730     0.8638     56.8715
12500     0.4626      0.8613     0.8638     57.1000
12600     0.4279      0.8711     0.8638     57.3092
12700     0.5120      0.8516     0.8638     56.9796
12800     0.4692      0.8555     0.8638     56.6643
12900     0.4536      0.8574     0.8638     57.3067
13000     0.4754      0.8633     0.8638     56.7501
13100     0.4645      0.8594     0.8638     56.9668
13200     0.4414      0.8789     0.8638     57.2961
13300     0.4439      0.8691     0.8638     56.6338
13400     0.4332      0.8652     0.8638     57.6257
13500     0.3932      0.8867     0.8638     56.9119
13600     0.4879      0.8613     0.8638     57.2291
13700     0.4405      0.8809     0.8638     57.4280
13800     0.4865      0.8613     0.8638     57.0262
13900     0.4593      0.8652     0.8638     56.6507
14000     0.4041      0.8750     0.8638     58.5416
14100     0.5342      0.8438     0.8638     56.5883
14200     0.3781      0.8926     0.8638     58.1319
14300     0.4661      0.8535     0.8638     57.3844
14400     0.4522      0.8555     0.8638     56.9939
14500     0.4575      0.8555     0.8638     57.5706
14600     0.4374      0.8789     0.8638     57.8222
14700     0.4100      0.8906     0.8638     57.3197
14800     0.4692      0.8555     0.8638     58.6817
14900     0.4533      0.8750     0.8638     57.4007
15000     0.4275      0.8789     0.8638     57.1965
15100     0.4262      0.8926     0.8638     56.9714
15200     0.4121      0.8848     0.8638     56.7530
15300     0.4460      0.8672     0.8638     57.1876
15400     0.4425      0.8691     0.8638     56.7295
15500     0.4447      0.8770     0.8638     56.9444
15600     0.3969      0.8828     0.8638     57.6003
15700     0.3953      0.8828     0.8638     56.8679
15800     0.4732      0.8555     0.8638     57.3908
15900     0.4468      0.8672     0.8638     56.9861
16000     0.4405      0.8633     0.8638     57.2063
16100     0.4540      0.8691     0.8638     57.6751
16200     0.4582      0.8672     0.8638     57.1060
16300     0.4238      0.8789     0.8638     57.7252
16400     0.5021      0.8516     0.8638     57.9249
16500     0.4801      0.8672     0.8638     57.2022
16600     0.4671      0.8535     0.8638     58.1490
16700     0.4881      0.8711     0.8638     57.5041
16800     0.4086      0.8906     0.8638     57.1771
16900     0.4592      0.8574     0.8638     57.4519
17000     0.4123      0.8809     0.8638     56.5631
17100     0.4585      0.8867     0.8638     57.3077
17200     0.5050      0.8672     0.8638     57.4111
17300     0.4265      0.8867     0.8638     57.1686
17400     0.4544      0.8535     0.8638     57.6976
17500     0.4438      0.8770     0.8638     56.6973
17600     0.4911      0.8633     0.8638     57.3060
17700     0.5210      0.8418     0.8638     57.6501
17800     0.3752      0.9043     0.8638     56.9131
17900     0.4400      0.8828     0.8638     57.2546
18000     0.4273      0.8867     0.8638     57.5321
18100     0.3898      0.8926     0.8638     57.5017
18200     0.4177      0.8711     0.8638     57.5204
18300     0.5101      0.8438     0.8638     56.7794
18400     0.4153      0.8672     0.8638     56.5046
18500     0.4191      0.8750     0.8638     57.6054
18600     0.4166      0.8691     0.8638     56.7674
18700     0.4326      0.8867     0.8638     56.5401
18800     0.4270      0.8672     0.8638     57.2196
18900     0.4873      0.8594     0.8638     57.4547
19000     0.4849      0.8594     0.8638     57.7670
19100     0.3623      0.8906     0.8638     56.9312
19200     0.4312      0.8711     0.8638     56.6714
19300     0.4493      0.8711     0.8638     57.4727
19400     0.4655      0.8457     0.8638     56.9244
19500     0.4054      0.8750     0.8638     56.8203
19600     0.3963      0.8926     0.8638     57.2052
19700     0.4358      0.8750     0.8638     56.9971
19800     0.4535      0.8691     0.8638     57.4227
19900     0.4177      0.8809     0.8638     56.8973
20000     0.3881      0.8809     0.8638     56.7705
20100     0.4022      0.8926     0.8638     57.4418
20199     0.3784      0.8809     0.8638     56.5614
Start finetuning with noise:
Epoch     Train Loss  Train Acc  Vali. Acc  Time (s)
00000     0.7819      0.7480     0.7836     9.7093
00100     0.4446      0.8594     0.8667     58.1132
00200     0.4572      0.8496     0.8827     56.5772
00300     0.4820      0.8652     0.8850     57.0010
00400     0.3803      0.8848     0.8850     57.4296
00500     0.4647      0.8633     0.8850     56.9514
00600     0.4770      0.8594     0.8850     57.4017
00700     0.4985      0.8555     0.8850     56.5609
00800     0.5329      0.8379     0.8850     56.8317
00900     0.5118      0.8438     0.8850     57.8724
01000     0.4365      0.8613     0.8850     56.6001
01100     0.4176      0.8828     0.8850     57.2115
01200     0.4789      0.8496     0.8867     56.8070
01300     0.4312      0.8633     0.8867     57.0949
01400     0.4334      0.8574     0.8867     57.0053
01500     0.4484      0.8594     0.8867     56.7847
01600     0.4729      0.8555     0.8867     56.5066
01700     0.4425      0.8789     0.8867     57.0352
01800     0.4820      0.8633     0.8867     56.9895
01900     0.4622      0.8652     0.8867     57.0047
02000     0.4753      0.8770     0.8903     56.5202
02100     0.3870      0.8828     0.8903     56.6850
02200     0.4421      0.8594     0.8903     57.5439
02300     0.5414      0.8398     0.8903     56.7683
02400     0.4550      0.8691     0.8903     57.5822
02500     0.5084      0.8594     0.8903     56.8310
02600     0.4768      0.8594     0.8903     56.6776
02700     0.4423      0.8633     0.8903     57.1686
02800     0.4161      0.8789     0.8903     56.3781
02900     0.4138      0.8770     0.8943     56.4938
03000     0.4155      0.8633     0.8943     57.4160
03100     0.4061      0.8867     0.8943     56.8481
03200     0.3381      0.9180     0.8943     57.3646
03300     0.4213      0.8809     0.8943     56.7043
03400     0.4273      0.8965     0.8943     56.9970
03500     0.3945      0.8789     0.8943     57.8022
03600     0.4303      0.8809     0.8943     56.7849
03700     0.4168      0.8711     0.8943     57.5189
03800     0.4008      0.8750     0.8943     57.0920
03900     0.3778      0.8906     0.8943     56.8002
04000     0.3697      0.9004     0.8943     56.9984
04100     0.3459      0.8965     0.8943     56.9889
04200     0.4088      0.8770     0.8943     57.0751
04300     0.4275      0.8594     0.8943     57.1260
04400     0.4773      0.8633     0.8943     56.5416
04500     0.4676      0.8496     0.8943     57.2701
04600     0.4517      0.8574     0.8943     56.8617
04700     0.3939      0.8848     0.8943     56.7018
04800     0.4469      0.8711     0.8943     57.5388
04900     0.4420      0.8672     0.8943     57.1123
05000     0.3676      0.9043     0.8943     57.2068
05100     0.4473      0.8691     0.8943     56.9920
05200     0.4415      0.8789     0.8943     57.0343
05300     0.3713      0.8945     0.8943     57.0418
05400     0.4076      0.8809     0.8943     56.5568
05500     0.4033      0.8711     0.8943     56.9329
05600     0.4487      0.8594     0.8943     57.3779
05700     0.4490      0.8633     0.8943     56.6858
05800     0.3832      0.8965     0.8943     57.1880
05900     0.4836      0.8496     0.8943     57.0040
06000     0.3948      0.8887     0.8943     56.8714
06100     0.3699      0.8965     0.8943     57.1701
06200     0.3924      0.8906     0.8943     56.6550
06300     0.5027      0.8672     0.8943     57.6614
06400     0.4821      0.8613     0.8943     56.9520
06500     0.4816      0.8555     0.8943     56.9429
06600     0.3632      0.8828     0.8943     57.6180
06700     0.4108      0.8809     0.8943     57.1165
06800     0.4878      0.8438     0.8943     56.8360
06900     0.5113      0.8379     0.8943     56.8095
07000     0.4003      0.8867     0.8943     56.5708
07100     0.4335      0.8555     0.8943     57.7256
07200     0.4286      0.8691     0.8943     56.9866
07300     0.4300      0.8594     0.8943     56.4562
07400     0.4319      0.8770     0.8943     57.8078
07500     0.4793      0.8633     0.8943     57.1609
07600     0.4143      0.8848     0.8943     57.6118
07700     0.4528      0.8555     0.8943     56.7279
07800     0.4253      0.8613     0.8943     57.1691
07900     0.3833      0.8848     0.8943     57.5341
08000     0.4401      0.8750     0.8943     57.1949
08100     0.3788      0.8867     0.8943     57.4191
08200     0.4192      0.8809     0.8943     56.9967
08300     0.3938      0.8945     0.8943     56.9472
08400     0.4122      0.8926     0.8943     57.5856
08500     0.4342      0.8809     0.8943     56.4021
08600     0.3645      0.9043     0.8943     56.7188
08700     0.4781      0.8457     0.8943     57.3855
08800     0.4109      0.8789     0.8943     56.3307
08900     0.4144      0.8887     0.8943     56.2737
09000     0.3829      0.8984     0.8943     56.9064
09100     0.4555      0.8477     0.8943     56.2086
09200     0.4138      0.8789     0.8943     56.7829
09300     0.4618      0.8652     0.8943     56.3383
09400     0.4150      0.8730     0.8943     56.3394
09500     0.4058      0.8867     0.8943     56.9647
09600     0.4346      0.8594     0.8943     56.4647
09700     0.4906      0.8438     0.8943     56.0222
09800     0.5043      0.8496     0.8943     56.6908
09900     0.4198      0.8789     0.8943     56.8751
Start testing:
Test Accuracy: 0.8861
